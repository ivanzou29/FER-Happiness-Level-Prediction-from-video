Epoch: 1| Step: 0
Training loss: 4.991784403351272
Validation loss: 5.857256768074974

Epoch: 6| Step: 1
Training loss: 6.311051306344181
Validation loss: 5.8386842693908445

Epoch: 6| Step: 2
Training loss: 5.201288489117046
Validation loss: 5.822599266280851

Epoch: 6| Step: 3
Training loss: 5.586248386508301
Validation loss: 5.809241065886284

Epoch: 6| Step: 4
Training loss: 5.543352754933155
Validation loss: 5.794649387302389

Epoch: 6| Step: 5
Training loss: 6.363974217945866
Validation loss: 5.778629690716146

Epoch: 6| Step: 6
Training loss: 6.1707159958046125
Validation loss: 5.75978405544813

Epoch: 6| Step: 7
Training loss: 5.376563798393751
Validation loss: 5.738220683176468

Epoch: 6| Step: 8
Training loss: 7.270132065613722
Validation loss: 5.714691351851005

Epoch: 6| Step: 9
Training loss: 5.993508005676753
Validation loss: 5.687146052873049

Epoch: 6| Step: 10
Training loss: 3.9592969156587907
Validation loss: 5.656091218617778

Epoch: 6| Step: 11
Training loss: 5.182614887324013
Validation loss: 5.621785252246047

Epoch: 6| Step: 12
Training loss: 6.780743417408198
Validation loss: 5.5831039717092965

Epoch: 6| Step: 13
Training loss: 4.686507463278933
Validation loss: 5.540583577862514

Epoch: 2| Step: 0
Training loss: 5.321844342558404
Validation loss: 5.4947340675658936

Epoch: 6| Step: 1
Training loss: 4.201345754915174
Validation loss: 5.443993197019081

Epoch: 6| Step: 2
Training loss: 5.916003696030397
Validation loss: 5.390910456635547

Epoch: 6| Step: 3
Training loss: 5.430088824253451
Validation loss: 5.337931970515637

Epoch: 6| Step: 4
Training loss: 4.475489516096692
Validation loss: 5.284053054010144

Epoch: 6| Step: 5
Training loss: 4.815152910920067
Validation loss: 5.230573022437722

Epoch: 6| Step: 6
Training loss: 4.595001880533292
Validation loss: 5.175555932373715

Epoch: 6| Step: 7
Training loss: 5.2708200245966905
Validation loss: 5.1204473091343

Epoch: 6| Step: 8
Training loss: 5.341495328268943
Validation loss: 5.061114884494553

Epoch: 6| Step: 9
Training loss: 4.913942562998796
Validation loss: 4.996329124942088

Epoch: 6| Step: 10
Training loss: 4.485143296622233
Validation loss: 4.928412304278466

Epoch: 6| Step: 11
Training loss: 5.991321008692426
Validation loss: 4.8636028846483885

Epoch: 6| Step: 12
Training loss: 5.894358456090596
Validation loss: 4.807534099654567

Epoch: 6| Step: 13
Training loss: 5.874556829092082
Validation loss: 4.751569319216476

Epoch: 3| Step: 0
Training loss: 5.517859851767712
Validation loss: 4.705045428522344

Epoch: 6| Step: 1
Training loss: 4.983132712792304
Validation loss: 4.6611182780776215

Epoch: 6| Step: 2
Training loss: 5.093937595400053
Validation loss: 4.623923224298282

Epoch: 6| Step: 3
Training loss: 3.8994280028864265
Validation loss: 4.588133969050643

Epoch: 6| Step: 4
Training loss: 5.00806806034238
Validation loss: 4.544280197801951

Epoch: 6| Step: 5
Training loss: 4.950477638705013
Validation loss: 4.508660845356144

Epoch: 6| Step: 6
Training loss: 4.6141791701301
Validation loss: 4.476982092596531

Epoch: 6| Step: 7
Training loss: 3.7704098008742126
Validation loss: 4.459745301779926

Epoch: 6| Step: 8
Training loss: 4.713684596920762
Validation loss: 4.444808021580938

Epoch: 6| Step: 9
Training loss: 3.6324618918933105
Validation loss: 4.43472697000142

Epoch: 6| Step: 10
Training loss: 4.574543981285867
Validation loss: 4.424634025692348

Epoch: 6| Step: 11
Training loss: 3.5745984152007977
Validation loss: 4.415616797410622

Epoch: 6| Step: 12
Training loss: 4.519579360428893
Validation loss: 4.399715272299197

Epoch: 6| Step: 13
Training loss: 5.8831940991979765
Validation loss: 4.380048428496843

Epoch: 4| Step: 0
Training loss: 4.553743084575838
Validation loss: 4.362211293518397

Epoch: 6| Step: 1
Training loss: 3.1479125235980727
Validation loss: 4.340515563795339

Epoch: 6| Step: 2
Training loss: 5.178004202760337
Validation loss: 4.327260363831945

Epoch: 6| Step: 3
Training loss: 5.16741825092617
Validation loss: 4.323787553548354

Epoch: 6| Step: 4
Training loss: 4.265165311196735
Validation loss: 4.317338535349661

Epoch: 6| Step: 5
Training loss: 4.926755199299357
Validation loss: 4.297080237893092

Epoch: 6| Step: 6
Training loss: 3.8875597786676983
Validation loss: 4.2766021824387

Epoch: 6| Step: 7
Training loss: 3.685298844620028
Validation loss: 4.267851614289745

Epoch: 6| Step: 8
Training loss: 5.200511195658269
Validation loss: 4.2614266413222595

Epoch: 6| Step: 9
Training loss: 3.0700385284491314
Validation loss: 4.242742609361976

Epoch: 6| Step: 10
Training loss: 5.289750160129663
Validation loss: 4.224342194112373

Epoch: 6| Step: 11
Training loss: 3.7361304335033463
Validation loss: 4.220297906431563

Epoch: 6| Step: 12
Training loss: 4.20543643968059
Validation loss: 4.204382628410539

Epoch: 6| Step: 13
Training loss: 4.719375467847261
Validation loss: 4.19073729744704

Epoch: 5| Step: 0
Training loss: 3.182894802012582
Validation loss: 4.182511953898465

Epoch: 6| Step: 1
Training loss: 2.9781672439701303
Validation loss: 4.1748525715180556

Epoch: 6| Step: 2
Training loss: 3.4557661769852306
Validation loss: 4.170190076163292

Epoch: 6| Step: 3
Training loss: 5.448489079451418
Validation loss: 4.16102745080886

Epoch: 6| Step: 4
Training loss: 3.7858778951615375
Validation loss: 4.145316578359273

Epoch: 6| Step: 5
Training loss: 3.3120769014598803
Validation loss: 4.131437305959401

Epoch: 6| Step: 6
Training loss: 4.4337178620431175
Validation loss: 4.123725286459812

Epoch: 6| Step: 7
Training loss: 5.2738046023843985
Validation loss: 4.112118599960216

Epoch: 6| Step: 8
Training loss: 4.438379764655424
Validation loss: 4.097960081287084

Epoch: 6| Step: 9
Training loss: 4.616071175593432
Validation loss: 4.091470081788115

Epoch: 6| Step: 10
Training loss: 3.990018789299725
Validation loss: 4.078338147259823

Epoch: 6| Step: 11
Training loss: 4.302478590473679
Validation loss: 4.072617090241451

Epoch: 6| Step: 12
Training loss: 4.382813370376473
Validation loss: 4.058552093246285

Epoch: 6| Step: 13
Training loss: 5.619285732778361
Validation loss: 4.041957944102736

Epoch: 6| Step: 0
Training loss: 3.481721560524967
Validation loss: 4.032442329134263

Epoch: 6| Step: 1
Training loss: 3.461880316522761
Validation loss: 4.02047760733221

Epoch: 6| Step: 2
Training loss: 4.362784853491683
Validation loss: 4.013904998478624

Epoch: 6| Step: 3
Training loss: 4.948183500891624
Validation loss: 4.003552920090538

Epoch: 6| Step: 4
Training loss: 3.128119170862763
Validation loss: 3.992321134740775

Epoch: 6| Step: 5
Training loss: 5.1397823118430574
Validation loss: 3.98471402345144

Epoch: 6| Step: 6
Training loss: 5.263315662227937
Validation loss: 3.972476807840903

Epoch: 6| Step: 7
Training loss: 4.296240631439587
Validation loss: 3.96001162136888

Epoch: 6| Step: 8
Training loss: 4.017995884028786
Validation loss: 3.9547575174132836

Epoch: 6| Step: 9
Training loss: 4.086849541194694
Validation loss: 3.9445253909159734

Epoch: 6| Step: 10
Training loss: 4.658739896353412
Validation loss: 3.9329004112216004

Epoch: 6| Step: 11
Training loss: 3.284159505727092
Validation loss: 3.9256897167688805

Epoch: 6| Step: 12
Training loss: 2.874257862667927
Validation loss: 3.9176558373691748

Epoch: 6| Step: 13
Training loss: 3.660183738515708
Validation loss: 3.904203460450291

Epoch: 7| Step: 0
Training loss: 4.4290101409747775
Validation loss: 3.89848299168664

Epoch: 6| Step: 1
Training loss: 3.710309362957172
Validation loss: 3.884850265618671

Epoch: 6| Step: 2
Training loss: 3.5741860643570367
Validation loss: 3.876811478397593

Epoch: 6| Step: 3
Training loss: 3.2001786241745216
Validation loss: 3.8703537951456517

Epoch: 6| Step: 4
Training loss: 3.572822072785285
Validation loss: 3.8587029396582464

Epoch: 6| Step: 5
Training loss: 3.8472817175740954
Validation loss: 3.8502625655396447

Epoch: 6| Step: 6
Training loss: 3.7806353108812063
Validation loss: 3.8461856656694833

Epoch: 6| Step: 7
Training loss: 3.518270760080206
Validation loss: 3.8402539860310956

Epoch: 6| Step: 8
Training loss: 4.637909888072731
Validation loss: 3.8260373820227076

Epoch: 6| Step: 9
Training loss: 4.102122822589705
Validation loss: 3.820105414799884

Epoch: 6| Step: 10
Training loss: 4.792295773989789
Validation loss: 3.8121467963321165

Epoch: 6| Step: 11
Training loss: 4.109113213500838
Validation loss: 3.8005461350551473

Epoch: 6| Step: 12
Training loss: 4.014281051702851
Validation loss: 3.792442837775249

Epoch: 6| Step: 13
Training loss: 4.827286039378798
Validation loss: 3.787755479470535

Epoch: 8| Step: 0
Training loss: 3.82526000890687
Validation loss: 3.7760405093294884

Epoch: 6| Step: 1
Training loss: 3.3858994985717916
Validation loss: 3.7688529190031406

Epoch: 6| Step: 2
Training loss: 3.4350026162039
Validation loss: 3.7614495420635903

Epoch: 6| Step: 3
Training loss: 3.8383515447619905
Validation loss: 3.7531749137623653

Epoch: 6| Step: 4
Training loss: 4.155698352429676
Validation loss: 3.7425758307104102

Epoch: 6| Step: 5
Training loss: 4.157634762560037
Validation loss: 3.7303390025944236

Epoch: 6| Step: 6
Training loss: 3.935797429214135
Validation loss: 3.7260987795221685

Epoch: 6| Step: 7
Training loss: 3.925273744792536
Validation loss: 3.7169662972472657

Epoch: 6| Step: 8
Training loss: 4.042126082170144
Validation loss: 3.706356985230303

Epoch: 6| Step: 9
Training loss: 3.753325704247246
Validation loss: 3.699248774371

Epoch: 6| Step: 10
Training loss: 3.8480488378395665
Validation loss: 3.690289312324295

Epoch: 6| Step: 11
Training loss: 4.254462647984549
Validation loss: 3.681615309101767

Epoch: 6| Step: 12
Training loss: 4.016601918649693
Validation loss: 3.6723260752998237

Epoch: 6| Step: 13
Training loss: 4.102183267755825
Validation loss: 3.6686631125064215

Epoch: 9| Step: 0
Training loss: 4.282222219745518
Validation loss: 3.6562251421547307

Epoch: 6| Step: 1
Training loss: 3.975614963201583
Validation loss: 3.649943508610624

Epoch: 6| Step: 2
Training loss: 3.9402291664050533
Validation loss: 3.638149648206813

Epoch: 6| Step: 3
Training loss: 5.029846753920036
Validation loss: 3.633441924065817

Epoch: 6| Step: 4
Training loss: 4.226834159765457
Validation loss: 3.623956894433111

Epoch: 6| Step: 5
Training loss: 4.133922521580286
Validation loss: 3.6150375550810976

Epoch: 6| Step: 6
Training loss: 3.098941031442422
Validation loss: 3.6102107734927547

Epoch: 6| Step: 7
Training loss: 3.444203170064822
Validation loss: 3.6282868298586557

Epoch: 6| Step: 8
Training loss: 2.8033527396577793
Validation loss: 3.590939643215675

Epoch: 6| Step: 9
Training loss: 3.0477368823046005
Validation loss: 3.5923310648177473

Epoch: 6| Step: 10
Training loss: 4.1812362511013905
Validation loss: 3.5944796156383045

Epoch: 6| Step: 11
Training loss: 4.405256010446853
Validation loss: 3.5866838479613103

Epoch: 6| Step: 12
Training loss: 2.8681866726424095
Validation loss: 3.569747489840822

Epoch: 6| Step: 13
Training loss: 2.5801453043806526
Validation loss: 3.55645251542088

Epoch: 10| Step: 0
Training loss: 3.93289205586623
Validation loss: 3.5569741109402044

Epoch: 6| Step: 1
Training loss: 4.176101658003957
Validation loss: 3.5511233457623153

Epoch: 6| Step: 2
Training loss: 3.9169725880290627
Validation loss: 3.534383757048277

Epoch: 6| Step: 3
Training loss: 3.351223872935558
Validation loss: 3.524754034819665

Epoch: 6| Step: 4
Training loss: 3.1229790828711583
Validation loss: 3.5213990043168377

Epoch: 6| Step: 5
Training loss: 3.5864587209706684
Validation loss: 3.5174560503828256

Epoch: 6| Step: 6
Training loss: 3.8603448923228485
Validation loss: 3.5158084215377703

Epoch: 6| Step: 7
Training loss: 3.6112125887836015
Validation loss: 3.5026351147234522

Epoch: 6| Step: 8
Training loss: 3.9823421303780253
Validation loss: 3.4942903341412976

Epoch: 6| Step: 9
Training loss: 3.908446647991413
Validation loss: 3.4835775684126697

Epoch: 6| Step: 10
Training loss: 3.574441538053177
Validation loss: 3.475455378855748

Epoch: 6| Step: 11
Training loss: 3.2116395503035857
Validation loss: 3.469762837359255

Epoch: 6| Step: 12
Training loss: 4.346671021075106
Validation loss: 3.4653226014206138

Epoch: 6| Step: 13
Training loss: 2.7762230866908366
Validation loss: 3.457649133376801

Epoch: 11| Step: 0
Training loss: 3.9840725653064406
Validation loss: 3.4494599716130754

Epoch: 6| Step: 1
Training loss: 4.060630015833539
Validation loss: 3.4408982380245376

Epoch: 6| Step: 2
Training loss: 3.063991728280956
Validation loss: 3.4357351640827183

Epoch: 6| Step: 3
Training loss: 3.3783541366119807
Validation loss: 3.427345217748055

Epoch: 6| Step: 4
Training loss: 3.722415688138262
Validation loss: 3.427063122621879

Epoch: 6| Step: 5
Training loss: 3.4000714462851733
Validation loss: 3.411862552945627

Epoch: 6| Step: 6
Training loss: 4.038714454751844
Validation loss: 3.408326018257113

Epoch: 6| Step: 7
Training loss: 3.7298881358536002
Validation loss: 3.401863944863224

Epoch: 6| Step: 8
Training loss: 2.4439845760376957
Validation loss: 3.387277896421515

Epoch: 6| Step: 9
Training loss: 3.7374081603539206
Validation loss: 3.383873868380076

Epoch: 6| Step: 10
Training loss: 3.9018199977759025
Validation loss: 3.3835525557655575

Epoch: 6| Step: 11
Training loss: 4.336151062800823
Validation loss: 3.3736305086494567

Epoch: 6| Step: 12
Training loss: 2.985231286677958
Validation loss: 3.370105407313716

Epoch: 6| Step: 13
Training loss: 3.6522910047996224
Validation loss: 3.3685288292142563

Epoch: 12| Step: 0
Training loss: 3.4077455929463274
Validation loss: 3.3612659746023974

Epoch: 6| Step: 1
Training loss: 3.9496880468486917
Validation loss: 3.3496890475189187

Epoch: 6| Step: 2
Training loss: 2.476601200222565
Validation loss: 3.341463086859188

Epoch: 6| Step: 3
Training loss: 3.9150047158009467
Validation loss: 3.3506150690104444

Epoch: 6| Step: 4
Training loss: 3.619857033704233
Validation loss: 3.331142838413343

Epoch: 6| Step: 5
Training loss: 3.505393232633547
Validation loss: 3.319341464327492

Epoch: 6| Step: 6
Training loss: 3.735763483210659
Validation loss: 3.311697540940245

Epoch: 6| Step: 7
Training loss: 3.5754696517507765
Validation loss: 3.3073892977044292

Epoch: 6| Step: 8
Training loss: 3.8605377052026078
Validation loss: 3.301544969036754

Epoch: 6| Step: 9
Training loss: 3.418817138012641
Validation loss: 3.2916151494181856

Epoch: 6| Step: 10
Training loss: 3.7046841586390324
Validation loss: 3.290457462274276

Epoch: 6| Step: 11
Training loss: 3.496899321252483
Validation loss: 3.279348352280053

Epoch: 6| Step: 12
Training loss: 3.2556888968190885
Validation loss: 3.2715610963986688

Epoch: 6| Step: 13
Training loss: 3.6611638102182025
Validation loss: 3.267778389525568

Epoch: 13| Step: 0
Training loss: 2.750416377363771
Validation loss: 3.25622417732666

Epoch: 6| Step: 1
Training loss: 3.609810806980724
Validation loss: 3.2489161739052146

Epoch: 6| Step: 2
Training loss: 3.9886955023676487
Validation loss: 3.2480153728311043

Epoch: 6| Step: 3
Training loss: 3.4097821881489256
Validation loss: 3.241491123120501

Epoch: 6| Step: 4
Training loss: 3.5569920038965073
Validation loss: 3.238311499457538

Epoch: 6| Step: 5
Training loss: 3.117631847098918
Validation loss: 3.230712398939108

Epoch: 6| Step: 6
Training loss: 3.6575367285448137
Validation loss: 3.2328315830372762

Epoch: 6| Step: 7
Training loss: 3.824577836818439
Validation loss: 3.2281303796521943

Epoch: 6| Step: 8
Training loss: 2.905287224231994
Validation loss: 3.217280035395349

Epoch: 6| Step: 9
Training loss: 3.78974766779519
Validation loss: 3.209077163681049

Epoch: 6| Step: 10
Training loss: 3.813210468069066
Validation loss: 3.202206256323456

Epoch: 6| Step: 11
Training loss: 3.4980286087078913
Validation loss: 3.202285082625466

Epoch: 6| Step: 12
Training loss: 3.1505011144798645
Validation loss: 3.1883001527618062

Epoch: 6| Step: 13
Training loss: 3.335530240573275
Validation loss: 3.181976337108642

Epoch: 14| Step: 0
Training loss: 3.625312002501097
Validation loss: 3.1763402534170586

Epoch: 6| Step: 1
Training loss: 3.8546861590571364
Validation loss: 3.1721562329026973

Epoch: 6| Step: 2
Training loss: 2.832059199754321
Validation loss: 3.1646394204408903

Epoch: 6| Step: 3
Training loss: 3.4748459596591292
Validation loss: 3.1593291895123623

Epoch: 6| Step: 4
Training loss: 2.4131537149652904
Validation loss: 3.158373980339468

Epoch: 6| Step: 5
Training loss: 3.6625554415832258
Validation loss: 3.1570558237116804

Epoch: 6| Step: 6
Training loss: 2.788154176321227
Validation loss: 3.1515575247365306

Epoch: 6| Step: 7
Training loss: 3.3117559785003503
Validation loss: 3.13656463855393

Epoch: 6| Step: 8
Training loss: 3.732732563272651
Validation loss: 3.1416070127586058

Epoch: 6| Step: 9
Training loss: 3.9346756266542866
Validation loss: 3.1288585967746037

Epoch: 6| Step: 10
Training loss: 3.5236049028708716
Validation loss: 3.1329050538082264

Epoch: 6| Step: 11
Training loss: 4.18944174313722
Validation loss: 3.1327703890106666

Epoch: 6| Step: 12
Training loss: 2.7612723889528996
Validation loss: 3.1214818876761363

Epoch: 6| Step: 13
Training loss: 3.022766038720382
Validation loss: 3.116101450192846

Epoch: 15| Step: 0
Training loss: 3.1926306030037876
Validation loss: 3.110948484087032

Epoch: 6| Step: 1
Training loss: 3.5042578819519914
Validation loss: 3.109486597319358

Epoch: 6| Step: 2
Training loss: 3.810220130508489
Validation loss: 3.103917632953422

Epoch: 6| Step: 3
Training loss: 3.5343873257342
Validation loss: 3.095412419954481

Epoch: 6| Step: 4
Training loss: 2.718988166988168
Validation loss: 3.1096997305073457

Epoch: 6| Step: 5
Training loss: 2.854620477466932
Validation loss: 3.0837036745733943

Epoch: 6| Step: 6
Training loss: 4.05877514783507
Validation loss: 3.085248474317003

Epoch: 6| Step: 7
Training loss: 3.1300465842975957
Validation loss: 3.07890097747776

Epoch: 6| Step: 8
Training loss: 3.224349105562395
Validation loss: 3.079556578816535

Epoch: 6| Step: 9
Training loss: 3.2035465614361005
Validation loss: 3.075394433675285

Epoch: 6| Step: 10
Training loss: 3.736468889263901
Validation loss: 3.0714179793329777

Epoch: 6| Step: 11
Training loss: 3.506038497556765
Validation loss: 3.063422587499054

Epoch: 6| Step: 12
Training loss: 3.325723417360394
Validation loss: 3.047281761142538

Epoch: 6| Step: 13
Training loss: 2.731311411570921
Validation loss: 3.0411987152623534

Epoch: 16| Step: 0
Training loss: 2.9724958379024953
Validation loss: 3.0360925675379566

Epoch: 6| Step: 1
Training loss: 3.678557204114525
Validation loss: 3.032988229721384

Epoch: 6| Step: 2
Training loss: 3.66985041217343
Validation loss: 3.028670122811748

Epoch: 6| Step: 3
Training loss: 2.6967604134308094
Validation loss: 3.0225161630662845

Epoch: 6| Step: 4
Training loss: 3.3339400375090813
Validation loss: 3.0303056567610454

Epoch: 6| Step: 5
Training loss: 3.7494882870267485
Validation loss: 3.02341583719877

Epoch: 6| Step: 6
Training loss: 3.2044273148089277
Validation loss: 3.013092147946491

Epoch: 6| Step: 7
Training loss: 2.9094466810072364
Validation loss: 3.0047907544588672

Epoch: 6| Step: 8
Training loss: 2.5480035259156555
Validation loss: 3.0082936394721624

Epoch: 6| Step: 9
Training loss: 3.848231362673833
Validation loss: 3.008055774197985

Epoch: 6| Step: 10
Training loss: 3.4274910604080713
Validation loss: 3.0020468644089853

Epoch: 6| Step: 11
Training loss: 3.495607890034868
Validation loss: 2.993995262853396

Epoch: 6| Step: 12
Training loss: 3.0760334480262834
Validation loss: 2.9887527521720094

Epoch: 6| Step: 13
Training loss: 3.5282397647704613
Validation loss: 3.000145272870165

Epoch: 17| Step: 0
Training loss: 3.6306313806846475
Validation loss: 2.990743693950996

Epoch: 6| Step: 1
Training loss: 3.815799098686361
Validation loss: 3.0080573363871252

Epoch: 6| Step: 2
Training loss: 2.5411403194387736
Validation loss: 3.0175888337974306

Epoch: 6| Step: 3
Training loss: 2.749049629371229
Validation loss: 3.0233184154211328

Epoch: 6| Step: 4
Training loss: 3.7122045020465464
Validation loss: 3.027979058679517

Epoch: 6| Step: 5
Training loss: 2.7758915749078765
Validation loss: 3.016644292351975

Epoch: 6| Step: 6
Training loss: 2.7035728480502157
Validation loss: 3.0066352013645736

Epoch: 6| Step: 7
Training loss: 3.3138514227151243
Validation loss: 2.9967047289282918

Epoch: 6| Step: 8
Training loss: 3.3095786690289897
Validation loss: 2.981301536156748

Epoch: 6| Step: 9
Training loss: 2.955410027884856
Validation loss: 2.9723769980036727

Epoch: 6| Step: 10
Training loss: 3.763315592730469
Validation loss: 2.9751336487961835

Epoch: 6| Step: 11
Training loss: 3.7094357312829995
Validation loss: 2.982457038634547

Epoch: 6| Step: 12
Training loss: 3.3040190757958188
Validation loss: 2.970201259564165

Epoch: 6| Step: 13
Training loss: 3.470730602733097
Validation loss: 2.9549428450699966

Epoch: 18| Step: 0
Training loss: 3.295481889186771
Validation loss: 2.963714288927422

Epoch: 6| Step: 1
Training loss: 3.6435792912429816
Validation loss: 2.940166285298302

Epoch: 6| Step: 2
Training loss: 2.48482734832599
Validation loss: 2.9433786726216917

Epoch: 6| Step: 3
Training loss: 2.974337489626432
Validation loss: 2.949875555616885

Epoch: 6| Step: 4
Training loss: 3.482129934657226
Validation loss: 2.9594091226906785

Epoch: 6| Step: 5
Training loss: 2.923606318824527
Validation loss: 2.9618149051371945

Epoch: 6| Step: 6
Training loss: 2.4583410704754987
Validation loss: 2.958916140638386

Epoch: 6| Step: 7
Training loss: 3.4650884324716205
Validation loss: 2.9533648632494898

Epoch: 6| Step: 8
Training loss: 3.7340864125874114
Validation loss: 2.9377565579510594

Epoch: 6| Step: 9
Training loss: 3.387543433780159
Validation loss: 2.923264881022052

Epoch: 6| Step: 10
Training loss: 4.053632712936228
Validation loss: 2.9521292666772494

Epoch: 6| Step: 11
Training loss: 2.696762446842908
Validation loss: 2.9677600688946564

Epoch: 6| Step: 12
Training loss: 3.5834976572609514
Validation loss: 2.9254374659790967

Epoch: 6| Step: 13
Training loss: 2.7074718474582617
Validation loss: 2.9155729077110166

Epoch: 19| Step: 0
Training loss: 3.519934289275564
Validation loss: 2.924894876188824

Epoch: 6| Step: 1
Training loss: 3.2276507521332882
Validation loss: 2.9273587408479744

Epoch: 6| Step: 2
Training loss: 2.810953944498999
Validation loss: 2.932283232777736

Epoch: 6| Step: 3
Training loss: 3.4627691722039198
Validation loss: 2.9352781654900526

Epoch: 6| Step: 4
Training loss: 3.016055377826186
Validation loss: 2.936108685791913

Epoch: 6| Step: 5
Training loss: 3.3335353949020763
Validation loss: 2.9270895450371253

Epoch: 6| Step: 6
Training loss: 2.446539915094856
Validation loss: 2.9241567277666802

Epoch: 6| Step: 7
Training loss: 2.7204034096625653
Validation loss: 2.916704036690054

Epoch: 6| Step: 8
Training loss: 2.9211877024728508
Validation loss: 2.9122080979548395

Epoch: 6| Step: 9
Training loss: 3.4977845946130675
Validation loss: 2.940878104543006

Epoch: 6| Step: 10
Training loss: 3.219301046959888
Validation loss: 2.939123730812886

Epoch: 6| Step: 11
Training loss: 3.103418673998964
Validation loss: 2.96057799033395

Epoch: 6| Step: 12
Training loss: 4.00975658714885
Validation loss: 2.9049154845999205

Epoch: 6| Step: 13
Training loss: 4.111970147317662
Validation loss: 2.9021152948605446

Epoch: 20| Step: 0
Training loss: 3.5150308742768024
Validation loss: 2.909301547731861

Epoch: 6| Step: 1
Training loss: 2.9500891203268917
Validation loss: 2.9317065490309306

Epoch: 6| Step: 2
Training loss: 2.8543625553228176
Validation loss: 2.9589443872696153

Epoch: 6| Step: 3
Training loss: 3.469708284544103
Validation loss: 2.9634848414998056

Epoch: 6| Step: 4
Training loss: 4.058616777635588
Validation loss: 2.952685239461476

Epoch: 6| Step: 5
Training loss: 2.803734322519813
Validation loss: 2.9106212375799427

Epoch: 6| Step: 6
Training loss: 3.1881186688974825
Validation loss: 2.879346081646165

Epoch: 6| Step: 7
Training loss: 3.463491629714731
Validation loss: 2.866703890033915

Epoch: 6| Step: 8
Training loss: 3.378298807392746
Validation loss: 2.9298156001480087

Epoch: 6| Step: 9
Training loss: 3.2115840214573064
Validation loss: 2.947497564863422

Epoch: 6| Step: 10
Training loss: 3.060012741560993
Validation loss: 2.9982431319515412

Epoch: 6| Step: 11
Training loss: 3.3529089866092923
Validation loss: 2.8991679149195444

Epoch: 6| Step: 12
Training loss: 3.0142964811655784
Validation loss: 2.8564481988371138

Epoch: 6| Step: 13
Training loss: 2.790775574384223
Validation loss: 2.8988641574621528

Epoch: 21| Step: 0
Training loss: 2.9057162215104095
Validation loss: 2.884829678743806

Epoch: 6| Step: 1
Training loss: 3.7725042922777314
Validation loss: 2.886223703527335

Epoch: 6| Step: 2
Training loss: 2.6873608930204997
Validation loss: 2.8842916179268574

Epoch: 6| Step: 3
Training loss: 3.523273878675587
Validation loss: 2.884391273436008

Epoch: 6| Step: 4
Training loss: 2.88709534290833
Validation loss: 2.8819792230982424

Epoch: 6| Step: 5
Training loss: 3.8693991761953446
Validation loss: 2.873251096354568

Epoch: 6| Step: 6
Training loss: 3.249500823199709
Validation loss: 2.8607250388481895

Epoch: 6| Step: 7
Training loss: 3.3455246823765363
Validation loss: 2.8471319639671533

Epoch: 6| Step: 8
Training loss: 3.1933946146755905
Validation loss: 2.856064477099705

Epoch: 6| Step: 9
Training loss: 3.3643763018865473
Validation loss: 2.8712046799566635

Epoch: 6| Step: 10
Training loss: 3.067802820655744
Validation loss: 2.8532867922733005

Epoch: 6| Step: 11
Training loss: 3.1135042462483087
Validation loss: 2.8486591729526665

Epoch: 6| Step: 12
Training loss: 2.733388842817655
Validation loss: 2.845368500906758

Epoch: 6| Step: 13
Training loss: 2.5588166350915973
Validation loss: 2.8502996584009384

Epoch: 22| Step: 0
Training loss: 3.068927481621417
Validation loss: 2.8567891466333353

Epoch: 6| Step: 1
Training loss: 2.8825627247188352
Validation loss: 2.8579088936753236

Epoch: 6| Step: 2
Training loss: 3.335847161027957
Validation loss: 2.846803931333939

Epoch: 6| Step: 3
Training loss: 3.077209956293575
Validation loss: 2.843217985900722

Epoch: 6| Step: 4
Training loss: 3.9621214291692803
Validation loss: 2.8353790200656532

Epoch: 6| Step: 5
Training loss: 3.4285971748429867
Validation loss: 2.8276696952192446

Epoch: 6| Step: 6
Training loss: 3.0771997290730777
Validation loss: 2.8292032000753187

Epoch: 6| Step: 7
Training loss: 2.823394106271279
Validation loss: 2.8281813063925783

Epoch: 6| Step: 8
Training loss: 3.1002022092765693
Validation loss: 2.832644397526261

Epoch: 6| Step: 9
Training loss: 3.3329418588277493
Validation loss: 2.8419297252426885

Epoch: 6| Step: 10
Training loss: 2.802430331123902
Validation loss: 2.8334863073709466

Epoch: 6| Step: 11
Training loss: 2.332049652487065
Validation loss: 2.8292259466960514

Epoch: 6| Step: 12
Training loss: 2.9475652442550997
Validation loss: 2.8248181233621694

Epoch: 6| Step: 13
Training loss: 4.228201895477662
Validation loss: 2.817158876314069

Epoch: 23| Step: 0
Training loss: 2.736599698364402
Validation loss: 2.8124886005494183

Epoch: 6| Step: 1
Training loss: 3.2961213755619685
Validation loss: 2.8113745384773106

Epoch: 6| Step: 2
Training loss: 3.111005959171877
Validation loss: 2.8135345306423227

Epoch: 6| Step: 3
Training loss: 3.2411394378600646
Validation loss: 2.8173959901756405

Epoch: 6| Step: 4
Training loss: 3.1582124859748406
Validation loss: 2.8197268394727586

Epoch: 6| Step: 5
Training loss: 2.829903564966785
Validation loss: 2.8159690363853613

Epoch: 6| Step: 6
Training loss: 3.5058510782892776
Validation loss: 2.812831222008309

Epoch: 6| Step: 7
Training loss: 3.4813605299546855
Validation loss: 2.811871800970718

Epoch: 6| Step: 8
Training loss: 2.228079937964735
Validation loss: 2.8134451842321306

Epoch: 6| Step: 9
Training loss: 2.8953179282700057
Validation loss: 2.836097903715361

Epoch: 6| Step: 10
Training loss: 3.047982821438657
Validation loss: 2.8635484047889155

Epoch: 6| Step: 11
Training loss: 3.657221477591592
Validation loss: 2.8509792556833546

Epoch: 6| Step: 12
Training loss: 3.2154538953391567
Validation loss: 2.8163219347860027

Epoch: 6| Step: 13
Training loss: 3.797640248484469
Validation loss: 2.793402372982769

Epoch: 24| Step: 0
Training loss: 2.414859973330938
Validation loss: 2.7962577580076133

Epoch: 6| Step: 1
Training loss: 3.261545215490349
Validation loss: 2.8086855855653017

Epoch: 6| Step: 2
Training loss: 3.365877042522626
Validation loss: 2.8047998041209454

Epoch: 6| Step: 3
Training loss: 3.0636694096238326
Validation loss: 2.792863905614183

Epoch: 6| Step: 4
Training loss: 2.643868705663745
Validation loss: 2.7905671123504145

Epoch: 6| Step: 5
Training loss: 3.228064974644994
Validation loss: 2.7904595639826892

Epoch: 6| Step: 6
Training loss: 2.608441214316029
Validation loss: 2.790195243220761

Epoch: 6| Step: 7
Training loss: 3.6966276832591207
Validation loss: 2.786438872060743

Epoch: 6| Step: 8
Training loss: 2.984173673060321
Validation loss: 2.7839957640509816

Epoch: 6| Step: 9
Training loss: 3.016174424477887
Validation loss: 2.784703802546117

Epoch: 6| Step: 10
Training loss: 3.1825761350528152
Validation loss: 2.780012207640377

Epoch: 6| Step: 11
Training loss: 3.429374663177579
Validation loss: 2.776926780359475

Epoch: 6| Step: 12
Training loss: 3.5911488282889903
Validation loss: 2.779596815161149

Epoch: 6| Step: 13
Training loss: 2.847586088009542
Validation loss: 2.7759073553524094

Epoch: 25| Step: 0
Training loss: 3.2214991116224265
Validation loss: 2.7957965208069475

Epoch: 6| Step: 1
Training loss: 2.8696597715739482
Validation loss: 2.813985669641403

Epoch: 6| Step: 2
Training loss: 3.312916567538591
Validation loss: 2.796551055305973

Epoch: 6| Step: 3
Training loss: 3.0785110154133744
Validation loss: 2.773012175379435

Epoch: 6| Step: 4
Training loss: 3.252482273249952
Validation loss: 2.7715143237686597

Epoch: 6| Step: 5
Training loss: 3.3684912191160183
Validation loss: 2.7711757655257165

Epoch: 6| Step: 6
Training loss: 2.7321188201701765
Validation loss: 2.7727257690733094

Epoch: 6| Step: 7
Training loss: 3.3046824498059078
Validation loss: 2.7771312223336935

Epoch: 6| Step: 8
Training loss: 3.141129182798725
Validation loss: 2.7757187220495547

Epoch: 6| Step: 9
Training loss: 3.906623639356947
Validation loss: 2.773559836601646

Epoch: 6| Step: 10
Training loss: 2.4930005316888675
Validation loss: 2.7694576276841563

Epoch: 6| Step: 11
Training loss: 2.6328793299312077
Validation loss: 2.7664475007863447

Epoch: 6| Step: 12
Training loss: 2.7655693091695968
Validation loss: 2.7688690073675066

Epoch: 6| Step: 13
Training loss: 3.369835752565335
Validation loss: 2.7784252877098816

Epoch: 26| Step: 0
Training loss: 2.8415184907138733
Validation loss: 2.8001329360408724

Epoch: 6| Step: 1
Training loss: 2.2060837593881786
Validation loss: 2.8594700280683965

Epoch: 6| Step: 2
Training loss: 3.393455343644811
Validation loss: 3.127306956640642

Epoch: 6| Step: 3
Training loss: 3.6126544800565896
Validation loss: 2.8489148083631317

Epoch: 6| Step: 4
Training loss: 2.309802982796571
Validation loss: 2.773550317073757

Epoch: 6| Step: 5
Training loss: 3.728074125085083
Validation loss: 2.7903886133783846

Epoch: 6| Step: 6
Training loss: 2.897858677568858
Validation loss: 2.806637705709351

Epoch: 6| Step: 7
Training loss: 3.6514747932292204
Validation loss: 2.8184318172448477

Epoch: 6| Step: 8
Training loss: 3.2825553114241037
Validation loss: 2.826406620038515

Epoch: 6| Step: 9
Training loss: 2.6271425313630083
Validation loss: 2.866767016679329

Epoch: 6| Step: 10
Training loss: 2.7546016166917693
Validation loss: 2.8888817512444014

Epoch: 6| Step: 11
Training loss: 3.268776864694516
Validation loss: 2.874959538778299

Epoch: 6| Step: 12
Training loss: 3.715760127017513
Validation loss: 2.8199384778387535

Epoch: 6| Step: 13
Training loss: 3.3518988167142267
Validation loss: 2.7983882309817747

Epoch: 27| Step: 0
Training loss: 3.1182088684437432
Validation loss: 2.79531324162087

Epoch: 6| Step: 1
Training loss: 3.658087374703726
Validation loss: 2.79912336938315

Epoch: 6| Step: 2
Training loss: 3.4331544631979884
Validation loss: 2.807889052715946

Epoch: 6| Step: 3
Training loss: 2.427683012611593
Validation loss: 2.7841098871736603

Epoch: 6| Step: 4
Training loss: 2.391149725080273
Validation loss: 2.7744150586036986

Epoch: 6| Step: 5
Training loss: 2.551296865508475
Validation loss: 2.7730037152971283

Epoch: 6| Step: 6
Training loss: 3.493115602604526
Validation loss: 2.7738709035592253

Epoch: 6| Step: 7
Training loss: 3.183047007787378
Validation loss: 2.7735791981428135

Epoch: 6| Step: 8
Training loss: 3.299426786172042
Validation loss: 2.7662125117477303

Epoch: 6| Step: 9
Training loss: 3.049506200621168
Validation loss: 2.7637558141681455

Epoch: 6| Step: 10
Training loss: 3.2056240846510256
Validation loss: 2.7595232275575428

Epoch: 6| Step: 11
Training loss: 3.3142508432122937
Validation loss: 2.7494177318397988

Epoch: 6| Step: 12
Training loss: 2.855160468246658
Validation loss: 2.7513479199628166

Epoch: 6| Step: 13
Training loss: 3.4401925379077225
Validation loss: 2.7637145248386803

Epoch: 28| Step: 0
Training loss: 3.090966716269987
Validation loss: 2.7577906851765714

Epoch: 6| Step: 1
Training loss: 2.991848839302194
Validation loss: 2.752494745030345

Epoch: 6| Step: 2
Training loss: 3.581945387082623
Validation loss: 2.741628128839652

Epoch: 6| Step: 3
Training loss: 3.6436205152271794
Validation loss: 2.7390562034082353

Epoch: 6| Step: 4
Training loss: 2.5802507364430767
Validation loss: 2.7377175897607744

Epoch: 6| Step: 5
Training loss: 2.787263519793558
Validation loss: 2.7376113195444285

Epoch: 6| Step: 6
Training loss: 2.7919393282913774
Validation loss: 2.7477164729857018

Epoch: 6| Step: 7
Training loss: 3.8026798285523595
Validation loss: 2.746751958135764

Epoch: 6| Step: 8
Training loss: 3.2411548854467105
Validation loss: 2.7429584524582

Epoch: 6| Step: 9
Training loss: 2.3538320748093664
Validation loss: 2.739171117879201

Epoch: 6| Step: 10
Training loss: 2.896715336322342
Validation loss: 2.7354840220278436

Epoch: 6| Step: 11
Training loss: 2.4356150063144986
Validation loss: 2.731072013816746

Epoch: 6| Step: 12
Training loss: 3.384930729347422
Validation loss: 2.7349071897735713

Epoch: 6| Step: 13
Training loss: 3.1399482263560934
Validation loss: 2.741927194944734

Epoch: 29| Step: 0
Training loss: 3.1541128949344377
Validation loss: 2.741409007547211

Epoch: 6| Step: 1
Training loss: 3.1771322944912477
Validation loss: 2.7429030323074928

Epoch: 6| Step: 2
Training loss: 3.081028067931758
Validation loss: 2.734368330189998

Epoch: 6| Step: 3
Training loss: 2.907105022705081
Validation loss: 2.731573857298327

Epoch: 6| Step: 4
Training loss: 2.9220674012835577
Validation loss: 2.724359953804083

Epoch: 6| Step: 5
Training loss: 3.342613668855734
Validation loss: 2.722347522907594

Epoch: 6| Step: 6
Training loss: 3.1337496528275324
Validation loss: 2.7202684308601874

Epoch: 6| Step: 7
Training loss: 2.7654820636946282
Validation loss: 2.7180656974083286

Epoch: 6| Step: 8
Training loss: 2.3402250103483766
Validation loss: 2.718049325542037

Epoch: 6| Step: 9
Training loss: 2.948798830566754
Validation loss: 2.715799653736811

Epoch: 6| Step: 10
Training loss: 3.8951974985687756
Validation loss: 2.717806815509215

Epoch: 6| Step: 11
Training loss: 3.1281691693856524
Validation loss: 2.7158334543556704

Epoch: 6| Step: 12
Training loss: 2.607927663373295
Validation loss: 2.7189274872471687

Epoch: 6| Step: 13
Training loss: 3.2794499864214948
Validation loss: 2.720211944330259

Epoch: 30| Step: 0
Training loss: 3.225712920648015
Validation loss: 2.7182085457277347

Epoch: 6| Step: 1
Training loss: 2.3659894574302096
Validation loss: 2.7190135412710332

Epoch: 6| Step: 2
Training loss: 1.921886040880697
Validation loss: 2.721429035051646

Epoch: 6| Step: 3
Training loss: 3.4267398619596103
Validation loss: 2.721159909233149

Epoch: 6| Step: 4
Training loss: 3.6357036284216377
Validation loss: 2.718026309689063

Epoch: 6| Step: 5
Training loss: 3.6815823416308384
Validation loss: 2.7153032401655497

Epoch: 6| Step: 6
Training loss: 3.299292667761371
Validation loss: 2.7122873892001396

Epoch: 6| Step: 7
Training loss: 2.1290131988259597
Validation loss: 2.712594216188039

Epoch: 6| Step: 8
Training loss: 3.2643266497016117
Validation loss: 2.7099825536620936

Epoch: 6| Step: 9
Training loss: 2.9291521320733054
Validation loss: 2.7092840979072217

Epoch: 6| Step: 10
Training loss: 2.8605099286135642
Validation loss: 2.709428726316295

Epoch: 6| Step: 11
Training loss: 3.1714114898113617
Validation loss: 2.713040727916495

Epoch: 6| Step: 12
Training loss: 3.238500645180267
Validation loss: 2.72672943188113

Epoch: 6| Step: 13
Training loss: 3.0840200913760647
Validation loss: 2.71854712559486

Epoch: 31| Step: 0
Training loss: 2.847282981741153
Validation loss: 2.7050653497839905

Epoch: 6| Step: 1
Training loss: 2.8502115472281417
Validation loss: 2.7055755580790377

Epoch: 6| Step: 2
Training loss: 3.107606778017416
Validation loss: 2.7020767809429724

Epoch: 6| Step: 3
Training loss: 3.022897913686439
Validation loss: 2.7024546568604824

Epoch: 6| Step: 4
Training loss: 3.6428370488571633
Validation loss: 2.701948177291944

Epoch: 6| Step: 5
Training loss: 2.7920293335179798
Validation loss: 2.7015868759187187

Epoch: 6| Step: 6
Training loss: 2.850410458638342
Validation loss: 2.703896027450531

Epoch: 6| Step: 7
Training loss: 3.209354056612227
Validation loss: 2.702371086596106

Epoch: 6| Step: 8
Training loss: 2.910475909195156
Validation loss: 2.701436060659772

Epoch: 6| Step: 9
Training loss: 2.7909952086869176
Validation loss: 2.703576215254556

Epoch: 6| Step: 10
Training loss: 3.287760497241129
Validation loss: 2.702504452102787

Epoch: 6| Step: 11
Training loss: 3.2841923191940396
Validation loss: 2.704710415375148

Epoch: 6| Step: 12
Training loss: 2.70102005516044
Validation loss: 2.7022262132684243

Epoch: 6| Step: 13
Training loss: 3.3605100621494377
Validation loss: 2.700316868809515

Epoch: 32| Step: 0
Training loss: 2.7851821020780383
Validation loss: 2.698930688923851

Epoch: 6| Step: 1
Training loss: 3.3531366666251343
Validation loss: 2.7010291336496657

Epoch: 6| Step: 2
Training loss: 2.834086673291506
Validation loss: 2.698435274382555

Epoch: 6| Step: 3
Training loss: 2.9052778689567043
Validation loss: 2.7007149289898797

Epoch: 6| Step: 4
Training loss: 2.890375899845421
Validation loss: 2.6991611220081775

Epoch: 6| Step: 5
Training loss: 2.9623036315477447
Validation loss: 2.702872729309209

Epoch: 6| Step: 6
Training loss: 3.780378509220159
Validation loss: 2.7138233121710864

Epoch: 6| Step: 7
Training loss: 2.401262340762166
Validation loss: 2.722990151273104

Epoch: 6| Step: 8
Training loss: 3.6826665202791524
Validation loss: 2.7493228134502723

Epoch: 6| Step: 9
Training loss: 3.1422928514086133
Validation loss: 2.7802043099825853

Epoch: 6| Step: 10
Training loss: 3.4766478110143764
Validation loss: 2.7639695984693553

Epoch: 6| Step: 11
Training loss: 2.789495696290644
Validation loss: 2.713886331604807

Epoch: 6| Step: 12
Training loss: 2.617028712679611
Validation loss: 2.6994507292818395

Epoch: 6| Step: 13
Training loss: 2.457878609536343
Validation loss: 2.7047668209195486

Epoch: 33| Step: 0
Training loss: 3.303948646718991
Validation loss: 2.7242332537323337

Epoch: 6| Step: 1
Training loss: 2.8715617310990713
Validation loss: 2.722991526774966

Epoch: 6| Step: 2
Training loss: 3.2398068431686604
Validation loss: 2.717999217096046

Epoch: 6| Step: 3
Training loss: 2.939984508655243
Validation loss: 2.7066130507827793

Epoch: 6| Step: 4
Training loss: 2.8536793833486653
Validation loss: 2.7014451567307396

Epoch: 6| Step: 5
Training loss: 3.001406181113426
Validation loss: 2.6982225409485157

Epoch: 6| Step: 6
Training loss: 2.9518670526252553
Validation loss: 2.691917130075623

Epoch: 6| Step: 7
Training loss: 3.140368854458537
Validation loss: 2.6890528676677765

Epoch: 6| Step: 8
Training loss: 3.0364552274833136
Validation loss: 2.6908248229757885

Epoch: 6| Step: 9
Training loss: 3.5390010601041038
Validation loss: 2.690298258183424

Epoch: 6| Step: 10
Training loss: 2.940049059711545
Validation loss: 2.692525171492887

Epoch: 6| Step: 11
Training loss: 2.3508017694066443
Validation loss: 2.6897782410815125

Epoch: 6| Step: 12
Training loss: 3.2268112123706216
Validation loss: 2.705529235302254

Epoch: 6| Step: 13
Training loss: 3.180429133124207
Validation loss: 2.7079952669511136

Epoch: 34| Step: 0
Training loss: 3.1370100026285854
Validation loss: 2.7184676024982046

Epoch: 6| Step: 1
Training loss: 3.1791357363041595
Validation loss: 2.7060249853277987

Epoch: 6| Step: 2
Training loss: 3.177581314499112
Validation loss: 2.6949686400274393

Epoch: 6| Step: 3
Training loss: 3.002129117116691
Validation loss: 2.6901481632841144

Epoch: 6| Step: 4
Training loss: 2.8864029026296025
Validation loss: 2.686456165176781

Epoch: 6| Step: 5
Training loss: 3.319755094526444
Validation loss: 2.6887194763538886

Epoch: 6| Step: 6
Training loss: 2.4908439817570023
Validation loss: 2.693607407690142

Epoch: 6| Step: 7
Training loss: 2.8946254781811667
Validation loss: 2.6954534171898166

Epoch: 6| Step: 8
Training loss: 2.3757752608801113
Validation loss: 2.694306246010639

Epoch: 6| Step: 9
Training loss: 2.9108884159194566
Validation loss: 2.69146008255136

Epoch: 6| Step: 10
Training loss: 3.9533684093941344
Validation loss: 2.6916434181459215

Epoch: 6| Step: 11
Training loss: 3.10670333409534
Validation loss: 2.686284436382644

Epoch: 6| Step: 12
Training loss: 2.9853649794227453
Validation loss: 2.6840302012318333

Epoch: 6| Step: 13
Training loss: 3.051226985083534
Validation loss: 2.679430732107718

Epoch: 35| Step: 0
Training loss: 3.4994859317911535
Validation loss: 2.676034300398303

Epoch: 6| Step: 1
Training loss: 2.8703974495798814
Validation loss: 2.67382195038294

Epoch: 6| Step: 2
Training loss: 2.8854964666429224
Validation loss: 2.6850272610245316

Epoch: 6| Step: 3
Training loss: 3.400879852667223
Validation loss: 2.6947682267926907

Epoch: 6| Step: 4
Training loss: 3.2930922660424287
Validation loss: 2.703354279923644

Epoch: 6| Step: 5
Training loss: 2.654732214431538
Validation loss: 2.7170830258905543

Epoch: 6| Step: 6
Training loss: 3.25758254888578
Validation loss: 2.7053758794299885

Epoch: 6| Step: 7
Training loss: 3.435909388740733
Validation loss: 2.728439911247774

Epoch: 6| Step: 8
Training loss: 2.8467381466445985
Validation loss: 2.73858256507343

Epoch: 6| Step: 9
Training loss: 2.8797624368174684
Validation loss: 2.7405123817201607

Epoch: 6| Step: 10
Training loss: 3.101813702535866
Validation loss: 2.736105227727377

Epoch: 6| Step: 11
Training loss: 2.9814417939301214
Validation loss: 2.69158332871196

Epoch: 6| Step: 12
Training loss: 2.665283191943328
Validation loss: 2.674242323912501

Epoch: 6| Step: 13
Training loss: 1.9398951953421977
Validation loss: 2.6684082459114578

Epoch: 36| Step: 0
Training loss: 2.9499001631572574
Validation loss: 2.6726056313549016

Epoch: 6| Step: 1
Training loss: 3.1742287908156324
Validation loss: 2.6766318991636133

Epoch: 6| Step: 2
Training loss: 3.402560834824291
Validation loss: 2.6791328280168987

Epoch: 6| Step: 3
Training loss: 3.4683680968503072
Validation loss: 2.682711552363913

Epoch: 6| Step: 4
Training loss: 3.084698469366434
Validation loss: 2.6825107337259753

Epoch: 6| Step: 5
Training loss: 2.935592681466469
Validation loss: 2.683638795900096

Epoch: 6| Step: 6
Training loss: 3.202504167635943
Validation loss: 2.6800088762979044

Epoch: 6| Step: 7
Training loss: 2.592313805481286
Validation loss: 2.6772628247748935

Epoch: 6| Step: 8
Training loss: 3.3505009746313847
Validation loss: 2.6766291081736724

Epoch: 6| Step: 9
Training loss: 3.491342463650097
Validation loss: 2.6731932441154154

Epoch: 6| Step: 10
Training loss: 2.487129268144292
Validation loss: 2.67299556687821

Epoch: 6| Step: 11
Training loss: 2.66254943658485
Validation loss: 2.670347462047474

Epoch: 6| Step: 12
Training loss: 2.615317607577917
Validation loss: 2.6746686113924625

Epoch: 6| Step: 13
Training loss: 2.718994743468355
Validation loss: 2.6726693109271245

Epoch: 37| Step: 0
Training loss: 2.9501970904186137
Validation loss: 2.6703305691285006

Epoch: 6| Step: 1
Training loss: 2.992103356356058
Validation loss: 2.664829914305074

Epoch: 6| Step: 2
Training loss: 3.7825821980926135
Validation loss: 2.665308622551081

Epoch: 6| Step: 3
Training loss: 2.813812458255158
Validation loss: 2.6711634377039055

Epoch: 6| Step: 4
Training loss: 3.031149321781674
Validation loss: 2.675417583948292

Epoch: 6| Step: 5
Training loss: 3.437314115180051
Validation loss: 2.67364240660985

Epoch: 6| Step: 6
Training loss: 3.1623343563802626
Validation loss: 2.6804739304744607

Epoch: 6| Step: 7
Training loss: 2.940994618908662
Validation loss: 2.6683607494947283

Epoch: 6| Step: 8
Training loss: 2.7018260149957136
Validation loss: 2.666236631907609

Epoch: 6| Step: 9
Training loss: 2.5522563226938937
Validation loss: 2.664283101209815

Epoch: 6| Step: 10
Training loss: 3.1712736113995863
Validation loss: 2.6619007694935624

Epoch: 6| Step: 11
Training loss: 2.5969094945496427
Validation loss: 2.6619244454246904

Epoch: 6| Step: 12
Training loss: 2.7954614572940026
Validation loss: 2.6586707534017258

Epoch: 6| Step: 13
Training loss: 3.038080914995045
Validation loss: 2.659573107742419

Epoch: 38| Step: 0
Training loss: 3.391362123305561
Validation loss: 2.660864068428452

Epoch: 6| Step: 1
Training loss: 2.408680369662912
Validation loss: 2.659187866674128

Epoch: 6| Step: 2
Training loss: 2.2544966375681628
Validation loss: 2.656585032345887

Epoch: 6| Step: 3
Training loss: 2.897052773429178
Validation loss: 2.655544483124946

Epoch: 6| Step: 4
Training loss: 2.740236376151933
Validation loss: 2.6589032351292796

Epoch: 6| Step: 5
Training loss: 3.1069287976472686
Validation loss: 2.656115845701233

Epoch: 6| Step: 6
Training loss: 2.9627913255340257
Validation loss: 2.657809815451833

Epoch: 6| Step: 7
Training loss: 3.419768220509518
Validation loss: 2.6584213232501384

Epoch: 6| Step: 8
Training loss: 3.5198761731822517
Validation loss: 2.666805134595581

Epoch: 6| Step: 9
Training loss: 3.635559224974323
Validation loss: 2.682913267559139

Epoch: 6| Step: 10
Training loss: 3.4350087241515936
Validation loss: 2.6826274645304737

Epoch: 6| Step: 11
Training loss: 2.367216708062895
Validation loss: 2.720397718652939

Epoch: 6| Step: 12
Training loss: 2.4014887086728725
Validation loss: 2.7951057563750394

Epoch: 6| Step: 13
Training loss: 3.025729628914732
Validation loss: 2.89227864181605

Epoch: 39| Step: 0
Training loss: 2.522111384812801
Validation loss: 3.002575923778774

Epoch: 6| Step: 1
Training loss: 3.7719010725913815
Validation loss: 2.8836397165317567

Epoch: 6| Step: 2
Training loss: 3.2666215608038818
Validation loss: 2.6822153365905717

Epoch: 6| Step: 3
Training loss: 2.8734538025468748
Validation loss: 2.652060062034493

Epoch: 6| Step: 4
Training loss: 2.840453868113313
Validation loss: 2.6758038033556724

Epoch: 6| Step: 5
Training loss: 2.597074469331862
Validation loss: 2.726153426389769

Epoch: 6| Step: 6
Training loss: 3.3954437750108974
Validation loss: 2.8427646953434125

Epoch: 6| Step: 7
Training loss: 2.8064221301738432
Validation loss: 2.8598186234632608

Epoch: 6| Step: 8
Training loss: 4.127520629165826
Validation loss: 2.8624604754726857

Epoch: 6| Step: 9
Training loss: 3.10977890995074
Validation loss: 2.813205173009399

Epoch: 6| Step: 10
Training loss: 3.6034738098861157
Validation loss: 2.744368469847944

Epoch: 6| Step: 11
Training loss: 2.737845088162609
Validation loss: 2.6851161143411546

Epoch: 6| Step: 12
Training loss: 2.753104711451342
Validation loss: 2.657195643031464

Epoch: 6| Step: 13
Training loss: 2.9227853264713493
Validation loss: 2.6545516083849487

Epoch: 40| Step: 0
Training loss: 3.4274683835542046
Validation loss: 2.6740831587438247

Epoch: 6| Step: 1
Training loss: 3.0820625023169916
Validation loss: 2.7115654287444038

Epoch: 6| Step: 2
Training loss: 2.5963246998442786
Validation loss: 2.768223614109709

Epoch: 6| Step: 3
Training loss: 3.1861710115311888
Validation loss: 2.7914718415465116

Epoch: 6| Step: 4
Training loss: 2.9992442768637244
Validation loss: 2.759066073977057

Epoch: 6| Step: 5
Training loss: 2.9033152211000597
Validation loss: 2.6985104002121467

Epoch: 6| Step: 6
Training loss: 2.8747070826707244
Validation loss: 2.678505242951495

Epoch: 6| Step: 7
Training loss: 3.8961403601661635
Validation loss: 2.6601589773207275

Epoch: 6| Step: 8
Training loss: 2.872217407402891
Validation loss: 2.6500824622714716

Epoch: 6| Step: 9
Training loss: 2.9084661094575748
Validation loss: 2.6489925312939455

Epoch: 6| Step: 10
Training loss: 1.9047923534094033
Validation loss: 2.6506874767674704

Epoch: 6| Step: 11
Training loss: 2.839696825259205
Validation loss: 2.6536242010492432

Epoch: 6| Step: 12
Training loss: 3.076841754939068
Validation loss: 2.6563179929115304

Epoch: 6| Step: 13
Training loss: 3.793232241748948
Validation loss: 2.6520072567343567

Epoch: 41| Step: 0
Training loss: 3.248962236658711
Validation loss: 2.653886849651559

Epoch: 6| Step: 1
Training loss: 3.119951977482486
Validation loss: 2.6527021578735357

Epoch: 6| Step: 2
Training loss: 3.3572758703346453
Validation loss: 2.651560878180917

Epoch: 6| Step: 3
Training loss: 3.355920195047818
Validation loss: 2.6480551629510782

Epoch: 6| Step: 4
Training loss: 2.5772890527430485
Validation loss: 2.6451006969817024

Epoch: 6| Step: 5
Training loss: 2.6474072638417554
Validation loss: 2.6424945936664694

Epoch: 6| Step: 6
Training loss: 3.1515470913844625
Validation loss: 2.6418443827774842

Epoch: 6| Step: 7
Training loss: 3.243088489032156
Validation loss: 2.6431331185267815

Epoch: 6| Step: 8
Training loss: 3.3175552947690408
Validation loss: 2.6407254593077942

Epoch: 6| Step: 9
Training loss: 2.5159264614899306
Validation loss: 2.6386292866003633

Epoch: 6| Step: 10
Training loss: 2.6115382929519146
Validation loss: 2.6385670614823837

Epoch: 6| Step: 11
Training loss: 2.829679114815365
Validation loss: 2.637395276928609

Epoch: 6| Step: 12
Training loss: 2.7357924792744317
Validation loss: 2.639267952043834

Epoch: 6| Step: 13
Training loss: 3.1572729284891894
Validation loss: 2.646365985992952

Epoch: 42| Step: 0
Training loss: 2.6631693755264045
Validation loss: 2.6447918713604897

Epoch: 6| Step: 1
Training loss: 3.015605076541213
Validation loss: 2.653627279011196

Epoch: 6| Step: 2
Training loss: 3.1895221952678816
Validation loss: 2.653701745087419

Epoch: 6| Step: 3
Training loss: 2.609247718493517
Validation loss: 2.657404421569311

Epoch: 6| Step: 4
Training loss: 3.33682875151593
Validation loss: 2.666047201833928

Epoch: 6| Step: 5
Training loss: 2.499648450929623
Validation loss: 2.6796753203025157

Epoch: 6| Step: 6
Training loss: 2.7727953530095997
Validation loss: 2.6983275216880958

Epoch: 6| Step: 7
Training loss: 3.1467907861163753
Validation loss: 2.6846161441617062

Epoch: 6| Step: 8
Training loss: 2.8515263803363116
Validation loss: 2.655766562202283

Epoch: 6| Step: 9
Training loss: 3.0706274438899164
Validation loss: 2.634506508759738

Epoch: 6| Step: 10
Training loss: 3.322317151046274
Validation loss: 2.6332235365076047

Epoch: 6| Step: 11
Training loss: 2.9549816289805095
Validation loss: 2.6399870556557214

Epoch: 6| Step: 12
Training loss: 3.101708242994206
Validation loss: 2.6433483036110133

Epoch: 6| Step: 13
Training loss: 3.472949124012754
Validation loss: 2.6449022819698915

Epoch: 43| Step: 0
Training loss: 2.6779103071763344
Validation loss: 2.647027572103126

Epoch: 6| Step: 1
Training loss: 3.225250198949718
Validation loss: 2.6478862952956215

Epoch: 6| Step: 2
Training loss: 3.193089987681774
Validation loss: 2.6465585751570315

Epoch: 6| Step: 3
Training loss: 3.394830863908165
Validation loss: 2.642673888540147

Epoch: 6| Step: 4
Training loss: 2.643916499529048
Validation loss: 2.6423888577724357

Epoch: 6| Step: 5
Training loss: 3.1869647006667488
Validation loss: 2.638792371540992

Epoch: 6| Step: 6
Training loss: 3.3016762146546133
Validation loss: 2.6388125314839224

Epoch: 6| Step: 7
Training loss: 3.139654664408677
Validation loss: 2.6353738854193955

Epoch: 6| Step: 8
Training loss: 3.855160779160671
Validation loss: 2.6334910554902593

Epoch: 6| Step: 9
Training loss: 2.334462800730216
Validation loss: 2.633639497652535

Epoch: 6| Step: 10
Training loss: 2.6554912605465053
Validation loss: 2.6330629637793916

Epoch: 6| Step: 11
Training loss: 2.823805656137914
Validation loss: 2.632215103286523

Epoch: 6| Step: 12
Training loss: 2.292446408409782
Validation loss: 2.63087416808223

Epoch: 6| Step: 13
Training loss: 2.891864737580481
Validation loss: 2.629001905013288

Epoch: 44| Step: 0
Training loss: 3.3733867038932708
Validation loss: 2.6270548273098915

Epoch: 6| Step: 1
Training loss: 2.918174317662775
Validation loss: 2.6243758331171607

Epoch: 6| Step: 2
Training loss: 2.7007336926542194
Validation loss: 2.6223443647409783

Epoch: 6| Step: 3
Training loss: 2.962117707257007
Validation loss: 2.628452920852086

Epoch: 6| Step: 4
Training loss: 3.8462007974546544
Validation loss: 2.63846168607848

Epoch: 6| Step: 5
Training loss: 2.4672721592183153
Validation loss: 2.6519677308547855

Epoch: 6| Step: 6
Training loss: 2.6083070316958925
Validation loss: 2.682569985665588

Epoch: 6| Step: 7
Training loss: 3.341094772095375
Validation loss: 2.7020799811313623

Epoch: 6| Step: 8
Training loss: 3.5300447550846137
Validation loss: 2.6734574808139877

Epoch: 6| Step: 9
Training loss: 2.8803518488322375
Validation loss: 2.6507998545847373

Epoch: 6| Step: 10
Training loss: 2.7234455644725606
Validation loss: 2.628056539268363

Epoch: 6| Step: 11
Training loss: 2.44306174340066
Validation loss: 2.6175874461123394

Epoch: 6| Step: 12
Training loss: 2.666462999752037
Validation loss: 2.6242025133755

Epoch: 6| Step: 13
Training loss: 3.055858182852014
Validation loss: 2.625874904342385

Epoch: 45| Step: 0
Training loss: 3.63654868673763
Validation loss: 2.6335578789102105

Epoch: 6| Step: 1
Training loss: 2.8523949649071456
Validation loss: 2.643505130807011

Epoch: 6| Step: 2
Training loss: 2.520218062124379
Validation loss: 2.6415309372577465

Epoch: 6| Step: 3
Training loss: 2.7874416054395383
Validation loss: 2.6397684077683023

Epoch: 6| Step: 4
Training loss: 3.246799507034488
Validation loss: 2.6338396721771646

Epoch: 6| Step: 5
Training loss: 3.3219840117086434
Validation loss: 2.6321950408552133

Epoch: 6| Step: 6
Training loss: 3.45830289222158
Validation loss: 2.6287734503961007

Epoch: 6| Step: 7
Training loss: 2.6937027256616606
Validation loss: 2.628150282103622

Epoch: 6| Step: 8
Training loss: 2.6498839442913136
Validation loss: 2.625685368459441

Epoch: 6| Step: 9
Training loss: 2.641801831450742
Validation loss: 2.6245437612330362

Epoch: 6| Step: 10
Training loss: 3.50360970321376
Validation loss: 2.626709937306667

Epoch: 6| Step: 11
Training loss: 2.68301283144504
Validation loss: 2.6242215554660824

Epoch: 6| Step: 12
Training loss: 2.961369062794136
Validation loss: 2.6217180626655576

Epoch: 6| Step: 13
Training loss: 2.723738029912352
Validation loss: 2.6201550413078083

Epoch: 46| Step: 0
Training loss: 3.116397155151714
Validation loss: 2.618869028380262

Epoch: 6| Step: 1
Training loss: 3.3195607489974
Validation loss: 2.6179922586617677

Epoch: 6| Step: 2
Training loss: 2.1811417954503445
Validation loss: 2.617440180303911

Epoch: 6| Step: 3
Training loss: 3.3463377164245873
Validation loss: 2.614780430526035

Epoch: 6| Step: 4
Training loss: 2.908615134358084
Validation loss: 2.6139184142005774

Epoch: 6| Step: 5
Training loss: 2.821552542917003
Validation loss: 2.612500689420527

Epoch: 6| Step: 6
Training loss: 3.4691726839439614
Validation loss: 2.610149103420145

Epoch: 6| Step: 7
Training loss: 2.7445412991516496
Validation loss: 2.609728383419538

Epoch: 6| Step: 8
Training loss: 2.6914487705276873
Validation loss: 2.607318694023547

Epoch: 6| Step: 9
Training loss: 3.474331600757401
Validation loss: 2.606525732479744

Epoch: 6| Step: 10
Training loss: 2.481867358374377
Validation loss: 2.6098036845898323

Epoch: 6| Step: 11
Training loss: 2.982673519487599
Validation loss: 2.605678405704778

Epoch: 6| Step: 12
Training loss: 2.7509448422359206
Validation loss: 2.6036670984989967

Epoch: 6| Step: 13
Training loss: 3.1234267279938686
Validation loss: 2.6063470670059945

Epoch: 47| Step: 0
Training loss: 2.9231060515045098
Validation loss: 2.6071319749522885

Epoch: 6| Step: 1
Training loss: 2.488725703382851
Validation loss: 2.6057591887766747

Epoch: 6| Step: 2
Training loss: 2.446660849211773
Validation loss: 2.6081927355338674

Epoch: 6| Step: 3
Training loss: 2.968733938073326
Validation loss: 2.6160075363968076

Epoch: 6| Step: 4
Training loss: 3.1058240387366967
Validation loss: 2.61729673997289

Epoch: 6| Step: 5
Training loss: 2.201612562120716
Validation loss: 2.6217240060163483

Epoch: 6| Step: 6
Training loss: 2.3642481315632033
Validation loss: 2.638779012130075

Epoch: 6| Step: 7
Training loss: 2.7762512547792007
Validation loss: 2.6510284355483402

Epoch: 6| Step: 8
Training loss: 3.4086363946038123
Validation loss: 2.645720898951745

Epoch: 6| Step: 9
Training loss: 3.3095051884821594
Validation loss: 2.6074653993324284

Epoch: 6| Step: 10
Training loss: 3.1080908495808237
Validation loss: 2.600043494402662

Epoch: 6| Step: 11
Training loss: 3.5302387241118343
Validation loss: 2.600480430320729

Epoch: 6| Step: 12
Training loss: 3.388573373028026
Validation loss: 2.604233974120208

Epoch: 6| Step: 13
Training loss: 3.2729410236568484
Validation loss: 2.6065736337304584

Epoch: 48| Step: 0
Training loss: 2.578343977442813
Validation loss: 2.611542363889514

Epoch: 6| Step: 1
Training loss: 3.1065437039884114
Validation loss: 2.6143887979436147

Epoch: 6| Step: 2
Training loss: 2.898197297787764
Validation loss: 2.613479509372538

Epoch: 6| Step: 3
Training loss: 3.113548353504306
Validation loss: 2.615774660718067

Epoch: 6| Step: 4
Training loss: 3.53586297321206
Validation loss: 2.6188816592430473

Epoch: 6| Step: 5
Training loss: 2.6612926414158955
Validation loss: 2.621357233594466

Epoch: 6| Step: 6
Training loss: 3.2110560040290084
Validation loss: 2.6213149717102806

Epoch: 6| Step: 7
Training loss: 2.9763622466881854
Validation loss: 2.6218410358475297

Epoch: 6| Step: 8
Training loss: 2.6409993103904648
Validation loss: 2.618451698921614

Epoch: 6| Step: 9
Training loss: 2.882653704976111
Validation loss: 2.61806718603514

Epoch: 6| Step: 10
Training loss: 3.0713168532625814
Validation loss: 2.61304725425736

Epoch: 6| Step: 11
Training loss: 2.393142870210889
Validation loss: 2.61069658773507

Epoch: 6| Step: 12
Training loss: 3.298524272288735
Validation loss: 2.6104812089921925

Epoch: 6| Step: 13
Training loss: 3.42460363280323
Validation loss: 2.613637793340863

Epoch: 49| Step: 0
Training loss: 2.864661327803036
Validation loss: 2.6106710336561028

Epoch: 6| Step: 1
Training loss: 3.2874860814021094
Validation loss: 2.6040020785259714

Epoch: 6| Step: 2
Training loss: 3.469845985270675
Validation loss: 2.6000364021012135

Epoch: 6| Step: 3
Training loss: 2.9968703157549417
Validation loss: 2.6015451899944857

Epoch: 6| Step: 4
Training loss: 3.2531543975879624
Validation loss: 2.5964607516979368

Epoch: 6| Step: 5
Training loss: 2.9536445604668136
Validation loss: 2.5973088213085966

Epoch: 6| Step: 6
Training loss: 2.710072668023276
Validation loss: 2.5951608131303883

Epoch: 6| Step: 7
Training loss: 3.1772586628214
Validation loss: 2.5937648594363036

Epoch: 6| Step: 8
Training loss: 2.427247616251911
Validation loss: 2.5937400498429763

Epoch: 6| Step: 9
Training loss: 2.6766682957430104
Validation loss: 2.59741544382233

Epoch: 6| Step: 10
Training loss: 2.691309336120244
Validation loss: 2.618310511641361

Epoch: 6| Step: 11
Training loss: 2.518187361129905
Validation loss: 2.6240313276469966

Epoch: 6| Step: 12
Training loss: 2.893903203414451
Validation loss: 2.6214225028229268

Epoch: 6| Step: 13
Training loss: 3.5790234899397526
Validation loss: 2.62592864713315

Epoch: 50| Step: 0
Training loss: 2.9614956211493735
Validation loss: 2.5949125426156625

Epoch: 6| Step: 1
Training loss: 2.8848952133215695
Validation loss: 2.589604040873595

Epoch: 6| Step: 2
Training loss: 2.7366602476217405
Validation loss: 2.597994140440299

Epoch: 6| Step: 3
Training loss: 3.1766701518223455
Validation loss: 2.601275248979287

Epoch: 6| Step: 4
Training loss: 3.0240358061310544
Validation loss: 2.611915651812703

Epoch: 6| Step: 5
Training loss: 3.1184700452262857
Validation loss: 2.6187260818847817

Epoch: 6| Step: 6
Training loss: 3.3825460773946956
Validation loss: 2.6332436339398777

Epoch: 6| Step: 7
Training loss: 3.202363309779555
Validation loss: 2.6401929128664237

Epoch: 6| Step: 8
Training loss: 2.743221685687723
Validation loss: 2.632106225335702

Epoch: 6| Step: 9
Training loss: 2.888073264019583
Validation loss: 2.625684257350054

Epoch: 6| Step: 10
Training loss: 2.9204163199676847
Validation loss: 2.6129521032469047

Epoch: 6| Step: 11
Training loss: 2.769127303823404
Validation loss: 2.6103275184823294

Epoch: 6| Step: 12
Training loss: 2.8134091073569243
Validation loss: 2.6087567173679873

Epoch: 6| Step: 13
Training loss: 3.488862436711273
Validation loss: 2.6063530483585984

Epoch: 51| Step: 0
Training loss: 2.9805012272456444
Validation loss: 2.604685806868444

Epoch: 6| Step: 1
Training loss: 2.6021166145470698
Validation loss: 2.6018508869050416

Epoch: 6| Step: 2
Training loss: 3.147296102280834
Validation loss: 2.598536900349913

Epoch: 6| Step: 3
Training loss: 3.1039481032742517
Validation loss: 2.5970570840044362

Epoch: 6| Step: 4
Training loss: 3.5563590678298973
Validation loss: 2.596742193192641

Epoch: 6| Step: 5
Training loss: 2.945543032257857
Validation loss: 2.591425299057632

Epoch: 6| Step: 6
Training loss: 3.2974850058630443
Validation loss: 2.6016153418092243

Epoch: 6| Step: 7
Training loss: 2.605424368736278
Validation loss: 2.6129479197134953

Epoch: 6| Step: 8
Training loss: 2.8912225878810056
Validation loss: 2.6378815315158493

Epoch: 6| Step: 9
Training loss: 3.313014872141353
Validation loss: 2.6435704733726606

Epoch: 6| Step: 10
Training loss: 2.9468504434413942
Validation loss: 2.6180876965119904

Epoch: 6| Step: 11
Training loss: 3.035330316960024
Validation loss: 2.5928149079015714

Epoch: 6| Step: 12
Training loss: 1.9096916273344318
Validation loss: 2.5848857338220315

Epoch: 6| Step: 13
Training loss: 3.0253690321110485
Validation loss: 2.582494869244751

Epoch: 52| Step: 0
Training loss: 2.3831522714978806
Validation loss: 2.5807533516752326

Epoch: 6| Step: 1
Training loss: 2.87697997537114
Validation loss: 2.5811014116906126

Epoch: 6| Step: 2
Training loss: 2.9050367550650384
Validation loss: 2.5858140229487714

Epoch: 6| Step: 3
Training loss: 3.138636932463397
Validation loss: 2.586566711316975

Epoch: 6| Step: 4
Training loss: 2.3028354250517005
Validation loss: 2.587835864406549

Epoch: 6| Step: 5
Training loss: 3.1396043931591815
Validation loss: 2.584228853288298

Epoch: 6| Step: 6
Training loss: 2.8524798864111856
Validation loss: 2.5835497610338627

Epoch: 6| Step: 7
Training loss: 2.8052692288204697
Validation loss: 2.5832628610335626

Epoch: 6| Step: 8
Training loss: 3.400119549388562
Validation loss: 2.5814130978706196

Epoch: 6| Step: 9
Training loss: 2.6270281812871934
Validation loss: 2.5841017786564033

Epoch: 6| Step: 10
Training loss: 3.4684974647216857
Validation loss: 2.580257047545006

Epoch: 6| Step: 11
Training loss: 3.2243807530818573
Validation loss: 2.580475872874508

Epoch: 6| Step: 12
Training loss: 3.048120488626326
Validation loss: 2.579564834717012

Epoch: 6| Step: 13
Training loss: 2.8694199853892544
Validation loss: 2.5786814884862013

Epoch: 53| Step: 0
Training loss: 3.4468652941237727
Validation loss: 2.5785841099555933

Epoch: 6| Step: 1
Training loss: 2.2827078981344866
Validation loss: 2.5769710858299706

Epoch: 6| Step: 2
Training loss: 3.3813710302102775
Validation loss: 2.5755416272211313

Epoch: 6| Step: 3
Training loss: 2.811870334924074
Validation loss: 2.572372631758359

Epoch: 6| Step: 4
Training loss: 2.852604589392019
Validation loss: 2.573378800207213

Epoch: 6| Step: 5
Training loss: 2.987979649302109
Validation loss: 2.5757155280365986

Epoch: 6| Step: 6
Training loss: 2.760896854500839
Validation loss: 2.5720966907880913

Epoch: 6| Step: 7
Training loss: 2.9403296294936263
Validation loss: 2.577505525138849

Epoch: 6| Step: 8
Training loss: 2.728242110877252
Validation loss: 2.584056840923345

Epoch: 6| Step: 9
Training loss: 2.503292204836057
Validation loss: 2.589288716941122

Epoch: 6| Step: 10
Training loss: 3.378413416912763
Validation loss: 2.597436321680952

Epoch: 6| Step: 11
Training loss: 2.4899932864301633
Validation loss: 2.597505726795176

Epoch: 6| Step: 12
Training loss: 3.2492306605655257
Validation loss: 2.613730418258922

Epoch: 6| Step: 13
Training loss: 3.2945489135297623
Validation loss: 2.586409270441726

Epoch: 54| Step: 0
Training loss: 3.093097039876975
Validation loss: 2.577576386919062

Epoch: 6| Step: 1
Training loss: 3.1984459918365533
Validation loss: 2.577096848801375

Epoch: 6| Step: 2
Training loss: 2.667555501944998
Validation loss: 2.5674018646185996

Epoch: 6| Step: 3
Training loss: 2.355920549021291
Validation loss: 2.569992668409852

Epoch: 6| Step: 4
Training loss: 2.996996965999296
Validation loss: 2.574414229956277

Epoch: 6| Step: 5
Training loss: 3.545920894870017
Validation loss: 2.5788793291233834

Epoch: 6| Step: 6
Training loss: 2.8664243758370076
Validation loss: 2.578189864699043

Epoch: 6| Step: 7
Training loss: 2.9908933704465874
Validation loss: 2.584089558162505

Epoch: 6| Step: 8
Training loss: 2.8612128016435725
Validation loss: 2.5860850276563982

Epoch: 6| Step: 9
Training loss: 3.1820923055499253
Validation loss: 2.5782439462040667

Epoch: 6| Step: 10
Training loss: 2.9749767655178574
Validation loss: 2.5840157319753283

Epoch: 6| Step: 11
Training loss: 2.863959634102164
Validation loss: 2.5862496669706445

Epoch: 6| Step: 12
Training loss: 2.784876227513456
Validation loss: 2.5831892930706197

Epoch: 6| Step: 13
Training loss: 2.7013235097871577
Validation loss: 2.580056576076306

Epoch: 55| Step: 0
Training loss: 3.1968477100687904
Validation loss: 2.5776283877193586

Epoch: 6| Step: 1
Training loss: 2.377549660020639
Validation loss: 2.5742931768190953

Epoch: 6| Step: 2
Training loss: 3.2515439987572816
Validation loss: 2.573453380822794

Epoch: 6| Step: 3
Training loss: 2.7097772319490674
Validation loss: 2.5776006862148177

Epoch: 6| Step: 4
Training loss: 3.021532666166289
Validation loss: 2.5819514611343153

Epoch: 6| Step: 5
Training loss: 3.0036747995806308
Validation loss: 2.5787250197193234

Epoch: 6| Step: 6
Training loss: 3.3416376304474813
Validation loss: 2.5704850830442143

Epoch: 6| Step: 7
Training loss: 3.4352395688489823
Validation loss: 2.569249310484216

Epoch: 6| Step: 8
Training loss: 2.985258920197321
Validation loss: 2.5726395656599146

Epoch: 6| Step: 9
Training loss: 2.455432559750106
Validation loss: 2.580471335666004

Epoch: 6| Step: 10
Training loss: 3.061140420369143
Validation loss: 2.6118517571935973

Epoch: 6| Step: 11
Training loss: 2.320565258092396
Validation loss: 2.6317602955330046

Epoch: 6| Step: 12
Training loss: 2.65896629124479
Validation loss: 2.6886424664736652

Epoch: 6| Step: 13
Training loss: 3.5376528865275407
Validation loss: 2.694982291657009

Epoch: 56| Step: 0
Training loss: 2.8379186850707563
Validation loss: 2.5690877977503637

Epoch: 6| Step: 1
Training loss: 3.0091665415595243
Validation loss: 2.5638041655346924

Epoch: 6| Step: 2
Training loss: 2.849590800841196
Validation loss: 2.5724404159829155

Epoch: 6| Step: 3
Training loss: 3.4092971750917194
Validation loss: 2.5796190792004423

Epoch: 6| Step: 4
Training loss: 2.870227999785375
Validation loss: 2.5955525280363116

Epoch: 6| Step: 5
Training loss: 3.214204115058924
Validation loss: 2.6085947702695003

Epoch: 6| Step: 6
Training loss: 2.3162278722621177
Validation loss: 2.6056352490113657

Epoch: 6| Step: 7
Training loss: 2.894529273152159
Validation loss: 2.606531545236193

Epoch: 6| Step: 8
Training loss: 3.0151036261468374
Validation loss: 2.603963112699977

Epoch: 6| Step: 9
Training loss: 3.064943545445426
Validation loss: 2.596167006440415

Epoch: 6| Step: 10
Training loss: 3.350089934551023
Validation loss: 2.5904305461671173

Epoch: 6| Step: 11
Training loss: 2.7720792641364485
Validation loss: 2.5871334058579007

Epoch: 6| Step: 12
Training loss: 3.059615197264923
Validation loss: 2.5836485360483863

Epoch: 6| Step: 13
Training loss: 2.5768453485567226
Validation loss: 2.5785815006638404

Epoch: 57| Step: 0
Training loss: 2.4819379645501796
Validation loss: 2.5736430787053566

Epoch: 6| Step: 1
Training loss: 2.933358052178471
Validation loss: 2.570831902640302

Epoch: 6| Step: 2
Training loss: 2.98870790714376
Validation loss: 2.56501060370588

Epoch: 6| Step: 3
Training loss: 2.692630572398312
Validation loss: 2.562948768891033

Epoch: 6| Step: 4
Training loss: 3.0968655398442793
Validation loss: 2.5748702340621987

Epoch: 6| Step: 5
Training loss: 3.553934608626413
Validation loss: 2.584776941087854

Epoch: 6| Step: 6
Training loss: 2.890284667729159
Validation loss: 2.5825341242442557

Epoch: 6| Step: 7
Training loss: 2.6007828560913295
Validation loss: 2.579200080460187

Epoch: 6| Step: 8
Training loss: 3.038905907055774
Validation loss: 2.5784658624935863

Epoch: 6| Step: 9
Training loss: 2.8748676435438743
Validation loss: 2.5802884359070375

Epoch: 6| Step: 10
Training loss: 3.261176040179227
Validation loss: 2.5848621213914216

Epoch: 6| Step: 11
Training loss: 2.5758398852907263
Validation loss: 2.5849491247128364

Epoch: 6| Step: 12
Training loss: 2.8825119399446275
Validation loss: 2.5813519182687683

Epoch: 6| Step: 13
Training loss: 3.3199801648937735
Validation loss: 2.5789653245106

Epoch: 58| Step: 0
Training loss: 2.5034261115194516
Validation loss: 2.5899686651915417

Epoch: 6| Step: 1
Training loss: 2.700579994213329
Validation loss: 2.5969595810778277

Epoch: 6| Step: 2
Training loss: 3.0144078461757524
Validation loss: 2.608734918894491

Epoch: 6| Step: 3
Training loss: 3.131155589007164
Validation loss: 2.6240701791429215

Epoch: 6| Step: 4
Training loss: 2.5623274954440722
Validation loss: 2.6276134776710007

Epoch: 6| Step: 5
Training loss: 3.313028833173427
Validation loss: 2.5936288549580526

Epoch: 6| Step: 6
Training loss: 2.5203187170265364
Validation loss: 2.5791842564516405

Epoch: 6| Step: 7
Training loss: 3.4528363827878827
Validation loss: 2.573513478086483

Epoch: 6| Step: 8
Training loss: 2.8283231044113006
Validation loss: 2.5629310555494222

Epoch: 6| Step: 9
Training loss: 2.3465002581085965
Validation loss: 2.5525165862303774

Epoch: 6| Step: 10
Training loss: 3.3482910409031477
Validation loss: 2.551530072779853

Epoch: 6| Step: 11
Training loss: 3.462516889289952
Validation loss: 2.5510437472642176

Epoch: 6| Step: 12
Training loss: 2.8992579859157903
Validation loss: 2.552406956183886

Epoch: 6| Step: 13
Training loss: 2.20301201713287
Validation loss: 2.5547908528612426

Epoch: 59| Step: 0
Training loss: 2.595111903017781
Validation loss: 2.5535309179132213

Epoch: 6| Step: 1
Training loss: 3.269713256582821
Validation loss: 2.5535639439386335

Epoch: 6| Step: 2
Training loss: 2.9002237496711762
Validation loss: 2.5513861709531334

Epoch: 6| Step: 3
Training loss: 2.511922065728639
Validation loss: 2.55186374130595

Epoch: 6| Step: 4
Training loss: 3.358657547388522
Validation loss: 2.5516682156407087

Epoch: 6| Step: 5
Training loss: 2.8925009748742414
Validation loss: 2.5501898372183773

Epoch: 6| Step: 6
Training loss: 2.3883007444335433
Validation loss: 2.551134656349751

Epoch: 6| Step: 7
Training loss: 2.357057860935141
Validation loss: 2.550632851327666

Epoch: 6| Step: 8
Training loss: 3.0561644746948287
Validation loss: 2.551722588992272

Epoch: 6| Step: 9
Training loss: 3.046058271445573
Validation loss: 2.5670396604371457

Epoch: 6| Step: 10
Training loss: 2.741579515488488
Validation loss: 2.569564422722635

Epoch: 6| Step: 11
Training loss: 3.433426819303237
Validation loss: 2.5788920773143826

Epoch: 6| Step: 12
Training loss: 2.9004225192901245
Validation loss: 2.5900452515226675

Epoch: 6| Step: 13
Training loss: 3.5142281024698674
Validation loss: 2.5992820407124655

Epoch: 60| Step: 0
Training loss: 3.2354871523995095
Validation loss: 2.6358031473008268

Epoch: 6| Step: 1
Training loss: 2.9864427682989665
Validation loss: 2.6727359672443805

Epoch: 6| Step: 2
Training loss: 3.1554372515314024
Validation loss: 2.6561425647567565

Epoch: 6| Step: 3
Training loss: 3.1055624126009076
Validation loss: 2.627943797618235

Epoch: 6| Step: 4
Training loss: 3.202158712386306
Validation loss: 2.5949509932966266

Epoch: 6| Step: 5
Training loss: 2.928431696997472
Validation loss: 2.5840200198925793

Epoch: 6| Step: 6
Training loss: 2.9463458689522835
Validation loss: 2.5704931913836213

Epoch: 6| Step: 7
Training loss: 3.133415335400581
Validation loss: 2.567767939397503

Epoch: 6| Step: 8
Training loss: 2.478533228457729
Validation loss: 2.5601578660359343

Epoch: 6| Step: 9
Training loss: 3.0850357476486425
Validation loss: 2.5717981112521575

Epoch: 6| Step: 10
Training loss: 2.5553927600073436
Validation loss: 2.5865324603867297

Epoch: 6| Step: 11
Training loss: 3.194786270944678
Validation loss: 2.596190966335594

Epoch: 6| Step: 12
Training loss: 2.6432811312273223
Validation loss: 2.6072454469254813

Epoch: 6| Step: 13
Training loss: 3.003630507652608
Validation loss: 2.6061491277376794

Epoch: 61| Step: 0
Training loss: 2.947677836274657
Validation loss: 2.5842878706493067

Epoch: 6| Step: 1
Training loss: 3.359800373913794
Validation loss: 2.5773337523795625

Epoch: 6| Step: 2
Training loss: 3.2650630777759053
Validation loss: 2.569833116772244

Epoch: 6| Step: 3
Training loss: 2.3582837947679325
Validation loss: 2.565182368163597

Epoch: 6| Step: 4
Training loss: 2.8737866702401216
Validation loss: 2.5645407997142216

Epoch: 6| Step: 5
Training loss: 2.985598967248834
Validation loss: 2.561165927319944

Epoch: 6| Step: 6
Training loss: 3.022084487865029
Validation loss: 2.5584855356810356

Epoch: 6| Step: 7
Training loss: 2.88179903901954
Validation loss: 2.5604164843306925

Epoch: 6| Step: 8
Training loss: 2.9060828048029848
Validation loss: 2.5568373195279586

Epoch: 6| Step: 9
Training loss: 3.352878267821993
Validation loss: 2.554809882481571

Epoch: 6| Step: 10
Training loss: 3.278859021217299
Validation loss: 2.5497567674278883

Epoch: 6| Step: 11
Training loss: 2.4961503907464944
Validation loss: 2.543938697672658

Epoch: 6| Step: 12
Training loss: 2.493686332941577
Validation loss: 2.539063109846649

Epoch: 6| Step: 13
Training loss: 2.680377482119874
Validation loss: 2.5496200202009742

Epoch: 62| Step: 0
Training loss: 3.0732739946618244
Validation loss: 2.5724657160148863

Epoch: 6| Step: 1
Training loss: 2.9930938065266126
Validation loss: 2.6137228089229088

Epoch: 6| Step: 2
Training loss: 2.9172561776362786
Validation loss: 2.585893004605613

Epoch: 6| Step: 3
Training loss: 3.2137839516055924
Validation loss: 2.576502290614471

Epoch: 6| Step: 4
Training loss: 2.401988826678078
Validation loss: 2.562682452685787

Epoch: 6| Step: 5
Training loss: 2.92316950717049
Validation loss: 2.5554722846245665

Epoch: 6| Step: 6
Training loss: 3.0844318048411803
Validation loss: 2.5447071672923283

Epoch: 6| Step: 7
Training loss: 2.78930192876038
Validation loss: 2.542463954892223

Epoch: 6| Step: 8
Training loss: 3.1139705569737983
Validation loss: 2.5352905441126268

Epoch: 6| Step: 9
Training loss: 2.7466789479428626
Validation loss: 2.5347188770786997

Epoch: 6| Step: 10
Training loss: 2.9061033150356392
Validation loss: 2.53429960982328

Epoch: 6| Step: 11
Training loss: 2.93591590536397
Validation loss: 2.53281756939517

Epoch: 6| Step: 12
Training loss: 2.6992901186747953
Validation loss: 2.5365312821051944

Epoch: 6| Step: 13
Training loss: 2.9984365203839918
Validation loss: 2.5352056413066735

Epoch: 63| Step: 0
Training loss: 3.3158897473973004
Validation loss: 2.5375035914584543

Epoch: 6| Step: 1
Training loss: 2.8514596580877067
Validation loss: 2.540558035779724

Epoch: 6| Step: 2
Training loss: 2.5596304853309486
Validation loss: 2.539627439945388

Epoch: 6| Step: 3
Training loss: 3.09183789898952
Validation loss: 2.5404443890038033

Epoch: 6| Step: 4
Training loss: 2.2177128046533303
Validation loss: 2.544218873553627

Epoch: 6| Step: 5
Training loss: 2.6466088747814207
Validation loss: 2.5441860345599463

Epoch: 6| Step: 6
Training loss: 2.908908571413557
Validation loss: 2.541541463779651

Epoch: 6| Step: 7
Training loss: 3.2499140214551194
Validation loss: 2.5348837437234972

Epoch: 6| Step: 8
Training loss: 2.862686657546018
Validation loss: 2.534256604261249

Epoch: 6| Step: 9
Training loss: 2.9068839089407397
Validation loss: 2.5316168690015544

Epoch: 6| Step: 10
Training loss: 3.094012008515463
Validation loss: 2.5368572634873625

Epoch: 6| Step: 11
Training loss: 3.0294316137024135
Validation loss: 2.5380725116340943

Epoch: 6| Step: 12
Training loss: 3.2236246769342207
Validation loss: 2.539083579001949

Epoch: 6| Step: 13
Training loss: 2.3247426700724674
Validation loss: 2.5476213523343594

Epoch: 64| Step: 0
Training loss: 3.1835049295029267
Validation loss: 2.55083435784225

Epoch: 6| Step: 1
Training loss: 2.9098291814947688
Validation loss: 2.5610533001257783

Epoch: 6| Step: 2
Training loss: 2.7232126026014356
Validation loss: 2.5590907021537292

Epoch: 6| Step: 3
Training loss: 2.6968932892675395
Validation loss: 2.5603668073733816

Epoch: 6| Step: 4
Training loss: 3.1925889325282784
Validation loss: 2.5647849763487094

Epoch: 6| Step: 5
Training loss: 3.052596291062894
Validation loss: 2.596174003653236

Epoch: 6| Step: 6
Training loss: 2.4578607611455943
Validation loss: 2.5782188505341357

Epoch: 6| Step: 7
Training loss: 2.8997161594787157
Validation loss: 2.5774653898874074

Epoch: 6| Step: 8
Training loss: 3.290027467792718
Validation loss: 2.5278484104806735

Epoch: 6| Step: 9
Training loss: 2.7150517006688264
Validation loss: 2.5244262245643574

Epoch: 6| Step: 10
Training loss: 2.9618417773181585
Validation loss: 2.526008724652872

Epoch: 6| Step: 11
Training loss: 2.2800992388627517
Validation loss: 2.5274487484531805

Epoch: 6| Step: 12
Training loss: 3.2476307963166886
Validation loss: 2.535808520339356

Epoch: 6| Step: 13
Training loss: 2.801417662859904
Validation loss: 2.5386573840635376

Epoch: 65| Step: 0
Training loss: 3.318104159917897
Validation loss: 2.5415167516361867

Epoch: 6| Step: 1
Training loss: 2.6466095954577513
Validation loss: 2.54325891733947

Epoch: 6| Step: 2
Training loss: 2.763438853222866
Validation loss: 2.5443242879905257

Epoch: 6| Step: 3
Training loss: 2.919029114669814
Validation loss: 2.5469020648552263

Epoch: 6| Step: 4
Training loss: 2.619602102976229
Validation loss: 2.54863759505311

Epoch: 6| Step: 5
Training loss: 2.403774226359671
Validation loss: 2.5476711369373306

Epoch: 6| Step: 6
Training loss: 2.813460461544956
Validation loss: 2.547192117262474

Epoch: 6| Step: 7
Training loss: 2.4176205583116115
Validation loss: 2.54857337843778

Epoch: 6| Step: 8
Training loss: 3.54997165292517
Validation loss: 2.5482612871907007

Epoch: 6| Step: 9
Training loss: 2.7961726665418434
Validation loss: 2.5473157263681525

Epoch: 6| Step: 10
Training loss: 3.504054718104194
Validation loss: 2.547868787528465

Epoch: 6| Step: 11
Training loss: 3.1224360820009815
Validation loss: 2.5435874361139676

Epoch: 6| Step: 12
Training loss: 2.666622688010282
Validation loss: 2.5419810221911736

Epoch: 6| Step: 13
Training loss: 3.596458277022924
Validation loss: 2.539412966032475

Epoch: 66| Step: 0
Training loss: 3.3470496893420636
Validation loss: 2.5402540154033457

Epoch: 6| Step: 1
Training loss: 2.5313856300468744
Validation loss: 2.5454198083696746

Epoch: 6| Step: 2
Training loss: 2.386274280774261
Validation loss: 2.5470604833092945

Epoch: 6| Step: 3
Training loss: 3.2220057345338695
Validation loss: 2.556412213146981

Epoch: 6| Step: 4
Training loss: 3.0924602864335937
Validation loss: 2.549570949361682

Epoch: 6| Step: 5
Training loss: 3.3540200779950244
Validation loss: 2.5461831168755884

Epoch: 6| Step: 6
Training loss: 2.4160527293533196
Validation loss: 2.5483166949847673

Epoch: 6| Step: 7
Training loss: 3.2979494486416705
Validation loss: 2.5510207461410674

Epoch: 6| Step: 8
Training loss: 2.971316543238379
Validation loss: 2.5563795237678333

Epoch: 6| Step: 9
Training loss: 2.9520271319251234
Validation loss: 2.559464546392127

Epoch: 6| Step: 10
Training loss: 2.703795421569772
Validation loss: 2.557349514610315

Epoch: 6| Step: 11
Training loss: 2.848113349051939
Validation loss: 2.5557771117515795

Epoch: 6| Step: 12
Training loss: 2.76861135165778
Validation loss: 2.5544354603995933

Epoch: 6| Step: 13
Training loss: 2.825685070321564
Validation loss: 2.538734966952205

Epoch: 67| Step: 0
Training loss: 2.8650612912104654
Validation loss: 2.5333353656518858

Epoch: 6| Step: 1
Training loss: 2.723064025378363
Validation loss: 2.5298706194539093

Epoch: 6| Step: 2
Training loss: 2.8647194200037602
Validation loss: 2.5250394066970645

Epoch: 6| Step: 3
Training loss: 2.7839583165402537
Validation loss: 2.525781713281618

Epoch: 6| Step: 4
Training loss: 2.8378956657517174
Validation loss: 2.5249617197730605

Epoch: 6| Step: 5
Training loss: 2.7501343780977714
Validation loss: 2.522798648906659

Epoch: 6| Step: 6
Training loss: 3.1901581096629736
Validation loss: 2.5198644482765653

Epoch: 6| Step: 7
Training loss: 2.7624579793228374
Validation loss: 2.520905601369321

Epoch: 6| Step: 8
Training loss: 2.881321136192244
Validation loss: 2.5226535532461427

Epoch: 6| Step: 9
Training loss: 2.717138404687824
Validation loss: 2.5227067095496523

Epoch: 6| Step: 10
Training loss: 3.20196005874112
Validation loss: 2.526459681627988

Epoch: 6| Step: 11
Training loss: 3.166110508583596
Validation loss: 2.536285053523351

Epoch: 6| Step: 12
Training loss: 2.837442130284317
Validation loss: 2.5232958161925363

Epoch: 6| Step: 13
Training loss: 3.122549698543331
Validation loss: 2.516333109726905

Epoch: 68| Step: 0
Training loss: 2.9842836056440776
Validation loss: 2.5129183469967122

Epoch: 6| Step: 1
Training loss: 2.3495834996042486
Validation loss: 2.51462990833325

Epoch: 6| Step: 2
Training loss: 2.5609480647578473
Validation loss: 2.5142516928513032

Epoch: 6| Step: 3
Training loss: 2.5986032695732373
Validation loss: 2.5141074519701045

Epoch: 6| Step: 4
Training loss: 2.897295868391598
Validation loss: 2.5152384785187127

Epoch: 6| Step: 5
Training loss: 3.1739122585002573
Validation loss: 2.5142044640458527

Epoch: 6| Step: 6
Training loss: 2.598464725280648
Validation loss: 2.512884505204799

Epoch: 6| Step: 7
Training loss: 3.0029239710311444
Validation loss: 2.5166260384817885

Epoch: 6| Step: 8
Training loss: 2.8574505776497583
Validation loss: 2.5146196542795627

Epoch: 6| Step: 9
Training loss: 3.314645468279119
Validation loss: 2.5131069179869345

Epoch: 6| Step: 10
Training loss: 3.6052392822512624
Validation loss: 2.5136700657343716

Epoch: 6| Step: 11
Training loss: 2.908654479592382
Validation loss: 2.51469897122158

Epoch: 6| Step: 12
Training loss: 3.2009168503510566
Validation loss: 2.52495225292455

Epoch: 6| Step: 13
Training loss: 1.8368746050055131
Validation loss: 2.5148420318559013

Epoch: 69| Step: 0
Training loss: 2.7736437223891572
Validation loss: 2.514164156454278

Epoch: 6| Step: 1
Training loss: 2.7511939144700595
Validation loss: 2.5127432843602535

Epoch: 6| Step: 2
Training loss: 2.7220616833931266
Validation loss: 2.511971257648402

Epoch: 6| Step: 3
Training loss: 3.266365806841978
Validation loss: 2.512532367231501

Epoch: 6| Step: 4
Training loss: 3.2967668126872924
Validation loss: 2.515340779953456

Epoch: 6| Step: 5
Training loss: 3.0597627824575078
Validation loss: 2.515701974536834

Epoch: 6| Step: 6
Training loss: 2.901186397829966
Validation loss: 2.5158141374679213

Epoch: 6| Step: 7
Training loss: 2.7517950961546362
Validation loss: 2.519288276949939

Epoch: 6| Step: 8
Training loss: 2.6281280045826687
Validation loss: 2.5179279404836614

Epoch: 6| Step: 9
Training loss: 2.685297573655857
Validation loss: 2.518226811276325

Epoch: 6| Step: 10
Training loss: 2.574562061438221
Validation loss: 2.5151013640984714

Epoch: 6| Step: 11
Training loss: 3.0370308721767527
Validation loss: 2.5181821039142327

Epoch: 6| Step: 12
Training loss: 3.3724552026577173
Validation loss: 2.517961830559129

Epoch: 6| Step: 13
Training loss: 2.5048289391745633
Validation loss: 2.522619353233614

Epoch: 70| Step: 0
Training loss: 2.5401637110178497
Validation loss: 2.5347257040886464

Epoch: 6| Step: 1
Training loss: 3.3452145230281007
Validation loss: 2.564941425740111

Epoch: 6| Step: 2
Training loss: 3.4795912675778884
Validation loss: 2.5696196135993628

Epoch: 6| Step: 3
Training loss: 3.2563275582417903
Validation loss: 2.5306647102723305

Epoch: 6| Step: 4
Training loss: 2.1767902329068396
Validation loss: 2.5141798467174157

Epoch: 6| Step: 5
Training loss: 2.885190401719582
Validation loss: 2.5050300918653767

Epoch: 6| Step: 6
Training loss: 2.348799187681645
Validation loss: 2.509366675007953

Epoch: 6| Step: 7
Training loss: 2.301755015808918
Validation loss: 2.51216838000962

Epoch: 6| Step: 8
Training loss: 3.0509874027558523
Validation loss: 2.5177296225738806

Epoch: 6| Step: 9
Training loss: 3.1922096912660196
Validation loss: 2.522103863460936

Epoch: 6| Step: 10
Training loss: 3.0888613213270255
Validation loss: 2.517141859223439

Epoch: 6| Step: 11
Training loss: 3.114130878224129
Validation loss: 2.519627822789278

Epoch: 6| Step: 12
Training loss: 2.762631450304655
Validation loss: 2.518864725290442

Epoch: 6| Step: 13
Training loss: 3.292039697169039
Validation loss: 2.5145345332130935

Epoch: 71| Step: 0
Training loss: 2.803952771701952
Validation loss: 2.511685219685088

Epoch: 6| Step: 1
Training loss: 3.195278624562563
Validation loss: 2.5080808927108644

Epoch: 6| Step: 2
Training loss: 2.3802438367425878
Validation loss: 2.5061056887721502

Epoch: 6| Step: 3
Training loss: 3.372039768269571
Validation loss: 2.506425016450841

Epoch: 6| Step: 4
Training loss: 2.825792225169625
Validation loss: 2.5049768623677124

Epoch: 6| Step: 5
Training loss: 2.671726512686834
Validation loss: 2.5023310161406154

Epoch: 6| Step: 6
Training loss: 3.042576031201135
Validation loss: 2.504236447538649

Epoch: 6| Step: 7
Training loss: 1.937013072692998
Validation loss: 2.5053336089764393

Epoch: 6| Step: 8
Training loss: 3.0531930999813826
Validation loss: 2.515224472034634

Epoch: 6| Step: 9
Training loss: 2.788927603913484
Validation loss: 2.5180717986050443

Epoch: 6| Step: 10
Training loss: 2.697933967760696
Validation loss: 2.5512927567216295

Epoch: 6| Step: 11
Training loss: 2.8921329998072736
Validation loss: 2.5844345173809264

Epoch: 6| Step: 12
Training loss: 3.248126150002202
Validation loss: 2.612030144288485

Epoch: 6| Step: 13
Training loss: 3.659266735270327
Validation loss: 2.672709928199671

Epoch: 72| Step: 0
Training loss: 2.494671101268345
Validation loss: 2.635167194507399

Epoch: 6| Step: 1
Training loss: 2.6090021837897206
Validation loss: 2.6325234332691045

Epoch: 6| Step: 2
Training loss: 3.1424404988994676
Validation loss: 2.582516501062799

Epoch: 6| Step: 3
Training loss: 2.385013751744065
Validation loss: 2.6259340693845905

Epoch: 6| Step: 4
Training loss: 2.874219539690959
Validation loss: 2.7085562088150827

Epoch: 6| Step: 5
Training loss: 2.9978792323854164
Validation loss: 2.687468102874685

Epoch: 6| Step: 6
Training loss: 3.110406599051928
Validation loss: 2.585469429242104

Epoch: 6| Step: 7
Training loss: 3.1727843836666962
Validation loss: 2.5280207224200093

Epoch: 6| Step: 8
Training loss: 2.4666730618608756
Validation loss: 2.5096769806018746

Epoch: 6| Step: 9
Training loss: 3.072183674606318
Validation loss: 2.534635868269427

Epoch: 6| Step: 10
Training loss: 3.0642390082144195
Validation loss: 2.557553566228058

Epoch: 6| Step: 11
Training loss: 3.460795311342715
Validation loss: 2.587877306943566

Epoch: 6| Step: 12
Training loss: 3.119995840754549
Validation loss: 2.586384902744395

Epoch: 6| Step: 13
Training loss: 2.3468385818033006
Validation loss: 2.605564921078067

Epoch: 73| Step: 0
Training loss: 3.3212481448247666
Validation loss: 2.647375087981924

Epoch: 6| Step: 1
Training loss: 3.4164812611462794
Validation loss: 2.6595660286356897

Epoch: 6| Step: 2
Training loss: 3.5191525542397573
Validation loss: 2.662536788532982

Epoch: 6| Step: 3
Training loss: 3.3746274106673817
Validation loss: 2.6356520179045524

Epoch: 6| Step: 4
Training loss: 2.4265379286072837
Validation loss: 2.6073504832176724

Epoch: 6| Step: 5
Training loss: 2.747389768379833
Validation loss: 2.587508048323002

Epoch: 6| Step: 6
Training loss: 3.3497178485533663
Validation loss: 2.563999514630936

Epoch: 6| Step: 7
Training loss: 2.081793279449671
Validation loss: 2.5500170367526533

Epoch: 6| Step: 8
Training loss: 3.0730964909756215
Validation loss: 2.5377974192781707

Epoch: 6| Step: 9
Training loss: 3.065215794494947
Validation loss: 2.5337209416210467

Epoch: 6| Step: 10
Training loss: 2.765935158444647
Validation loss: 2.5283649186693307

Epoch: 6| Step: 11
Training loss: 2.861832360114313
Validation loss: 2.523477171828563

Epoch: 6| Step: 12
Training loss: 2.5955888595287893
Validation loss: 2.522244723339517

Epoch: 6| Step: 13
Training loss: 2.6932957277253537
Validation loss: 2.5152466803583136

Epoch: 74| Step: 0
Training loss: 3.2352364540061176
Validation loss: 2.5091218941499425

Epoch: 6| Step: 1
Training loss: 3.091717909999191
Validation loss: 2.507460122085247

Epoch: 6| Step: 2
Training loss: 2.921410834831202
Validation loss: 2.5121065233172732

Epoch: 6| Step: 3
Training loss: 2.49753315335589
Validation loss: 2.510729895843108

Epoch: 6| Step: 4
Training loss: 2.2318918795496145
Validation loss: 2.5228154439170676

Epoch: 6| Step: 5
Training loss: 3.293419640783399
Validation loss: 2.5278244528851688

Epoch: 6| Step: 6
Training loss: 2.6377537031323968
Validation loss: 2.5316216608546007

Epoch: 6| Step: 7
Training loss: 3.2467890796913386
Validation loss: 2.561095762629085

Epoch: 6| Step: 8
Training loss: 2.6360633770294797
Validation loss: 2.5470028730721266

Epoch: 6| Step: 9
Training loss: 2.8827506371453486
Validation loss: 2.549504996553343

Epoch: 6| Step: 10
Training loss: 3.2016359915613357
Validation loss: 2.5477163149634174

Epoch: 6| Step: 11
Training loss: 3.053967793552777
Validation loss: 2.5568425223297253

Epoch: 6| Step: 12
Training loss: 2.5759667813072507
Validation loss: 2.538980181474205

Epoch: 6| Step: 13
Training loss: 2.970471816428297
Validation loss: 2.5409359253846087

Epoch: 75| Step: 0
Training loss: 3.234991926982069
Validation loss: 2.535430051563824

Epoch: 6| Step: 1
Training loss: 3.553346083178617
Validation loss: 2.519282627200008

Epoch: 6| Step: 2
Training loss: 3.0722404813616695
Validation loss: 2.518787298809561

Epoch: 6| Step: 3
Training loss: 3.07929497960347
Validation loss: 2.5101307184567334

Epoch: 6| Step: 4
Training loss: 3.41273818529736
Validation loss: 2.512303419753536

Epoch: 6| Step: 5
Training loss: 2.46967858342952
Validation loss: 2.5097419146156796

Epoch: 6| Step: 6
Training loss: 2.3075713138161444
Validation loss: 2.5071009187126734

Epoch: 6| Step: 7
Training loss: 2.9599578766789683
Validation loss: 2.5086269978472884

Epoch: 6| Step: 8
Training loss: 2.5493948237047492
Validation loss: 2.513341332988075

Epoch: 6| Step: 9
Training loss: 2.247479192278888
Validation loss: 2.5151944824854136

Epoch: 6| Step: 10
Training loss: 2.735535990971047
Validation loss: 2.5195880231973717

Epoch: 6| Step: 11
Training loss: 2.9075881994486843
Validation loss: 2.5184751861009858

Epoch: 6| Step: 12
Training loss: 2.8957577885349526
Validation loss: 2.5213106173575626

Epoch: 6| Step: 13
Training loss: 2.9252673361133907
Validation loss: 2.5200365797790814

Epoch: 76| Step: 0
Training loss: 2.517555964737129
Validation loss: 2.525491002689285

Epoch: 6| Step: 1
Training loss: 2.950092191387094
Validation loss: 2.5280611619791777

Epoch: 6| Step: 2
Training loss: 2.646234997475703
Validation loss: 2.5235185334904022

Epoch: 6| Step: 3
Training loss: 2.7960065633563063
Validation loss: 2.5266409309126985

Epoch: 6| Step: 4
Training loss: 3.0694860069103838
Validation loss: 2.5262259135887346

Epoch: 6| Step: 5
Training loss: 3.24519197904145
Validation loss: 2.5237721115574163

Epoch: 6| Step: 6
Training loss: 2.948247686521867
Validation loss: 2.519683244678178

Epoch: 6| Step: 7
Training loss: 3.2563195043603352
Validation loss: 2.523478446294764

Epoch: 6| Step: 8
Training loss: 2.7948493654880657
Validation loss: 2.5199037928136767

Epoch: 6| Step: 9
Training loss: 2.2973574177163485
Validation loss: 2.5160282448491484

Epoch: 6| Step: 10
Training loss: 3.1890868182752676
Validation loss: 2.518441161438939

Epoch: 6| Step: 11
Training loss: 3.3310065890876603
Validation loss: 2.517008878803506

Epoch: 6| Step: 12
Training loss: 2.4063470994258163
Validation loss: 2.529689041657415

Epoch: 6| Step: 13
Training loss: 2.8859383195298647
Validation loss: 2.525012560325531

Epoch: 77| Step: 0
Training loss: 2.8487056269083504
Validation loss: 2.5382445799598123

Epoch: 6| Step: 1
Training loss: 3.1831833276473582
Validation loss: 2.540550200217634

Epoch: 6| Step: 2
Training loss: 3.352200818576352
Validation loss: 2.5297132248131566

Epoch: 6| Step: 3
Training loss: 2.715496528141427
Validation loss: 2.5065510532562025

Epoch: 6| Step: 4
Training loss: 2.8496525469013805
Validation loss: 2.506292190587384

Epoch: 6| Step: 5
Training loss: 2.9022653807870733
Validation loss: 2.502464889228101

Epoch: 6| Step: 6
Training loss: 3.435597812433287
Validation loss: 2.5011200682507866

Epoch: 6| Step: 7
Training loss: 2.4271908410183047
Validation loss: 2.501637283622339

Epoch: 6| Step: 8
Training loss: 3.1148767917814206
Validation loss: 2.504931739422524

Epoch: 6| Step: 9
Training loss: 2.8140787567906065
Validation loss: 2.506089241600917

Epoch: 6| Step: 10
Training loss: 2.3620580108392395
Validation loss: 2.5073408924734637

Epoch: 6| Step: 11
Training loss: 3.022028789532082
Validation loss: 2.50763773418607

Epoch: 6| Step: 12
Training loss: 2.44419245312113
Validation loss: 2.501165583854643

Epoch: 6| Step: 13
Training loss: 2.9347683806389298
Validation loss: 2.4947868999817846

Epoch: 78| Step: 0
Training loss: 3.0553982973571814
Validation loss: 2.503760918569937

Epoch: 6| Step: 1
Training loss: 3.140922266617963
Validation loss: 2.5029479806444934

Epoch: 6| Step: 2
Training loss: 3.2583051350211623
Validation loss: 2.503458498005285

Epoch: 6| Step: 3
Training loss: 3.2096375295801023
Validation loss: 2.512134916926239

Epoch: 6| Step: 4
Training loss: 2.3180463108706046
Validation loss: 2.504989447318168

Epoch: 6| Step: 5
Training loss: 2.1806067712469632
Validation loss: 2.4988352348985696

Epoch: 6| Step: 6
Training loss: 3.5129240384162332
Validation loss: 2.506333724358214

Epoch: 6| Step: 7
Training loss: 2.2804262947294216
Validation loss: 2.5036414196121304

Epoch: 6| Step: 8
Training loss: 2.8627749381814294
Validation loss: 2.5173064309078415

Epoch: 6| Step: 9
Training loss: 2.650407716587761
Validation loss: 2.5117270490691412

Epoch: 6| Step: 10
Training loss: 2.7439672014817553
Validation loss: 2.509548588788254

Epoch: 6| Step: 11
Training loss: 2.911629241312703
Validation loss: 2.51039276874169

Epoch: 6| Step: 12
Training loss: 3.2919167210368774
Validation loss: 2.506628897585735

Epoch: 6| Step: 13
Training loss: 2.218980804044014
Validation loss: 2.502923298183202

Epoch: 79| Step: 0
Training loss: 3.1825704416059186
Validation loss: 2.5054877091748673

Epoch: 6| Step: 1
Training loss: 2.9653869854969828
Validation loss: 2.5091849636183006

Epoch: 6| Step: 2
Training loss: 2.906601423120436
Validation loss: 2.5109178981108666

Epoch: 6| Step: 3
Training loss: 2.4690018537475007
Validation loss: 2.508788594486352

Epoch: 6| Step: 4
Training loss: 2.6597882210887542
Validation loss: 2.5109997613685833

Epoch: 6| Step: 5
Training loss: 3.4228207748704524
Validation loss: 2.516496031996875

Epoch: 6| Step: 6
Training loss: 2.9457149483625935
Validation loss: 2.5084784825277793

Epoch: 6| Step: 7
Training loss: 2.8399493632033446
Validation loss: 2.5257753929372564

Epoch: 6| Step: 8
Training loss: 2.734125965222007
Validation loss: 2.5169221905070303

Epoch: 6| Step: 9
Training loss: 3.0276760780294314
Validation loss: 2.5243747476371072

Epoch: 6| Step: 10
Training loss: 2.4087485680330905
Validation loss: 2.5110478860926477

Epoch: 6| Step: 11
Training loss: 3.029910706063588
Validation loss: 2.515834989390224

Epoch: 6| Step: 12
Training loss: 2.540582946954826
Validation loss: 2.516835688683719

Epoch: 6| Step: 13
Training loss: 2.9704938083719634
Validation loss: 2.511938889065358

Epoch: 80| Step: 0
Training loss: 2.8008976995927544
Validation loss: 2.503871474473219

Epoch: 6| Step: 1
Training loss: 2.4446478479703244
Validation loss: 2.5031230042488173

Epoch: 6| Step: 2
Training loss: 2.4349671922858724
Validation loss: 2.4967210431954956

Epoch: 6| Step: 3
Training loss: 3.154087950221029
Validation loss: 2.4970017912868103

Epoch: 6| Step: 4
Training loss: 2.5767976060227906
Validation loss: 2.4984908984085794

Epoch: 6| Step: 5
Training loss: 2.3600024469007317
Validation loss: 2.4967186507433796

Epoch: 6| Step: 6
Training loss: 2.3054908516114336
Validation loss: 2.5011883300713094

Epoch: 6| Step: 7
Training loss: 3.488209620100226
Validation loss: 2.5131204109200898

Epoch: 6| Step: 8
Training loss: 3.61462799543039
Validation loss: 2.524044254348468

Epoch: 6| Step: 9
Training loss: 2.6499299202055764
Validation loss: 2.546594861652087

Epoch: 6| Step: 10
Training loss: 3.0732075871403848
Validation loss: 2.5721623902395025

Epoch: 6| Step: 11
Training loss: 3.1850998761912437
Validation loss: 2.605095738877012

Epoch: 6| Step: 12
Training loss: 2.9227013057039
Validation loss: 2.6344844309541755

Epoch: 6| Step: 13
Training loss: 2.9326445038063453
Validation loss: 2.5844392618934386

Epoch: 81| Step: 0
Training loss: 2.9026897321750416
Validation loss: 2.554187481998839

Epoch: 6| Step: 1
Training loss: 3.0353164925122385
Validation loss: 2.5337800234717105

Epoch: 6| Step: 2
Training loss: 2.7028533269820127
Validation loss: 2.5221618315191083

Epoch: 6| Step: 3
Training loss: 2.533921233358293
Validation loss: 2.516494005733685

Epoch: 6| Step: 4
Training loss: 2.514768466492189
Validation loss: 2.508565523930105

Epoch: 6| Step: 5
Training loss: 3.7448117287018383
Validation loss: 2.504682157703706

Epoch: 6| Step: 6
Training loss: 2.4605024649583096
Validation loss: 2.5061486797439754

Epoch: 6| Step: 7
Training loss: 2.4938534516791777
Validation loss: 2.500707200221254

Epoch: 6| Step: 8
Training loss: 3.203044313484518
Validation loss: 2.51161109845318

Epoch: 6| Step: 9
Training loss: 3.256515427679238
Validation loss: 2.5245611742536718

Epoch: 6| Step: 10
Training loss: 3.054373254246659
Validation loss: 2.5291606484239995

Epoch: 6| Step: 11
Training loss: 2.4163748027779133
Validation loss: 2.5369457605235897

Epoch: 6| Step: 12
Training loss: 2.618319685002172
Validation loss: 2.5663185416180405

Epoch: 6| Step: 13
Training loss: 3.4841110061059553
Validation loss: 2.609567931909089

Epoch: 82| Step: 0
Training loss: 2.6812296619844367
Validation loss: 2.5952229271804335

Epoch: 6| Step: 1
Training loss: 2.469710247789876
Validation loss: 2.554638732221901

Epoch: 6| Step: 2
Training loss: 2.298207007079148
Validation loss: 2.529947643408685

Epoch: 6| Step: 3
Training loss: 3.3577012261529298
Validation loss: 2.5173454976873577

Epoch: 6| Step: 4
Training loss: 3.310015484407746
Validation loss: 2.519179655438358

Epoch: 6| Step: 5
Training loss: 3.190386942053715
Validation loss: 2.514129170511213

Epoch: 6| Step: 6
Training loss: 2.6032095612601815
Validation loss: 2.517065509267018

Epoch: 6| Step: 7
Training loss: 3.01355003956171
Validation loss: 2.512973081345568

Epoch: 6| Step: 8
Training loss: 2.5392496362828427
Validation loss: 2.513977771070742

Epoch: 6| Step: 9
Training loss: 3.469924453000914
Validation loss: 2.5194946764636272

Epoch: 6| Step: 10
Training loss: 3.0600174164120566
Validation loss: 2.513209940669198

Epoch: 6| Step: 11
Training loss: 2.929043874613212
Validation loss: 2.5233647663557277

Epoch: 6| Step: 12
Training loss: 2.410508091499755
Validation loss: 2.5314535395658995

Epoch: 6| Step: 13
Training loss: 2.7526217014664907
Validation loss: 2.543500070235693

Epoch: 83| Step: 0
Training loss: 3.338742936961288
Validation loss: 2.5784673976156993

Epoch: 6| Step: 1
Training loss: 2.324634059697381
Validation loss: 2.584975772086487

Epoch: 6| Step: 2
Training loss: 2.7360411009462
Validation loss: 2.628940731921282

Epoch: 6| Step: 3
Training loss: 2.8324467636990907
Validation loss: 2.603009932687202

Epoch: 6| Step: 4
Training loss: 3.538255073111935
Validation loss: 2.673849555796284

Epoch: 6| Step: 5
Training loss: 3.570038432280384
Validation loss: 2.6109463442561704

Epoch: 6| Step: 6
Training loss: 2.7395887967092674
Validation loss: 2.5503689286118143

Epoch: 6| Step: 7
Training loss: 2.3619354703843554
Validation loss: 2.538216479435086

Epoch: 6| Step: 8
Training loss: 3.118338847914087
Validation loss: 2.5118496199426734

Epoch: 6| Step: 9
Training loss: 2.637037379474753
Validation loss: 2.5141483365931636

Epoch: 6| Step: 10
Training loss: 3.262091225925444
Validation loss: 2.5180645497459735

Epoch: 6| Step: 11
Training loss: 2.2692099785380764
Validation loss: 2.503412747594135

Epoch: 6| Step: 12
Training loss: 2.714119135255283
Validation loss: 2.513442737567019

Epoch: 6| Step: 13
Training loss: 2.8092698940011633
Validation loss: 2.501016826369355

Epoch: 84| Step: 0
Training loss: 2.706076273286561
Validation loss: 2.4935093186240413

Epoch: 6| Step: 1
Training loss: 2.7042612322792423
Validation loss: 2.4918644208632155

Epoch: 6| Step: 2
Training loss: 3.235061793732098
Validation loss: 2.4887716671273923

Epoch: 6| Step: 3
Training loss: 3.0880269763044943
Validation loss: 2.4947838079343425

Epoch: 6| Step: 4
Training loss: 2.5294730460183694
Validation loss: 2.500442545699061

Epoch: 6| Step: 5
Training loss: 3.095058896393339
Validation loss: 2.5187173714093456

Epoch: 6| Step: 6
Training loss: 2.5710955415509082
Validation loss: 2.510692911765409

Epoch: 6| Step: 7
Training loss: 3.3526279561019194
Validation loss: 2.5098256681051203

Epoch: 6| Step: 8
Training loss: 2.7972153611042723
Validation loss: 2.5011326182829055

Epoch: 6| Step: 9
Training loss: 2.734018705860767
Validation loss: 2.497810296956589

Epoch: 6| Step: 10
Training loss: 2.7772739578684784
Validation loss: 2.4925735899265984

Epoch: 6| Step: 11
Training loss: 2.624161359245172
Validation loss: 2.4860287058376445

Epoch: 6| Step: 12
Training loss: 2.8478489772637605
Validation loss: 2.4864813121354867

Epoch: 6| Step: 13
Training loss: 2.8819146968016094
Validation loss: 2.486373386567667

Epoch: 85| Step: 0
Training loss: 2.2392391797973703
Validation loss: 2.483913527107982

Epoch: 6| Step: 1
Training loss: 2.9206922452610993
Validation loss: 2.488616949047314

Epoch: 6| Step: 2
Training loss: 2.9735731598080712
Validation loss: 2.486912509871285

Epoch: 6| Step: 3
Training loss: 2.6216187271161706
Validation loss: 2.482300521525443

Epoch: 6| Step: 4
Training loss: 2.9128425233012094
Validation loss: 2.4846647793591843

Epoch: 6| Step: 5
Training loss: 3.037295889570323
Validation loss: 2.4896613162151473

Epoch: 6| Step: 6
Training loss: 2.796035384842441
Validation loss: 2.503193913152895

Epoch: 6| Step: 7
Training loss: 2.7304151363320974
Validation loss: 2.5327117914273063

Epoch: 6| Step: 8
Training loss: 3.000344892385974
Validation loss: 2.5896718007365354

Epoch: 6| Step: 9
Training loss: 2.898687881389754
Validation loss: 2.6348628351825347

Epoch: 6| Step: 10
Training loss: 2.8524071683496968
Validation loss: 2.651094682990457

Epoch: 6| Step: 11
Training loss: 3.1223893514167673
Validation loss: 2.689893908349897

Epoch: 6| Step: 12
Training loss: 3.4443723472788506
Validation loss: 2.651510961588146

Epoch: 6| Step: 13
Training loss: 2.6997108728523265
Validation loss: 2.526944000039117

Epoch: 86| Step: 0
Training loss: 2.8863090668355103
Validation loss: 2.487461234112348

Epoch: 6| Step: 1
Training loss: 2.253633743816757
Validation loss: 2.4862270756311573

Epoch: 6| Step: 2
Training loss: 2.701410179631772
Validation loss: 2.495241096463264

Epoch: 6| Step: 3
Training loss: 3.3784258374154104
Validation loss: 2.501047702413748

Epoch: 6| Step: 4
Training loss: 2.8147319838213853
Validation loss: 2.5086434978712497

Epoch: 6| Step: 5
Training loss: 2.973510779772267
Validation loss: 2.507052837353239

Epoch: 6| Step: 6
Training loss: 3.038025196023694
Validation loss: 2.512287494805043

Epoch: 6| Step: 7
Training loss: 2.9469798905767695
Validation loss: 2.5133913294441537

Epoch: 6| Step: 8
Training loss: 3.227410378249687
Validation loss: 2.5150986884368285

Epoch: 6| Step: 9
Training loss: 2.743184400233882
Validation loss: 2.5100349017190333

Epoch: 6| Step: 10
Training loss: 2.815971266849187
Validation loss: 2.5094449530097322

Epoch: 6| Step: 11
Training loss: 2.90876677452139
Validation loss: 2.506802395052928

Epoch: 6| Step: 12
Training loss: 2.5514475958344804
Validation loss: 2.5040670752823204

Epoch: 6| Step: 13
Training loss: 3.399661209954348
Validation loss: 2.5022752847144423

Epoch: 87| Step: 0
Training loss: 2.814588661292337
Validation loss: 2.499336720739758

Epoch: 6| Step: 1
Training loss: 3.138759837315562
Validation loss: 2.500407938335259

Epoch: 6| Step: 2
Training loss: 2.7277100342493927
Validation loss: 2.4998248561912746

Epoch: 6| Step: 3
Training loss: 3.345263985108786
Validation loss: 2.493203172363746

Epoch: 6| Step: 4
Training loss: 2.715392571830606
Validation loss: 2.4882385576844386

Epoch: 6| Step: 5
Training loss: 2.902034861170558
Validation loss: 2.4918665978119967

Epoch: 6| Step: 6
Training loss: 2.9699078359978257
Validation loss: 2.4889368354074124

Epoch: 6| Step: 7
Training loss: 2.9096390847099034
Validation loss: 2.499208369588988

Epoch: 6| Step: 8
Training loss: 2.9103735106610658
Validation loss: 2.4909481768435153

Epoch: 6| Step: 9
Training loss: 3.0067371378193974
Validation loss: 2.485589308830466

Epoch: 6| Step: 10
Training loss: 2.568233309940814
Validation loss: 2.4823346718311923

Epoch: 6| Step: 11
Training loss: 3.2788287721014426
Validation loss: 2.4799459702153297

Epoch: 6| Step: 12
Training loss: 2.1645604190831285
Validation loss: 2.481755385287139

Epoch: 6| Step: 13
Training loss: 2.1148633975020688
Validation loss: 2.4842942675485857

Epoch: 88| Step: 0
Training loss: 3.0782746409335617
Validation loss: 2.4846408748419835

Epoch: 6| Step: 1
Training loss: 2.7269276371731808
Validation loss: 2.4861193380446767

Epoch: 6| Step: 2
Training loss: 2.4711894284255322
Validation loss: 2.4956574450971867

Epoch: 6| Step: 3
Training loss: 2.791787235066696
Validation loss: 2.512941873363982

Epoch: 6| Step: 4
Training loss: 2.9999809264530386
Validation loss: 2.5349591514789864

Epoch: 6| Step: 5
Training loss: 2.750293542627346
Validation loss: 2.544006971457148

Epoch: 6| Step: 6
Training loss: 2.7518976773157857
Validation loss: 2.5505079113439484

Epoch: 6| Step: 7
Training loss: 2.580384807278091
Validation loss: 2.5507024515308236

Epoch: 6| Step: 8
Training loss: 2.7268756151078426
Validation loss: 2.5623478687986947

Epoch: 6| Step: 9
Training loss: 3.6333755549039566
Validation loss: 2.541207721117341

Epoch: 6| Step: 10
Training loss: 2.934773579950619
Validation loss: 2.4825623970305735

Epoch: 6| Step: 11
Training loss: 2.7448753812257984
Validation loss: 2.477550606176035

Epoch: 6| Step: 12
Training loss: 3.2419601176854544
Validation loss: 2.4836798420447215

Epoch: 6| Step: 13
Training loss: 1.807276070680486
Validation loss: 2.4884229205561375

Epoch: 89| Step: 0
Training loss: 2.541173532726838
Validation loss: 2.4952399765839375

Epoch: 6| Step: 1
Training loss: 2.8999901080784958
Validation loss: 2.4957638772800737

Epoch: 6| Step: 2
Training loss: 3.0241878081236475
Validation loss: 2.499536154385693

Epoch: 6| Step: 3
Training loss: 3.3419937798141404
Validation loss: 2.497444605481278

Epoch: 6| Step: 4
Training loss: 2.650345556653251
Validation loss: 2.495069793263251

Epoch: 6| Step: 5
Training loss: 3.5505322661776986
Validation loss: 2.4899186893465735

Epoch: 6| Step: 6
Training loss: 2.4400806948341374
Validation loss: 2.487266709367228

Epoch: 6| Step: 7
Training loss: 2.758247319596633
Validation loss: 2.48369519485804

Epoch: 6| Step: 8
Training loss: 2.4371977398605047
Validation loss: 2.4988949456099836

Epoch: 6| Step: 9
Training loss: 2.656282581802702
Validation loss: 2.5117992123562107

Epoch: 6| Step: 10
Training loss: 3.36655584725771
Validation loss: 2.5107595568421095

Epoch: 6| Step: 11
Training loss: 2.991469972001021
Validation loss: 2.520158721331856

Epoch: 6| Step: 12
Training loss: 2.527042046629246
Validation loss: 2.517359016784393

Epoch: 6| Step: 13
Training loss: 2.783996083585681
Validation loss: 2.5163155649077695

Epoch: 90| Step: 0
Training loss: 2.865565202228464
Validation loss: 2.5209032715306288

Epoch: 6| Step: 1
Training loss: 1.8921445934619106
Validation loss: 2.52921332029951

Epoch: 6| Step: 2
Training loss: 2.8515379186119443
Validation loss: 2.529224293637777

Epoch: 6| Step: 3
Training loss: 3.3875877734985314
Validation loss: 2.5175614544151306

Epoch: 6| Step: 4
Training loss: 2.475714216351007
Validation loss: 2.5086310569490897

Epoch: 6| Step: 5
Training loss: 2.9560025843504474
Validation loss: 2.5107626802681677

Epoch: 6| Step: 6
Training loss: 3.3714938849017915
Validation loss: 2.501418156522926

Epoch: 6| Step: 7
Training loss: 2.93312347340539
Validation loss: 2.5049530636718353

Epoch: 6| Step: 8
Training loss: 2.4901692699822764
Validation loss: 2.4954613833922656

Epoch: 6| Step: 9
Training loss: 3.0654055766712673
Validation loss: 2.504361357382508

Epoch: 6| Step: 10
Training loss: 3.074630535228846
Validation loss: 2.5076464260455884

Epoch: 6| Step: 11
Training loss: 2.2836656713758705
Validation loss: 2.501922226315306

Epoch: 6| Step: 12
Training loss: 3.2070119209032306
Validation loss: 2.5053668139703924

Epoch: 6| Step: 13
Training loss: 2.210005277221731
Validation loss: 2.498932467976598

Epoch: 91| Step: 0
Training loss: 2.794748019629873
Validation loss: 2.4879669404601343

Epoch: 6| Step: 1
Training loss: 2.656770901934567
Validation loss: 2.489775866241316

Epoch: 6| Step: 2
Training loss: 2.9780260227723954
Validation loss: 2.4846096060903853

Epoch: 6| Step: 3
Training loss: 3.174643525063951
Validation loss: 2.482385030462561

Epoch: 6| Step: 4
Training loss: 2.1318720606121286
Validation loss: 2.4834336111988558

Epoch: 6| Step: 5
Training loss: 2.9976594059469965
Validation loss: 2.5018778157681827

Epoch: 6| Step: 6
Training loss: 2.7295148981613324
Validation loss: 2.5098645305971856

Epoch: 6| Step: 7
Training loss: 2.529788313441984
Validation loss: 2.513087034981983

Epoch: 6| Step: 8
Training loss: 2.9966787391337384
Validation loss: 2.5024752545532416

Epoch: 6| Step: 9
Training loss: 3.1100657860277505
Validation loss: 2.4956505646302056

Epoch: 6| Step: 10
Training loss: 2.706861790669073
Validation loss: 2.506497397845109

Epoch: 6| Step: 11
Training loss: 2.784185681826676
Validation loss: 2.5014196251681993

Epoch: 6| Step: 12
Training loss: 2.767566753571381
Validation loss: 2.486881367614285

Epoch: 6| Step: 13
Training loss: 3.1926134270672266
Validation loss: 2.480266539742078

Epoch: 92| Step: 0
Training loss: 2.9564400287224863
Validation loss: 2.469336821456597

Epoch: 6| Step: 1
Training loss: 2.2739877985188266
Validation loss: 2.465830660318767

Epoch: 6| Step: 2
Training loss: 3.158624039475719
Validation loss: 2.464160574957656

Epoch: 6| Step: 3
Training loss: 2.4094729946607645
Validation loss: 2.4610695174082924

Epoch: 6| Step: 4
Training loss: 3.316169865140037
Validation loss: 2.4702428434807553

Epoch: 6| Step: 5
Training loss: 3.2475804345533987
Validation loss: 2.462404108957522

Epoch: 6| Step: 6
Training loss: 2.8899345604374576
Validation loss: 2.4639103395568824

Epoch: 6| Step: 7
Training loss: 2.2633317588197666
Validation loss: 2.467493369058067

Epoch: 6| Step: 8
Training loss: 2.942716471342914
Validation loss: 2.472853289416649

Epoch: 6| Step: 9
Training loss: 2.7230364453317515
Validation loss: 2.4843392731051925

Epoch: 6| Step: 10
Training loss: 2.3508155625014324
Validation loss: 2.531288439378918

Epoch: 6| Step: 11
Training loss: 3.287335084905282
Validation loss: 2.5756886186401515

Epoch: 6| Step: 12
Training loss: 2.666597027664056
Validation loss: 2.574015280483124

Epoch: 6| Step: 13
Training loss: 2.7905210640103224
Validation loss: 2.550765713018425

Epoch: 93| Step: 0
Training loss: 2.52079071975714
Validation loss: 2.523085044548482

Epoch: 6| Step: 1
Training loss: 3.061531692323401
Validation loss: 2.507033332749679

Epoch: 6| Step: 2
Training loss: 3.4125104296179045
Validation loss: 2.5073532876522355

Epoch: 6| Step: 3
Training loss: 3.025480147158399
Validation loss: 2.494945200702742

Epoch: 6| Step: 4
Training loss: 2.2083403629215086
Validation loss: 2.499869112464686

Epoch: 6| Step: 5
Training loss: 2.127270439305294
Validation loss: 2.4835504149025374

Epoch: 6| Step: 6
Training loss: 2.639538151917116
Validation loss: 2.484932137694602

Epoch: 6| Step: 7
Training loss: 2.927815482636536
Validation loss: 2.4811096278164078

Epoch: 6| Step: 8
Training loss: 3.0985984956636607
Validation loss: 2.4800060592250524

Epoch: 6| Step: 9
Training loss: 2.096881517380252
Validation loss: 2.477904200912779

Epoch: 6| Step: 10
Training loss: 2.7932366089321907
Validation loss: 2.474873400603641

Epoch: 6| Step: 11
Training loss: 3.592335165406033
Validation loss: 2.478832689433971

Epoch: 6| Step: 12
Training loss: 2.5779884071000656
Validation loss: 2.473868951310839

Epoch: 6| Step: 13
Training loss: 2.770043157661338
Validation loss: 2.47462689021645

Epoch: 94| Step: 0
Training loss: 2.895391544883205
Validation loss: 2.477373235734805

Epoch: 6| Step: 1
Training loss: 2.6090604855247155
Validation loss: 2.483388011132494

Epoch: 6| Step: 2
Training loss: 2.3634077558470414
Validation loss: 2.480409235486986

Epoch: 6| Step: 3
Training loss: 2.9917120892008326
Validation loss: 2.482385184339755

Epoch: 6| Step: 4
Training loss: 3.023003441064449
Validation loss: 2.4635197678534677

Epoch: 6| Step: 5
Training loss: 3.251851654760763
Validation loss: 2.468998966147896

Epoch: 6| Step: 6
Training loss: 2.780875534394333
Validation loss: 2.4690566105526424

Epoch: 6| Step: 7
Training loss: 2.7621015097315005
Validation loss: 2.4747648931882105

Epoch: 6| Step: 8
Training loss: 3.1042643964774173
Validation loss: 2.4862412372258715

Epoch: 6| Step: 9
Training loss: 2.5440976966716433
Validation loss: 2.518681026267435

Epoch: 6| Step: 10
Training loss: 2.9663755859847662
Validation loss: 2.5534493449323543

Epoch: 6| Step: 11
Training loss: 2.6183586574748663
Validation loss: 2.5627140122872043

Epoch: 6| Step: 12
Training loss: 2.9405154719614597
Validation loss: 2.5757496540738956

Epoch: 6| Step: 13
Training loss: 1.6468740705744458
Validation loss: 2.5196780083889467

Epoch: 95| Step: 0
Training loss: 2.945729193314735
Validation loss: 2.4813845991939765

Epoch: 6| Step: 1
Training loss: 2.173002101526767
Validation loss: 2.4692423298298953

Epoch: 6| Step: 2
Training loss: 2.9294570221841827
Validation loss: 2.472839897114102

Epoch: 6| Step: 3
Training loss: 2.25412023311981
Validation loss: 2.472055616991042

Epoch: 6| Step: 4
Training loss: 2.625156216287349
Validation loss: 2.4640351545492982

Epoch: 6| Step: 5
Training loss: 3.1152313136740446
Validation loss: 2.4714520819031716

Epoch: 6| Step: 6
Training loss: 3.062646667705247
Validation loss: 2.4791746200581475

Epoch: 6| Step: 7
Training loss: 2.9853085959964134
Validation loss: 2.484129586264939

Epoch: 6| Step: 8
Training loss: 2.563245711331662
Validation loss: 2.490183098249222

Epoch: 6| Step: 9
Training loss: 2.5414918516314744
Validation loss: 2.4907013877180284

Epoch: 6| Step: 10
Training loss: 2.6800364275492083
Validation loss: 2.4902592594094544

Epoch: 6| Step: 11
Training loss: 2.999836440396112
Validation loss: 2.495483500494548

Epoch: 6| Step: 12
Training loss: 3.203946040924269
Validation loss: 2.5112296301048973

Epoch: 6| Step: 13
Training loss: 2.6191358914713003
Validation loss: 2.509456263081102

Epoch: 96| Step: 0
Training loss: 2.989057612078629
Validation loss: 2.498919815589867

Epoch: 6| Step: 1
Training loss: 2.4802593953850867
Validation loss: 2.493355830177552

Epoch: 6| Step: 2
Training loss: 3.0877218375260194
Validation loss: 2.476995654178258

Epoch: 6| Step: 3
Training loss: 2.4215302252701645
Validation loss: 2.4781211026321195

Epoch: 6| Step: 4
Training loss: 2.9811863666516993
Validation loss: 2.4716181586458337

Epoch: 6| Step: 5
Training loss: 2.7088441709336335
Validation loss: 2.471365686673727

Epoch: 6| Step: 6
Training loss: 2.0498619126123616
Validation loss: 2.4734948510473096

Epoch: 6| Step: 7
Training loss: 2.514810465805437
Validation loss: 2.467207010415907

Epoch: 6| Step: 8
Training loss: 2.4692522334828038
Validation loss: 2.47491882089991

Epoch: 6| Step: 9
Training loss: 2.6217224913334594
Validation loss: 2.485797933009659

Epoch: 6| Step: 10
Training loss: 3.0629589748941735
Validation loss: 2.512352976657587

Epoch: 6| Step: 11
Training loss: 3.28217092033876
Validation loss: 2.57932175708353

Epoch: 6| Step: 12
Training loss: 3.212774563348238
Validation loss: 2.5850405719896115

Epoch: 6| Step: 13
Training loss: 3.1679071874316573
Validation loss: 2.518418700838347

Epoch: 97| Step: 0
Training loss: 2.6802464561157877
Validation loss: 2.471411389272644

Epoch: 6| Step: 1
Training loss: 2.800377724918851
Validation loss: 2.4828447708947237

Epoch: 6| Step: 2
Training loss: 2.6238228111105375
Validation loss: 2.4833694500253785

Epoch: 6| Step: 3
Training loss: 3.1003753927038566
Validation loss: 2.4902614007006507

Epoch: 6| Step: 4
Training loss: 2.9168623903997952
Validation loss: 2.495813424652463

Epoch: 6| Step: 5
Training loss: 3.6230510865878487
Validation loss: 2.4974926762714356

Epoch: 6| Step: 6
Training loss: 3.088208717210462
Validation loss: 2.4935890402608942

Epoch: 6| Step: 7
Training loss: 2.5161172136497747
Validation loss: 2.4917184786530586

Epoch: 6| Step: 8
Training loss: 3.1432884651075104
Validation loss: 2.4893740298575375

Epoch: 6| Step: 9
Training loss: 2.93400544428304
Validation loss: 2.4881963344627307

Epoch: 6| Step: 10
Training loss: 2.7168131100397823
Validation loss: 2.4836230366360157

Epoch: 6| Step: 11
Training loss: 3.0043617964681197
Validation loss: 2.485610367897854

Epoch: 6| Step: 12
Training loss: 2.3047553618186964
Validation loss: 2.4915292694646523

Epoch: 6| Step: 13
Training loss: 1.8627959765088757
Validation loss: 2.486675783884187

Epoch: 98| Step: 0
Training loss: 2.929009687215584
Validation loss: 2.4877948107464993

Epoch: 6| Step: 1
Training loss: 3.236874117058403
Validation loss: 2.482945376617149

Epoch: 6| Step: 2
Training loss: 2.6988957124839064
Validation loss: 2.4846640364736454

Epoch: 6| Step: 3
Training loss: 2.3669684280397907
Validation loss: 2.491954551742234

Epoch: 6| Step: 4
Training loss: 3.0517748437024763
Validation loss: 2.491670703901817

Epoch: 6| Step: 5
Training loss: 2.9548961031184513
Validation loss: 2.5319310432643904

Epoch: 6| Step: 6
Training loss: 2.759388157434081
Validation loss: 2.6038629924955465

Epoch: 6| Step: 7
Training loss: 1.6349355228787914
Validation loss: 2.683378087794529

Epoch: 6| Step: 8
Training loss: 3.0504533149411976
Validation loss: 2.7827875492212577

Epoch: 6| Step: 9
Training loss: 2.0636905355221766
Validation loss: 2.7237962965437013

Epoch: 6| Step: 10
Training loss: 3.0332174306950956
Validation loss: 2.654197534001846

Epoch: 6| Step: 11
Training loss: 2.837444651059673
Validation loss: 2.6510790415236047

Epoch: 6| Step: 12
Training loss: 3.175908272655752
Validation loss: 2.5875409725877248

Epoch: 6| Step: 13
Training loss: 3.429288593286147
Validation loss: 2.5325192533362744

Epoch: 99| Step: 0
Training loss: 2.6661499436745415
Validation loss: 2.4832970263143683

Epoch: 6| Step: 1
Training loss: 2.7550655008273175
Validation loss: 2.4655615727627214

Epoch: 6| Step: 2
Training loss: 2.953040550679506
Validation loss: 2.4676490832643014

Epoch: 6| Step: 3
Training loss: 2.645945048539394
Validation loss: 2.465668401563984

Epoch: 6| Step: 4
Training loss: 2.6933632699165093
Validation loss: 2.475244161561923

Epoch: 6| Step: 5
Training loss: 2.7132491663720013
Validation loss: 2.474846599461693

Epoch: 6| Step: 6
Training loss: 2.825752991779454
Validation loss: 2.4762264370603044

Epoch: 6| Step: 7
Training loss: 2.5150456679579523
Validation loss: 2.4841396214866625

Epoch: 6| Step: 8
Training loss: 2.5602380026510243
Validation loss: 2.4705723715713748

Epoch: 6| Step: 9
Training loss: 2.880336783903663
Validation loss: 2.4815172499456226

Epoch: 6| Step: 10
Training loss: 2.866003971978235
Validation loss: 2.5082569420431082

Epoch: 6| Step: 11
Training loss: 2.6132518726744696
Validation loss: 2.5311343765335934

Epoch: 6| Step: 12
Training loss: 3.217226130664439
Validation loss: 2.5583267765242392

Epoch: 6| Step: 13
Training loss: 3.023036565490723
Validation loss: 2.5846883896337696

Epoch: 100| Step: 0
Training loss: 2.729820949565559
Validation loss: 2.632177098587252

Epoch: 6| Step: 1
Training loss: 2.8851628013968234
Validation loss: 2.680355852926363

Epoch: 6| Step: 2
Training loss: 2.7644898407187637
Validation loss: 2.7444244946782144

Epoch: 6| Step: 3
Training loss: 2.9190400594149506
Validation loss: 2.7933564851844843

Epoch: 6| Step: 4
Training loss: 3.058941388988133
Validation loss: 2.709098514408452

Epoch: 6| Step: 5
Training loss: 2.437409619343007
Validation loss: 2.6085256071309595

Epoch: 6| Step: 6
Training loss: 3.009116625761062
Validation loss: 2.5160267612959037

Epoch: 6| Step: 7
Training loss: 2.4671510757743595
Validation loss: 2.492413840513185

Epoch: 6| Step: 8
Training loss: 2.78766825899435
Validation loss: 2.481039416034592

Epoch: 6| Step: 9
Training loss: 2.915298322589368
Validation loss: 2.4720139532316354

Epoch: 6| Step: 10
Training loss: 2.8350924659464476
Validation loss: 2.479879374203611

Epoch: 6| Step: 11
Training loss: 2.608173482057147
Validation loss: 2.4800115059083883

Epoch: 6| Step: 12
Training loss: 3.2409774544463423
Validation loss: 2.4823450303304777

Epoch: 6| Step: 13
Training loss: 2.4208950890582934
Validation loss: 2.4814281811791745

Epoch: 101| Step: 0
Training loss: 3.241194607474304
Validation loss: 2.480953185719465

Epoch: 6| Step: 1
Training loss: 2.128067719393054
Validation loss: 2.4741168666404487

Epoch: 6| Step: 2
Training loss: 2.7664193774399064
Validation loss: 2.4739224760417855

Epoch: 6| Step: 3
Training loss: 2.995943187526383
Validation loss: 2.4752525715308944

Epoch: 6| Step: 4
Training loss: 2.7830981853845844
Validation loss: 2.4764300465872933

Epoch: 6| Step: 5
Training loss: 2.620773727960219
Validation loss: 2.4732855933382356

Epoch: 6| Step: 6
Training loss: 3.297630187408114
Validation loss: 2.482142799409564

Epoch: 6| Step: 7
Training loss: 3.000663842822056
Validation loss: 2.4899399413982

Epoch: 6| Step: 8
Training loss: 2.579968989429849
Validation loss: 2.5044489917618065

Epoch: 6| Step: 9
Training loss: 3.058094670877971
Validation loss: 2.5042912600578404

Epoch: 6| Step: 10
Training loss: 2.6481910073231294
Validation loss: 2.515543880389112

Epoch: 6| Step: 11
Training loss: 2.4836379583359482
Validation loss: 2.494125672598464

Epoch: 6| Step: 12
Training loss: 2.8576581626500652
Validation loss: 2.492845456976255

Epoch: 6| Step: 13
Training loss: 2.635278469478005
Validation loss: 2.4984834767924875

Epoch: 102| Step: 0
Training loss: 2.504020033704426
Validation loss: 2.4988045110569104

Epoch: 6| Step: 1
Training loss: 2.916265641753838
Validation loss: 2.48833889999983

Epoch: 6| Step: 2
Training loss: 2.885833067519899
Validation loss: 2.4706071239573593

Epoch: 6| Step: 3
Training loss: 2.6064006734422125
Validation loss: 2.4664486872594544

Epoch: 6| Step: 4
Training loss: 2.3643743840014357
Validation loss: 2.463875057788989

Epoch: 6| Step: 5
Training loss: 2.575626156424152
Validation loss: 2.468259210947277

Epoch: 6| Step: 6
Training loss: 2.7777813487559784
Validation loss: 2.467315044954938

Epoch: 6| Step: 7
Training loss: 2.6631061705662447
Validation loss: 2.466829567790201

Epoch: 6| Step: 8
Training loss: 2.7354473518070233
Validation loss: 2.4763236934321675

Epoch: 6| Step: 9
Training loss: 3.4888106368401544
Validation loss: 2.4824629293139835

Epoch: 6| Step: 10
Training loss: 2.9192509419389965
Validation loss: 2.4770780351358708

Epoch: 6| Step: 11
Training loss: 2.6086656896971347
Validation loss: 2.4827875188977893

Epoch: 6| Step: 12
Training loss: 2.286103402803676
Validation loss: 2.4908246250340897

Epoch: 6| Step: 13
Training loss: 3.218159000989829
Validation loss: 2.5067001348110085

Epoch: 103| Step: 0
Training loss: 2.7375054450830105
Validation loss: 2.5891335882410904

Epoch: 6| Step: 1
Training loss: 2.367756085991854
Validation loss: 2.6354104403159146

Epoch: 6| Step: 2
Training loss: 3.496304877528683
Validation loss: 2.74519260097582

Epoch: 6| Step: 3
Training loss: 3.0676313734952485
Validation loss: 2.75048871512737

Epoch: 6| Step: 4
Training loss: 2.749894313515367
Validation loss: 2.6988145017913014

Epoch: 6| Step: 5
Training loss: 2.501884322519537
Validation loss: 2.6174612789492935

Epoch: 6| Step: 6
Training loss: 3.017500694293785
Validation loss: 2.5302020128741

Epoch: 6| Step: 7
Training loss: 2.769235527918876
Validation loss: 2.486473620628365

Epoch: 6| Step: 8
Training loss: 2.6665758375752757
Validation loss: 2.4767741430572072

Epoch: 6| Step: 9
Training loss: 2.565947562790209
Validation loss: 2.4649601844760802

Epoch: 6| Step: 10
Training loss: 3.0264927609374577
Validation loss: 2.474675850737322

Epoch: 6| Step: 11
Training loss: 2.706989678838449
Validation loss: 2.476195038241128

Epoch: 6| Step: 12
Training loss: 3.195829988467088
Validation loss: 2.4766879107068287

Epoch: 6| Step: 13
Training loss: 2.0626810167812537
Validation loss: 2.472341704902356

Epoch: 104| Step: 0
Training loss: 2.629736940550397
Validation loss: 2.4698153266689142

Epoch: 6| Step: 1
Training loss: 3.3786553262140386
Validation loss: 2.4711636735103717

Epoch: 6| Step: 2
Training loss: 2.1002027232095144
Validation loss: 2.462589550727227

Epoch: 6| Step: 3
Training loss: 2.417363252766368
Validation loss: 2.465848653754939

Epoch: 6| Step: 4
Training loss: 3.455165760574221
Validation loss: 2.474478831088837

Epoch: 6| Step: 5
Training loss: 3.0734623483759753
Validation loss: 2.4756992924555283

Epoch: 6| Step: 6
Training loss: 2.8015181717182
Validation loss: 2.484273686467957

Epoch: 6| Step: 7
Training loss: 2.575693359356487
Validation loss: 2.48205221927754

Epoch: 6| Step: 8
Training loss: 2.699580961841551
Validation loss: 2.487324124180377

Epoch: 6| Step: 9
Training loss: 2.494312779303613
Validation loss: 2.4883591862880707

Epoch: 6| Step: 10
Training loss: 2.437323343771254
Validation loss: 2.47585146599424

Epoch: 6| Step: 11
Training loss: 2.421404392911021
Validation loss: 2.4819782480604173

Epoch: 6| Step: 12
Training loss: 2.8286476284623823
Validation loss: 2.4727823927408217

Epoch: 6| Step: 13
Training loss: 2.988340130394182
Validation loss: 2.4614015567823815

Epoch: 105| Step: 0
Training loss: 2.9424529828485646
Validation loss: 2.4815464914996173

Epoch: 6| Step: 1
Training loss: 2.6977045474132777
Validation loss: 2.485111898376887

Epoch: 6| Step: 2
Training loss: 2.7832266823290324
Validation loss: 2.495357364942615

Epoch: 6| Step: 3
Training loss: 3.0245464962327056
Validation loss: 2.4977881224536445

Epoch: 6| Step: 4
Training loss: 2.3664186947716677
Validation loss: 2.4938454642332335

Epoch: 6| Step: 5
Training loss: 3.0640035558435907
Validation loss: 2.5005028065223174

Epoch: 6| Step: 6
Training loss: 2.6778700646009215
Validation loss: 2.4955599909337782

Epoch: 6| Step: 7
Training loss: 2.499156046513346
Validation loss: 2.4997364141089347

Epoch: 6| Step: 8
Training loss: 2.3377582509386086
Validation loss: 2.5113973280322597

Epoch: 6| Step: 9
Training loss: 2.895877828644969
Validation loss: 2.5029131272749474

Epoch: 6| Step: 10
Training loss: 3.2084957593616665
Validation loss: 2.502454609913862

Epoch: 6| Step: 11
Training loss: 2.708751225631852
Validation loss: 2.505706805739365

Epoch: 6| Step: 12
Training loss: 2.4828736189082807
Validation loss: 2.4996287695790493

Epoch: 6| Step: 13
Training loss: 2.2220072946986678
Validation loss: 2.495231900081541

Epoch: 106| Step: 0
Training loss: 2.6279631647637727
Validation loss: 2.4997764764349966

Epoch: 6| Step: 1
Training loss: 2.398923942616459
Validation loss: 2.496506405447471

Epoch: 6| Step: 2
Training loss: 2.7941560795742038
Validation loss: 2.4890253810752694

Epoch: 6| Step: 3
Training loss: 2.5069220081944468
Validation loss: 2.491293564604943

Epoch: 6| Step: 4
Training loss: 3.0387621733634647
Validation loss: 2.4910018291008162

Epoch: 6| Step: 5
Training loss: 2.499657416712885
Validation loss: 2.4894483209337857

Epoch: 6| Step: 6
Training loss: 3.171431637299773
Validation loss: 2.495948029407699

Epoch: 6| Step: 7
Training loss: 2.7290382840388134
Validation loss: 2.497586939758137

Epoch: 6| Step: 8
Training loss: 2.1363867688356217
Validation loss: 2.4951111162136406

Epoch: 6| Step: 9
Training loss: 3.2791694948086465
Validation loss: 2.5017815712541482

Epoch: 6| Step: 10
Training loss: 2.6804598482095883
Validation loss: 2.506920924211917

Epoch: 6| Step: 11
Training loss: 2.8805336150565406
Validation loss: 2.5108107115164344

Epoch: 6| Step: 12
Training loss: 2.3562147183394977
Validation loss: 2.508532974457829

Epoch: 6| Step: 13
Training loss: 2.809335836067527
Validation loss: 2.5045414744164813

Epoch: 107| Step: 0
Training loss: 2.8000402175194847
Validation loss: 2.4826705140834253

Epoch: 6| Step: 1
Training loss: 2.8901198487031925
Validation loss: 2.4827293135331274

Epoch: 6| Step: 2
Training loss: 2.6836555840310754
Validation loss: 2.4839542667536425

Epoch: 6| Step: 3
Training loss: 2.5041204828032972
Validation loss: 2.483961754486016

Epoch: 6| Step: 4
Training loss: 2.9087671023834485
Validation loss: 2.4761129146976084

Epoch: 6| Step: 5
Training loss: 2.623077506016469
Validation loss: 2.4630125637418394

Epoch: 6| Step: 6
Training loss: 3.01211517535267
Validation loss: 2.4564419579725723

Epoch: 6| Step: 7
Training loss: 2.822246026099769
Validation loss: 2.4593866735890857

Epoch: 6| Step: 8
Training loss: 2.813983949605992
Validation loss: 2.4619851133495056

Epoch: 6| Step: 9
Training loss: 2.458927169025305
Validation loss: 2.4631519891804743

Epoch: 6| Step: 10
Training loss: 2.9323306761473718
Validation loss: 2.4656160982022395

Epoch: 6| Step: 11
Training loss: 2.3602659046569383
Validation loss: 2.4616955796798825

Epoch: 6| Step: 12
Training loss: 2.267098204354738
Validation loss: 2.4566051032020386

Epoch: 6| Step: 13
Training loss: 3.189787548727668
Validation loss: 2.4882206880381754

Epoch: 108| Step: 0
Training loss: 3.1092681099619672
Validation loss: 2.538597096842216

Epoch: 6| Step: 1
Training loss: 2.927152712034812
Validation loss: 2.5526913295141664

Epoch: 6| Step: 2
Training loss: 2.3699423494047176
Validation loss: 2.531125990183258

Epoch: 6| Step: 3
Training loss: 2.4043250372792144
Validation loss: 2.522656983079483

Epoch: 6| Step: 4
Training loss: 2.93422565181706
Validation loss: 2.504573227211492

Epoch: 6| Step: 5
Training loss: 2.337528261228042
Validation loss: 2.491459792232875

Epoch: 6| Step: 6
Training loss: 2.932335554554491
Validation loss: 2.4787656198229047

Epoch: 6| Step: 7
Training loss: 2.7159514657854165
Validation loss: 2.47277669064597

Epoch: 6| Step: 8
Training loss: 2.9802289193829172
Validation loss: 2.4646205361269597

Epoch: 6| Step: 9
Training loss: 2.8091303666888128
Validation loss: 2.471095922672083

Epoch: 6| Step: 10
Training loss: 2.904701558736234
Validation loss: 2.468672714176322

Epoch: 6| Step: 11
Training loss: 2.316881102190402
Validation loss: 2.4636772004183123

Epoch: 6| Step: 12
Training loss: 2.786644664963499
Validation loss: 2.4639682382685453

Epoch: 6| Step: 13
Training loss: 2.1463133142427058
Validation loss: 2.467854536925714

Epoch: 109| Step: 0
Training loss: 2.5542096265589187
Validation loss: 2.4742165527988838

Epoch: 6| Step: 1
Training loss: 2.3738745984425673
Validation loss: 2.466958341451149

Epoch: 6| Step: 2
Training loss: 3.0216879501757465
Validation loss: 2.468661296180809

Epoch: 6| Step: 3
Training loss: 2.482655632004081
Validation loss: 2.473281698046434

Epoch: 6| Step: 4
Training loss: 2.8925266918114683
Validation loss: 2.4918089419779816

Epoch: 6| Step: 5
Training loss: 2.207184318703297
Validation loss: 2.5160468024368057

Epoch: 6| Step: 6
Training loss: 3.2334170189740927
Validation loss: 2.5471294333110057

Epoch: 6| Step: 7
Training loss: 1.8796443639357654
Validation loss: 2.5496060728848104

Epoch: 6| Step: 8
Training loss: 2.5587704197115877
Validation loss: 2.54784383590141

Epoch: 6| Step: 9
Training loss: 2.8741981383256254
Validation loss: 2.50418513092637

Epoch: 6| Step: 10
Training loss: 3.2903503651142563
Validation loss: 2.493453350439511

Epoch: 6| Step: 11
Training loss: 3.1688094334793955
Validation loss: 2.474758755915204

Epoch: 6| Step: 12
Training loss: 2.4795268034950717
Validation loss: 2.4669175424794205

Epoch: 6| Step: 13
Training loss: 2.748571024606628
Validation loss: 2.4603568078705984

Epoch: 110| Step: 0
Training loss: 2.4152975040522553
Validation loss: 2.457078178081904

Epoch: 6| Step: 1
Training loss: 2.652580688860018
Validation loss: 2.4666873398751226

Epoch: 6| Step: 2
Training loss: 3.0602517731253736
Validation loss: 2.4611446857889856

Epoch: 6| Step: 3
Training loss: 1.903249536708393
Validation loss: 2.4644645098975353

Epoch: 6| Step: 4
Training loss: 2.832049939324108
Validation loss: 2.4648911773017526

Epoch: 6| Step: 5
Training loss: 3.2744497404762947
Validation loss: 2.46953492299017

Epoch: 6| Step: 6
Training loss: 2.6638383966866512
Validation loss: 2.460974623970939

Epoch: 6| Step: 7
Training loss: 2.7288811997848246
Validation loss: 2.464561008181004

Epoch: 6| Step: 8
Training loss: 2.660455762495738
Validation loss: 2.463334141655268

Epoch: 6| Step: 9
Training loss: 2.7352815705578015
Validation loss: 2.483102134050327

Epoch: 6| Step: 10
Training loss: 2.3752258594933573
Validation loss: 2.4961259892377545

Epoch: 6| Step: 11
Training loss: 2.8176609582966905
Validation loss: 2.51595936670673

Epoch: 6| Step: 12
Training loss: 2.5293229856579384
Validation loss: 2.5329340785561802

Epoch: 6| Step: 13
Training loss: 2.9848359236359765
Validation loss: 2.52851150044524

Epoch: 111| Step: 0
Training loss: 2.517707294615938
Validation loss: 2.532184286109376

Epoch: 6| Step: 1
Training loss: 2.5894355279586687
Validation loss: 2.5356711776016168

Epoch: 6| Step: 2
Training loss: 2.8047205675986318
Validation loss: 2.5283300344244863

Epoch: 6| Step: 3
Training loss: 3.03164623069761
Validation loss: 2.509615409821117

Epoch: 6| Step: 4
Training loss: 2.2422202503324637
Validation loss: 2.4820437848357355

Epoch: 6| Step: 5
Training loss: 3.1783869002465215
Validation loss: 2.4683441107785944

Epoch: 6| Step: 6
Training loss: 3.3191300743844905
Validation loss: 2.450559199263678

Epoch: 6| Step: 7
Training loss: 2.147707728507551
Validation loss: 2.445583100639667

Epoch: 6| Step: 8
Training loss: 2.9526951391220067
Validation loss: 2.441897131220053

Epoch: 6| Step: 9
Training loss: 2.528910085199681
Validation loss: 2.4445104310034336

Epoch: 6| Step: 10
Training loss: 2.4532511490380164
Validation loss: 2.4454797360401095

Epoch: 6| Step: 11
Training loss: 2.9940995364719853
Validation loss: 2.4443085854666013

Epoch: 6| Step: 12
Training loss: 2.1284900503430078
Validation loss: 2.4457639358563434

Epoch: 6| Step: 13
Training loss: 2.467588996712288
Validation loss: 2.4853549654168323

Epoch: 112| Step: 0
Training loss: 2.8852852654863126
Validation loss: 2.511530914859092

Epoch: 6| Step: 1
Training loss: 3.2837848862739607
Validation loss: 2.544680413566213

Epoch: 6| Step: 2
Training loss: 2.438573160882019
Validation loss: 2.5730831631584925

Epoch: 6| Step: 3
Training loss: 1.6557908771346972
Validation loss: 2.5856690348766604

Epoch: 6| Step: 4
Training loss: 2.9525887138611457
Validation loss: 2.563206702025559

Epoch: 6| Step: 5
Training loss: 2.815916148384079
Validation loss: 2.5552774825389877

Epoch: 6| Step: 6
Training loss: 2.850568038816281
Validation loss: 2.5631006208750096

Epoch: 6| Step: 7
Training loss: 2.632329970155213
Validation loss: 2.5791973053018893

Epoch: 6| Step: 8
Training loss: 2.977323339748167
Validation loss: 2.5084421934343037

Epoch: 6| Step: 9
Training loss: 2.8465228971615426
Validation loss: 2.4553600282602837

Epoch: 6| Step: 10
Training loss: 3.025661547494662
Validation loss: 2.435154052294592

Epoch: 6| Step: 11
Training loss: 2.1815313381870514
Validation loss: 2.437532643998043

Epoch: 6| Step: 12
Training loss: 2.6779247302530513
Validation loss: 2.4353000903885604

Epoch: 6| Step: 13
Training loss: 2.2772746525484466
Validation loss: 2.42985798538636

Epoch: 113| Step: 0
Training loss: 3.2040011068430307
Validation loss: 2.437225953234415

Epoch: 6| Step: 1
Training loss: 2.8259817189936713
Validation loss: 2.4533547938530886

Epoch: 6| Step: 2
Training loss: 2.5159153741089413
Validation loss: 2.4576109618612176

Epoch: 6| Step: 3
Training loss: 2.724547420330555
Validation loss: 2.4765009237242626

Epoch: 6| Step: 4
Training loss: 2.7822543270376676
Validation loss: 2.488219180693026

Epoch: 6| Step: 5
Training loss: 2.5949465278305457
Validation loss: 2.50843573230994

Epoch: 6| Step: 6
Training loss: 2.6884079220146946
Validation loss: 2.50850467600199

Epoch: 6| Step: 7
Training loss: 2.7473978389111955
Validation loss: 2.508793624095697

Epoch: 6| Step: 8
Training loss: 2.327857699826622
Validation loss: 2.475796405929115

Epoch: 6| Step: 9
Training loss: 3.0038722796559365
Validation loss: 2.45744412836209

Epoch: 6| Step: 10
Training loss: 2.622078768753822
Validation loss: 2.451217114280657

Epoch: 6| Step: 11
Training loss: 2.2013659658217932
Validation loss: 2.442245234496541

Epoch: 6| Step: 12
Training loss: 2.4427934535527087
Validation loss: 2.446388872390737

Epoch: 6| Step: 13
Training loss: 3.2874142827800052
Validation loss: 2.441484091290984

Epoch: 114| Step: 0
Training loss: 3.1570333887218998
Validation loss: 2.4406293486239097

Epoch: 6| Step: 1
Training loss: 2.878424056728613
Validation loss: 2.4655769739301125

Epoch: 6| Step: 2
Training loss: 2.7016144870501795
Validation loss: 2.510345299343349

Epoch: 6| Step: 3
Training loss: 2.520403948908553
Validation loss: 2.5448340965714613

Epoch: 6| Step: 4
Training loss: 3.162883020660165
Validation loss: 2.570194502372793

Epoch: 6| Step: 5
Training loss: 2.6959252586984457
Validation loss: 2.6077941267477085

Epoch: 6| Step: 6
Training loss: 2.6189581958978176
Validation loss: 2.6041865594575815

Epoch: 6| Step: 7
Training loss: 2.3768668617512447
Validation loss: 2.568534764172128

Epoch: 6| Step: 8
Training loss: 3.1959639726883156
Validation loss: 2.5166225484762124

Epoch: 6| Step: 9
Training loss: 2.320784292838677
Validation loss: 2.471393657287421

Epoch: 6| Step: 10
Training loss: 2.48020287237697
Validation loss: 2.4348951945891613

Epoch: 6| Step: 11
Training loss: 1.9609710568904157
Validation loss: 2.4229257231548997

Epoch: 6| Step: 12
Training loss: 2.7750339265416253
Validation loss: 2.423425704212133

Epoch: 6| Step: 13
Training loss: 2.883484139293153
Validation loss: 2.4187748592212444

Epoch: 115| Step: 0
Training loss: 2.3509861439402453
Validation loss: 2.4183072277289566

Epoch: 6| Step: 1
Training loss: 2.440677430277207
Validation loss: 2.419521183769356

Epoch: 6| Step: 2
Training loss: 2.893898095446454
Validation loss: 2.435586881642454

Epoch: 6| Step: 3
Training loss: 2.1865230013398405
Validation loss: 2.449117280130217

Epoch: 6| Step: 4
Training loss: 3.2154780674054257
Validation loss: 2.4721431298589605

Epoch: 6| Step: 5
Training loss: 2.8192375015288897
Validation loss: 2.501857438767768

Epoch: 6| Step: 6
Training loss: 2.3793282471240187
Validation loss: 2.548461059848505

Epoch: 6| Step: 7
Training loss: 3.314799248556018
Validation loss: 2.6077542236861686

Epoch: 6| Step: 8
Training loss: 2.5038137909813596
Validation loss: 2.6238071291997827

Epoch: 6| Step: 9
Training loss: 3.096061382562485
Validation loss: 2.631176523732796

Epoch: 6| Step: 10
Training loss: 3.035294970235257
Validation loss: 2.5555255558053718

Epoch: 6| Step: 11
Training loss: 2.2810089820106327
Validation loss: 2.504322603479839

Epoch: 6| Step: 12
Training loss: 2.6329307642786324
Validation loss: 2.451504426714073

Epoch: 6| Step: 13
Training loss: 2.7467288589226415
Validation loss: 2.432271752619939

Epoch: 116| Step: 0
Training loss: 2.6697958726039883
Validation loss: 2.4267680152442055

Epoch: 6| Step: 1
Training loss: 2.670439127982697
Validation loss: 2.4353210306088133

Epoch: 6| Step: 2
Training loss: 2.6825502635160974
Validation loss: 2.447999936487327

Epoch: 6| Step: 3
Training loss: 2.771756374568908
Validation loss: 2.46765906603422

Epoch: 6| Step: 4
Training loss: 2.8450319523684717
Validation loss: 2.4895571707604853

Epoch: 6| Step: 5
Training loss: 2.850436220749303
Validation loss: 2.5619103558409417

Epoch: 6| Step: 6
Training loss: 3.049629101323577
Validation loss: 2.570151392220076

Epoch: 6| Step: 7
Training loss: 2.3414810387647855
Validation loss: 2.573996815142911

Epoch: 6| Step: 8
Training loss: 2.795976633037017
Validation loss: 2.5469113766369755

Epoch: 6| Step: 9
Training loss: 2.5496083201866018
Validation loss: 2.502081954723568

Epoch: 6| Step: 10
Training loss: 2.711477528192844
Validation loss: 2.4599089140194015

Epoch: 6| Step: 11
Training loss: 2.5234407123746
Validation loss: 2.4343943661816643

Epoch: 6| Step: 12
Training loss: 3.077745443266841
Validation loss: 2.4361596293917884

Epoch: 6| Step: 13
Training loss: 2.1596577571012534
Validation loss: 2.442311712552523

Epoch: 117| Step: 0
Training loss: 2.5199465387213906
Validation loss: 2.4379053727967865

Epoch: 6| Step: 1
Training loss: 2.761303299838011
Validation loss: 2.4386320322519546

Epoch: 6| Step: 2
Training loss: 2.7790034790017284
Validation loss: 2.428703044038687

Epoch: 6| Step: 3
Training loss: 2.968706632598541
Validation loss: 2.437921000204561

Epoch: 6| Step: 4
Training loss: 3.111630795610385
Validation loss: 2.432165629337908

Epoch: 6| Step: 5
Training loss: 2.511093512710675
Validation loss: 2.4526549223907836

Epoch: 6| Step: 6
Training loss: 3.438284489674116
Validation loss: 2.4856964371681936

Epoch: 6| Step: 7
Training loss: 2.5843652284522793
Validation loss: 2.5334170560273837

Epoch: 6| Step: 8
Training loss: 2.818291359951131
Validation loss: 2.553816202559919

Epoch: 6| Step: 9
Training loss: 3.0802007347187796
Validation loss: 2.5575249611490065

Epoch: 6| Step: 10
Training loss: 2.6885426295289943
Validation loss: 2.5503989106504577

Epoch: 6| Step: 11
Training loss: 2.2815045776455025
Validation loss: 2.5249123819135844

Epoch: 6| Step: 12
Training loss: 1.8307426166356888
Validation loss: 2.505640207878234

Epoch: 6| Step: 13
Training loss: 2.1035891832615072
Validation loss: 2.4773262833404304

Epoch: 118| Step: 0
Training loss: 2.928762223677743
Validation loss: 2.4520272230428297

Epoch: 6| Step: 1
Training loss: 2.233358298584434
Validation loss: 2.4450363972540816

Epoch: 6| Step: 2
Training loss: 2.4819938716889616
Validation loss: 2.4508713658337125

Epoch: 6| Step: 3
Training loss: 2.6180538736254224
Validation loss: 2.4478777955441817

Epoch: 6| Step: 4
Training loss: 2.697702161197749
Validation loss: 2.4675194334494663

Epoch: 6| Step: 5
Training loss: 2.4764909703848517
Validation loss: 2.4904163264686745

Epoch: 6| Step: 6
Training loss: 2.6093580051257
Validation loss: 2.4867818149236087

Epoch: 6| Step: 7
Training loss: 2.9121270591766875
Validation loss: 2.5089128500662117

Epoch: 6| Step: 8
Training loss: 3.199629201863701
Validation loss: 2.515302040802065

Epoch: 6| Step: 9
Training loss: 2.7653227387681785
Validation loss: 2.5358014131706645

Epoch: 6| Step: 10
Training loss: 2.6298906090327248
Validation loss: 2.5243086006463007

Epoch: 6| Step: 11
Training loss: 2.439943116259782
Validation loss: 2.5168667220793663

Epoch: 6| Step: 12
Training loss: 2.645391192000877
Validation loss: 2.5106802088769524

Epoch: 6| Step: 13
Training loss: 3.255610831235891
Validation loss: 2.4906804061772845

Epoch: 119| Step: 0
Training loss: 3.0066683091570083
Validation loss: 2.4904780672027207

Epoch: 6| Step: 1
Training loss: 2.617869183028106
Validation loss: 2.4901374930915665

Epoch: 6| Step: 2
Training loss: 2.7476269279783168
Validation loss: 2.485425891292995

Epoch: 6| Step: 3
Training loss: 3.299206528493081
Validation loss: 2.4738100708002086

Epoch: 6| Step: 4
Training loss: 2.7616245626011446
Validation loss: 2.459780597792158

Epoch: 6| Step: 5
Training loss: 2.9531337192951637
Validation loss: 2.4635315322762916

Epoch: 6| Step: 6
Training loss: 2.7591499349151514
Validation loss: 2.4587118856032486

Epoch: 6| Step: 7
Training loss: 2.6019651542400397
Validation loss: 2.4458454778776826

Epoch: 6| Step: 8
Training loss: 2.824051341560002
Validation loss: 2.4546851954991067

Epoch: 6| Step: 9
Training loss: 2.0129286599780762
Validation loss: 2.4613796927419163

Epoch: 6| Step: 10
Training loss: 2.291358915530051
Validation loss: 2.466504275085194

Epoch: 6| Step: 11
Training loss: 2.1316281338396155
Validation loss: 2.4839938209219317

Epoch: 6| Step: 12
Training loss: 2.4857105048390467
Validation loss: 2.5011656310036483

Epoch: 6| Step: 13
Training loss: 3.034787502988217
Validation loss: 2.5058734716828828

Epoch: 120| Step: 0
Training loss: 2.8559788614874555
Validation loss: 2.504840068464034

Epoch: 6| Step: 1
Training loss: 3.175031622781742
Validation loss: 2.5010894872775142

Epoch: 6| Step: 2
Training loss: 2.763859416947242
Validation loss: 2.4914825879588967

Epoch: 6| Step: 3
Training loss: 2.12020703744288
Validation loss: 2.4940892527904346

Epoch: 6| Step: 4
Training loss: 2.607781111869456
Validation loss: 2.4733349535547897

Epoch: 6| Step: 5
Training loss: 2.1250567709127326
Validation loss: 2.4721695029924913

Epoch: 6| Step: 6
Training loss: 2.7402702215002117
Validation loss: 2.447579592134058

Epoch: 6| Step: 7
Training loss: 2.5972591698976975
Validation loss: 2.434567882138671

Epoch: 6| Step: 8
Training loss: 2.589557154291169
Validation loss: 2.4331457668262066

Epoch: 6| Step: 9
Training loss: 2.579497925454548
Validation loss: 2.426928614604049

Epoch: 6| Step: 10
Training loss: 2.7015000238469766
Validation loss: 2.4284450378638374

Epoch: 6| Step: 11
Training loss: 2.6890049646174976
Validation loss: 2.4334666209169837

Epoch: 6| Step: 12
Training loss: 2.956702915845344
Validation loss: 2.43968874819696

Epoch: 6| Step: 13
Training loss: 2.4704433379972275
Validation loss: 2.4540741364967875

Epoch: 121| Step: 0
Training loss: 2.140016343954545
Validation loss: 2.4837012661810505

Epoch: 6| Step: 1
Training loss: 2.4432434493632527
Validation loss: 2.492149732063542

Epoch: 6| Step: 2
Training loss: 2.586841068123621
Validation loss: 2.510456045239582

Epoch: 6| Step: 3
Training loss: 1.880294825994028
Validation loss: 2.5089233302729115

Epoch: 6| Step: 4
Training loss: 3.7007655099312995
Validation loss: 2.4969224127043583

Epoch: 6| Step: 5
Training loss: 2.153685108520133
Validation loss: 2.500546672998944

Epoch: 6| Step: 6
Training loss: 2.439685428697174
Validation loss: 2.4762029542431656

Epoch: 6| Step: 7
Training loss: 2.9850982269023194
Validation loss: 2.4587884825398656

Epoch: 6| Step: 8
Training loss: 2.2891818591561375
Validation loss: 2.4752773787014877

Epoch: 6| Step: 9
Training loss: 2.5882280698333733
Validation loss: 2.491272191353246

Epoch: 6| Step: 10
Training loss: 2.018029013932708
Validation loss: 2.494602732021221

Epoch: 6| Step: 11
Training loss: 3.2974465403586817
Validation loss: 2.522615483307295

Epoch: 6| Step: 12
Training loss: 3.003197237827045
Validation loss: 2.5172795203969045

Epoch: 6| Step: 13
Training loss: 3.0588746703088745
Validation loss: 2.523149841791547

Epoch: 122| Step: 0
Training loss: 3.040441048400516
Validation loss: 2.4818413516991065

Epoch: 6| Step: 1
Training loss: 1.9168175002193972
Validation loss: 2.4559429102084396

Epoch: 6| Step: 2
Training loss: 2.869024452262166
Validation loss: 2.446955994815458

Epoch: 6| Step: 3
Training loss: 2.8824174811932903
Validation loss: 2.4291043641692083

Epoch: 6| Step: 4
Training loss: 2.505323940049706
Validation loss: 2.4413499636488774

Epoch: 6| Step: 5
Training loss: 2.5864557858988557
Validation loss: 2.440635187798651

Epoch: 6| Step: 6
Training loss: 2.939082105166619
Validation loss: 2.448061976225074

Epoch: 6| Step: 7
Training loss: 2.873432893318935
Validation loss: 2.462378441226781

Epoch: 6| Step: 8
Training loss: 2.59473317827064
Validation loss: 2.486656556575582

Epoch: 6| Step: 9
Training loss: 2.77915146769686
Validation loss: 2.532347777971854

Epoch: 6| Step: 10
Training loss: 2.9200365126952836
Validation loss: 2.578244258425535

Epoch: 6| Step: 11
Training loss: 2.164092686852298
Validation loss: 2.5898774226092356

Epoch: 6| Step: 12
Training loss: 2.2818868087926907
Validation loss: 2.5742699751227143

Epoch: 6| Step: 13
Training loss: 2.160893623088646
Validation loss: 2.5574865271750182

Epoch: 123| Step: 0
Training loss: 2.8648096353555803
Validation loss: 2.533760485827924

Epoch: 6| Step: 1
Training loss: 1.8272553682257549
Validation loss: 2.5331374868532284

Epoch: 6| Step: 2
Training loss: 2.6468398414988035
Validation loss: 2.5229147021525287

Epoch: 6| Step: 3
Training loss: 1.8803697624191924
Validation loss: 2.495493483894501

Epoch: 6| Step: 4
Training loss: 2.6329264177529286
Validation loss: 2.4752641766197296

Epoch: 6| Step: 5
Training loss: 2.8290506607864745
Validation loss: 2.4592505774400886

Epoch: 6| Step: 6
Training loss: 2.631957959688681
Validation loss: 2.4443480113551743

Epoch: 6| Step: 7
Training loss: 2.5193164349437014
Validation loss: 2.429181374401722

Epoch: 6| Step: 8
Training loss: 3.2898144077721847
Validation loss: 2.425749666402281

Epoch: 6| Step: 9
Training loss: 2.763844148368527
Validation loss: 2.435422802652314

Epoch: 6| Step: 10
Training loss: 2.6552245909675896
Validation loss: 2.4659153001656247

Epoch: 6| Step: 11
Training loss: 2.842296354569612
Validation loss: 2.490795996734309

Epoch: 6| Step: 12
Training loss: 2.366788421328668
Validation loss: 2.517198858429434

Epoch: 6| Step: 13
Training loss: 3.060115742457917
Validation loss: 2.5234203662504617

Epoch: 124| Step: 0
Training loss: 2.2232853915847337
Validation loss: 2.494339014644769

Epoch: 6| Step: 1
Training loss: 2.6860928954863743
Validation loss: 2.465241961228878

Epoch: 6| Step: 2
Training loss: 3.030945438146889
Validation loss: 2.454708568762763

Epoch: 6| Step: 3
Training loss: 2.577380818554002
Validation loss: 2.440823221924687

Epoch: 6| Step: 4
Training loss: 2.1709844286486293
Validation loss: 2.4312653214175155

Epoch: 6| Step: 5
Training loss: 3.0393461668704855
Validation loss: 2.422698668356426

Epoch: 6| Step: 6
Training loss: 2.8648865325454604
Validation loss: 2.4298801657653866

Epoch: 6| Step: 7
Training loss: 2.499214239614268
Validation loss: 2.4328038823691442

Epoch: 6| Step: 8
Training loss: 2.3569764330352965
Validation loss: 2.4321191502607387

Epoch: 6| Step: 9
Training loss: 2.642272435317093
Validation loss: 2.4389201408547447

Epoch: 6| Step: 10
Training loss: 2.875533676372126
Validation loss: 2.4530132794183093

Epoch: 6| Step: 11
Training loss: 3.0195615700288863
Validation loss: 2.469845239248877

Epoch: 6| Step: 12
Training loss: 2.380069341851511
Validation loss: 2.5117546039050174

Epoch: 6| Step: 13
Training loss: 2.6492230085981605
Validation loss: 2.5948598300740384

Epoch: 125| Step: 0
Training loss: 2.6295720202543587
Validation loss: 2.654651559986

Epoch: 6| Step: 1
Training loss: 2.492874098761875
Validation loss: 2.638132223429712

Epoch: 6| Step: 2
Training loss: 2.5912860059283815
Validation loss: 2.5700152204326274

Epoch: 6| Step: 3
Training loss: 2.7271682372450514
Validation loss: 2.4890634384603083

Epoch: 6| Step: 4
Training loss: 3.0061160369019917
Validation loss: 2.4543848020967327

Epoch: 6| Step: 5
Training loss: 2.8496903636109363
Validation loss: 2.4355329786219126

Epoch: 6| Step: 6
Training loss: 2.3701594364281395
Validation loss: 2.446613915165812

Epoch: 6| Step: 7
Training loss: 2.585099928822538
Validation loss: 2.4463589360339486

Epoch: 6| Step: 8
Training loss: 2.1214717296377295
Validation loss: 2.4579695444073186

Epoch: 6| Step: 9
Training loss: 2.7910310866272656
Validation loss: 2.4509946498806015

Epoch: 6| Step: 10
Training loss: 2.996414107760806
Validation loss: 2.4793386678022715

Epoch: 6| Step: 11
Training loss: 3.0624707278974137
Validation loss: 2.5259666940878955

Epoch: 6| Step: 12
Training loss: 2.7291871475344247
Validation loss: 2.589775260123975

Epoch: 6| Step: 13
Training loss: 2.8854460640154556
Validation loss: 2.6271941424270224

Epoch: 126| Step: 0
Training loss: 2.8215358120506076
Validation loss: 2.556151713910186

Epoch: 6| Step: 1
Training loss: 2.7570224743446956
Validation loss: 2.527223123288571

Epoch: 6| Step: 2
Training loss: 2.623053964703282
Validation loss: 2.502608971388782

Epoch: 6| Step: 3
Training loss: 2.6771237757456867
Validation loss: 2.4895867169076116

Epoch: 6| Step: 4
Training loss: 2.939820043025759
Validation loss: 2.4820060572452998

Epoch: 6| Step: 5
Training loss: 2.304134703466785
Validation loss: 2.4710298226802063

Epoch: 6| Step: 6
Training loss: 2.788866223179415
Validation loss: 2.4628332495256826

Epoch: 6| Step: 7
Training loss: 2.601856386923115
Validation loss: 2.4574135859898596

Epoch: 6| Step: 8
Training loss: 1.8822324500368386
Validation loss: 2.448399526030392

Epoch: 6| Step: 9
Training loss: 2.574852548469971
Validation loss: 2.4431856051519745

Epoch: 6| Step: 10
Training loss: 2.5315813389307795
Validation loss: 2.4448011069908517

Epoch: 6| Step: 11
Training loss: 2.923891075746008
Validation loss: 2.4535033900398258

Epoch: 6| Step: 12
Training loss: 2.48521936810761
Validation loss: 2.4451552301851742

Epoch: 6| Step: 13
Training loss: 2.616059472974132
Validation loss: 2.457228733265305

Epoch: 127| Step: 0
Training loss: 2.0328599858290444
Validation loss: 2.473707320341086

Epoch: 6| Step: 1
Training loss: 2.7951457042624166
Validation loss: 2.4962573647044217

Epoch: 6| Step: 2
Training loss: 2.534047970444939
Validation loss: 2.5376456043871722

Epoch: 6| Step: 3
Training loss: 2.3835802186014847
Validation loss: 2.5529848463213978

Epoch: 6| Step: 4
Training loss: 2.505623500839702
Validation loss: 2.530471761120219

Epoch: 6| Step: 5
Training loss: 2.6494424989259056
Validation loss: 2.530857978988832

Epoch: 6| Step: 6
Training loss: 2.1967683899190997
Validation loss: 2.5098713363487257

Epoch: 6| Step: 7
Training loss: 2.5662836193589817
Validation loss: 2.523128186673634

Epoch: 6| Step: 8
Training loss: 2.901244744818323
Validation loss: 2.517093430373691

Epoch: 6| Step: 9
Training loss: 2.182632043433782
Validation loss: 2.4999518338557447

Epoch: 6| Step: 10
Training loss: 2.899966430469767
Validation loss: 2.4953621483453956

Epoch: 6| Step: 11
Training loss: 2.8310651301980565
Validation loss: 2.4922278050553506

Epoch: 6| Step: 12
Training loss: 2.968274851470475
Validation loss: 2.485614754407559

Epoch: 6| Step: 13
Training loss: 2.8535702678610537
Validation loss: 2.4926520178382083

Epoch: 128| Step: 0
Training loss: 2.542150128353054
Validation loss: 2.4837688940596365

Epoch: 6| Step: 1
Training loss: 2.7751505115003607
Validation loss: 2.48724664865612

Epoch: 6| Step: 2
Training loss: 2.2485181379140173
Validation loss: 2.5010358612774515

Epoch: 6| Step: 3
Training loss: 3.0435448861382
Validation loss: 2.491343474601246

Epoch: 6| Step: 4
Training loss: 1.9504332036772847
Validation loss: 2.5111318808581844

Epoch: 6| Step: 5
Training loss: 2.4939494347237643
Validation loss: 2.538215647686326

Epoch: 6| Step: 6
Training loss: 2.932896029502759
Validation loss: 2.544055396866202

Epoch: 6| Step: 7
Training loss: 2.7752450130312343
Validation loss: 2.522282334380945

Epoch: 6| Step: 8
Training loss: 2.7924294544622237
Validation loss: 2.498046883210064

Epoch: 6| Step: 9
Training loss: 2.2123790125983707
Validation loss: 2.4787608674816917

Epoch: 6| Step: 10
Training loss: 2.125125320329536
Validation loss: 2.4647266828241152

Epoch: 6| Step: 11
Training loss: 2.378045989840685
Validation loss: 2.4611797827469877

Epoch: 6| Step: 12
Training loss: 2.9970173313682196
Validation loss: 2.4540130834457936

Epoch: 6| Step: 13
Training loss: 2.9712062913683224
Validation loss: 2.455185977982408

Epoch: 129| Step: 0
Training loss: 2.40122212844075
Validation loss: 2.513848965614744

Epoch: 6| Step: 1
Training loss: 2.91643454218379
Validation loss: 2.55696340202163

Epoch: 6| Step: 2
Training loss: 2.3401368837993672
Validation loss: 2.5694250449930376

Epoch: 6| Step: 3
Training loss: 2.8030755556438707
Validation loss: 2.5550536636227688

Epoch: 6| Step: 4
Training loss: 2.6497705756147183
Validation loss: 2.541708606136586

Epoch: 6| Step: 5
Training loss: 2.5519611147725265
Validation loss: 2.5456628452103214

Epoch: 6| Step: 6
Training loss: 1.3825457040775253
Validation loss: 2.549391720458542

Epoch: 6| Step: 7
Training loss: 2.9528818181743226
Validation loss: 2.5580488804454165

Epoch: 6| Step: 8
Training loss: 2.449635350085533
Validation loss: 2.6003412717674035

Epoch: 6| Step: 9
Training loss: 2.6204911518555303
Validation loss: 2.5976794026943897

Epoch: 6| Step: 10
Training loss: 2.612545075096156
Validation loss: 2.6160563909897085

Epoch: 6| Step: 11
Training loss: 2.9371009717968963
Validation loss: 2.559230544531019

Epoch: 6| Step: 12
Training loss: 2.3790569790097504
Validation loss: 2.5097751950981326

Epoch: 6| Step: 13
Training loss: 2.9755686304673907
Validation loss: 2.4864665147269145

Epoch: 130| Step: 0
Training loss: 3.1676929885373846
Validation loss: 2.4774805320477427

Epoch: 6| Step: 1
Training loss: 2.659319192769225
Validation loss: 2.466008735182249

Epoch: 6| Step: 2
Training loss: 2.360566804515458
Validation loss: 2.4660804492891857

Epoch: 6| Step: 3
Training loss: 2.5887241619005046
Validation loss: 2.4649725244401064

Epoch: 6| Step: 4
Training loss: 2.683841077576595
Validation loss: 2.4721813807659463

Epoch: 6| Step: 5
Training loss: 2.2502793032752675
Validation loss: 2.503202067404704

Epoch: 6| Step: 6
Training loss: 2.608817583496393
Validation loss: 2.527831716351519

Epoch: 6| Step: 7
Training loss: 2.382921360218246
Validation loss: 2.5855616077070858

Epoch: 6| Step: 8
Training loss: 2.5550801853206035
Validation loss: 2.5962094535384663

Epoch: 6| Step: 9
Training loss: 2.5824005740026807
Validation loss: 2.6031245164868673

Epoch: 6| Step: 10
Training loss: 2.4687668039257025
Validation loss: 2.5916794143688753

Epoch: 6| Step: 11
Training loss: 2.3836224289490695
Validation loss: 2.5724701138719244

Epoch: 6| Step: 12
Training loss: 2.318941989569547
Validation loss: 2.534117734800923

Epoch: 6| Step: 13
Training loss: 3.2728821640737555
Validation loss: 2.5189277411350393

Epoch: 131| Step: 0
Training loss: 2.9390454386611884
Validation loss: 2.5106921607544193

Epoch: 6| Step: 1
Training loss: 2.5221597843965684
Validation loss: 2.5151431906905315

Epoch: 6| Step: 2
Training loss: 2.5489977082294204
Validation loss: 2.5180351779088324

Epoch: 6| Step: 3
Training loss: 2.9787658386193576
Validation loss: 2.518115039855698

Epoch: 6| Step: 4
Training loss: 2.9244456686998386
Validation loss: 2.5092480325230966

Epoch: 6| Step: 5
Training loss: 2.029253168892909
Validation loss: 2.5094971672944877

Epoch: 6| Step: 6
Training loss: 1.7935586594665673
Validation loss: 2.5286520150422103

Epoch: 6| Step: 7
Training loss: 2.1474448390355545
Validation loss: 2.5479737249694407

Epoch: 6| Step: 8
Training loss: 2.417725977627131
Validation loss: 2.577319355238268

Epoch: 6| Step: 9
Training loss: 2.5629218149751063
Validation loss: 2.5886586649085372

Epoch: 6| Step: 10
Training loss: 2.3621016151347445
Validation loss: 2.5758427128341186

Epoch: 6| Step: 11
Training loss: 2.424252628897372
Validation loss: 2.5783626999943117

Epoch: 6| Step: 12
Training loss: 3.0763033206043464
Validation loss: 2.55335885059125

Epoch: 6| Step: 13
Training loss: 2.504459409269438
Validation loss: 2.5407005560472737

Epoch: 132| Step: 0
Training loss: 2.2203304410849847
Validation loss: 2.585997185202484

Epoch: 6| Step: 1
Training loss: 2.1731697449618337
Validation loss: 2.621118855249553

Epoch: 6| Step: 2
Training loss: 2.7487478873612083
Validation loss: 2.6387625204361607

Epoch: 6| Step: 3
Training loss: 3.0177326644287032
Validation loss: 2.6375929552035244

Epoch: 6| Step: 4
Training loss: 1.8073926855285622
Validation loss: 2.6151218638389158

Epoch: 6| Step: 5
Training loss: 2.5635516172048276
Validation loss: 2.6255672855593772

Epoch: 6| Step: 6
Training loss: 2.6880446480948503
Validation loss: 2.6114284527922234

Epoch: 6| Step: 7
Training loss: 2.562546520276091
Validation loss: 2.529587679259892

Epoch: 6| Step: 8
Training loss: 2.7621936956241857
Validation loss: 2.5043565630175197

Epoch: 6| Step: 9
Training loss: 2.5410876839321466
Validation loss: 2.4789319118979765

Epoch: 6| Step: 10
Training loss: 2.561217568649716
Validation loss: 2.457782338086422

Epoch: 6| Step: 11
Training loss: 2.5656226136060574
Validation loss: 2.455371354627558

Epoch: 6| Step: 12
Training loss: 2.8423460125039957
Validation loss: 2.453715787087202

Epoch: 6| Step: 13
Training loss: 3.2800494222871834
Validation loss: 2.461216493708675

Epoch: 133| Step: 0
Training loss: 2.703968776364603
Validation loss: 2.472643080579435

Epoch: 6| Step: 1
Training loss: 1.853916265602884
Validation loss: 2.480825330706389

Epoch: 6| Step: 2
Training loss: 2.5355559094282314
Validation loss: 2.486762329675165

Epoch: 6| Step: 3
Training loss: 2.632629931540951
Validation loss: 2.5263919047514896

Epoch: 6| Step: 4
Training loss: 2.349890028632719
Validation loss: 2.543661601171908

Epoch: 6| Step: 5
Training loss: 3.268810416077213
Validation loss: 2.564025195907232

Epoch: 6| Step: 6
Training loss: 2.900335713390126
Validation loss: 2.5782872591140973

Epoch: 6| Step: 7
Training loss: 2.5032402974028978
Validation loss: 2.5843465880742156

Epoch: 6| Step: 8
Training loss: 2.1824914541874656
Validation loss: 2.602040408145668

Epoch: 6| Step: 9
Training loss: 2.258828538416937
Validation loss: 2.5986719638088567

Epoch: 6| Step: 10
Training loss: 2.607903711039241
Validation loss: 2.571360378741464

Epoch: 6| Step: 11
Training loss: 2.524672734420062
Validation loss: 2.533111576430456

Epoch: 6| Step: 12
Training loss: 2.5708941233499565
Validation loss: 2.508338554869381

Epoch: 6| Step: 13
Training loss: 2.657535197185502
Validation loss: 2.4727680286203584

Epoch: 134| Step: 0
Training loss: 2.620669471296066
Validation loss: 2.4463426845058325

Epoch: 6| Step: 1
Training loss: 2.4626025183391
Validation loss: 2.4405048865323775

Epoch: 6| Step: 2
Training loss: 2.424188997391594
Validation loss: 2.4350485536592257

Epoch: 6| Step: 3
Training loss: 2.3907429784448087
Validation loss: 2.433040350752056

Epoch: 6| Step: 4
Training loss: 2.5084089003762555
Validation loss: 2.4272686174919698

Epoch: 6| Step: 5
Training loss: 3.084498898260322
Validation loss: 2.4322138466866776

Epoch: 6| Step: 6
Training loss: 2.731709778100078
Validation loss: 2.429952684474631

Epoch: 6| Step: 7
Training loss: 2.5798741735781148
Validation loss: 2.445639426885914

Epoch: 6| Step: 8
Training loss: 2.4119897727812747
Validation loss: 2.469519898426864

Epoch: 6| Step: 9
Training loss: 2.222347298387099
Validation loss: 2.5113859991407192

Epoch: 6| Step: 10
Training loss: 3.13144812042809
Validation loss: 2.5759720002091293

Epoch: 6| Step: 11
Training loss: 2.744245056120089
Validation loss: 2.6334110053328716

Epoch: 6| Step: 12
Training loss: 2.6372457698130316
Validation loss: 2.7209483419554488

Epoch: 6| Step: 13
Training loss: 1.5267718481009416
Validation loss: 2.689618149017468

Epoch: 135| Step: 0
Training loss: 2.19918817800409
Validation loss: 2.679911487117423

Epoch: 6| Step: 1
Training loss: 2.094631180447883
Validation loss: 2.675052898605787

Epoch: 6| Step: 2
Training loss: 2.571848123625517
Validation loss: 2.64561256131634

Epoch: 6| Step: 3
Training loss: 2.2291030042833
Validation loss: 2.617048233090745

Epoch: 6| Step: 4
Training loss: 2.485595691134988
Validation loss: 2.5783101422756056

Epoch: 6| Step: 5
Training loss: 2.5307976589340977
Validation loss: 2.5605499246784893

Epoch: 6| Step: 6
Training loss: 2.885633623433752
Validation loss: 2.5462552228788438

Epoch: 6| Step: 7
Training loss: 2.5334572293285147
Validation loss: 2.555773865793109

Epoch: 6| Step: 8
Training loss: 2.3760773072558377
Validation loss: 2.5252263517483855

Epoch: 6| Step: 9
Training loss: 2.459656978665471
Validation loss: 2.4986127265289078

Epoch: 6| Step: 10
Training loss: 2.550504286773854
Validation loss: 2.486784334458898

Epoch: 6| Step: 11
Training loss: 2.5498581715953272
Validation loss: 2.4983944423587703

Epoch: 6| Step: 12
Training loss: 2.898030789777102
Validation loss: 2.5018724033677855

Epoch: 6| Step: 13
Training loss: 2.7642361014424623
Validation loss: 2.5082418509783855

Epoch: 136| Step: 0
Training loss: 2.85653695766446
Validation loss: 2.500227018529004

Epoch: 6| Step: 1
Training loss: 1.8183752401378912
Validation loss: 2.5109427909249864

Epoch: 6| Step: 2
Training loss: 2.9531311358029844
Validation loss: 2.520998413842218

Epoch: 6| Step: 3
Training loss: 3.016062176101458
Validation loss: 2.51633385039453

Epoch: 6| Step: 4
Training loss: 2.0963241922514166
Validation loss: 2.512792148011201

Epoch: 6| Step: 5
Training loss: 2.5960699529759648
Validation loss: 2.5148499240711595

Epoch: 6| Step: 6
Training loss: 2.769255071517076
Validation loss: 2.507284248906472

Epoch: 6| Step: 7
Training loss: 2.0479346873776634
Validation loss: 2.5196312282523112

Epoch: 6| Step: 8
Training loss: 2.4787056972528525
Validation loss: 2.5255861609772885

Epoch: 6| Step: 9
Training loss: 2.623529022056271
Validation loss: 2.5477592201141945

Epoch: 6| Step: 10
Training loss: 2.674634166907379
Validation loss: 2.5563321200659854

Epoch: 6| Step: 11
Training loss: 2.4805910095361945
Validation loss: 2.5709815136011094

Epoch: 6| Step: 12
Training loss: 1.99525275682403
Validation loss: 2.587753598499047

Epoch: 6| Step: 13
Training loss: 1.3710469294973882
Validation loss: 2.624403431143897

Epoch: 137| Step: 0
Training loss: 2.571964185426804
Validation loss: 2.663089090202657

Epoch: 6| Step: 1
Training loss: 1.7354621572893905
Validation loss: 2.7025365749164365

Epoch: 6| Step: 2
Training loss: 2.5385627137319835
Validation loss: 2.7239932056813467

Epoch: 6| Step: 3
Training loss: 1.5522433290219952
Validation loss: 2.6605017513286318

Epoch: 6| Step: 4
Training loss: 2.6787454712046666
Validation loss: 2.5940284297870098

Epoch: 6| Step: 5
Training loss: 2.9557344725080865
Validation loss: 2.520258426492425

Epoch: 6| Step: 6
Training loss: 2.363203567615314
Validation loss: 2.4975666960882528

Epoch: 6| Step: 7
Training loss: 2.492552727511751
Validation loss: 2.491389035045262

Epoch: 6| Step: 8
Training loss: 2.440557860404647
Validation loss: 2.484787084180431

Epoch: 6| Step: 9
Training loss: 2.8100746717747036
Validation loss: 2.4726796586740867

Epoch: 6| Step: 10
Training loss: 3.0994761485821893
Validation loss: 2.499759012317988

Epoch: 6| Step: 11
Training loss: 2.538361156141653
Validation loss: 2.5016814432724814

Epoch: 6| Step: 12
Training loss: 2.7201679960430343
Validation loss: 2.5157090049928295

Epoch: 6| Step: 13
Training loss: 2.001248923402076
Validation loss: 2.5306092543469454

Epoch: 138| Step: 0
Training loss: 2.58011250036241
Validation loss: 2.5475597275220445

Epoch: 6| Step: 1
Training loss: 2.8117803712596707
Validation loss: 2.526994765202574

Epoch: 6| Step: 2
Training loss: 2.6388506825229996
Validation loss: 2.4988578453610604

Epoch: 6| Step: 3
Training loss: 2.5580884594687663
Validation loss: 2.485843029941857

Epoch: 6| Step: 4
Training loss: 1.685915591687111
Validation loss: 2.476622804633611

Epoch: 6| Step: 5
Training loss: 2.4507002783384277
Validation loss: 2.4602859892555697

Epoch: 6| Step: 6
Training loss: 2.2679248580864115
Validation loss: 2.4646510744828727

Epoch: 6| Step: 7
Training loss: 2.4392734824893023
Validation loss: 2.450576731547629

Epoch: 6| Step: 8
Training loss: 2.610244332054465
Validation loss: 2.460371979041077

Epoch: 6| Step: 9
Training loss: 2.4673869559105954
Validation loss: 2.488313498764299

Epoch: 6| Step: 10
Training loss: 2.8806392262281943
Validation loss: 2.521847376461391

Epoch: 6| Step: 11
Training loss: 2.7580526533737055
Validation loss: 2.5537674397262933

Epoch: 6| Step: 12
Training loss: 1.6370948231987184
Validation loss: 2.629658395413868

Epoch: 6| Step: 13
Training loss: 2.6702764536576997
Validation loss: 2.6792597777779243

Epoch: 139| Step: 0
Training loss: 3.083596725211147
Validation loss: 2.7128484358217557

Epoch: 6| Step: 1
Training loss: 2.411645759785404
Validation loss: 2.6894536254404784

Epoch: 6| Step: 2
Training loss: 2.1790411753167
Validation loss: 2.6346809391050563

Epoch: 6| Step: 3
Training loss: 1.9428287567641522
Validation loss: 2.587101576340635

Epoch: 6| Step: 4
Training loss: 2.0886224618506266
Validation loss: 2.5301252170441635

Epoch: 6| Step: 5
Training loss: 2.40824074572703
Validation loss: 2.501854154618359

Epoch: 6| Step: 6
Training loss: 3.083419970420711
Validation loss: 2.4692429891056444

Epoch: 6| Step: 7
Training loss: 2.558874403142829
Validation loss: 2.4652184268135673

Epoch: 6| Step: 8
Training loss: 2.1288554054278177
Validation loss: 2.468348970937472

Epoch: 6| Step: 9
Training loss: 2.2275116620684345
Validation loss: 2.456529226105736

Epoch: 6| Step: 10
Training loss: 2.4671714661399116
Validation loss: 2.4641699237116637

Epoch: 6| Step: 11
Training loss: 2.4670449658437263
Validation loss: 2.4634314897033094

Epoch: 6| Step: 12
Training loss: 2.8825848910824594
Validation loss: 2.453335257332615

Epoch: 6| Step: 13
Training loss: 2.7812411704619766
Validation loss: 2.472323323727711

Epoch: 140| Step: 0
Training loss: 2.29665160876885
Validation loss: 2.505408955539199

Epoch: 6| Step: 1
Training loss: 2.7265621502283084
Validation loss: 2.5814569852453317

Epoch: 6| Step: 2
Training loss: 2.7825113929316503
Validation loss: 2.6525214725595743

Epoch: 6| Step: 3
Training loss: 2.8966861996652407
Validation loss: 2.6607719049758987

Epoch: 6| Step: 4
Training loss: 2.049723964808232
Validation loss: 2.636487494642429

Epoch: 6| Step: 5
Training loss: 2.4250058518171986
Validation loss: 2.6001662454393184

Epoch: 6| Step: 6
Training loss: 2.6221703309601896
Validation loss: 2.5315262463856802

Epoch: 6| Step: 7
Training loss: 2.351815954598012
Validation loss: 2.4836699381188914

Epoch: 6| Step: 8
Training loss: 2.0479462128296513
Validation loss: 2.468708049915683

Epoch: 6| Step: 9
Training loss: 2.6347446313563174
Validation loss: 2.4547511476348784

Epoch: 6| Step: 10
Training loss: 2.2842523335765046
Validation loss: 2.4498152855124418

Epoch: 6| Step: 11
Training loss: 2.57691936624043
Validation loss: 2.4601729489796726

Epoch: 6| Step: 12
Training loss: 2.487213911924476
Validation loss: 2.469094574945661

Epoch: 6| Step: 13
Training loss: 2.9622506724838225
Validation loss: 2.4977642378150993

Epoch: 141| Step: 0
Training loss: 2.465893118510558
Validation loss: 2.534864992316476

Epoch: 6| Step: 1
Training loss: 2.068802999411088
Validation loss: 2.618608414525894

Epoch: 6| Step: 2
Training loss: 2.245693429848669
Validation loss: 2.689707566476703

Epoch: 6| Step: 3
Training loss: 2.20545944011911
Validation loss: 2.742396256483222

Epoch: 6| Step: 4
Training loss: 3.075450684309715
Validation loss: 2.7548218867112078

Epoch: 6| Step: 5
Training loss: 2.5644197483607223
Validation loss: 2.696290967909212

Epoch: 6| Step: 6
Training loss: 2.920306758966819
Validation loss: 2.6486336214411113

Epoch: 6| Step: 7
Training loss: 2.5190556040249357
Validation loss: 2.567463583325462

Epoch: 6| Step: 8
Training loss: 2.4633085908107435
Validation loss: 2.526989372105466

Epoch: 6| Step: 9
Training loss: 2.219768021981381
Validation loss: 2.4951606621639275

Epoch: 6| Step: 10
Training loss: 2.499034790633404
Validation loss: 2.4896557197099423

Epoch: 6| Step: 11
Training loss: 2.256594213715336
Validation loss: 2.4703938919850175

Epoch: 6| Step: 12
Training loss: 2.5194033094938715
Validation loss: 2.4603459056180013

Epoch: 6| Step: 13
Training loss: 2.7360001448469515
Validation loss: 2.478455340562364

Epoch: 142| Step: 0
Training loss: 1.8936525804496784
Validation loss: 2.470279546366629

Epoch: 6| Step: 1
Training loss: 2.298128162472115
Validation loss: 2.487382889891685

Epoch: 6| Step: 2
Training loss: 2.256340101139023
Validation loss: 2.510328265164468

Epoch: 6| Step: 3
Training loss: 2.064617803640631
Validation loss: 2.521387710129726

Epoch: 6| Step: 4
Training loss: 2.352596828563822
Validation loss: 2.5248391881495236

Epoch: 6| Step: 5
Training loss: 2.5701494252148316
Validation loss: 2.5435240636211076

Epoch: 6| Step: 6
Training loss: 2.9312476314705918
Validation loss: 2.536466222301319

Epoch: 6| Step: 7
Training loss: 2.6667846017825827
Validation loss: 2.55267040003897

Epoch: 6| Step: 8
Training loss: 3.1013131221572965
Validation loss: 2.576822115121433

Epoch: 6| Step: 9
Training loss: 2.5757906431397677
Validation loss: 2.566687169557123

Epoch: 6| Step: 10
Training loss: 2.4520717230570295
Validation loss: 2.5657449541041752

Epoch: 6| Step: 11
Training loss: 2.169930994545137
Validation loss: 2.531013378455789

Epoch: 6| Step: 12
Training loss: 2.1784490533089245
Validation loss: 2.531872557184727

Epoch: 6| Step: 13
Training loss: 1.8981755707818013
Validation loss: 2.4907909801865644

Epoch: 143| Step: 0
Training loss: 2.1588824139546636
Validation loss: 2.502487655392349

Epoch: 6| Step: 1
Training loss: 1.6765411716892797
Validation loss: 2.493131526862313

Epoch: 6| Step: 2
Training loss: 2.706402858443277
Validation loss: 2.4838767640434143

Epoch: 6| Step: 3
Training loss: 2.6660350309790513
Validation loss: 2.491516205974998

Epoch: 6| Step: 4
Training loss: 3.2681355294521044
Validation loss: 2.514653820363014

Epoch: 6| Step: 5
Training loss: 2.6422415756566275
Validation loss: 2.5329364155469243

Epoch: 6| Step: 6
Training loss: 3.0270295010790416
Validation loss: 2.5292581458734893

Epoch: 6| Step: 7
Training loss: 1.9231173833845605
Validation loss: 2.531107488965619

Epoch: 6| Step: 8
Training loss: 1.6254271899425972
Validation loss: 2.5351227434931882

Epoch: 6| Step: 9
Training loss: 2.694380977927496
Validation loss: 2.5363186973018697

Epoch: 6| Step: 10
Training loss: 2.0846688631048407
Validation loss: 2.521081016479364

Epoch: 6| Step: 11
Training loss: 2.3096993472807457
Validation loss: 2.535988859848766

Epoch: 6| Step: 12
Training loss: 2.151989916398486
Validation loss: 2.525783828517792

Epoch: 6| Step: 13
Training loss: 1.6122858334044872
Validation loss: 2.5404286238092286

Epoch: 144| Step: 0
Training loss: 2.4702927802768815
Validation loss: 2.5801402340185318

Epoch: 6| Step: 1
Training loss: 2.1669025170510325
Validation loss: 2.5692263466211864

Epoch: 6| Step: 2
Training loss: 3.3163035885722447
Validation loss: 2.5819692986675227

Epoch: 6| Step: 3
Training loss: 2.1323552462428017
Validation loss: 2.570783225721779

Epoch: 6| Step: 4
Training loss: 2.6104030753895877
Validation loss: 2.5725015743254027

Epoch: 6| Step: 5
Training loss: 1.9925735398796736
Validation loss: 2.56389512211919

Epoch: 6| Step: 6
Training loss: 2.9208029345909052
Validation loss: 2.5484883191084173

Epoch: 6| Step: 7
Training loss: 2.248055677620418
Validation loss: 2.535641841373937

Epoch: 6| Step: 8
Training loss: 2.1851732140851947
Validation loss: 2.5496087475256446

Epoch: 6| Step: 9
Training loss: 1.7214927290542135
Validation loss: 2.545793350067138

Epoch: 6| Step: 10
Training loss: 2.541368394036761
Validation loss: 2.559303056243473

Epoch: 6| Step: 11
Training loss: 2.105131274409061
Validation loss: 2.586862413860129

Epoch: 6| Step: 12
Training loss: 2.062308447063353
Validation loss: 2.5966154327807995

Epoch: 6| Step: 13
Training loss: 2.2854229111239848
Validation loss: 2.5839785432709825

Epoch: 145| Step: 0
Training loss: 2.589744876683059
Validation loss: 2.5498260698109907

Epoch: 6| Step: 1
Training loss: 2.615024321754798
Validation loss: 2.5210490536575665

Epoch: 6| Step: 2
Training loss: 2.5091868404851536
Validation loss: 2.5183414403192677

Epoch: 6| Step: 3
Training loss: 1.7455670204463285
Validation loss: 2.506182778717466

Epoch: 6| Step: 4
Training loss: 2.6642277909625993
Validation loss: 2.506649491482116

Epoch: 6| Step: 5
Training loss: 2.148576877581287
Validation loss: 2.5083168301379946

Epoch: 6| Step: 6
Training loss: 2.854261484914719
Validation loss: 2.5133094543220715

Epoch: 6| Step: 7
Training loss: 1.9493540867854444
Validation loss: 2.5365342676745164

Epoch: 6| Step: 8
Training loss: 2.4453193250079477
Validation loss: 2.5448014186420944

Epoch: 6| Step: 9
Training loss: 1.6424922671201343
Validation loss: 2.559490779991603

Epoch: 6| Step: 10
Training loss: 2.4277918249500283
Validation loss: 2.562638712964555

Epoch: 6| Step: 11
Training loss: 2.4692841929715543
Validation loss: 2.553970260815977

Epoch: 6| Step: 12
Training loss: 2.1623318910028924
Validation loss: 2.5700861550697303

Epoch: 6| Step: 13
Training loss: 2.2201744127923315
Validation loss: 2.574019919704736

Epoch: 146| Step: 0
Training loss: 2.3320837989394545
Validation loss: 2.6279503609825934

Epoch: 6| Step: 1
Training loss: 2.442733721136864
Validation loss: 2.6655424229147693

Epoch: 6| Step: 2
Training loss: 2.793768494820021
Validation loss: 2.6137032646567313

Epoch: 6| Step: 3
Training loss: 1.884018950725418
Validation loss: 2.5087347845971357

Epoch: 6| Step: 4
Training loss: 2.800711105511099
Validation loss: 2.462677557689178

Epoch: 6| Step: 5
Training loss: 2.6264067014182864
Validation loss: 2.4534609849111306

Epoch: 6| Step: 6
Training loss: 2.448385726683191
Validation loss: 2.4546568130813458

Epoch: 6| Step: 7
Training loss: 1.9234738406134595
Validation loss: 2.486660884537155

Epoch: 6| Step: 8
Training loss: 2.597054456255179
Validation loss: 2.4998329527225134

Epoch: 6| Step: 9
Training loss: 2.3235300277828443
Validation loss: 2.482974647814292

Epoch: 6| Step: 10
Training loss: 2.3248256371858624
Validation loss: 2.472493903119409

Epoch: 6| Step: 11
Training loss: 2.450924609353552
Validation loss: 2.437974126963365

Epoch: 6| Step: 12
Training loss: 1.6750933606903253
Validation loss: 2.428409748614753

Epoch: 6| Step: 13
Training loss: 2.72191278056959
Validation loss: 2.4522916429766495

Epoch: 147| Step: 0
Training loss: 2.2588634750834684
Validation loss: 2.4737325119467166

Epoch: 6| Step: 1
Training loss: 2.1465223619676657
Validation loss: 2.5035085402722377

Epoch: 6| Step: 2
Training loss: 2.2842995105382338
Validation loss: 2.537031239177265

Epoch: 6| Step: 3
Training loss: 2.5887808020880727
Validation loss: 2.5857810697550603

Epoch: 6| Step: 4
Training loss: 2.2346884767671313
Validation loss: 2.620956987306855

Epoch: 6| Step: 5
Training loss: 2.8204957685866567
Validation loss: 2.643325152317059

Epoch: 6| Step: 6
Training loss: 1.8340989667133822
Validation loss: 2.6182171635976097

Epoch: 6| Step: 7
Training loss: 1.7056169390461713
Validation loss: 2.6080992081881056

Epoch: 6| Step: 8
Training loss: 2.318210047449687
Validation loss: 2.5783856779372885

Epoch: 6| Step: 9
Training loss: 2.064785586810585
Validation loss: 2.569394670348201

Epoch: 6| Step: 10
Training loss: 2.7437304207140163
Validation loss: 2.578837750933686

Epoch: 6| Step: 11
Training loss: 2.286131665320355
Validation loss: 2.5753583046906376

Epoch: 6| Step: 12
Training loss: 2.4971443078777917
Validation loss: 2.5585423643251173

Epoch: 6| Step: 13
Training loss: 2.8268656587583423
Validation loss: 2.547186486126482

Epoch: 148| Step: 0
Training loss: 2.66708202902161
Validation loss: 2.5345575985389086

Epoch: 6| Step: 1
Training loss: 2.452003368369149
Validation loss: 2.5298827547799463

Epoch: 6| Step: 2
Training loss: 2.6136124975449206
Validation loss: 2.523266229405226

Epoch: 6| Step: 3
Training loss: 2.526635378775551
Validation loss: 2.5014722898283894

Epoch: 6| Step: 4
Training loss: 1.6806194470173041
Validation loss: 2.467447261413562

Epoch: 6| Step: 5
Training loss: 2.275782264109476
Validation loss: 2.447037159770553

Epoch: 6| Step: 6
Training loss: 2.470183620191454
Validation loss: 2.449633330263086

Epoch: 6| Step: 7
Training loss: 2.0971654108140685
Validation loss: 2.4372498757569665

Epoch: 6| Step: 8
Training loss: 2.3376752328723436
Validation loss: 2.4407817586928404

Epoch: 6| Step: 9
Training loss: 2.28740791177131
Validation loss: 2.4629844963878753

Epoch: 6| Step: 10
Training loss: 2.2380637284546907
Validation loss: 2.4999307899739422

Epoch: 6| Step: 11
Training loss: 2.070911479168828
Validation loss: 2.5503218806174184

Epoch: 6| Step: 12
Training loss: 2.1043392400192893
Validation loss: 2.569374241119755

Epoch: 6| Step: 13
Training loss: 2.278209175051335
Validation loss: 2.602460414627224

Epoch: 149| Step: 0
Training loss: 2.142608169442381
Validation loss: 2.6424154507683983

Epoch: 6| Step: 1
Training loss: 2.263182276917767
Validation loss: 2.6234423561654427

Epoch: 6| Step: 2
Training loss: 2.0396727602417184
Validation loss: 2.615187455827625

Epoch: 6| Step: 3
Training loss: 2.012447010551055
Validation loss: 2.5570051383880275

Epoch: 6| Step: 4
Training loss: 2.3613110613462767
Validation loss: 2.533179263587944

Epoch: 6| Step: 5
Training loss: 2.2156231835792317
Validation loss: 2.525793821056285

Epoch: 6| Step: 6
Training loss: 2.06690058717442
Validation loss: 2.555557548878258

Epoch: 6| Step: 7
Training loss: 1.9149668522846868
Validation loss: 2.594210037687887

Epoch: 6| Step: 8
Training loss: 2.4811107375414263
Validation loss: 2.61967513864716

Epoch: 6| Step: 9
Training loss: 2.4502377375262894
Validation loss: 2.6308972632879013

Epoch: 6| Step: 10
Training loss: 2.277674341825378
Validation loss: 2.593120147102851

Epoch: 6| Step: 11
Training loss: 2.6714931460561617
Validation loss: 2.5290654979203047

Epoch: 6| Step: 12
Training loss: 2.438367053633359
Validation loss: 2.47306421017048

Epoch: 6| Step: 13
Training loss: 2.779713066716845
Validation loss: 2.4435017615589474

Epoch: 150| Step: 0
Training loss: 2.4022620706706546
Validation loss: 2.4169345434264624

Epoch: 6| Step: 1
Training loss: 2.2540472505519813
Validation loss: 2.420436133993674

Epoch: 6| Step: 2
Training loss: 2.0921697773941306
Validation loss: 2.4437540657759294

Epoch: 6| Step: 3
Training loss: 2.167495593426195
Validation loss: 2.426721253194697

Epoch: 6| Step: 4
Training loss: 2.061511178389059
Validation loss: 2.4471422054696736

Epoch: 6| Step: 5
Training loss: 1.793746413915153
Validation loss: 2.465057739227723

Epoch: 6| Step: 6
Training loss: 2.71568160266951
Validation loss: 2.5279668381714684

Epoch: 6| Step: 7
Training loss: 2.1685657126101683
Validation loss: 2.5666067763328795

Epoch: 6| Step: 8
Training loss: 2.077650890907157
Validation loss: 2.607445799314663

Epoch: 6| Step: 9
Training loss: 3.025794084217875
Validation loss: 2.688484701176011

Epoch: 6| Step: 10
Training loss: 2.273704696631766
Validation loss: 2.6711690167063886

Epoch: 6| Step: 11
Training loss: 2.379833723398631
Validation loss: 2.636745289342741

Epoch: 6| Step: 12
Training loss: 1.9472095477601232
Validation loss: 2.562595979907457

Epoch: 6| Step: 13
Training loss: 2.589611842846829
Validation loss: 2.484873933043686

Epoch: 151| Step: 0
Training loss: 2.6116045717778666
Validation loss: 2.4521524353631183

Epoch: 6| Step: 1
Training loss: 2.130472429563998
Validation loss: 2.4408552963259136

Epoch: 6| Step: 2
Training loss: 2.5893545940238716
Validation loss: 2.4344012260116914

Epoch: 6| Step: 3
Training loss: 2.654534358052473
Validation loss: 2.4309233681225297

Epoch: 6| Step: 4
Training loss: 1.7414383581939972
Validation loss: 2.441343334937827

Epoch: 6| Step: 5
Training loss: 2.341359967094747
Validation loss: 2.4115421806207324

Epoch: 6| Step: 6
Training loss: 1.8833311007776221
Validation loss: 2.434242254962477

Epoch: 6| Step: 7
Training loss: 2.3311582941012357
Validation loss: 2.481891740998912

Epoch: 6| Step: 8
Training loss: 2.2095623794800354
Validation loss: 2.617986273544958

Epoch: 6| Step: 9
Training loss: 1.6596831960448115
Validation loss: 2.748968194643886

Epoch: 6| Step: 10
Training loss: 2.204750165041991
Validation loss: 2.835925103647517

Epoch: 6| Step: 11
Training loss: 2.4527606086038842
Validation loss: 2.873966750973613

Epoch: 6| Step: 12
Training loss: 2.628767488850433
Validation loss: 2.7810691349757084

Epoch: 6| Step: 13
Training loss: 2.679522183938082
Validation loss: 2.647067797389118

Epoch: 152| Step: 0
Training loss: 2.3904339084421227
Validation loss: 2.5289092164300024

Epoch: 6| Step: 1
Training loss: 2.8439919505052353
Validation loss: 2.485695972026104

Epoch: 6| Step: 2
Training loss: 2.091331337522982
Validation loss: 2.425754719171445

Epoch: 6| Step: 3
Training loss: 2.3446490788556216
Validation loss: 2.4165225666720818

Epoch: 6| Step: 4
Training loss: 2.578835961540482
Validation loss: 2.419076580586516

Epoch: 6| Step: 5
Training loss: 2.3005977227112924
Validation loss: 2.4323798010806383

Epoch: 6| Step: 6
Training loss: 2.3946764156777474
Validation loss: 2.497556885208984

Epoch: 6| Step: 7
Training loss: 1.8041832950286805
Validation loss: 2.5478782456989024

Epoch: 6| Step: 8
Training loss: 1.840167752061792
Validation loss: 2.564275439155919

Epoch: 6| Step: 9
Training loss: 1.8672524923720881
Validation loss: 2.575243859233613

Epoch: 6| Step: 10
Training loss: 2.180221546884572
Validation loss: 2.5807250693086

Epoch: 6| Step: 11
Training loss: 2.506755094929904
Validation loss: 2.564059645508137

Epoch: 6| Step: 12
Training loss: 2.2237864538831884
Validation loss: 2.527925320099353

Epoch: 6| Step: 13
Training loss: 2.6513479439333882
Validation loss: 2.497653859100608

Epoch: 153| Step: 0
Training loss: 1.714765890174361
Validation loss: 2.432530945355508

Epoch: 6| Step: 1
Training loss: 2.537364685509698
Validation loss: 2.3723455528005517

Epoch: 6| Step: 2
Training loss: 2.0457327181540546
Validation loss: 2.3735547496109306

Epoch: 6| Step: 3
Training loss: 2.8020512083875753
Validation loss: 2.3795933897872894

Epoch: 6| Step: 4
Training loss: 2.1528654210561324
Validation loss: 2.3927351477542977

Epoch: 6| Step: 5
Training loss: 1.7043560373659352
Validation loss: 2.4601937379305507

Epoch: 6| Step: 6
Training loss: 2.1306334602185752
Validation loss: 2.498281155640732

Epoch: 6| Step: 7
Training loss: 2.652784173324181
Validation loss: 2.5196618517761356

Epoch: 6| Step: 8
Training loss: 2.307335111899335
Validation loss: 2.5823417438288945

Epoch: 6| Step: 9
Training loss: 2.111842090252052
Validation loss: 2.5852094169992452

Epoch: 6| Step: 10
Training loss: 1.9471175922841366
Validation loss: 2.578401531703702

Epoch: 6| Step: 11
Training loss: 2.243059261333395
Validation loss: 2.582811847440005

Epoch: 6| Step: 12
Training loss: 2.6838616871791667
Validation loss: 2.5849352222299116

Epoch: 6| Step: 13
Training loss: 2.533344391330209
Validation loss: 2.5606565129649255

Epoch: 154| Step: 0
Training loss: 1.5445472851520463
Validation loss: 2.5363244243311494

Epoch: 6| Step: 1
Training loss: 2.178435701077553
Validation loss: 2.551923876852302

Epoch: 6| Step: 2
Training loss: 1.9147074955586778
Validation loss: 2.587934634329205

Epoch: 6| Step: 3
Training loss: 2.7214874867661987
Validation loss: 2.5462897256281862

Epoch: 6| Step: 4
Training loss: 2.1895607097508325
Validation loss: 2.519749593467215

Epoch: 6| Step: 5
Training loss: 2.4083096495347087
Validation loss: 2.5032480561911274

Epoch: 6| Step: 6
Training loss: 2.210885198938317
Validation loss: 2.4469335679540674

Epoch: 6| Step: 7
Training loss: 2.126525779928425
Validation loss: 2.4342024410684573

Epoch: 6| Step: 8
Training loss: 2.7250145833036434
Validation loss: 2.433708390872913

Epoch: 6| Step: 9
Training loss: 2.441063061816539
Validation loss: 2.4462233150434223

Epoch: 6| Step: 10
Training loss: 1.7625976535446908
Validation loss: 2.470411743222029

Epoch: 6| Step: 11
Training loss: 1.919822899876035
Validation loss: 2.5081381755155188

Epoch: 6| Step: 12
Training loss: 2.0451572577862875
Validation loss: 2.5340702879638033

Epoch: 6| Step: 13
Training loss: 2.919081551061156
Validation loss: 2.559393157107268

Epoch: 155| Step: 0
Training loss: 1.2636514983384184
Validation loss: 2.5348421063300424

Epoch: 6| Step: 1
Training loss: 2.5422120264859625
Validation loss: 2.4893939549953092

Epoch: 6| Step: 2
Training loss: 2.061488510406992
Validation loss: 2.4921196242869534

Epoch: 6| Step: 3
Training loss: 2.3930915624156
Validation loss: 2.481689811971108

Epoch: 6| Step: 4
Training loss: 1.9751421382313328
Validation loss: 2.4755381600167334

Epoch: 6| Step: 5
Training loss: 2.538061889675418
Validation loss: 2.4542176714671937

Epoch: 6| Step: 6
Training loss: 2.1543032939189275
Validation loss: 2.4312333704063915

Epoch: 6| Step: 7
Training loss: 2.641952361613732
Validation loss: 2.409682871253426

Epoch: 6| Step: 8
Training loss: 2.8865709073376977
Validation loss: 2.480379022408273

Epoch: 6| Step: 9
Training loss: 2.188584631005334
Validation loss: 2.506770787080632

Epoch: 6| Step: 10
Training loss: 2.0282916313571615
Validation loss: 2.5117248250315334

Epoch: 6| Step: 11
Training loss: 1.7048922128251054
Validation loss: 2.5210696955122507

Epoch: 6| Step: 12
Training loss: 1.7077619093148964
Validation loss: 2.5375264150498507

Epoch: 6| Step: 13
Training loss: 1.956367121106306
Validation loss: 2.573364020805183

Epoch: 156| Step: 0
Training loss: 2.3365240987183507
Validation loss: 2.5951919491216753

Epoch: 6| Step: 1
Training loss: 2.2650236186633963
Validation loss: 2.5827260357142046

Epoch: 6| Step: 2
Training loss: 1.884711230189817
Validation loss: 2.5595837928674996

Epoch: 6| Step: 3
Training loss: 2.42785339800252
Validation loss: 2.564333226195517

Epoch: 6| Step: 4
Training loss: 1.3648521750615348
Validation loss: 2.5425631049937434

Epoch: 6| Step: 5
Training loss: 2.373167837066258
Validation loss: 2.53866214445015

Epoch: 6| Step: 6
Training loss: 2.1348789809722746
Validation loss: 2.5454914641547983

Epoch: 6| Step: 7
Training loss: 1.6981255182483768
Validation loss: 2.510617901965678

Epoch: 6| Step: 8
Training loss: 2.0235598022733265
Validation loss: 2.502980371268473

Epoch: 6| Step: 9
Training loss: 2.139215381138471
Validation loss: 2.495603062387633

Epoch: 6| Step: 10
Training loss: 2.1954313089931383
Validation loss: 2.4922353677096623

Epoch: 6| Step: 11
Training loss: 2.3203235972745992
Validation loss: 2.5093692934405585

Epoch: 6| Step: 12
Training loss: 2.2576845126191807
Validation loss: 2.476358376493732

Epoch: 6| Step: 13
Training loss: 2.536345075288925
Validation loss: 2.474597711933263

Epoch: 157| Step: 0
Training loss: 1.9742625730939467
Validation loss: 2.475161058760889

Epoch: 6| Step: 1
Training loss: 1.7729288455650913
Validation loss: 2.5340357675552694

Epoch: 6| Step: 2
Training loss: 2.2090285184650393
Validation loss: 2.543814406464328

Epoch: 6| Step: 3
Training loss: 1.7724635604965095
Validation loss: 2.5758479319873713

Epoch: 6| Step: 4
Training loss: 1.9326583295069697
Validation loss: 2.5991248117901047

Epoch: 6| Step: 5
Training loss: 2.0776451532033553
Validation loss: 2.537624589209471

Epoch: 6| Step: 6
Training loss: 2.0614602907340798
Validation loss: 2.503019355405587

Epoch: 6| Step: 7
Training loss: 2.262318902117995
Validation loss: 2.4335342869005236

Epoch: 6| Step: 8
Training loss: 1.9188898986114769
Validation loss: 2.394153082458457

Epoch: 6| Step: 9
Training loss: 2.037628254042519
Validation loss: 2.403557660343183

Epoch: 6| Step: 10
Training loss: 2.1948259024778025
Validation loss: 2.4163410252375748

Epoch: 6| Step: 11
Training loss: 2.386045070539071
Validation loss: 2.4453435887529302

Epoch: 6| Step: 12
Training loss: 2.6993586590911103
Validation loss: 2.4577822984497364

Epoch: 6| Step: 13
Training loss: 2.2884909271000375
Validation loss: 2.508190767587034

Epoch: 158| Step: 0
Training loss: 2.4151557496198617
Validation loss: 2.506866249979376

Epoch: 6| Step: 1
Training loss: 1.9514664588917339
Validation loss: 2.4929049275726047

Epoch: 6| Step: 2
Training loss: 2.5116435702687325
Validation loss: 2.484987465771614

Epoch: 6| Step: 3
Training loss: 1.7126953292047982
Validation loss: 2.5222967214419043

Epoch: 6| Step: 4
Training loss: 2.313058734100252
Validation loss: 2.543062580383051

Epoch: 6| Step: 5
Training loss: 1.9193802435308622
Validation loss: 2.5688329495785958

Epoch: 6| Step: 6
Training loss: 2.0554384665032663
Validation loss: 2.6102506413517736

Epoch: 6| Step: 7
Training loss: 2.3122022282419654
Validation loss: 2.626305818370786

Epoch: 6| Step: 8
Training loss: 1.5418733595710628
Validation loss: 2.6005656488054507

Epoch: 6| Step: 9
Training loss: 1.9717171728265543
Validation loss: 2.5895049237711985

Epoch: 6| Step: 10
Training loss: 2.5021706694247383
Validation loss: 2.573602428984298

Epoch: 6| Step: 11
Training loss: 2.3739461066307928
Validation loss: 2.477883890589421

Epoch: 6| Step: 12
Training loss: 1.819903610213167
Validation loss: 2.4569284875482285

Epoch: 6| Step: 13
Training loss: 1.5432482502487253
Validation loss: 2.4446124224296417

Epoch: 159| Step: 0
Training loss: 1.5213105833329017
Validation loss: 2.4801231669996078

Epoch: 6| Step: 1
Training loss: 2.234589039614299
Validation loss: 2.5233860984023715

Epoch: 6| Step: 2
Training loss: 2.228743920095969
Validation loss: 2.5132366433115574

Epoch: 6| Step: 3
Training loss: 2.053276592911508
Validation loss: 2.5282513616081905

Epoch: 6| Step: 4
Training loss: 1.844414930541563
Validation loss: 2.5251561914880405

Epoch: 6| Step: 5
Training loss: 2.404870467633334
Validation loss: 2.518639268606252

Epoch: 6| Step: 6
Training loss: 1.1879591305217954
Validation loss: 2.482852367292019

Epoch: 6| Step: 7
Training loss: 1.7673714701364138
Validation loss: 2.466012205334766

Epoch: 6| Step: 8
Training loss: 2.425956584595404
Validation loss: 2.4680499445541706

Epoch: 6| Step: 9
Training loss: 1.4673308151173825
Validation loss: 2.4815364788501464

Epoch: 6| Step: 10
Training loss: 1.9984243862267645
Validation loss: 2.508959477019571

Epoch: 6| Step: 11
Training loss: 2.3821324112712756
Validation loss: 2.5368393785741845

Epoch: 6| Step: 12
Training loss: 2.405199292147894
Validation loss: 2.5358105973879526

Epoch: 6| Step: 13
Training loss: 2.727402172484585
Validation loss: 2.561512096108557

Epoch: 160| Step: 0
Training loss: 2.256483062795532
Validation loss: 2.5244705959010156

Epoch: 6| Step: 1
Training loss: 2.380580169103389
Validation loss: 2.4537162541124626

Epoch: 6| Step: 2
Training loss: 2.216048194058214
Validation loss: 2.4508518157931647

Epoch: 6| Step: 3
Training loss: 2.0728858932690124
Validation loss: 2.455153748827426

Epoch: 6| Step: 4
Training loss: 2.212913573668793
Validation loss: 2.437500218763991

Epoch: 6| Step: 5
Training loss: 1.3673646212695774
Validation loss: 2.4636198484656195

Epoch: 6| Step: 6
Training loss: 2.0187630289797
Validation loss: 2.5126798409997004

Epoch: 6| Step: 7
Training loss: 2.009444349169
Validation loss: 2.5465713260546967

Epoch: 6| Step: 8
Training loss: 1.9107694987123571
Validation loss: 2.534149248519399

Epoch: 6| Step: 9
Training loss: 1.3172335416410341
Validation loss: 2.530247469379051

Epoch: 6| Step: 10
Training loss: 2.328621318576898
Validation loss: 2.511703820588409

Epoch: 6| Step: 11
Training loss: 2.0205413240333554
Validation loss: 2.486045048546461

Epoch: 6| Step: 12
Training loss: 2.283168247501468
Validation loss: 2.488009556034978

Epoch: 6| Step: 13
Training loss: 1.9917051441939622
Validation loss: 2.4797512736005687

Epoch: 161| Step: 0
Training loss: 2.088698370961489
Validation loss: 2.503125437691832

Epoch: 6| Step: 1
Training loss: 2.0908790377013857
Validation loss: 2.5287378673183234

Epoch: 6| Step: 2
Training loss: 2.0454406371751586
Validation loss: 2.5411361690083125

Epoch: 6| Step: 3
Training loss: 2.037161924245818
Validation loss: 2.5472435145160413

Epoch: 6| Step: 4
Training loss: 2.255327910203735
Validation loss: 2.5141226475248004

Epoch: 6| Step: 5
Training loss: 1.8174853329283145
Validation loss: 2.4709220397870855

Epoch: 6| Step: 6
Training loss: 1.6166730421799207
Validation loss: 2.463213366282921

Epoch: 6| Step: 7
Training loss: 1.9002182383433013
Validation loss: 2.4939689810044454

Epoch: 6| Step: 8
Training loss: 1.5632688538051744
Validation loss: 2.503360019300351

Epoch: 6| Step: 9
Training loss: 1.7267858438467822
Validation loss: 2.510769856271213

Epoch: 6| Step: 10
Training loss: 2.2927148213714306
Validation loss: 2.49566086477878

Epoch: 6| Step: 11
Training loss: 2.4386267380919158
Validation loss: 2.4888411637765984

Epoch: 6| Step: 12
Training loss: 2.238312246697185
Validation loss: 2.551918934258106

Epoch: 6| Step: 13
Training loss: 2.2502904280671494
Validation loss: 2.520200149663129

Epoch: 162| Step: 0
Training loss: 1.8351480288136681
Validation loss: 2.4216766816077175

Epoch: 6| Step: 1
Training loss: 2.0822216755597025
Validation loss: 2.342132374369398

Epoch: 6| Step: 2
Training loss: 2.0149208435716215
Validation loss: 2.3809566549769654

Epoch: 6| Step: 3
Training loss: 2.373810922203485
Validation loss: 2.420238810971435

Epoch: 6| Step: 4
Training loss: 1.8435741276023712
Validation loss: 2.4854191196874695

Epoch: 6| Step: 5
Training loss: 1.862582285554664
Validation loss: 2.5149076204889464

Epoch: 6| Step: 6
Training loss: 2.0436571759250524
Validation loss: 2.524713052953586

Epoch: 6| Step: 7
Training loss: 1.966063467831035
Validation loss: 2.5152260233315014

Epoch: 6| Step: 8
Training loss: 2.1840042519999603
Validation loss: 2.533942903480968

Epoch: 6| Step: 9
Training loss: 1.7283154042980722
Validation loss: 2.5501609414464492

Epoch: 6| Step: 10
Training loss: 2.5090680648432224
Validation loss: 2.552139403828258

Epoch: 6| Step: 11
Training loss: 1.6967030109617347
Validation loss: 2.4735753503974505

Epoch: 6| Step: 12
Training loss: 2.469303117428516
Validation loss: 2.438491917108765

Epoch: 6| Step: 13
Training loss: 2.01043008065237
Validation loss: 2.4152431036360196

Epoch: 163| Step: 0
Training loss: 1.8582795706984905
Validation loss: 2.4198914316525175

Epoch: 6| Step: 1
Training loss: 1.8820393981377166
Validation loss: 2.4247067729941008

Epoch: 6| Step: 2
Training loss: 1.758461726353789
Validation loss: 2.430531609725385

Epoch: 6| Step: 3
Training loss: 2.41977172090146
Validation loss: 2.4524504810166006

Epoch: 6| Step: 4
Training loss: 1.6071273515346889
Validation loss: 2.4933862325079708

Epoch: 6| Step: 5
Training loss: 0.8393469117123988
Validation loss: 2.5701413427226116

Epoch: 6| Step: 6
Training loss: 2.5249655617357756
Validation loss: 2.6769747128880472

Epoch: 6| Step: 7
Training loss: 1.8736614535673803
Validation loss: 2.6425519352783446

Epoch: 6| Step: 8
Training loss: 2.198381010304466
Validation loss: 2.5080491495705295

Epoch: 6| Step: 9
Training loss: 1.9911335270134072
Validation loss: 2.4492345763206065

Epoch: 6| Step: 10
Training loss: 2.7673325951315713
Validation loss: 2.4143959317183383

Epoch: 6| Step: 11
Training loss: 2.2333827449779373
Validation loss: 2.426808140199142

Epoch: 6| Step: 12
Training loss: 2.0705979636258305
Validation loss: 2.40940172902471

Epoch: 6| Step: 13
Training loss: 2.071584634586382
Validation loss: 2.402968761750866

Epoch: 164| Step: 0
Training loss: 1.766935081904239
Validation loss: 2.385376257437056

Epoch: 6| Step: 1
Training loss: 2.4255966632291646
Validation loss: 2.3906414168950514

Epoch: 6| Step: 2
Training loss: 2.0816423292648776
Validation loss: 2.4272636956726923

Epoch: 6| Step: 3
Training loss: 1.844175386656727
Validation loss: 2.434433703053263

Epoch: 6| Step: 4
Training loss: 2.3016127944081517
Validation loss: 2.4750342376137393

Epoch: 6| Step: 5
Training loss: 2.197711543293809
Validation loss: 2.5070973014329607

Epoch: 6| Step: 6
Training loss: 1.999907729881444
Validation loss: 2.526349425187516

Epoch: 6| Step: 7
Training loss: 1.8079509869995984
Validation loss: 2.5648686186485437

Epoch: 6| Step: 8
Training loss: 1.8607529735282604
Validation loss: 2.60880584433911

Epoch: 6| Step: 9
Training loss: 2.0633487688924146
Validation loss: 2.5707874469574676

Epoch: 6| Step: 10
Training loss: 1.948019511181917
Validation loss: 2.480881849866354

Epoch: 6| Step: 11
Training loss: 2.073860669163616
Validation loss: 2.442922479443504

Epoch: 6| Step: 12
Training loss: 2.2837693399112577
Validation loss: 2.411676823312821

Epoch: 6| Step: 13
Training loss: 0.9598310660198405
Validation loss: 2.397947044739994

Epoch: 165| Step: 0
Training loss: 2.383441379294384
Validation loss: 2.4024859039776483

Epoch: 6| Step: 1
Training loss: 1.8183107297886951
Validation loss: 2.41173617052479

Epoch: 6| Step: 2
Training loss: 1.3383568167298767
Validation loss: 2.4092259866336114

Epoch: 6| Step: 3
Training loss: 1.9913606012139997
Validation loss: 2.4285369777364374

Epoch: 6| Step: 4
Training loss: 2.076190822798357
Validation loss: 2.455508825706648

Epoch: 6| Step: 5
Training loss: 1.9831402158348816
Validation loss: 2.4892271589175095

Epoch: 6| Step: 6
Training loss: 1.1104558058682474
Validation loss: 2.470124611356871

Epoch: 6| Step: 7
Training loss: 2.149946868594841
Validation loss: 2.4632811955067404

Epoch: 6| Step: 8
Training loss: 2.2929976557350735
Validation loss: 2.458478765494526

Epoch: 6| Step: 9
Training loss: 2.0338909880121263
Validation loss: 2.474250846801815

Epoch: 6| Step: 10
Training loss: 1.263394356254447
Validation loss: 2.5010567595331374

Epoch: 6| Step: 11
Training loss: 2.131895434064407
Validation loss: 2.5300283660459324

Epoch: 6| Step: 12
Training loss: 2.4273208916664903
Validation loss: 2.568139134922382

Epoch: 6| Step: 13
Training loss: 1.8726587937663612
Validation loss: 2.563496193181204

Epoch: 166| Step: 0
Training loss: 1.9138559658182088
Validation loss: 2.547488146517684

Epoch: 6| Step: 1
Training loss: 1.9658540286012027
Validation loss: 2.5050161869298226

Epoch: 6| Step: 2
Training loss: 2.1151471318140476
Validation loss: 2.4829046069948415

Epoch: 6| Step: 3
Training loss: 1.9906853491529684
Validation loss: 2.460615750042764

Epoch: 6| Step: 4
Training loss: 1.458017950923391
Validation loss: 2.4316409665881027

Epoch: 6| Step: 5
Training loss: 2.03929809144716
Validation loss: 2.407467022269788

Epoch: 6| Step: 6
Training loss: 1.8133847938042247
Validation loss: 2.4089470546720344

Epoch: 6| Step: 7
Training loss: 1.3838485026599585
Validation loss: 2.409615021694623

Epoch: 6| Step: 8
Training loss: 2.305682882242655
Validation loss: 2.4538399081990914

Epoch: 6| Step: 9
Training loss: 1.801329026722272
Validation loss: 2.4992628466892053

Epoch: 6| Step: 10
Training loss: 1.9195256333662931
Validation loss: 2.53740698571528

Epoch: 6| Step: 11
Training loss: 1.848910206629558
Validation loss: 2.5395050006530497

Epoch: 6| Step: 12
Training loss: 2.3665245814945837
Validation loss: 2.499541278510609

Epoch: 6| Step: 13
Training loss: 2.284451576546265
Validation loss: 2.4664020102294453

Epoch: 167| Step: 0
Training loss: 2.215063659407619
Validation loss: 2.4427619471837474

Epoch: 6| Step: 1
Training loss: 2.221938622445013
Validation loss: 2.4408256145479843

Epoch: 6| Step: 2
Training loss: 1.5106160876287797
Validation loss: 2.435368272819216

Epoch: 6| Step: 3
Training loss: 2.0024277733815676
Validation loss: 2.4496365305820262

Epoch: 6| Step: 4
Training loss: 1.8142596943451592
Validation loss: 2.445075689781985

Epoch: 6| Step: 5
Training loss: 2.0253025723687696
Validation loss: 2.4625510701485287

Epoch: 6| Step: 6
Training loss: 1.5796809316134461
Validation loss: 2.4898372573340843

Epoch: 6| Step: 7
Training loss: 2.3084011352533627
Validation loss: 2.4930970576051235

Epoch: 6| Step: 8
Training loss: 2.0683457731640815
Validation loss: 2.5239219189133784

Epoch: 6| Step: 9
Training loss: 2.367649750852605
Validation loss: 2.5514833767639744

Epoch: 6| Step: 10
Training loss: 1.3435796585346642
Validation loss: 2.5083322774470385

Epoch: 6| Step: 11
Training loss: 1.3777777184508595
Validation loss: 2.446767680744697

Epoch: 6| Step: 12
Training loss: 1.9575298230078355
Validation loss: 2.4151189536501625

Epoch: 6| Step: 13
Training loss: 2.014761214767884
Validation loss: 2.416387174425262

Epoch: 168| Step: 0
Training loss: 2.097214750005731
Validation loss: 2.4846072772981227

Epoch: 6| Step: 1
Training loss: 1.906233990711463
Validation loss: 2.4939287524024842

Epoch: 6| Step: 2
Training loss: 2.1228026080227465
Validation loss: 2.5241115539321575

Epoch: 6| Step: 3
Training loss: 1.7021632847710177
Validation loss: 2.5812600979782148

Epoch: 6| Step: 4
Training loss: 1.8762036592681044
Validation loss: 2.629442542867723

Epoch: 6| Step: 5
Training loss: 1.7863877470642615
Validation loss: 2.547566068287824

Epoch: 6| Step: 6
Training loss: 1.8204800115303634
Validation loss: 2.535439669381306

Epoch: 6| Step: 7
Training loss: 1.5954342338901277
Validation loss: 2.533484114726793

Epoch: 6| Step: 8
Training loss: 2.0875996457376087
Validation loss: 2.422436961044393

Epoch: 6| Step: 9
Training loss: 1.9740549699652692
Validation loss: 2.3790270165173113

Epoch: 6| Step: 10
Training loss: 1.567229703751393
Validation loss: 2.3555184818245345

Epoch: 6| Step: 11
Training loss: 2.253122282621779
Validation loss: 2.3463885035620136

Epoch: 6| Step: 12
Training loss: 2.213816140305281
Validation loss: 2.402888007331287

Epoch: 6| Step: 13
Training loss: 2.2431736282855463
Validation loss: 2.4831653781255465

Epoch: 169| Step: 0
Training loss: 2.028641654627703
Validation loss: 2.5548575953901334

Epoch: 6| Step: 1
Training loss: 2.37752087974004
Validation loss: 2.6222830646921453

Epoch: 6| Step: 2
Training loss: 2.0881482242867238
Validation loss: 2.5678862462010095

Epoch: 6| Step: 3
Training loss: 1.7265862510739105
Validation loss: 2.4451780198691586

Epoch: 6| Step: 4
Training loss: 1.8790806552575152
Validation loss: 2.387689110331203

Epoch: 6| Step: 5
Training loss: 1.7618053609530233
Validation loss: 2.391971955850982

Epoch: 6| Step: 6
Training loss: 2.3161642581820763
Validation loss: 2.415875561753969

Epoch: 6| Step: 7
Training loss: 1.9572170877450976
Validation loss: 2.433289120848836

Epoch: 6| Step: 8
Training loss: 1.889224582026125
Validation loss: 2.4676030969661564

Epoch: 6| Step: 9
Training loss: 1.928999434515048
Validation loss: 2.5102785345046623

Epoch: 6| Step: 10
Training loss: 1.817363790159381
Validation loss: 2.539101479407036

Epoch: 6| Step: 11
Training loss: 1.87290990684684
Validation loss: 2.5703427371296734

Epoch: 6| Step: 12
Training loss: 2.164127059689647
Validation loss: 2.5629485743385207

Epoch: 6| Step: 13
Training loss: 1.3344824825836539
Validation loss: 2.5320612545368015

Epoch: 170| Step: 0
Training loss: 2.001599268460159
Validation loss: 2.4909211508752134

Epoch: 6| Step: 1
Training loss: 1.473729721331026
Validation loss: 2.4900734449419764

Epoch: 6| Step: 2
Training loss: 2.079935188934318
Validation loss: 2.4769768712923965

Epoch: 6| Step: 3
Training loss: 1.7443723108208709
Validation loss: 2.4313127594747073

Epoch: 6| Step: 4
Training loss: 2.3573478427946757
Validation loss: 2.427343441646242

Epoch: 6| Step: 5
Training loss: 2.4794464167201173
Validation loss: 2.40021743349198

Epoch: 6| Step: 6
Training loss: 1.4820076787237448
Validation loss: 2.420135370772005

Epoch: 6| Step: 7
Training loss: 2.1816172380824588
Validation loss: 2.447878136437412

Epoch: 6| Step: 8
Training loss: 1.6483469572105307
Validation loss: 2.4687102026306307

Epoch: 6| Step: 9
Training loss: 1.9790890795574636
Validation loss: 2.4748454413491454

Epoch: 6| Step: 10
Training loss: 1.79142984407162
Validation loss: 2.51850210419454

Epoch: 6| Step: 11
Training loss: 1.583439856844244
Validation loss: 2.569577474530685

Epoch: 6| Step: 12
Training loss: 2.1230492333027957
Validation loss: 2.654133429595823

Epoch: 6| Step: 13
Training loss: 1.9360387891083508
Validation loss: 2.6174657843595046

Epoch: 171| Step: 0
Training loss: 2.256391559925737
Validation loss: 2.51834506638888

Epoch: 6| Step: 1
Training loss: 2.1517026192133386
Validation loss: 2.408423005251474

Epoch: 6| Step: 2
Training loss: 1.6076664419904996
Validation loss: 2.380083865808544

Epoch: 6| Step: 3
Training loss: 2.2594547832748444
Validation loss: 2.372718750606513

Epoch: 6| Step: 4
Training loss: 1.8617705612881057
Validation loss: 2.355241869920681

Epoch: 6| Step: 5
Training loss: 1.7767946170493474
Validation loss: 2.3649299464755145

Epoch: 6| Step: 6
Training loss: 1.603204992131591
Validation loss: 2.371291225121978

Epoch: 6| Step: 7
Training loss: 2.1382989427631083
Validation loss: 2.419250176295407

Epoch: 6| Step: 8
Training loss: 1.8843783756561157
Validation loss: 2.485006318096591

Epoch: 6| Step: 9
Training loss: 2.3158479117362107
Validation loss: 2.5974877482248306

Epoch: 6| Step: 10
Training loss: 1.7150358818015148
Validation loss: 2.7276050471279247

Epoch: 6| Step: 11
Training loss: 2.623222748519155
Validation loss: 2.839686231037065

Epoch: 6| Step: 12
Training loss: 1.3439472852087324
Validation loss: 2.6890604821587414

Epoch: 6| Step: 13
Training loss: 2.2827799643953264
Validation loss: 2.5341641397809545

Epoch: 172| Step: 0
Training loss: 2.1713304454459053
Validation loss: 2.425643786220723

Epoch: 6| Step: 1
Training loss: 1.7211522047840746
Validation loss: 2.361808803182426

Epoch: 6| Step: 2
Training loss: 2.467332263876556
Validation loss: 2.3330705739396547

Epoch: 6| Step: 3
Training loss: 2.0572338498518037
Validation loss: 2.317773754767692

Epoch: 6| Step: 4
Training loss: 1.307791073713629
Validation loss: 2.3273104178382873

Epoch: 6| Step: 5
Training loss: 1.904791852738325
Validation loss: 2.349389329572767

Epoch: 6| Step: 6
Training loss: 1.648418625276087
Validation loss: 2.365174870081945

Epoch: 6| Step: 7
Training loss: 1.6361023827999837
Validation loss: 2.418112111094675

Epoch: 6| Step: 8
Training loss: 2.1252987595517707
Validation loss: 2.4535934850524423

Epoch: 6| Step: 9
Training loss: 1.4864586911894972
Validation loss: 2.514400879107334

Epoch: 6| Step: 10
Training loss: 1.3808302815568008
Validation loss: 2.533603529642639

Epoch: 6| Step: 11
Training loss: 1.8069669521584026
Validation loss: 2.568498759653872

Epoch: 6| Step: 12
Training loss: 2.304366060244273
Validation loss: 2.6270025538136887

Epoch: 6| Step: 13
Training loss: 2.6353437893608036
Validation loss: 2.541660801722556

Epoch: 173| Step: 0
Training loss: 1.7203351427297988
Validation loss: 2.498849507654999

Epoch: 6| Step: 1
Training loss: 1.8126332464239034
Validation loss: 2.4545684511746697

Epoch: 6| Step: 2
Training loss: 2.326157115822195
Validation loss: 2.409980390301534

Epoch: 6| Step: 3
Training loss: 2.012152467657604
Validation loss: 2.37547110724706

Epoch: 6| Step: 4
Training loss: 1.7318434927093573
Validation loss: 2.3667766883727586

Epoch: 6| Step: 5
Training loss: 1.893845077998361
Validation loss: 2.3353573448441782

Epoch: 6| Step: 6
Training loss: 1.8723842176590835
Validation loss: 2.3406468358020693

Epoch: 6| Step: 7
Training loss: 1.9466766109386746
Validation loss: 2.356992086901405

Epoch: 6| Step: 8
Training loss: 1.6467484050932384
Validation loss: 2.3772906864200793

Epoch: 6| Step: 9
Training loss: 1.991665461339014
Validation loss: 2.424331494706475

Epoch: 6| Step: 10
Training loss: 1.2306618190854512
Validation loss: 2.4566885509004055

Epoch: 6| Step: 11
Training loss: 2.1073157537431646
Validation loss: 2.4966408218962184

Epoch: 6| Step: 12
Training loss: 2.0206609698326146
Validation loss: 2.543481976019409

Epoch: 6| Step: 13
Training loss: 1.287531735667969
Validation loss: 2.5635828250985204

Epoch: 174| Step: 0
Training loss: 2.030300211764294
Validation loss: 2.588089603104428

Epoch: 6| Step: 1
Training loss: 1.906176581297791
Validation loss: 2.584853556278016

Epoch: 6| Step: 2
Training loss: 2.1544799173569555
Validation loss: 2.58817159980571

Epoch: 6| Step: 3
Training loss: 1.677572791031067
Validation loss: 2.570985008590969

Epoch: 6| Step: 4
Training loss: 1.8014927449415121
Validation loss: 2.5332605531883314

Epoch: 6| Step: 5
Training loss: 2.1183778013918886
Validation loss: 2.477835217972021

Epoch: 6| Step: 6
Training loss: 1.5835952374865825
Validation loss: 2.429951299237324

Epoch: 6| Step: 7
Training loss: 2.2498077734319923
Validation loss: 2.3824485636712383

Epoch: 6| Step: 8
Training loss: 2.0932273781844644
Validation loss: 2.376741498348301

Epoch: 6| Step: 9
Training loss: 1.402908339053209
Validation loss: 2.3673858564887498

Epoch: 6| Step: 10
Training loss: 2.074795917448282
Validation loss: 2.3564414563393585

Epoch: 6| Step: 11
Training loss: 1.42728017113141
Validation loss: 2.3615155749665067

Epoch: 6| Step: 12
Training loss: 1.5805300524858465
Validation loss: 2.3832865872292506

Epoch: 6| Step: 13
Training loss: 1.547016792349245
Validation loss: 2.3752532180657315

Epoch: 175| Step: 0
Training loss: 2.0799765691904675
Validation loss: 2.436619324215042

Epoch: 6| Step: 1
Training loss: 1.937413921289882
Validation loss: 2.5048012457178586

Epoch: 6| Step: 2
Training loss: 1.5392486924374535
Validation loss: 2.5667266333560836

Epoch: 6| Step: 3
Training loss: 1.44703637494052
Validation loss: 2.585694031964224

Epoch: 6| Step: 4
Training loss: 2.314270990987564
Validation loss: 2.611632046595873

Epoch: 6| Step: 5
Training loss: 2.0257642175184225
Validation loss: 2.5957234675032517

Epoch: 6| Step: 6
Training loss: 2.303029954623541
Validation loss: 2.5645025628550737

Epoch: 6| Step: 7
Training loss: 1.7742370953906923
Validation loss: 2.5222972885882284

Epoch: 6| Step: 8
Training loss: 1.6282326748730263
Validation loss: 2.476668688667936

Epoch: 6| Step: 9
Training loss: 1.3221504014719916
Validation loss: 2.419524169357576

Epoch: 6| Step: 10
Training loss: 1.5942958570192869
Validation loss: 2.3855673575845104

Epoch: 6| Step: 11
Training loss: 1.7391866661911761
Validation loss: 2.377241814473179

Epoch: 6| Step: 12
Training loss: 1.7528616804356658
Validation loss: 2.381989240656141

Epoch: 6| Step: 13
Training loss: 1.6325925103887426
Validation loss: 2.3826638333888708

Epoch: 176| Step: 0
Training loss: 1.9014706341032925
Validation loss: 2.4071121141688314

Epoch: 6| Step: 1
Training loss: 1.7117759009065165
Validation loss: 2.3963475157563394

Epoch: 6| Step: 2
Training loss: 1.7799220572776553
Validation loss: 2.3961534943386327

Epoch: 6| Step: 3
Training loss: 2.087800754650545
Validation loss: 2.415732869158184

Epoch: 6| Step: 4
Training loss: 1.9702698426919192
Validation loss: 2.430412858171825

Epoch: 6| Step: 5
Training loss: 1.6589891347868497
Validation loss: 2.4530697903815812

Epoch: 6| Step: 6
Training loss: 1.3015307767191648
Validation loss: 2.429765861267642

Epoch: 6| Step: 7
Training loss: 1.4053535995258692
Validation loss: 2.4476295472117635

Epoch: 6| Step: 8
Training loss: 2.3523794383696286
Validation loss: 2.4424135176407846

Epoch: 6| Step: 9
Training loss: 1.6660383788653534
Validation loss: 2.45645345729098

Epoch: 6| Step: 10
Training loss: 1.6298196916374355
Validation loss: 2.473420076650454

Epoch: 6| Step: 11
Training loss: 1.913516468871341
Validation loss: 2.4150311561986055

Epoch: 6| Step: 12
Training loss: 1.741035388391865
Validation loss: 2.4282994357896306

Epoch: 6| Step: 13
Training loss: 1.5788512872022558
Validation loss: 2.399764169408549

Epoch: 177| Step: 0
Training loss: 1.2708369187267985
Validation loss: 2.4024921437100537

Epoch: 6| Step: 1
Training loss: 1.58590352440131
Validation loss: 2.434458837711371

Epoch: 6| Step: 2
Training loss: 1.5127987022850244
Validation loss: 2.4489685675031985

Epoch: 6| Step: 3
Training loss: 1.6974471755650065
Validation loss: 2.45074484324353

Epoch: 6| Step: 4
Training loss: 1.7302956246946013
Validation loss: 2.4440527480083647

Epoch: 6| Step: 5
Training loss: 1.986669580448924
Validation loss: 2.458917243595868

Epoch: 6| Step: 6
Training loss: 2.358419768116529
Validation loss: 2.4611457430590846

Epoch: 6| Step: 7
Training loss: 1.7061973647498252
Validation loss: 2.458918513469222

Epoch: 6| Step: 8
Training loss: 1.7545143891269295
Validation loss: 2.419437831631035

Epoch: 6| Step: 9
Training loss: 1.8209374136897718
Validation loss: 2.4124184607591075

Epoch: 6| Step: 10
Training loss: 1.9398985137153475
Validation loss: 2.3934680623500095

Epoch: 6| Step: 11
Training loss: 1.698060230512379
Validation loss: 2.392468062567499

Epoch: 6| Step: 12
Training loss: 2.043358497568393
Validation loss: 2.38617448760758

Epoch: 6| Step: 13
Training loss: 0.9823818615752516
Validation loss: 2.385533501693821

Epoch: 178| Step: 0
Training loss: 1.868807293578181
Validation loss: 2.3979564763049064

Epoch: 6| Step: 1
Training loss: 1.7820731820771802
Validation loss: 2.4300022824130387

Epoch: 6| Step: 2
Training loss: 1.9545573971123404
Validation loss: 2.460867816428739

Epoch: 6| Step: 3
Training loss: 1.5659921912221497
Validation loss: 2.490891143005073

Epoch: 6| Step: 4
Training loss: 2.122545395400506
Validation loss: 2.502059518891451

Epoch: 6| Step: 5
Training loss: 2.0056777470714624
Validation loss: 2.494814919367595

Epoch: 6| Step: 6
Training loss: 1.4881816039681313
Validation loss: 2.4915581640635476

Epoch: 6| Step: 7
Training loss: 1.1251892354543191
Validation loss: 2.4409494217255894

Epoch: 6| Step: 8
Training loss: 1.8028634501461964
Validation loss: 2.390913170959265

Epoch: 6| Step: 9
Training loss: 2.2868814851638155
Validation loss: 2.3811582635971713

Epoch: 6| Step: 10
Training loss: 1.7584452528686885
Validation loss: 2.3740694050318876

Epoch: 6| Step: 11
Training loss: 1.2947890269630309
Validation loss: 2.367943388714516

Epoch: 6| Step: 12
Training loss: 1.4432411816511115
Validation loss: 2.383082474628623

Epoch: 6| Step: 13
Training loss: 2.0265720199561357
Validation loss: 2.423504686317163

Epoch: 179| Step: 0
Training loss: 2.2897518018458674
Validation loss: 2.45270439781467

Epoch: 6| Step: 1
Training loss: 1.3270147732082669
Validation loss: 2.491866186290634

Epoch: 6| Step: 2
Training loss: 1.7427170979183841
Validation loss: 2.537927383106425

Epoch: 6| Step: 3
Training loss: 1.883004966174205
Validation loss: 2.5209228711933576

Epoch: 6| Step: 4
Training loss: 1.3731555705776024
Validation loss: 2.5155553485183453

Epoch: 6| Step: 5
Training loss: 1.8782512450989022
Validation loss: 2.490450179167133

Epoch: 6| Step: 6
Training loss: 1.5856874766839013
Validation loss: 2.498953543347775

Epoch: 6| Step: 7
Training loss: 2.18070254738208
Validation loss: 2.4257414377956286

Epoch: 6| Step: 8
Training loss: 1.4326780965451773
Validation loss: 2.375128653495362

Epoch: 6| Step: 9
Training loss: 1.7638880194847921
Validation loss: 2.3693765862040923

Epoch: 6| Step: 10
Training loss: 1.86918743716687
Validation loss: 2.343538608657237

Epoch: 6| Step: 11
Training loss: 1.8344318465566538
Validation loss: 2.367332938937061

Epoch: 6| Step: 12
Training loss: 1.6128282257300899
Validation loss: 2.407322849393991

Epoch: 6| Step: 13
Training loss: 1.2128103232714358
Validation loss: 2.4071568875091987

Epoch: 180| Step: 0
Training loss: 1.8667065627512154
Validation loss: 2.467259941935714

Epoch: 6| Step: 1
Training loss: 1.1688338428146083
Validation loss: 2.4844428499666504

Epoch: 6| Step: 2
Training loss: 1.6817142058434003
Validation loss: 2.5116254047623197

Epoch: 6| Step: 3
Training loss: 2.2909501660711937
Validation loss: 2.5218315077150897

Epoch: 6| Step: 4
Training loss: 1.7728812399827474
Validation loss: 2.4775319309730115

Epoch: 6| Step: 5
Training loss: 1.778950662774701
Validation loss: 2.4666949756375853

Epoch: 6| Step: 6
Training loss: 1.7457904594941207
Validation loss: 2.4810679414735253

Epoch: 6| Step: 7
Training loss: 1.5144493026232553
Validation loss: 2.4737604941432405

Epoch: 6| Step: 8
Training loss: 1.964325064723225
Validation loss: 2.491875236658523

Epoch: 6| Step: 9
Training loss: 1.7309752814402855
Validation loss: 2.5091763220433876

Epoch: 6| Step: 10
Training loss: 1.8021554068850958
Validation loss: 2.5387154250287005

Epoch: 6| Step: 11
Training loss: 1.8082179426771554
Validation loss: 2.5150018102399243

Epoch: 6| Step: 12
Training loss: 1.4280178973147604
Validation loss: 2.4854999630978662

Epoch: 6| Step: 13
Training loss: 1.2547535158037026
Validation loss: 2.469133981957936

Epoch: 181| Step: 0
Training loss: 1.5722476555407527
Validation loss: 2.4540130071846806

Epoch: 6| Step: 1
Training loss: 1.752796527193321
Validation loss: 2.3991023823531186

Epoch: 6| Step: 2
Training loss: 1.6843478113927812
Validation loss: 2.39762129301136

Epoch: 6| Step: 3
Training loss: 2.204404851475006
Validation loss: 2.356200977523884

Epoch: 6| Step: 4
Training loss: 1.5337847535813502
Validation loss: 2.388053311872885

Epoch: 6| Step: 5
Training loss: 1.550002356496681
Validation loss: 2.376990053945953

Epoch: 6| Step: 6
Training loss: 1.391311090504204
Validation loss: 2.3950488719389194

Epoch: 6| Step: 7
Training loss: 1.9400388019653378
Validation loss: 2.4080668828039777

Epoch: 6| Step: 8
Training loss: 1.3005985440996477
Validation loss: 2.4374581999411697

Epoch: 6| Step: 9
Training loss: 1.3829275826880274
Validation loss: 2.4376236879210613

Epoch: 6| Step: 10
Training loss: 2.073045991732429
Validation loss: 2.420434240206358

Epoch: 6| Step: 11
Training loss: 2.3462710237043813
Validation loss: 2.448493193889064

Epoch: 6| Step: 12
Training loss: 1.3045816892558513
Validation loss: 2.4488126934943923

Epoch: 6| Step: 13
Training loss: 1.4279225612875286
Validation loss: 2.4483486950663105

Epoch: 182| Step: 0
Training loss: 1.791233128576692
Validation loss: 2.4530738724391035

Epoch: 6| Step: 1
Training loss: 2.029414594832038
Validation loss: 2.453455745737592

Epoch: 6| Step: 2
Training loss: 1.8353515988729776
Validation loss: 2.47474596388118

Epoch: 6| Step: 3
Training loss: 1.8530229691565623
Validation loss: 2.455267873770543

Epoch: 6| Step: 4
Training loss: 1.5074719454557222
Validation loss: 2.450760563994642

Epoch: 6| Step: 5
Training loss: 1.1764181952871338
Validation loss: 2.4552306723868043

Epoch: 6| Step: 6
Training loss: 1.7792522537883952
Validation loss: 2.413256610943007

Epoch: 6| Step: 7
Training loss: 1.8161052495478622
Validation loss: 2.406545827809056

Epoch: 6| Step: 8
Training loss: 1.6549758328520143
Validation loss: 2.37562913878683

Epoch: 6| Step: 9
Training loss: 1.1562953630776198
Validation loss: 2.357439935738738

Epoch: 6| Step: 10
Training loss: 1.3252273364544414
Validation loss: 2.3383486840066836

Epoch: 6| Step: 11
Training loss: 2.037736366463175
Validation loss: 2.3857539894439

Epoch: 6| Step: 12
Training loss: 1.8319564836614364
Validation loss: 2.392901609818439

Epoch: 6| Step: 13
Training loss: 1.4492103596981682
Validation loss: 2.3959298467969896

Epoch: 183| Step: 0
Training loss: 1.845148881317189
Validation loss: 2.4136632704750003

Epoch: 6| Step: 1
Training loss: 1.6523482951649588
Validation loss: 2.451164184605313

Epoch: 6| Step: 2
Training loss: 2.297345898164177
Validation loss: 2.4648245073601625

Epoch: 6| Step: 3
Training loss: 1.4393398286909367
Validation loss: 2.4834887961417613

Epoch: 6| Step: 4
Training loss: 1.1094502168328109
Validation loss: 2.4818018835671003

Epoch: 6| Step: 5
Training loss: 1.8403108493864249
Validation loss: 2.483933938297891

Epoch: 6| Step: 6
Training loss: 1.381270632007182
Validation loss: 2.46443510518837

Epoch: 6| Step: 7
Training loss: 1.9946845468256917
Validation loss: 2.4501280532866665

Epoch: 6| Step: 8
Training loss: 1.3657053733935052
Validation loss: 2.4176601182658666

Epoch: 6| Step: 9
Training loss: 1.7205135661052904
Validation loss: 2.374178733083932

Epoch: 6| Step: 10
Training loss: 1.8375092616464148
Validation loss: 2.348426046039166

Epoch: 6| Step: 11
Training loss: 1.795581119584085
Validation loss: 2.353940624202549

Epoch: 6| Step: 12
Training loss: 1.6302328950861804
Validation loss: 2.3872018308450342

Epoch: 6| Step: 13
Training loss: 0.39080005537967816
Validation loss: 2.3883309995500457

Epoch: 184| Step: 0
Training loss: 1.765972812119295
Validation loss: 2.417033034769087

Epoch: 6| Step: 1
Training loss: 1.693542118617585
Validation loss: 2.482161317015205

Epoch: 6| Step: 2
Training loss: 1.7015321615017318
Validation loss: 2.5375196955941077

Epoch: 6| Step: 3
Training loss: 1.5110456362888283
Validation loss: 2.4532300023830924

Epoch: 6| Step: 4
Training loss: 2.1488795709961344
Validation loss: 2.3996550530014447

Epoch: 6| Step: 5
Training loss: 1.4195541052482117
Validation loss: 2.364457121688062

Epoch: 6| Step: 6
Training loss: 1.4018101008882522
Validation loss: 2.315294886291789

Epoch: 6| Step: 7
Training loss: 1.555877383561514
Validation loss: 2.3525860644159566

Epoch: 6| Step: 8
Training loss: 2.1265719714153515
Validation loss: 2.4000143613009626

Epoch: 6| Step: 9
Training loss: 1.6914748250614102
Validation loss: 2.3929359997122064

Epoch: 6| Step: 10
Training loss: 1.8128054131321838
Validation loss: 2.394274158163131

Epoch: 6| Step: 11
Training loss: 1.1622609272076185
Validation loss: 2.3957930457609877

Epoch: 6| Step: 12
Training loss: 1.7837253570458336
Validation loss: 2.378438012316434

Epoch: 6| Step: 13
Training loss: 1.6775527518288231
Validation loss: 2.369249795820707

Epoch: 185| Step: 0
Training loss: 1.1910092552267453
Validation loss: 2.394483798440355

Epoch: 6| Step: 1
Training loss: 1.8756364378029033
Validation loss: 2.4341815512569367

Epoch: 6| Step: 2
Training loss: 1.5665986283049755
Validation loss: 2.407900998817823

Epoch: 6| Step: 3
Training loss: 1.2521757264199003
Validation loss: 2.393344373491015

Epoch: 6| Step: 4
Training loss: 1.746216430450621
Validation loss: 2.3671968158625387

Epoch: 6| Step: 5
Training loss: 1.6465202861593826
Validation loss: 2.379907848952184

Epoch: 6| Step: 6
Training loss: 1.2169452411459667
Validation loss: 2.3712334560715562

Epoch: 6| Step: 7
Training loss: 2.0051850817104295
Validation loss: 2.37044300610711

Epoch: 6| Step: 8
Training loss: 1.2082985949729674
Validation loss: 2.3805395977243724

Epoch: 6| Step: 9
Training loss: 1.4715522655261588
Validation loss: 2.3578411773070855

Epoch: 6| Step: 10
Training loss: 1.889419044942804
Validation loss: 2.379251561694752

Epoch: 6| Step: 11
Training loss: 1.8777151158779344
Validation loss: 2.386560405440189

Epoch: 6| Step: 12
Training loss: 1.7962864450939804
Validation loss: 2.3869132569270732

Epoch: 6| Step: 13
Training loss: 2.144142212869755
Validation loss: 2.4364848288113468

Epoch: 186| Step: 0
Training loss: 1.8494845935337634
Validation loss: 2.4416448776609574

Epoch: 6| Step: 1
Training loss: 1.6287835896000697
Validation loss: 2.460017909968479

Epoch: 6| Step: 2
Training loss: 1.549498904660808
Validation loss: 2.4794769606516325

Epoch: 6| Step: 3
Training loss: 1.7975070670974451
Validation loss: 2.4688495269605677

Epoch: 6| Step: 4
Training loss: 1.2942271030132981
Validation loss: 2.481906340503625

Epoch: 6| Step: 5
Training loss: 1.305877873469131
Validation loss: 2.475488385991602

Epoch: 6| Step: 6
Training loss: 1.7040671446201947
Validation loss: 2.427991967824997

Epoch: 6| Step: 7
Training loss: 1.751520177787576
Validation loss: 2.3512477007892265

Epoch: 6| Step: 8
Training loss: 1.7263885686616132
Validation loss: 2.340278327243749

Epoch: 6| Step: 9
Training loss: 1.439173015115884
Validation loss: 2.311647504690177

Epoch: 6| Step: 10
Training loss: 1.459030947267476
Validation loss: 2.321200168710516

Epoch: 6| Step: 11
Training loss: 1.4122880371530302
Validation loss: 2.34604244659274

Epoch: 6| Step: 12
Training loss: 1.9118182668907275
Validation loss: 2.332402774021722

Epoch: 6| Step: 13
Training loss: 1.8594191089174656
Validation loss: 2.3658686381788527

Epoch: 187| Step: 0
Training loss: 1.3534873530089777
Validation loss: 2.4165469105644837

Epoch: 6| Step: 1
Training loss: 1.5993446438447598
Validation loss: 2.4415400012715662

Epoch: 6| Step: 2
Training loss: 2.3158698401697073
Validation loss: 2.4670342297966643

Epoch: 6| Step: 3
Training loss: 1.5793991233151803
Validation loss: 2.4364548759851714

Epoch: 6| Step: 4
Training loss: 1.0675559082399468
Validation loss: 2.3937370135225113

Epoch: 6| Step: 5
Training loss: 1.644441362040154
Validation loss: 2.3201040914127526

Epoch: 6| Step: 6
Training loss: 1.684882005244667
Validation loss: 2.2668292790135536

Epoch: 6| Step: 7
Training loss: 1.8523197476598905
Validation loss: 2.289624439289906

Epoch: 6| Step: 8
Training loss: 1.6317110133679433
Validation loss: 2.3311469173847095

Epoch: 6| Step: 9
Training loss: 2.0453816564654796
Validation loss: 2.3461187904112712

Epoch: 6| Step: 10
Training loss: 1.7664829296558122
Validation loss: 2.314358648037961

Epoch: 6| Step: 11
Training loss: 1.256800653278028
Validation loss: 2.310807027152906

Epoch: 6| Step: 12
Training loss: 1.5533051599286085
Validation loss: 2.3280094732237617

Epoch: 6| Step: 13
Training loss: 0.8440702325155314
Validation loss: 2.3486001313868665

Epoch: 188| Step: 0
Training loss: 1.6516474706469029
Validation loss: 2.372750912603623

Epoch: 6| Step: 1
Training loss: 1.3970987742354128
Validation loss: 2.4079808068668704

Epoch: 6| Step: 2
Training loss: 1.6746487291862617
Validation loss: 2.425726659841292

Epoch: 6| Step: 3
Training loss: 1.8671207176619593
Validation loss: 2.4486445643126946

Epoch: 6| Step: 4
Training loss: 1.5716170021508373
Validation loss: 2.456344723779134

Epoch: 6| Step: 5
Training loss: 1.4996908187114086
Validation loss: 2.464888776836563

Epoch: 6| Step: 6
Training loss: 1.3975215021707326
Validation loss: 2.44703081099798

Epoch: 6| Step: 7
Training loss: 1.3604454999422022
Validation loss: 2.4215926223722692

Epoch: 6| Step: 8
Training loss: 1.6076185400462055
Validation loss: 2.3894085889551264

Epoch: 6| Step: 9
Training loss: 1.5250895614589957
Validation loss: 2.397697527781352

Epoch: 6| Step: 10
Training loss: 2.0767850069195464
Validation loss: 2.366428147994613

Epoch: 6| Step: 11
Training loss: 1.8235582221829831
Validation loss: 2.3296685771011734

Epoch: 6| Step: 12
Training loss: 1.4075780743208661
Validation loss: 2.3722856659312175

Epoch: 6| Step: 13
Training loss: 1.9969272254478232
Validation loss: 2.385556259688515

Epoch: 189| Step: 0
Training loss: 1.570794937649412
Validation loss: 2.3840096115790668

Epoch: 6| Step: 1
Training loss: 1.50740828556164
Validation loss: 2.3812791893553213

Epoch: 6| Step: 2
Training loss: 1.5859645596906402
Validation loss: 2.378336605473888

Epoch: 6| Step: 3
Training loss: 1.4698998029127792
Validation loss: 2.3816828419086793

Epoch: 6| Step: 4
Training loss: 1.881626879685627
Validation loss: 2.326619199412055

Epoch: 6| Step: 5
Training loss: 1.6622548202190026
Validation loss: 2.279802648494305

Epoch: 6| Step: 6
Training loss: 1.9059249100438953
Validation loss: 2.275755077293921

Epoch: 6| Step: 7
Training loss: 2.4818399799290285
Validation loss: 2.245788251548514

Epoch: 6| Step: 8
Training loss: 1.4849652371688624
Validation loss: 2.2715528975103947

Epoch: 6| Step: 9
Training loss: 1.0658679763456953
Validation loss: 2.286061329661088

Epoch: 6| Step: 10
Training loss: 1.5946923256333299
Validation loss: 2.335506692372431

Epoch: 6| Step: 11
Training loss: 1.4837617109137031
Validation loss: 2.3431582135124636

Epoch: 6| Step: 12
Training loss: 1.299788545870847
Validation loss: 2.379727202921804

Epoch: 6| Step: 13
Training loss: 1.0358060704513183
Validation loss: 2.43948558312641

Epoch: 190| Step: 0
Training loss: 0.9627689493613089
Validation loss: 2.511332326571966

Epoch: 6| Step: 1
Training loss: 1.9592432248522995
Validation loss: 2.4900403314921795

Epoch: 6| Step: 2
Training loss: 1.9282407704224809
Validation loss: 2.451669918132961

Epoch: 6| Step: 3
Training loss: 1.7615273111539556
Validation loss: 2.430379130166382

Epoch: 6| Step: 4
Training loss: 1.909000975691005
Validation loss: 2.391432220840787

Epoch: 6| Step: 5
Training loss: 1.4966639136959548
Validation loss: 2.369844739811629

Epoch: 6| Step: 6
Training loss: 1.3872424359152327
Validation loss: 2.346599860496592

Epoch: 6| Step: 7
Training loss: 1.541638537313538
Validation loss: 2.3564399033204184

Epoch: 6| Step: 8
Training loss: 2.0453280361625845
Validation loss: 2.3922129866276505

Epoch: 6| Step: 9
Training loss: 1.074145116883342
Validation loss: 2.39695955818201

Epoch: 6| Step: 10
Training loss: 1.1098858906112454
Validation loss: 2.4341436616207806

Epoch: 6| Step: 11
Training loss: 1.2427603882052507
Validation loss: 2.442050601713218

Epoch: 6| Step: 12
Training loss: 1.4589239922505164
Validation loss: 2.4566555499260003

Epoch: 6| Step: 13
Training loss: 1.9408301375567392
Validation loss: 2.443131134730062

Epoch: 191| Step: 0
Training loss: 1.5632351481032472
Validation loss: 2.3914608304879383

Epoch: 6| Step: 1
Training loss: 1.0718315093445556
Validation loss: 2.3476820428984166

Epoch: 6| Step: 2
Training loss: 1.7539625944674926
Validation loss: 2.3341454031822266

Epoch: 6| Step: 3
Training loss: 1.6370814975266403
Validation loss: 2.322810083724601

Epoch: 6| Step: 4
Training loss: 1.7846124601276934
Validation loss: 2.3205144777789704

Epoch: 6| Step: 5
Training loss: 1.4359773781835519
Validation loss: 2.337738482023186

Epoch: 6| Step: 6
Training loss: 1.1487778658802639
Validation loss: 2.344458556179741

Epoch: 6| Step: 7
Training loss: 1.4659673425177702
Validation loss: 2.367428011219832

Epoch: 6| Step: 8
Training loss: 1.8330692693521666
Validation loss: 2.4231340304070867

Epoch: 6| Step: 9
Training loss: 1.7448306710357766
Validation loss: 2.485590466062722

Epoch: 6| Step: 10
Training loss: 1.1654988586243449
Validation loss: 2.5519743691228807

Epoch: 6| Step: 11
Training loss: 1.4322080645582005
Validation loss: 2.5682882918530074

Epoch: 6| Step: 12
Training loss: 1.76739487514791
Validation loss: 2.5257719805334204

Epoch: 6| Step: 13
Training loss: 1.8327556552510917
Validation loss: 2.4444218313203505

Epoch: 192| Step: 0
Training loss: 1.5809556106522356
Validation loss: 2.3608070845750575

Epoch: 6| Step: 1
Training loss: 1.3334416951412331
Validation loss: 2.334491359604892

Epoch: 6| Step: 2
Training loss: 1.8186137905437438
Validation loss: 2.3362836310729094

Epoch: 6| Step: 3
Training loss: 1.574044873500735
Validation loss: 2.3300768366208757

Epoch: 6| Step: 4
Training loss: 2.056499422013461
Validation loss: 2.3414230876943765

Epoch: 6| Step: 5
Training loss: 1.405150598348991
Validation loss: 2.3442278740569864

Epoch: 6| Step: 6
Training loss: 1.8270405218081118
Validation loss: 2.392432640194601

Epoch: 6| Step: 7
Training loss: 1.3245105253170772
Validation loss: 2.4119197841860274

Epoch: 6| Step: 8
Training loss: 1.4748922922928123
Validation loss: 2.5315181256453725

Epoch: 6| Step: 9
Training loss: 1.3784986979741647
Validation loss: 2.5553174967618015

Epoch: 6| Step: 10
Training loss: 1.1996482055302131
Validation loss: 2.572428152548225

Epoch: 6| Step: 11
Training loss: 1.3812960051733212
Validation loss: 2.515374734442645

Epoch: 6| Step: 12
Training loss: 1.0400445069363071
Validation loss: 2.423930918984232

Epoch: 6| Step: 13
Training loss: 2.005837979894417
Validation loss: 2.3615330040696443

Epoch: 193| Step: 0
Training loss: 1.6231175302855803
Validation loss: 2.2901966511190266

Epoch: 6| Step: 1
Training loss: 1.1874482494923613
Validation loss: 2.269569902558794

Epoch: 6| Step: 2
Training loss: 1.4649737898008506
Validation loss: 2.253997153992938

Epoch: 6| Step: 3
Training loss: 1.489294393121432
Validation loss: 2.2852259617316277

Epoch: 6| Step: 4
Training loss: 1.8356844463860043
Validation loss: 2.310182884426456

Epoch: 6| Step: 5
Training loss: 1.7708200641209733
Validation loss: 2.3287797530069105

Epoch: 6| Step: 6
Training loss: 1.4898578773152789
Validation loss: 2.4216708824685

Epoch: 6| Step: 7
Training loss: 1.714722370553015
Validation loss: 2.5109454133294307

Epoch: 6| Step: 8
Training loss: 1.5859657623328418
Validation loss: 2.643279256466803

Epoch: 6| Step: 9
Training loss: 1.8739094423705662
Validation loss: 2.6605152521946955

Epoch: 6| Step: 10
Training loss: 1.1880961979440692
Validation loss: 2.544264878668783

Epoch: 6| Step: 11
Training loss: 0.8649741786895445
Validation loss: 2.4204908297025955

Epoch: 6| Step: 12
Training loss: 1.380967973596021
Validation loss: 2.335626279428352

Epoch: 6| Step: 13
Training loss: 1.616791460130006
Validation loss: 2.297600202581892

Epoch: 194| Step: 0
Training loss: 1.5052146867488192
Validation loss: 2.2690091463688122

Epoch: 6| Step: 1
Training loss: 1.6616309942219007
Validation loss: 2.2519053990309055

Epoch: 6| Step: 2
Training loss: 1.4435068759734628
Validation loss: 2.258772524902459

Epoch: 6| Step: 3
Training loss: 1.4777947183863365
Validation loss: 2.245474210509054

Epoch: 6| Step: 4
Training loss: 1.997451827861304
Validation loss: 2.266001285455191

Epoch: 6| Step: 5
Training loss: 1.2639765423758915
Validation loss: 2.3392343095335053

Epoch: 6| Step: 6
Training loss: 1.4977864781424772
Validation loss: 2.4452230549899974

Epoch: 6| Step: 7
Training loss: 1.5102790068234206
Validation loss: 2.539544110053851

Epoch: 6| Step: 8
Training loss: 1.5980449474158045
Validation loss: 2.6585075518319874

Epoch: 6| Step: 9
Training loss: 1.8464374030271726
Validation loss: 2.739781551301349

Epoch: 6| Step: 10
Training loss: 1.6822895565748022
Validation loss: 2.630810455211668

Epoch: 6| Step: 11
Training loss: 1.4273387188501745
Validation loss: 2.5769566916366653

Epoch: 6| Step: 12
Training loss: 1.352170862422154
Validation loss: 2.425525399934777

Epoch: 6| Step: 13
Training loss: 1.1056745357856343
Validation loss: 2.3387479755277116

Epoch: 195| Step: 0
Training loss: 1.5140786688761103
Validation loss: 2.2942633146431666

Epoch: 6| Step: 1
Training loss: 1.6190915899823917
Validation loss: 2.2499047684199103

Epoch: 6| Step: 2
Training loss: 1.7611292764239619
Validation loss: 2.2653693270241635

Epoch: 6| Step: 3
Training loss: 1.293654982906174
Validation loss: 2.2984716109141057

Epoch: 6| Step: 4
Training loss: 1.4935081988512855
Validation loss: 2.382530888902519

Epoch: 6| Step: 5
Training loss: 1.496702224858452
Validation loss: 2.4582413068189854

Epoch: 6| Step: 6
Training loss: 1.499731675626328
Validation loss: 2.5395069182053076

Epoch: 6| Step: 7
Training loss: 1.5667235702298974
Validation loss: 2.5793810596592324

Epoch: 6| Step: 8
Training loss: 1.93747034357816
Validation loss: 2.579256081959914

Epoch: 6| Step: 9
Training loss: 1.9505710841497905
Validation loss: 2.5161637723068955

Epoch: 6| Step: 10
Training loss: 1.3259153047673369
Validation loss: 2.4359486567701083

Epoch: 6| Step: 11
Training loss: 1.1579502080447643
Validation loss: 2.3814703007043967

Epoch: 6| Step: 12
Training loss: 1.1370361514219778
Validation loss: 2.353030990873098

Epoch: 6| Step: 13
Training loss: 1.2280592806172694
Validation loss: 2.3635297904415387

Epoch: 196| Step: 0
Training loss: 1.28440310779268
Validation loss: 2.317000500967077

Epoch: 6| Step: 1
Training loss: 1.7200232904542891
Validation loss: 2.305558587485887

Epoch: 6| Step: 2
Training loss: 1.807688080537664
Validation loss: 2.30811093289163

Epoch: 6| Step: 3
Training loss: 1.7427168927055645
Validation loss: 2.2951527407701566

Epoch: 6| Step: 4
Training loss: 1.5746481335884686
Validation loss: 2.334771549364658

Epoch: 6| Step: 5
Training loss: 1.3238241682446539
Validation loss: 2.398535295134541

Epoch: 6| Step: 6
Training loss: 1.5891641199858006
Validation loss: 2.457462182160895

Epoch: 6| Step: 7
Training loss: 0.9996418609647275
Validation loss: 2.4921986589964047

Epoch: 6| Step: 8
Training loss: 1.4025500897525305
Validation loss: 2.5072134689776755

Epoch: 6| Step: 9
Training loss: 1.395101991432097
Validation loss: 2.4911696260070864

Epoch: 6| Step: 10
Training loss: 2.0012498764810043
Validation loss: 2.4558604601833363

Epoch: 6| Step: 11
Training loss: 1.4422224674965902
Validation loss: 2.4605209005779094

Epoch: 6| Step: 12
Training loss: 1.3019000216829746
Validation loss: 2.4415859959873614

Epoch: 6| Step: 13
Training loss: 0.6992103746647815
Validation loss: 2.424903891024441

Epoch: 197| Step: 0
Training loss: 1.1903977425023178
Validation loss: 2.413425085161722

Epoch: 6| Step: 1
Training loss: 1.2848612901950969
Validation loss: 2.4043824334823514

Epoch: 6| Step: 2
Training loss: 1.5519231248476877
Validation loss: 2.3987731346021905

Epoch: 6| Step: 3
Training loss: 1.181624483360824
Validation loss: 2.43269215380839

Epoch: 6| Step: 4
Training loss: 1.520823343670927
Validation loss: 2.4538348683368394

Epoch: 6| Step: 5
Training loss: 1.4641041427112846
Validation loss: 2.4813969143024663

Epoch: 6| Step: 6
Training loss: 1.5353912872466755
Validation loss: 2.487438485014701

Epoch: 6| Step: 7
Training loss: 1.4224411392772116
Validation loss: 2.521728977551849

Epoch: 6| Step: 8
Training loss: 1.3455433079310626
Validation loss: 2.55352835882016

Epoch: 6| Step: 9
Training loss: 2.0367551881539834
Validation loss: 2.519211110772301

Epoch: 6| Step: 10
Training loss: 1.5159087554527806
Validation loss: 2.491212765649455

Epoch: 6| Step: 11
Training loss: 1.050063558198895
Validation loss: 2.422501785726535

Epoch: 6| Step: 12
Training loss: 1.5288505438790965
Validation loss: 2.37045589542187

Epoch: 6| Step: 13
Training loss: 1.4824314443809632
Validation loss: 2.3683944241325054

Epoch: 198| Step: 0
Training loss: 1.772293393962421
Validation loss: 2.373361121374251

Epoch: 6| Step: 1
Training loss: 1.9942436826791659
Validation loss: 2.3917510712747934

Epoch: 6| Step: 2
Training loss: 1.2989354139486633
Validation loss: 2.429240164166154

Epoch: 6| Step: 3
Training loss: 1.1548323317200582
Validation loss: 2.47515714466989

Epoch: 6| Step: 4
Training loss: 1.2550839511559209
Validation loss: 2.5433309913363953

Epoch: 6| Step: 5
Training loss: 1.4289924307770205
Validation loss: 2.5936563897219282

Epoch: 6| Step: 6
Training loss: 1.3267896334211238
Validation loss: 2.6209706194903637

Epoch: 6| Step: 7
Training loss: 1.316293683318901
Validation loss: 2.634361188676181

Epoch: 6| Step: 8
Training loss: 1.33086041763126
Validation loss: 2.573339249139564

Epoch: 6| Step: 9
Training loss: 1.0419594671086836
Validation loss: 2.4701192295340526

Epoch: 6| Step: 10
Training loss: 1.2387567323257316
Validation loss: 2.4151413113194904

Epoch: 6| Step: 11
Training loss: 1.377472302111631
Validation loss: 2.3871390051918318

Epoch: 6| Step: 12
Training loss: 1.8976874734093123
Validation loss: 2.3324734004864567

Epoch: 6| Step: 13
Training loss: 1.2686315077147652
Validation loss: 2.329526742847253

Epoch: 199| Step: 0
Training loss: 1.6037710453356282
Validation loss: 2.3165877856499093

Epoch: 6| Step: 1
Training loss: 1.4784003229169782
Validation loss: 2.328576948344032

Epoch: 6| Step: 2
Training loss: 1.2908119121733586
Validation loss: 2.3761842421934385

Epoch: 6| Step: 3
Training loss: 1.4636020127567486
Validation loss: 2.4039246993062586

Epoch: 6| Step: 4
Training loss: 0.9329511274271566
Validation loss: 2.454320988739804

Epoch: 6| Step: 5
Training loss: 1.1255391206657581
Validation loss: 2.4790627135559027

Epoch: 6| Step: 6
Training loss: 1.4838877631155833
Validation loss: 2.4744745139373974

Epoch: 6| Step: 7
Training loss: 1.3828565515085687
Validation loss: 2.4755152071108237

Epoch: 6| Step: 8
Training loss: 1.4917745767841244
Validation loss: 2.4484912024412284

Epoch: 6| Step: 9
Training loss: 1.212056092478181
Validation loss: 2.4516702056925253

Epoch: 6| Step: 10
Training loss: 1.821382265544158
Validation loss: 2.4185839682809624

Epoch: 6| Step: 11
Training loss: 0.7782597727665922
Validation loss: 2.414763838342724

Epoch: 6| Step: 12
Training loss: 1.4593109214813715
Validation loss: 2.408574799243743

Epoch: 6| Step: 13
Training loss: 1.7228230851884578
Validation loss: 2.404872845923353

Epoch: 200| Step: 0
Training loss: 2.1276886704159876
Validation loss: 2.3955306095089184

Epoch: 6| Step: 1
Training loss: 1.4302560478339963
Validation loss: 2.4038840057573383

Epoch: 6| Step: 2
Training loss: 0.9499384659363741
Validation loss: 2.3982084431533077

Epoch: 6| Step: 3
Training loss: 0.8682371780279139
Validation loss: 2.4177852850393386

Epoch: 6| Step: 4
Training loss: 1.138851249791344
Validation loss: 2.4547699481076006

Epoch: 6| Step: 5
Training loss: 1.1320747867318246
Validation loss: 2.4931971416596785

Epoch: 6| Step: 6
Training loss: 1.3089408755723149
Validation loss: 2.513963271136916

Epoch: 6| Step: 7
Training loss: 1.5060205435193907
Validation loss: 2.4867402906762184

Epoch: 6| Step: 8
Training loss: 1.5395362241125132
Validation loss: 2.4524633506664117

Epoch: 6| Step: 9
Training loss: 1.3771212427843385
Validation loss: 2.378897362561155

Epoch: 6| Step: 10
Training loss: 1.27675183315082
Validation loss: 2.3646042268896172

Epoch: 6| Step: 11
Training loss: 1.5507415843282033
Validation loss: 2.365421319244651

Epoch: 6| Step: 12
Training loss: 1.4732187345363275
Validation loss: 2.3440455643433773

Epoch: 6| Step: 13
Training loss: 0.994386114301734
Validation loss: 2.374173023089232

Testing loss: 2.4824399427064403

Epoch: 1| Step: 0
Training loss: 6.294319152832031
Validation loss: 5.225263103362052

Epoch: 6| Step: 1
Training loss: 4.728137493133545
Validation loss: 5.211437297123735

Epoch: 6| Step: 2
Training loss: 3.723794460296631
Validation loss: 5.199074119649907

Epoch: 6| Step: 3
Training loss: 5.9422760009765625
Validation loss: 5.186731061627788

Epoch: 6| Step: 4
Training loss: 4.979789733886719
Validation loss: 5.172998110453288

Epoch: 6| Step: 5
Training loss: 5.402691841125488
Validation loss: 5.157394188706593

Epoch: 6| Step: 6
Training loss: 2.8768820762634277
Validation loss: 5.138992324952157

Epoch: 6| Step: 7
Training loss: 3.832040786743164
Validation loss: 5.118590970193186

Epoch: 6| Step: 8
Training loss: 4.707138538360596
Validation loss: 5.094544220996159

Epoch: 6| Step: 9
Training loss: 5.0837860107421875
Validation loss: 5.0678075308440835

Epoch: 6| Step: 10
Training loss: 5.463456153869629
Validation loss: 5.038024707507062

Epoch: 6| Step: 11
Training loss: 4.765303611755371
Validation loss: 5.0048157938065065

Epoch: 6| Step: 12
Training loss: 6.250082015991211
Validation loss: 4.96888336571314

Epoch: 6| Step: 13
Training loss: 4.479889869689941
Validation loss: 4.932006374482186

Epoch: 2| Step: 0
Training loss: 5.082155704498291
Validation loss: 4.892929774458691

Epoch: 6| Step: 1
Training loss: 5.530180931091309
Validation loss: 4.853496243876796

Epoch: 6| Step: 2
Training loss: 5.096324443817139
Validation loss: 4.812611697822489

Epoch: 6| Step: 3
Training loss: 2.954774856567383
Validation loss: 4.768948478083456

Epoch: 6| Step: 4
Training loss: 4.565065383911133
Validation loss: 4.724394577805714

Epoch: 6| Step: 5
Training loss: 3.37555193901062
Validation loss: 4.679926087779384

Epoch: 6| Step: 6
Training loss: 4.553493976593018
Validation loss: 4.6349852520932435

Epoch: 6| Step: 7
Training loss: 4.28265380859375
Validation loss: 4.591470959366009

Epoch: 6| Step: 8
Training loss: 4.739477157592773
Validation loss: 4.55238014651883

Epoch: 6| Step: 9
Training loss: 4.569323539733887
Validation loss: 4.513797549791233

Epoch: 6| Step: 10
Training loss: 4.096404075622559
Validation loss: 4.472781612027076

Epoch: 6| Step: 11
Training loss: 4.8857221603393555
Validation loss: 4.431797376243017

Epoch: 6| Step: 12
Training loss: 3.8251352310180664
Validation loss: 4.39398213355772

Epoch: 6| Step: 13
Training loss: 4.566397190093994
Validation loss: 4.362885736650036

Epoch: 3| Step: 0
Training loss: 4.770941734313965
Validation loss: 4.3314087980537

Epoch: 6| Step: 1
Training loss: 2.529851198196411
Validation loss: 4.29788250307883

Epoch: 6| Step: 2
Training loss: 4.269131660461426
Validation loss: 4.25457945177632

Epoch: 6| Step: 3
Training loss: 4.1530046463012695
Validation loss: 4.204425140093732

Epoch: 6| Step: 4
Training loss: 4.013814449310303
Validation loss: 4.156549776754072

Epoch: 6| Step: 5
Training loss: 4.564839839935303
Validation loss: 4.1128589158417075

Epoch: 6| Step: 6
Training loss: 2.7595252990722656
Validation loss: 4.075307692250898

Epoch: 6| Step: 7
Training loss: 4.675909042358398
Validation loss: 4.04031354124828

Epoch: 6| Step: 8
Training loss: 4.049141883850098
Validation loss: 3.9963000359073764

Epoch: 6| Step: 9
Training loss: 2.6372249126434326
Validation loss: 3.956295303119126

Epoch: 6| Step: 10
Training loss: 4.355649948120117
Validation loss: 3.9179445030868694

Epoch: 6| Step: 11
Training loss: 3.4710605144500732
Validation loss: 3.8788807725393646

Epoch: 6| Step: 12
Training loss: 4.723277568817139
Validation loss: 3.85137907151253

Epoch: 6| Step: 13
Training loss: 3.6416730880737305
Validation loss: 3.8199800522096696

Epoch: 4| Step: 0
Training loss: 2.524289608001709
Validation loss: 3.7839503416451077

Epoch: 6| Step: 1
Training loss: 3.526590347290039
Validation loss: 3.7484220279160367

Epoch: 6| Step: 2
Training loss: 4.439901828765869
Validation loss: 3.7117551834352556

Epoch: 6| Step: 3
Training loss: 4.064339637756348
Validation loss: 3.675418646104874

Epoch: 6| Step: 4
Training loss: 3.530761241912842
Validation loss: 3.644150164819533

Epoch: 6| Step: 5
Training loss: 3.9132041931152344
Validation loss: 3.634823481241862

Epoch: 6| Step: 6
Training loss: 3.463930130004883
Validation loss: 3.58829177066844

Epoch: 6| Step: 7
Training loss: 3.28389048576355
Validation loss: 3.573496162250478

Epoch: 6| Step: 8
Training loss: 2.5564191341400146
Validation loss: 3.5642956405557613

Epoch: 6| Step: 9
Training loss: 2.450974941253662
Validation loss: 3.5534137961685017

Epoch: 6| Step: 10
Training loss: 4.979950904846191
Validation loss: 3.5400901327851

Epoch: 6| Step: 11
Training loss: 3.4466214179992676
Validation loss: 3.522972322279407

Epoch: 6| Step: 12
Training loss: 3.5668723583221436
Validation loss: 3.5063010543905277

Epoch: 6| Step: 13
Training loss: 3.7484092712402344
Validation loss: 3.4859047448763283

Epoch: 5| Step: 0
Training loss: 3.45928955078125
Validation loss: 3.4731056433852

Epoch: 6| Step: 1
Training loss: 3.14420747756958
Validation loss: 3.457047193281112

Epoch: 6| Step: 2
Training loss: 3.312406063079834
Validation loss: 3.4410580153106363

Epoch: 6| Step: 3
Training loss: 2.620727300643921
Validation loss: 3.432475274608981

Epoch: 6| Step: 4
Training loss: 4.589435577392578
Validation loss: 3.4170086204364734

Epoch: 6| Step: 5
Training loss: 3.337486505508423
Validation loss: 3.401077257689609

Epoch: 6| Step: 6
Training loss: 2.8380444049835205
Validation loss: 3.3930979621025825

Epoch: 6| Step: 7
Training loss: 3.1307578086853027
Validation loss: 3.385494368050688

Epoch: 6| Step: 8
Training loss: 2.386946678161621
Validation loss: 3.370948055739044

Epoch: 6| Step: 9
Training loss: 4.014973163604736
Validation loss: 3.3598869359621437

Epoch: 6| Step: 10
Training loss: 4.535391807556152
Validation loss: 3.3503458730636106

Epoch: 6| Step: 11
Training loss: 3.032362699508667
Validation loss: 3.34149137620003

Epoch: 6| Step: 12
Training loss: 3.6379852294921875
Validation loss: 3.327155905385171

Epoch: 6| Step: 13
Training loss: 2.4568755626678467
Validation loss: 3.320320280649329

Epoch: 6| Step: 0
Training loss: 3.276315689086914
Validation loss: 3.310957911194012

Epoch: 6| Step: 1
Training loss: 3.1021292209625244
Validation loss: 3.2982671158288115

Epoch: 6| Step: 2
Training loss: 4.261786937713623
Validation loss: 3.2937091242882515

Epoch: 6| Step: 3
Training loss: 3.2203938961029053
Validation loss: 3.2810878907480547

Epoch: 6| Step: 4
Training loss: 3.827307939529419
Validation loss: 3.2709785917753815

Epoch: 6| Step: 5
Training loss: 2.8680100440979004
Validation loss: 3.2715790912669194

Epoch: 6| Step: 6
Training loss: 3.1691184043884277
Validation loss: 3.2845548532342397

Epoch: 6| Step: 7
Training loss: 3.568329334259033
Validation loss: 3.261830235040316

Epoch: 6| Step: 8
Training loss: 3.214400291442871
Validation loss: 3.243112705087149

Epoch: 6| Step: 9
Training loss: 2.62679123878479
Validation loss: 3.235731137696133

Epoch: 6| Step: 10
Training loss: 2.641779661178589
Validation loss: 3.238213021268127

Epoch: 6| Step: 11
Training loss: 3.3132729530334473
Validation loss: 3.225026725440897

Epoch: 6| Step: 12
Training loss: 3.260711193084717
Validation loss: 3.2146523562810754

Epoch: 6| Step: 13
Training loss: 2.8476388454437256
Validation loss: 3.201438319298529

Epoch: 7| Step: 0
Training loss: 3.8031933307647705
Validation loss: 3.1966390148285897

Epoch: 6| Step: 1
Training loss: 2.2579681873321533
Validation loss: 3.186420020236764

Epoch: 6| Step: 2
Training loss: 2.675654888153076
Validation loss: 3.175612152263682

Epoch: 6| Step: 3
Training loss: 3.2921860218048096
Validation loss: 3.1714800275782102

Epoch: 6| Step: 4
Training loss: 3.474264144897461
Validation loss: 3.158281010966147

Epoch: 6| Step: 5
Training loss: 2.2845962047576904
Validation loss: 3.1498860236137145

Epoch: 6| Step: 6
Training loss: 3.4179556369781494
Validation loss: 3.144515875847109

Epoch: 6| Step: 7
Training loss: 4.009710788726807
Validation loss: 3.1340651486509588

Epoch: 6| Step: 8
Training loss: 3.0276095867156982
Validation loss: 3.125412364159861

Epoch: 6| Step: 9
Training loss: 3.6011877059936523
Validation loss: 3.1191745368383264

Epoch: 6| Step: 10
Training loss: 3.0695416927337646
Validation loss: 3.1124635537465415

Epoch: 6| Step: 11
Training loss: 3.193821907043457
Validation loss: 3.1081461316795758

Epoch: 6| Step: 12
Training loss: 3.030089855194092
Validation loss: 3.1020089272529847

Epoch: 6| Step: 13
Training loss: 3.044787645339966
Validation loss: 3.0968879781743532

Epoch: 8| Step: 0
Training loss: 2.6133878231048584
Validation loss: 3.0879365141673754

Epoch: 6| Step: 1
Training loss: 2.8233163356781006
Validation loss: 3.0832499509216635

Epoch: 6| Step: 2
Training loss: 2.1853384971618652
Validation loss: 3.079911590904318

Epoch: 6| Step: 3
Training loss: 3.1825308799743652
Validation loss: 3.0690742154275217

Epoch: 6| Step: 4
Training loss: 3.2424631118774414
Validation loss: 3.066809759345106

Epoch: 6| Step: 5
Training loss: 3.594083547592163
Validation loss: 3.0610245658505346

Epoch: 6| Step: 6
Training loss: 3.4592363834381104
Validation loss: 3.057508284045804

Epoch: 6| Step: 7
Training loss: 3.1585915088653564
Validation loss: 3.051086687272595

Epoch: 6| Step: 8
Training loss: 3.409144878387451
Validation loss: 3.0509429362512406

Epoch: 6| Step: 9
Training loss: 2.4358253479003906
Validation loss: 3.0439265235777824

Epoch: 6| Step: 10
Training loss: 3.632172107696533
Validation loss: 3.0383038700267835

Epoch: 6| Step: 11
Training loss: 2.576289415359497
Validation loss: 3.038676631066107

Epoch: 6| Step: 12
Training loss: 3.8046891689300537
Validation loss: 3.0283462488523094

Epoch: 6| Step: 13
Training loss: 3.3682472705841064
Validation loss: 3.027355768347299

Epoch: 9| Step: 0
Training loss: 3.0837488174438477
Validation loss: 3.0238011626787085

Epoch: 6| Step: 1
Training loss: 3.6075263023376465
Validation loss: 3.024816495116039

Epoch: 6| Step: 2
Training loss: 2.349332571029663
Validation loss: 3.017855462207589

Epoch: 6| Step: 3
Training loss: 2.8239381313323975
Validation loss: 3.008392810821533

Epoch: 6| Step: 4
Training loss: 3.2263574600219727
Validation loss: 3.006661617627708

Epoch: 6| Step: 5
Training loss: 3.1520698070526123
Validation loss: 3.0139300951393704

Epoch: 6| Step: 6
Training loss: 3.167701482772827
Validation loss: 2.9987353535108667

Epoch: 6| Step: 7
Training loss: 2.934647798538208
Validation loss: 2.99576150473728

Epoch: 6| Step: 8
Training loss: 3.756483793258667
Validation loss: 2.989448006435107

Epoch: 6| Step: 9
Training loss: 3.0772392749786377
Validation loss: 2.9877831679518505

Epoch: 6| Step: 10
Training loss: 3.0432209968566895
Validation loss: 2.993267220835532

Epoch: 6| Step: 11
Training loss: 2.453101634979248
Validation loss: 2.995362973982288

Epoch: 6| Step: 12
Training loss: 3.3199665546417236
Validation loss: 2.988710134260116

Epoch: 6| Step: 13
Training loss: 2.8428874015808105
Validation loss: 2.998982065467424

Epoch: 10| Step: 0
Training loss: 3.485323905944824
Validation loss: 2.997602683241649

Epoch: 6| Step: 1
Training loss: 2.810274600982666
Validation loss: 2.9814179199998097

Epoch: 6| Step: 2
Training loss: 2.7534091472625732
Validation loss: 3.0020369816851873

Epoch: 6| Step: 3
Training loss: 3.481696844100952
Validation loss: 3.0252891509763655

Epoch: 6| Step: 4
Training loss: 2.8331260681152344
Validation loss: 3.0226441096234065

Epoch: 6| Step: 5
Training loss: 3.807788372039795
Validation loss: 2.981826138752763

Epoch: 6| Step: 6
Training loss: 2.0871806144714355
Validation loss: 2.9673206344727547

Epoch: 6| Step: 7
Training loss: 3.121419906616211
Validation loss: 2.978286845709688

Epoch: 6| Step: 8
Training loss: 3.7616164684295654
Validation loss: 2.997542678668935

Epoch: 6| Step: 9
Training loss: 2.2698543071746826
Validation loss: 2.9851404466936664

Epoch: 6| Step: 10
Training loss: 3.358919620513916
Validation loss: 2.9723074410551336

Epoch: 6| Step: 11
Training loss: 2.8625402450561523
Validation loss: 2.9601353112087456

Epoch: 6| Step: 12
Training loss: 2.8802719116210938
Validation loss: 2.957438399714808

Epoch: 6| Step: 13
Training loss: 3.4026107788085938
Validation loss: 2.9532762009610414

Epoch: 11| Step: 0
Training loss: 2.988557815551758
Validation loss: 2.9573127941418718

Epoch: 6| Step: 1
Training loss: 3.120605945587158
Validation loss: 2.959185169589135

Epoch: 6| Step: 2
Training loss: 3.2525382041931152
Validation loss: 2.956882446042953

Epoch: 6| Step: 3
Training loss: 3.1679859161376953
Validation loss: 2.950433067096177

Epoch: 6| Step: 4
Training loss: 3.391636848449707
Validation loss: 2.9415637857170513

Epoch: 6| Step: 5
Training loss: 3.2179806232452393
Validation loss: 2.933546373921056

Epoch: 6| Step: 6
Training loss: 3.9714226722717285
Validation loss: 2.9306769704306

Epoch: 6| Step: 7
Training loss: 2.616447687149048
Validation loss: 2.929028703320411

Epoch: 6| Step: 8
Training loss: 2.095653533935547
Validation loss: 2.927034601088493

Epoch: 6| Step: 9
Training loss: 3.2276806831359863
Validation loss: 2.9272298146319646

Epoch: 6| Step: 10
Training loss: 2.5879056453704834
Validation loss: 2.9248311981078117

Epoch: 6| Step: 11
Training loss: 3.1103832721710205
Validation loss: 2.9238437580805954

Epoch: 6| Step: 12
Training loss: 2.8828043937683105
Validation loss: 2.921115626570999

Epoch: 6| Step: 13
Training loss: 2.4291648864746094
Validation loss: 2.9219239065724034

Epoch: 12| Step: 0
Training loss: 2.8712821006774902
Validation loss: 2.918674297230218

Epoch: 6| Step: 1
Training loss: 2.7874045372009277
Validation loss: 2.919014641033706

Epoch: 6| Step: 2
Training loss: 3.5689187049865723
Validation loss: 2.92089629942371

Epoch: 6| Step: 3
Training loss: 3.121795177459717
Validation loss: 2.9206853579449397

Epoch: 6| Step: 4
Training loss: 2.9477739334106445
Validation loss: 2.917748074377737

Epoch: 6| Step: 5
Training loss: 2.5869979858398438
Validation loss: 2.9175609542477514

Epoch: 6| Step: 6
Training loss: 2.334517002105713
Validation loss: 2.9112180945693806

Epoch: 6| Step: 7
Training loss: 3.6796021461486816
Validation loss: 2.908642171531595

Epoch: 6| Step: 8
Training loss: 3.7431979179382324
Validation loss: 2.9094130915980183

Epoch: 6| Step: 9
Training loss: 2.4906020164489746
Validation loss: 2.914751401511572

Epoch: 6| Step: 10
Training loss: 2.9394850730895996
Validation loss: 2.903858395032985

Epoch: 6| Step: 11
Training loss: 2.540215253829956
Validation loss: 2.9032616282022126

Epoch: 6| Step: 12
Training loss: 3.4429521560668945
Validation loss: 2.902496417363485

Epoch: 6| Step: 13
Training loss: 2.948692798614502
Validation loss: 2.9007201681854906

Epoch: 13| Step: 0
Training loss: 2.7742700576782227
Validation loss: 2.9027584393819175

Epoch: 6| Step: 1
Training loss: 3.551974296569824
Validation loss: 2.902332951945643

Epoch: 6| Step: 2
Training loss: 2.950498580932617
Validation loss: 2.9052644083576817

Epoch: 6| Step: 3
Training loss: 3.1069109439849854
Validation loss: 2.9006831389601513

Epoch: 6| Step: 4
Training loss: 2.6813855171203613
Validation loss: 2.897606949652395

Epoch: 6| Step: 5
Training loss: 3.0298290252685547
Validation loss: 2.8942985457758748

Epoch: 6| Step: 6
Training loss: 3.8873934745788574
Validation loss: 2.8904208470416326

Epoch: 6| Step: 7
Training loss: 2.6534132957458496
Validation loss: 2.8903081288901706

Epoch: 6| Step: 8
Training loss: 2.602047920227051
Validation loss: 2.8924598847666094

Epoch: 6| Step: 9
Training loss: 2.8303287029266357
Validation loss: 2.8912035931823072

Epoch: 6| Step: 10
Training loss: 3.2513952255249023
Validation loss: 2.8919278139709146

Epoch: 6| Step: 11
Training loss: 2.3958940505981445
Validation loss: 2.8973365573472876

Epoch: 6| Step: 12
Training loss: 3.801023006439209
Validation loss: 2.920401544981105

Epoch: 6| Step: 13
Training loss: 1.9646426439285278
Validation loss: 2.8864952671912407

Epoch: 14| Step: 0
Training loss: 2.422086238861084
Validation loss: 2.885608847423266

Epoch: 6| Step: 1
Training loss: 3.645418167114258
Validation loss: 2.889104671375726

Epoch: 6| Step: 2
Training loss: 1.9965438842773438
Validation loss: 2.893434827045728

Epoch: 6| Step: 3
Training loss: 2.513807535171509
Validation loss: 2.9004141694755963

Epoch: 6| Step: 4
Training loss: 2.9984130859375
Validation loss: 2.8899101621361187

Epoch: 6| Step: 5
Training loss: 3.293789863586426
Validation loss: 2.8827257899827856

Epoch: 6| Step: 6
Training loss: 3.1299149990081787
Validation loss: 2.8813794582120833

Epoch: 6| Step: 7
Training loss: 3.119011402130127
Validation loss: 2.879432196258217

Epoch: 6| Step: 8
Training loss: 2.353641986846924
Validation loss: 2.879083976950697

Epoch: 6| Step: 9
Training loss: 4.26591682434082
Validation loss: 2.880694696980138

Epoch: 6| Step: 10
Training loss: 2.3434395790100098
Validation loss: 2.8811289443764636

Epoch: 6| Step: 11
Training loss: 3.596752643585205
Validation loss: 2.8820154820719073

Epoch: 6| Step: 12
Training loss: 2.745128631591797
Validation loss: 2.880084240308372

Epoch: 6| Step: 13
Training loss: 3.74509334564209
Validation loss: 2.8813550728623585

Epoch: 15| Step: 0
Training loss: 2.9319045543670654
Validation loss: 2.8800725090888237

Epoch: 6| Step: 1
Training loss: 2.2597780227661133
Validation loss: 2.871005981199203

Epoch: 6| Step: 2
Training loss: 2.558021068572998
Validation loss: 2.8742916430196455

Epoch: 6| Step: 3
Training loss: 3.5467910766601562
Validation loss: 2.87355129180416

Epoch: 6| Step: 4
Training loss: 2.139853000640869
Validation loss: 2.872063693179879

Epoch: 6| Step: 5
Training loss: 3.426572322845459
Validation loss: 2.869128780980264

Epoch: 6| Step: 6
Training loss: 3.0585737228393555
Validation loss: 2.8687977944650958

Epoch: 6| Step: 7
Training loss: 3.4814043045043945
Validation loss: 2.876259094925337

Epoch: 6| Step: 8
Training loss: 2.884944438934326
Validation loss: 2.930314399862802

Epoch: 6| Step: 9
Training loss: 2.9750938415527344
Validation loss: 2.9553105677327802

Epoch: 6| Step: 10
Training loss: 2.703603982925415
Validation loss: 2.9232583943233696

Epoch: 6| Step: 11
Training loss: 3.219303607940674
Validation loss: 2.855966273174491

Epoch: 6| Step: 12
Training loss: 3.333528995513916
Validation loss: 2.856697056883125

Epoch: 6| Step: 13
Training loss: 3.2829771041870117
Validation loss: 2.8595725951656217

Epoch: 16| Step: 0
Training loss: 1.9772791862487793
Validation loss: 2.863509706271592

Epoch: 6| Step: 1
Training loss: 2.3996801376342773
Validation loss: 2.872719728818504

Epoch: 6| Step: 2
Training loss: 3.3351211547851562
Validation loss: 2.872384763533069

Epoch: 6| Step: 3
Training loss: 2.8904149532318115
Validation loss: 2.864353146604312

Epoch: 6| Step: 4
Training loss: 3.19887638092041
Validation loss: 2.864623474818404

Epoch: 6| Step: 5
Training loss: 2.718987464904785
Validation loss: 2.859795965174193

Epoch: 6| Step: 6
Training loss: 3.7377359867095947
Validation loss: 2.8549196797032512

Epoch: 6| Step: 7
Training loss: 3.600574493408203
Validation loss: 2.8540581426312848

Epoch: 6| Step: 8
Training loss: 2.502969741821289
Validation loss: 2.847660151861047

Epoch: 6| Step: 9
Training loss: 2.1915533542633057
Validation loss: 2.8477783369761642

Epoch: 6| Step: 10
Training loss: 2.996260643005371
Validation loss: 2.8452809138964583

Epoch: 6| Step: 11
Training loss: 3.078644037246704
Validation loss: 2.843286322009179

Epoch: 6| Step: 12
Training loss: 3.632481575012207
Validation loss: 2.8409402344816472

Epoch: 6| Step: 13
Training loss: 3.5748748779296875
Validation loss: 2.8416738587041057

Epoch: 17| Step: 0
Training loss: 3.8965649604797363
Validation loss: 2.8364718549995014

Epoch: 6| Step: 1
Training loss: 2.7768702507019043
Validation loss: 2.8328976118436424

Epoch: 6| Step: 2
Training loss: 3.5283584594726562
Validation loss: 2.8343393213005474

Epoch: 6| Step: 3
Training loss: 3.370084762573242
Validation loss: 2.8303334712982178

Epoch: 6| Step: 4
Training loss: 2.3871374130249023
Validation loss: 2.83087432512673

Epoch: 6| Step: 5
Training loss: 3.271512031555176
Validation loss: 2.8314669362960325

Epoch: 6| Step: 6
Training loss: 2.0418412685394287
Validation loss: 2.8313205780521518

Epoch: 6| Step: 7
Training loss: 2.7764220237731934
Validation loss: 2.8403549194335938

Epoch: 6| Step: 8
Training loss: 3.343243360519409
Validation loss: 2.8344444074938373

Epoch: 6| Step: 9
Training loss: 2.8118855953216553
Validation loss: 2.82660525332215

Epoch: 6| Step: 10
Training loss: 2.2005701065063477
Validation loss: 2.8222876441094185

Epoch: 6| Step: 11
Training loss: 3.529296398162842
Validation loss: 2.8231848875681558

Epoch: 6| Step: 12
Training loss: 2.5187828540802
Validation loss: 2.822404482031381

Epoch: 6| Step: 13
Training loss: 2.8108670711517334
Validation loss: 2.8173342033099105

Epoch: 18| Step: 0
Training loss: 3.065293788909912
Validation loss: 2.8174423325446343

Epoch: 6| Step: 1
Training loss: 2.380012035369873
Validation loss: 2.8181610184331096

Epoch: 6| Step: 2
Training loss: 2.8140387535095215
Validation loss: 2.8180581754253757

Epoch: 6| Step: 3
Training loss: 2.6917903423309326
Validation loss: 2.821230776848332

Epoch: 6| Step: 4
Training loss: 3.0013272762298584
Validation loss: 2.81956224800438

Epoch: 6| Step: 5
Training loss: 3.4164648056030273
Validation loss: 2.81879243030343

Epoch: 6| Step: 6
Training loss: 1.9420067071914673
Validation loss: 2.8135381206389396

Epoch: 6| Step: 7
Training loss: 3.3572731018066406
Validation loss: 2.8119692751156387

Epoch: 6| Step: 8
Training loss: 3.484614849090576
Validation loss: 2.8090636217465965

Epoch: 6| Step: 9
Training loss: 3.478846549987793
Validation loss: 2.8134741911324124

Epoch: 6| Step: 10
Training loss: 2.2115426063537598
Validation loss: 2.816925264173938

Epoch: 6| Step: 11
Training loss: 3.2319629192352295
Validation loss: 2.8204377389723256

Epoch: 6| Step: 12
Training loss: 2.6715097427368164
Validation loss: 2.817435977279499

Epoch: 6| Step: 13
Training loss: 3.6900503635406494
Validation loss: 2.8088728689378306

Epoch: 19| Step: 0
Training loss: 3.4470794200897217
Validation loss: 2.8024672923549527

Epoch: 6| Step: 1
Training loss: 3.203216552734375
Validation loss: 2.801778152424802

Epoch: 6| Step: 2
Training loss: 2.9707984924316406
Validation loss: 2.7960755440496627

Epoch: 6| Step: 3
Training loss: 3.444363832473755
Validation loss: 2.7916570325051584

Epoch: 6| Step: 4
Training loss: 2.615936040878296
Validation loss: 2.7899142029464885

Epoch: 6| Step: 5
Training loss: 1.9125562906265259
Validation loss: 2.791914463043213

Epoch: 6| Step: 6
Training loss: 2.709588050842285
Validation loss: 2.789553988364435

Epoch: 6| Step: 7
Training loss: 2.8464126586914062
Validation loss: 2.8035812736839376

Epoch: 6| Step: 8
Training loss: 2.7014245986938477
Validation loss: 2.7809112841083157

Epoch: 6| Step: 9
Training loss: 3.2965376377105713
Validation loss: 2.7655739220239783

Epoch: 6| Step: 10
Training loss: 2.4680566787719727
Validation loss: 2.766057668193694

Epoch: 6| Step: 11
Training loss: 3.752232074737549
Validation loss: 2.7678632736206055

Epoch: 6| Step: 12
Training loss: 2.874209403991699
Validation loss: 2.7702228894797702

Epoch: 6| Step: 13
Training loss: 2.348371982574463
Validation loss: 2.777329150066581

Epoch: 20| Step: 0
Training loss: 2.687757968902588
Validation loss: 2.8087549094230897

Epoch: 6| Step: 1
Training loss: 3.1065430641174316
Validation loss: 2.8046707286629626

Epoch: 6| Step: 2
Training loss: 2.945868492126465
Validation loss: 2.7532361681743334

Epoch: 6| Step: 3
Training loss: 3.355504035949707
Validation loss: 2.7572330685072046

Epoch: 6| Step: 4
Training loss: 3.294234275817871
Validation loss: 2.7722990692302747

Epoch: 6| Step: 5
Training loss: 3.0174193382263184
Validation loss: 2.787062229648713

Epoch: 6| Step: 6
Training loss: 3.132431983947754
Validation loss: 2.804640370030557

Epoch: 6| Step: 7
Training loss: 3.8475875854492188
Validation loss: 2.8044115292128695

Epoch: 6| Step: 8
Training loss: 2.704120635986328
Validation loss: 2.7619193164251183

Epoch: 6| Step: 9
Training loss: 1.7879964113235474
Validation loss: 2.7571182097158125

Epoch: 6| Step: 10
Training loss: 2.134570837020874
Validation loss: 2.7720553516059794

Epoch: 6| Step: 11
Training loss: 2.182985782623291
Validation loss: 2.7891704549071608

Epoch: 6| Step: 12
Training loss: 3.8405113220214844
Validation loss: 2.822603130853304

Epoch: 6| Step: 13
Training loss: 2.78292179107666
Validation loss: 2.8128997561752156

Epoch: 21| Step: 0
Training loss: 2.5291500091552734
Validation loss: 2.766048398069156

Epoch: 6| Step: 1
Training loss: 2.524608612060547
Validation loss: 2.754266377418272

Epoch: 6| Step: 2
Training loss: 2.9941751956939697
Validation loss: 2.746713574214648

Epoch: 6| Step: 3
Training loss: 2.361842632293701
Validation loss: 2.7515378485443773

Epoch: 6| Step: 4
Training loss: 2.9913523197174072
Validation loss: 2.7736665920544694

Epoch: 6| Step: 5
Training loss: 2.9812912940979004
Validation loss: 2.804204176830989

Epoch: 6| Step: 6
Training loss: 2.909322738647461
Validation loss: 2.816538231347197

Epoch: 6| Step: 7
Training loss: 2.591090202331543
Validation loss: 2.824984081329838

Epoch: 6| Step: 8
Training loss: 2.8025875091552734
Validation loss: 2.7392607555594495

Epoch: 6| Step: 9
Training loss: 2.6088054180145264
Validation loss: 2.7313552415499123

Epoch: 6| Step: 10
Training loss: 3.575204372406006
Validation loss: 2.728420175531859

Epoch: 6| Step: 11
Training loss: 2.348220109939575
Validation loss: 2.7264180388501895

Epoch: 6| Step: 12
Training loss: 4.159761905670166
Validation loss: 2.7250857225028415

Epoch: 6| Step: 13
Training loss: 3.4176042079925537
Validation loss: 2.7227606491375993

Epoch: 22| Step: 0
Training loss: 2.3633906841278076
Validation loss: 2.7188066128761537

Epoch: 6| Step: 1
Training loss: 2.6044464111328125
Validation loss: 2.7168661702063774

Epoch: 6| Step: 2
Training loss: 2.490652322769165
Validation loss: 2.711498709135158

Epoch: 6| Step: 3
Training loss: 3.5215351581573486
Validation loss: 2.7107435298222367

Epoch: 6| Step: 4
Training loss: 3.1525044441223145
Validation loss: 2.7102321245337047

Epoch: 6| Step: 5
Training loss: 2.854759693145752
Validation loss: 2.710848767270324

Epoch: 6| Step: 6
Training loss: 2.4559388160705566
Validation loss: 2.7549182573954263

Epoch: 6| Step: 7
Training loss: 3.255894899368286
Validation loss: 2.834025695759763

Epoch: 6| Step: 8
Training loss: 2.4285972118377686
Validation loss: 2.753450180894585

Epoch: 6| Step: 9
Training loss: 2.811007261276245
Validation loss: 2.702599786943005

Epoch: 6| Step: 10
Training loss: 2.9307947158813477
Validation loss: 2.704804669144333

Epoch: 6| Step: 11
Training loss: 2.9781527519226074
Validation loss: 2.7186257480293192

Epoch: 6| Step: 12
Training loss: 3.4967222213745117
Validation loss: 2.749934942491593

Epoch: 6| Step: 13
Training loss: 3.2285070419311523
Validation loss: 2.7567198481611026

Epoch: 23| Step: 0
Training loss: 2.630291700363159
Validation loss: 2.780197189700219

Epoch: 6| Step: 1
Training loss: 3.0671539306640625
Validation loss: 2.771444243769492

Epoch: 6| Step: 2
Training loss: 2.980462074279785
Validation loss: 2.7522251990533646

Epoch: 6| Step: 3
Training loss: 1.8443636894226074
Validation loss: 2.723132715430311

Epoch: 6| Step: 4
Training loss: 3.1099603176116943
Validation loss: 2.7277574872457855

Epoch: 6| Step: 5
Training loss: 3.0963356494903564
Validation loss: 2.7558900951057352

Epoch: 6| Step: 6
Training loss: 2.972818613052368
Validation loss: 2.7608513421909784

Epoch: 6| Step: 7
Training loss: 3.442154884338379
Validation loss: 2.7173525030894945

Epoch: 6| Step: 8
Training loss: 2.5430784225463867
Validation loss: 2.7015779915676323

Epoch: 6| Step: 9
Training loss: 3.6289162635803223
Validation loss: 2.7014574722577165

Epoch: 6| Step: 10
Training loss: 3.0368220806121826
Validation loss: 2.710839266418129

Epoch: 6| Step: 11
Training loss: 2.7444119453430176
Validation loss: 2.7094395622130363

Epoch: 6| Step: 12
Training loss: 2.7113797664642334
Validation loss: 2.7112821789198023

Epoch: 6| Step: 13
Training loss: 2.3261160850524902
Validation loss: 2.719002995439755

Epoch: 24| Step: 0
Training loss: 3.2525479793548584
Validation loss: 2.712531305128528

Epoch: 6| Step: 1
Training loss: 2.9385173320770264
Validation loss: 2.7062894272547897

Epoch: 6| Step: 2
Training loss: 2.4772439002990723
Validation loss: 2.7057635117602605

Epoch: 6| Step: 3
Training loss: 2.4485559463500977
Validation loss: 2.703010079681232

Epoch: 6| Step: 4
Training loss: 2.816046953201294
Validation loss: 2.698038398578603

Epoch: 6| Step: 5
Training loss: 2.419830083847046
Validation loss: 2.691984150999336

Epoch: 6| Step: 6
Training loss: 3.408121109008789
Validation loss: 2.6931218844588085

Epoch: 6| Step: 7
Training loss: 3.2621893882751465
Validation loss: 2.6949405618893203

Epoch: 6| Step: 8
Training loss: 3.1377944946289062
Validation loss: 2.6902096963697866

Epoch: 6| Step: 9
Training loss: 3.1031618118286133
Validation loss: 2.688355630443942

Epoch: 6| Step: 10
Training loss: 2.073822021484375
Validation loss: 2.6843724404611895

Epoch: 6| Step: 11
Training loss: 2.8075270652770996
Validation loss: 2.6825067304795787

Epoch: 6| Step: 12
Training loss: 3.36723256111145
Validation loss: 2.6862014826907905

Epoch: 6| Step: 13
Training loss: 2.37192702293396
Validation loss: 2.6793995185564925

Epoch: 25| Step: 0
Training loss: 2.8987584114074707
Validation loss: 2.6781103021355084

Epoch: 6| Step: 1
Training loss: 3.367037296295166
Validation loss: 2.6780682097199144

Epoch: 6| Step: 2
Training loss: 2.0899851322174072
Validation loss: 2.6744327378529373

Epoch: 6| Step: 3
Training loss: 2.771390438079834
Validation loss: 2.6738438785717054

Epoch: 6| Step: 4
Training loss: 3.1960699558258057
Validation loss: 2.675486897909513

Epoch: 6| Step: 5
Training loss: 1.9611189365386963
Validation loss: 2.679952580441711

Epoch: 6| Step: 6
Training loss: 3.7696940898895264
Validation loss: 2.6739498107664046

Epoch: 6| Step: 7
Training loss: 2.938936233520508
Validation loss: 2.669699704775246

Epoch: 6| Step: 8
Training loss: 2.6559698581695557
Validation loss: 2.668810572675479

Epoch: 6| Step: 9
Training loss: 3.0487160682678223
Validation loss: 2.663975105490736

Epoch: 6| Step: 10
Training loss: 3.4040322303771973
Validation loss: 2.673135454936694

Epoch: 6| Step: 11
Training loss: 1.89906644821167
Validation loss: 2.6703584578729447

Epoch: 6| Step: 12
Training loss: 2.7873947620391846
Validation loss: 2.6749860727658836

Epoch: 6| Step: 13
Training loss: 3.1314938068389893
Validation loss: 2.674673854663808

Epoch: 26| Step: 0
Training loss: 2.8890650272369385
Validation loss: 2.671336320138747

Epoch: 6| Step: 1
Training loss: 3.250566244125366
Validation loss: 2.6581521316241195

Epoch: 6| Step: 2
Training loss: 2.237516164779663
Validation loss: 2.658190709288402

Epoch: 6| Step: 3
Training loss: 3.34552001953125
Validation loss: 2.657536950162662

Epoch: 6| Step: 4
Training loss: 2.6332993507385254
Validation loss: 2.6544954289672194

Epoch: 6| Step: 5
Training loss: 3.244724750518799
Validation loss: 2.6514464296320432

Epoch: 6| Step: 6
Training loss: 2.9727349281311035
Validation loss: 2.6563740443157893

Epoch: 6| Step: 7
Training loss: 3.3516688346862793
Validation loss: 2.654954195022583

Epoch: 6| Step: 8
Training loss: 2.4853105545043945
Validation loss: 2.6482648618759645

Epoch: 6| Step: 9
Training loss: 2.7586474418640137
Validation loss: 2.649108556009108

Epoch: 6| Step: 10
Training loss: 2.5599913597106934
Validation loss: 2.6466771120666177

Epoch: 6| Step: 11
Training loss: 2.7805261611938477
Validation loss: 2.6493975577815885

Epoch: 6| Step: 12
Training loss: 2.6286840438842773
Validation loss: 2.644942081102761

Epoch: 6| Step: 13
Training loss: 2.117126703262329
Validation loss: 2.643938749067245

Epoch: 27| Step: 0
Training loss: 2.7788431644439697
Validation loss: 2.64578648536436

Epoch: 6| Step: 1
Training loss: 2.6990084648132324
Validation loss: 2.643969847309974

Epoch: 6| Step: 2
Training loss: 2.8173794746398926
Validation loss: 2.650597157016877

Epoch: 6| Step: 3
Training loss: 3.6718432903289795
Validation loss: 2.64288527734818

Epoch: 6| Step: 4
Training loss: 3.058004140853882
Validation loss: 2.6430226807953208

Epoch: 6| Step: 5
Training loss: 2.2907402515411377
Validation loss: 2.6411160217818392

Epoch: 6| Step: 6
Training loss: 3.0781822204589844
Validation loss: 2.6418431523025676

Epoch: 6| Step: 7
Training loss: 2.4894051551818848
Validation loss: 2.6415458212616625

Epoch: 6| Step: 8
Training loss: 2.7977848052978516
Validation loss: 2.6414055849916194

Epoch: 6| Step: 9
Training loss: 2.9496970176696777
Validation loss: 2.638717994895033

Epoch: 6| Step: 10
Training loss: 2.2126498222351074
Validation loss: 2.6406613165332424

Epoch: 6| Step: 11
Training loss: 2.96085524559021
Validation loss: 2.642262371637488

Epoch: 6| Step: 12
Training loss: 3.1751554012298584
Validation loss: 2.6436190528254353

Epoch: 6| Step: 13
Training loss: 2.2475979328155518
Validation loss: 2.650513938678208

Epoch: 28| Step: 0
Training loss: 3.009859085083008
Validation loss: 2.652055791629258

Epoch: 6| Step: 1
Training loss: 2.3339486122131348
Validation loss: 2.653014454790341

Epoch: 6| Step: 2
Training loss: 2.862215995788574
Validation loss: 2.6450505513016895

Epoch: 6| Step: 3
Training loss: 3.815105676651001
Validation loss: 2.634972820999802

Epoch: 6| Step: 4
Training loss: 2.842864513397217
Validation loss: 2.638676853590114

Epoch: 6| Step: 5
Training loss: 2.3770596981048584
Validation loss: 2.6395486529155443

Epoch: 6| Step: 6
Training loss: 3.639359474182129
Validation loss: 2.6433288794691845

Epoch: 6| Step: 7
Training loss: 3.1396191120147705
Validation loss: 2.6440131228457213

Epoch: 6| Step: 8
Training loss: 2.5591678619384766
Validation loss: 2.642909403770201

Epoch: 6| Step: 9
Training loss: 2.6093075275421143
Validation loss: 2.640538533528646

Epoch: 6| Step: 10
Training loss: 2.442321300506592
Validation loss: 2.6357877228849675

Epoch: 6| Step: 11
Training loss: 3.085395097732544
Validation loss: 2.633707202890868

Epoch: 6| Step: 12
Training loss: 1.9698164463043213
Validation loss: 2.63650833406756

Epoch: 6| Step: 13
Training loss: 2.5877294540405273
Validation loss: 2.641731527543837

Epoch: 29| Step: 0
Training loss: 2.477285623550415
Validation loss: 2.6361988949519333

Epoch: 6| Step: 1
Training loss: 3.0639305114746094
Validation loss: 2.6330116743682535

Epoch: 6| Step: 2
Training loss: 2.7487235069274902
Validation loss: 2.6304149499503513

Epoch: 6| Step: 3
Training loss: 2.9983298778533936
Validation loss: 2.6282901071733042

Epoch: 6| Step: 4
Training loss: 2.466123104095459
Validation loss: 2.627144464882471

Epoch: 6| Step: 5
Training loss: 3.195833683013916
Validation loss: 2.626995699380034

Epoch: 6| Step: 6
Training loss: 1.9498043060302734
Validation loss: 2.6244410571231636

Epoch: 6| Step: 7
Training loss: 3.141338348388672
Validation loss: 2.630234310703893

Epoch: 6| Step: 8
Training loss: 2.5844626426696777
Validation loss: 2.6342999089148735

Epoch: 6| Step: 9
Training loss: 3.0878515243530273
Validation loss: 2.6375792744339153

Epoch: 6| Step: 10
Training loss: 3.1578726768493652
Validation loss: 2.631862009725263

Epoch: 6| Step: 11
Training loss: 2.8332056999206543
Validation loss: 2.626512437738398

Epoch: 6| Step: 12
Training loss: 2.168447971343994
Validation loss: 2.620530830916538

Epoch: 6| Step: 13
Training loss: 3.6228137016296387
Validation loss: 2.6286369959513345

Epoch: 30| Step: 0
Training loss: 3.5849738121032715
Validation loss: 2.63092028966514

Epoch: 6| Step: 1
Training loss: 2.624323844909668
Validation loss: 2.6644307746682117

Epoch: 6| Step: 2
Training loss: 2.8672585487365723
Validation loss: 2.6467543161043556

Epoch: 6| Step: 3
Training loss: 2.4669923782348633
Validation loss: 2.6349196818567093

Epoch: 6| Step: 4
Training loss: 3.051724910736084
Validation loss: 2.625898922643354

Epoch: 6| Step: 5
Training loss: 2.7616524696350098
Validation loss: 2.62667558270116

Epoch: 6| Step: 6
Training loss: 2.5916194915771484
Validation loss: 2.622878587374123

Epoch: 6| Step: 7
Training loss: 2.603349208831787
Validation loss: 2.622994469058129

Epoch: 6| Step: 8
Training loss: 2.732654571533203
Validation loss: 2.619234408101728

Epoch: 6| Step: 9
Training loss: 3.015374183654785
Validation loss: 2.61872592536352

Epoch: 6| Step: 10
Training loss: 2.697918176651001
Validation loss: 2.633157027665005

Epoch: 6| Step: 11
Training loss: 3.186023473739624
Validation loss: 2.6517614446660525

Epoch: 6| Step: 12
Training loss: 2.4028282165527344
Validation loss: 2.671243897048376

Epoch: 6| Step: 13
Training loss: 2.582381010055542
Validation loss: 2.6737453040256294

Epoch: 31| Step: 0
Training loss: 3.127054214477539
Validation loss: 2.6683650350057952

Epoch: 6| Step: 1
Training loss: 2.116558074951172
Validation loss: 2.6508278308376187

Epoch: 6| Step: 2
Training loss: 2.559113025665283
Validation loss: 2.626137935987083

Epoch: 6| Step: 3
Training loss: 1.5439327955245972
Validation loss: 2.6095907944504932

Epoch: 6| Step: 4
Training loss: 2.7971787452697754
Validation loss: 2.6104534928516676

Epoch: 6| Step: 5
Training loss: 3.5013833045959473
Validation loss: 2.6138401313494612

Epoch: 6| Step: 6
Training loss: 3.0948102474212646
Validation loss: 2.616043734294112

Epoch: 6| Step: 7
Training loss: 2.5134575366973877
Validation loss: 2.616890186904579

Epoch: 6| Step: 8
Training loss: 3.5278213024139404
Validation loss: 2.6175325070658038

Epoch: 6| Step: 9
Training loss: 3.5127878189086914
Validation loss: 2.6149244436653714

Epoch: 6| Step: 10
Training loss: 2.584146499633789
Validation loss: 2.6161758745870283

Epoch: 6| Step: 11
Training loss: 2.0190043449401855
Validation loss: 2.6099206401455786

Epoch: 6| Step: 12
Training loss: 3.3025097846984863
Validation loss: 2.6101072552383586

Epoch: 6| Step: 13
Training loss: 3.2446093559265137
Validation loss: 2.605436581437306

Epoch: 32| Step: 0
Training loss: 2.840555191040039
Validation loss: 2.6059056764007895

Epoch: 6| Step: 1
Training loss: 2.088920831680298
Validation loss: 2.604243410530911

Epoch: 6| Step: 2
Training loss: 3.7556357383728027
Validation loss: 2.602476809614448

Epoch: 6| Step: 3
Training loss: 2.9488089084625244
Validation loss: 2.6013855370142127

Epoch: 6| Step: 4
Training loss: 2.660923957824707
Validation loss: 2.6026336146939184

Epoch: 6| Step: 5
Training loss: 2.518998146057129
Validation loss: 2.6022026051757154

Epoch: 6| Step: 6
Training loss: 2.2913455963134766
Validation loss: 2.5998444634099163

Epoch: 6| Step: 7
Training loss: 2.695983409881592
Validation loss: 2.600546572798042

Epoch: 6| Step: 8
Training loss: 2.155585765838623
Validation loss: 2.6031507599738335

Epoch: 6| Step: 9
Training loss: 2.9630091190338135
Validation loss: 2.6017105194830124

Epoch: 6| Step: 10
Training loss: 2.980346441268921
Validation loss: 2.602998347692592

Epoch: 6| Step: 11
Training loss: 2.615968942642212
Validation loss: 2.6032254183164207

Epoch: 6| Step: 12
Training loss: 3.199240207672119
Validation loss: 2.6004113663909254

Epoch: 6| Step: 13
Training loss: 3.237962484359741
Validation loss: 2.6010289499836583

Epoch: 33| Step: 0
Training loss: 2.2823894023895264
Validation loss: 2.6066121183415896

Epoch: 6| Step: 1
Training loss: 2.6415834426879883
Validation loss: 2.617269818500806

Epoch: 6| Step: 2
Training loss: 2.5486724376678467
Validation loss: 2.6246187840738604

Epoch: 6| Step: 3
Training loss: 2.655941963195801
Validation loss: 2.643458127975464

Epoch: 6| Step: 4
Training loss: 2.668919563293457
Validation loss: 2.66594757059569

Epoch: 6| Step: 5
Training loss: 2.7158946990966797
Validation loss: 2.6242390627502115

Epoch: 6| Step: 6
Training loss: 2.22807240486145
Validation loss: 2.6021344020802486

Epoch: 6| Step: 7
Training loss: 3.1010642051696777
Validation loss: 2.596009003218784

Epoch: 6| Step: 8
Training loss: 3.5386650562286377
Validation loss: 2.6031592430606967

Epoch: 6| Step: 9
Training loss: 3.898054838180542
Validation loss: 2.6229999937036985

Epoch: 6| Step: 10
Training loss: 2.2479612827301025
Validation loss: 2.6301646950424358

Epoch: 6| Step: 11
Training loss: 2.8321473598480225
Validation loss: 2.6234042413773073

Epoch: 6| Step: 12
Training loss: 2.914215564727783
Validation loss: 2.6133820215861

Epoch: 6| Step: 13
Training loss: 2.5056562423706055
Validation loss: 2.6123545246739543

Epoch: 34| Step: 0
Training loss: 2.7611865997314453
Validation loss: 2.589047844691943

Epoch: 6| Step: 1
Training loss: 2.085564136505127
Validation loss: 2.584330076812416

Epoch: 6| Step: 2
Training loss: 2.7642714977264404
Validation loss: 2.584189820033248

Epoch: 6| Step: 3
Training loss: 2.6048965454101562
Validation loss: 2.5778682257539485

Epoch: 6| Step: 4
Training loss: 2.5575294494628906
Validation loss: 2.5964867312421083

Epoch: 6| Step: 5
Training loss: 3.371551990509033
Validation loss: 2.6333114741950907

Epoch: 6| Step: 6
Training loss: 3.4987592697143555
Validation loss: 2.648488308793755

Epoch: 6| Step: 7
Training loss: 2.3469438552856445
Validation loss: 2.6450855834509737

Epoch: 6| Step: 8
Training loss: 3.012256383895874
Validation loss: 2.6750762667707217

Epoch: 6| Step: 9
Training loss: 2.679638385772705
Validation loss: 2.6734346753807476

Epoch: 6| Step: 10
Training loss: 2.7686548233032227
Validation loss: 2.625255210425264

Epoch: 6| Step: 11
Training loss: 2.751272439956665
Validation loss: 2.590283001622846

Epoch: 6| Step: 12
Training loss: 2.578097343444824
Validation loss: 2.5834112321176836

Epoch: 6| Step: 13
Training loss: 2.861154556274414
Validation loss: 2.5806817111148628

Epoch: 35| Step: 0
Training loss: 3.126906156539917
Validation loss: 2.6133364246737574

Epoch: 6| Step: 1
Training loss: 2.8423500061035156
Validation loss: 2.617408662713984

Epoch: 6| Step: 2
Training loss: 2.7144055366516113
Validation loss: 2.5952808113508326

Epoch: 6| Step: 3
Training loss: 2.562452554702759
Validation loss: 2.5769534803205922

Epoch: 6| Step: 4
Training loss: 3.1838414669036865
Validation loss: 2.5710294015945925

Epoch: 6| Step: 5
Training loss: 2.463892698287964
Validation loss: 2.5747033703711724

Epoch: 6| Step: 6
Training loss: 3.272458791732788
Validation loss: 2.5843178149192565

Epoch: 6| Step: 7
Training loss: 3.3365626335144043
Validation loss: 2.5980266140353296

Epoch: 6| Step: 8
Training loss: 2.478238344192505
Validation loss: 2.6032063422664518

Epoch: 6| Step: 9
Training loss: 1.9251508712768555
Validation loss: 2.6121401658622165

Epoch: 6| Step: 10
Training loss: 2.716390609741211
Validation loss: 2.6100940550527265

Epoch: 6| Step: 11
Training loss: 3.5131804943084717
Validation loss: 2.608092554153935

Epoch: 6| Step: 12
Training loss: 2.054328680038452
Validation loss: 2.5862637924891647

Epoch: 6| Step: 13
Training loss: 2.176985740661621
Validation loss: 2.564418690178984

Epoch: 36| Step: 0
Training loss: 2.7499353885650635
Validation loss: 2.5635020861061673

Epoch: 6| Step: 1
Training loss: 2.7128639221191406
Validation loss: 2.5669731645173925

Epoch: 6| Step: 2
Training loss: 2.6995763778686523
Validation loss: 2.5696698670746176

Epoch: 6| Step: 3
Training loss: 3.4503560066223145
Validation loss: 2.5788002321797032

Epoch: 6| Step: 4
Training loss: 2.2085394859313965
Validation loss: 2.5839006952060166

Epoch: 6| Step: 5
Training loss: 2.219672203063965
Validation loss: 2.57862756329198

Epoch: 6| Step: 6
Training loss: 2.4403762817382812
Validation loss: 2.5790872881489415

Epoch: 6| Step: 7
Training loss: 3.4796392917633057
Validation loss: 2.579705592124693

Epoch: 6| Step: 8
Training loss: 3.199725866317749
Validation loss: 2.5655544265624015

Epoch: 6| Step: 9
Training loss: 3.71681809425354
Validation loss: 2.571594403636071

Epoch: 6| Step: 10
Training loss: 2.1521365642547607
Validation loss: 2.5681765489680792

Epoch: 6| Step: 11
Training loss: 2.5833334922790527
Validation loss: 2.5583247138607885

Epoch: 6| Step: 12
Training loss: 2.4977617263793945
Validation loss: 2.553537919957151

Epoch: 6| Step: 13
Training loss: 2.0560030937194824
Validation loss: 2.55884002870129

Epoch: 37| Step: 0
Training loss: 1.8192647695541382
Validation loss: 2.5594938570453274

Epoch: 6| Step: 1
Training loss: 2.812201499938965
Validation loss: 2.5580388653662895

Epoch: 6| Step: 2
Training loss: 3.079512119293213
Validation loss: 2.5897552044160905

Epoch: 6| Step: 3
Training loss: 2.9126439094543457
Validation loss: 2.6216885684638895

Epoch: 6| Step: 4
Training loss: 3.368279457092285
Validation loss: 2.576507173558717

Epoch: 6| Step: 5
Training loss: 2.7901220321655273
Validation loss: 2.568088375112062

Epoch: 6| Step: 6
Training loss: 3.251458168029785
Validation loss: 2.568170967922416

Epoch: 6| Step: 7
Training loss: 3.3456645011901855
Validation loss: 2.5695620044585197

Epoch: 6| Step: 8
Training loss: 2.059525489807129
Validation loss: 2.570056676864624

Epoch: 6| Step: 9
Training loss: 1.991182565689087
Validation loss: 2.568247884832403

Epoch: 6| Step: 10
Training loss: 2.343153953552246
Validation loss: 2.5679378765885548

Epoch: 6| Step: 11
Training loss: 2.056314468383789
Validation loss: 2.5577949067597747

Epoch: 6| Step: 12
Training loss: 3.1703009605407715
Validation loss: 2.5526088488999235

Epoch: 6| Step: 13
Training loss: 3.9625914096832275
Validation loss: 2.5504235785494567

Epoch: 38| Step: 0
Training loss: 2.753434658050537
Validation loss: 2.5475395212891283

Epoch: 6| Step: 1
Training loss: 2.9664320945739746
Validation loss: 2.5522095182890534

Epoch: 6| Step: 2
Training loss: 3.006601572036743
Validation loss: 2.5854097104841665

Epoch: 6| Step: 3
Training loss: 3.0947954654693604
Validation loss: 2.602938113674041

Epoch: 6| Step: 4
Training loss: 2.0072174072265625
Validation loss: 2.5571556116945002

Epoch: 6| Step: 5
Training loss: 2.8176558017730713
Validation loss: 2.5480389338667675

Epoch: 6| Step: 6
Training loss: 2.4816362857818604
Validation loss: 2.5450761343843196

Epoch: 6| Step: 7
Training loss: 2.9251937866210938
Validation loss: 2.54072965088711

Epoch: 6| Step: 8
Training loss: 2.882174253463745
Validation loss: 2.5386590444913475

Epoch: 6| Step: 9
Training loss: 2.60899019241333
Validation loss: 2.5384622055997133

Epoch: 6| Step: 10
Training loss: 3.150211811065674
Validation loss: 2.536670961687642

Epoch: 6| Step: 11
Training loss: 2.8036253452301025
Validation loss: 2.5355322489174466

Epoch: 6| Step: 12
Training loss: 2.0867042541503906
Validation loss: 2.5346098407622306

Epoch: 6| Step: 13
Training loss: 2.5418918132781982
Validation loss: 2.5362175126229562

Epoch: 39| Step: 0
Training loss: 2.751371383666992
Validation loss: 2.5350625566256944

Epoch: 6| Step: 1
Training loss: 3.2918739318847656
Validation loss: 2.538648215673303

Epoch: 6| Step: 2
Training loss: 2.895578384399414
Validation loss: 2.5463001138420513

Epoch: 6| Step: 3
Training loss: 1.9793877601623535
Validation loss: 2.5488236258106847

Epoch: 6| Step: 4
Training loss: 3.187897205352783
Validation loss: 2.550592686540337

Epoch: 6| Step: 5
Training loss: 1.7657697200775146
Validation loss: 2.5595818027373283

Epoch: 6| Step: 6
Training loss: 2.3037705421447754
Validation loss: 2.5592500471299693

Epoch: 6| Step: 7
Training loss: 2.652123212814331
Validation loss: 2.5409463913209978

Epoch: 6| Step: 8
Training loss: 3.1426444053649902
Validation loss: 2.539237799183015

Epoch: 6| Step: 9
Training loss: 2.9778759479522705
Validation loss: 2.5353181951789447

Epoch: 6| Step: 10
Training loss: 3.3513760566711426
Validation loss: 2.5360314640947568

Epoch: 6| Step: 11
Training loss: 2.3296523094177246
Validation loss: 2.540696423540833

Epoch: 6| Step: 12
Training loss: 2.6934494972229004
Validation loss: 2.544360347973403

Epoch: 6| Step: 13
Training loss: 2.7948503494262695
Validation loss: 2.549494210109916

Epoch: 40| Step: 0
Training loss: 2.81496524810791
Validation loss: 2.546265766184817

Epoch: 6| Step: 1
Training loss: 2.9251201152801514
Validation loss: 2.540138301029

Epoch: 6| Step: 2
Training loss: 2.458400011062622
Validation loss: 2.546368460501394

Epoch: 6| Step: 3
Training loss: 3.1175525188446045
Validation loss: 2.5529965739096365

Epoch: 6| Step: 4
Training loss: 2.424091339111328
Validation loss: 2.5554091135660806

Epoch: 6| Step: 5
Training loss: 3.1920888423919678
Validation loss: 2.5247053920581775

Epoch: 6| Step: 6
Training loss: 2.411482334136963
Validation loss: 2.521674986808531

Epoch: 6| Step: 7
Training loss: 3.2774643898010254
Validation loss: 2.5195628417435514

Epoch: 6| Step: 8
Training loss: 1.9239652156829834
Validation loss: 2.520622739227869

Epoch: 6| Step: 9
Training loss: 2.098611354827881
Validation loss: 2.5228293736775718

Epoch: 6| Step: 10
Training loss: 2.00763201713562
Validation loss: 2.523478191385987

Epoch: 6| Step: 11
Training loss: 3.3198909759521484
Validation loss: 2.5218409107577417

Epoch: 6| Step: 12
Training loss: 3.250335216522217
Validation loss: 2.5253085487632343

Epoch: 6| Step: 13
Training loss: 2.992784261703491
Validation loss: 2.522619047472554

Epoch: 41| Step: 0
Training loss: 2.4738636016845703
Validation loss: 2.5246852059518137

Epoch: 6| Step: 1
Training loss: 2.333707809448242
Validation loss: 2.5274701272287676

Epoch: 6| Step: 2
Training loss: 2.6593453884124756
Validation loss: 2.5312984079442997

Epoch: 6| Step: 3
Training loss: 3.110750913619995
Validation loss: 2.536105432818013

Epoch: 6| Step: 4
Training loss: 2.5840561389923096
Validation loss: 2.531986839027815

Epoch: 6| Step: 5
Training loss: 2.56249737739563
Validation loss: 2.532401828355687

Epoch: 6| Step: 6
Training loss: 2.8385355472564697
Validation loss: 2.5301971409910466

Epoch: 6| Step: 7
Training loss: 2.2483034133911133
Validation loss: 2.529223954805764

Epoch: 6| Step: 8
Training loss: 3.228997230529785
Validation loss: 2.5296752350304716

Epoch: 6| Step: 9
Training loss: 3.191781997680664
Validation loss: 2.527685626860588

Epoch: 6| Step: 10
Training loss: 2.8380913734436035
Validation loss: 2.5239517970751693

Epoch: 6| Step: 11
Training loss: 3.0161666870117188
Validation loss: 2.520665499471849

Epoch: 6| Step: 12
Training loss: 2.2772903442382812
Validation loss: 2.517695019322057

Epoch: 6| Step: 13
Training loss: 2.6765127182006836
Validation loss: 2.514783723379976

Epoch: 42| Step: 0
Training loss: 2.7087836265563965
Validation loss: 2.5125610866854267

Epoch: 6| Step: 1
Training loss: 2.916837453842163
Validation loss: 2.510956753966629

Epoch: 6| Step: 2
Training loss: 2.398078441619873
Validation loss: 2.5113904296710925

Epoch: 6| Step: 3
Training loss: 2.6221237182617188
Validation loss: 2.5101696906551236

Epoch: 6| Step: 4
Training loss: 3.126624584197998
Validation loss: 2.5110323941835793

Epoch: 6| Step: 5
Training loss: 4.316299915313721
Validation loss: 2.5108724563352522

Epoch: 6| Step: 6
Training loss: 2.9104690551757812
Validation loss: 2.5110566872422413

Epoch: 6| Step: 7
Training loss: 2.194021701812744
Validation loss: 2.5088277991100023

Epoch: 6| Step: 8
Training loss: 2.2000722885131836
Validation loss: 2.5108418541569866

Epoch: 6| Step: 9
Training loss: 3.0247323513031006
Validation loss: 2.5089361206177743

Epoch: 6| Step: 10
Training loss: 2.6058404445648193
Validation loss: 2.5111414719653387

Epoch: 6| Step: 11
Training loss: 3.03914737701416
Validation loss: 2.506201649224886

Epoch: 6| Step: 12
Training loss: 1.8438407182693481
Validation loss: 2.5074621374889086

Epoch: 6| Step: 13
Training loss: 1.5214500427246094
Validation loss: 2.504073650606217

Epoch: 43| Step: 0
Training loss: 2.933743476867676
Validation loss: 2.5043031579704693

Epoch: 6| Step: 1
Training loss: 3.137019157409668
Validation loss: 2.502757764631702

Epoch: 6| Step: 2
Training loss: 2.17536997795105
Validation loss: 2.5057897131930114

Epoch: 6| Step: 3
Training loss: 2.5139451026916504
Validation loss: 2.5097655224543747

Epoch: 6| Step: 4
Training loss: 2.7935032844543457
Validation loss: 2.5250591155021422

Epoch: 6| Step: 5
Training loss: 2.751950740814209
Validation loss: 2.5389812851464875

Epoch: 6| Step: 6
Training loss: 2.862004041671753
Validation loss: 2.5224954876848447

Epoch: 6| Step: 7
Training loss: 2.0208187103271484
Validation loss: 2.512869204244306

Epoch: 6| Step: 8
Training loss: 2.478487730026245
Validation loss: 2.5079389285015803

Epoch: 6| Step: 9
Training loss: 2.557779312133789
Validation loss: 2.5020683709011284

Epoch: 6| Step: 10
Training loss: 3.7109241485595703
Validation loss: 2.50414792952999

Epoch: 6| Step: 11
Training loss: 2.779554843902588
Validation loss: 2.501077967305337

Epoch: 6| Step: 12
Training loss: 2.3999269008636475
Validation loss: 2.500780866992089

Epoch: 6| Step: 13
Training loss: 2.767914056777954
Validation loss: 2.5037807290272047

Epoch: 44| Step: 0
Training loss: 2.05462908744812
Validation loss: 2.5061725929219234

Epoch: 6| Step: 1
Training loss: 2.947657823562622
Validation loss: 2.5082683204322733

Epoch: 6| Step: 2
Training loss: 2.318688154220581
Validation loss: 2.5125707093105523

Epoch: 6| Step: 3
Training loss: 3.296462297439575
Validation loss: 2.52093493810264

Epoch: 6| Step: 4
Training loss: 2.6355438232421875
Validation loss: 2.5164082281051146

Epoch: 6| Step: 5
Training loss: 2.6530966758728027
Validation loss: 2.516794184202789

Epoch: 6| Step: 6
Training loss: 2.3047962188720703
Validation loss: 2.5097596645355225

Epoch: 6| Step: 7
Training loss: 2.944911479949951
Validation loss: 2.5029332586514053

Epoch: 6| Step: 8
Training loss: 2.3451693058013916
Validation loss: 2.5014764032056256

Epoch: 6| Step: 9
Training loss: 3.1355037689208984
Validation loss: 2.499433291855679

Epoch: 6| Step: 10
Training loss: 2.7849531173706055
Validation loss: 2.494899626701109

Epoch: 6| Step: 11
Training loss: 2.078192949295044
Validation loss: 2.4977611085420013

Epoch: 6| Step: 12
Training loss: 3.6933462619781494
Validation loss: 2.4981323903606785

Epoch: 6| Step: 13
Training loss: 2.5934431552886963
Validation loss: 2.4963519393756823

Epoch: 45| Step: 0
Training loss: 2.9126908779144287
Validation loss: 2.508761009862346

Epoch: 6| Step: 1
Training loss: 2.133406639099121
Validation loss: 2.513945569274246

Epoch: 6| Step: 2
Training loss: 3.2310028076171875
Validation loss: 2.5364459740218295

Epoch: 6| Step: 3
Training loss: 2.3190016746520996
Validation loss: 2.5494140937764156

Epoch: 6| Step: 4
Training loss: 2.029170274734497
Validation loss: 2.577590229690716

Epoch: 6| Step: 5
Training loss: 3.648371696472168
Validation loss: 2.5869714367774224

Epoch: 6| Step: 6
Training loss: 4.145519256591797
Validation loss: 2.596871937474897

Epoch: 6| Step: 7
Training loss: 2.383878231048584
Validation loss: 2.5496215307584373

Epoch: 6| Step: 8
Training loss: 1.8453574180603027
Validation loss: 2.5231544561283563

Epoch: 6| Step: 9
Training loss: 2.7830088138580322
Validation loss: 2.5014893213907876

Epoch: 6| Step: 10
Training loss: 2.634147882461548
Validation loss: 2.4967588096536617

Epoch: 6| Step: 11
Training loss: 2.070807933807373
Validation loss: 2.4982500640294885

Epoch: 6| Step: 12
Training loss: 2.944395065307617
Validation loss: 2.547593447469896

Epoch: 6| Step: 13
Training loss: 3.048522472381592
Validation loss: 2.578569786522978

Epoch: 46| Step: 0
Training loss: 2.926210403442383
Validation loss: 2.52885760799531

Epoch: 6| Step: 1
Training loss: 2.5763907432556152
Validation loss: 2.504025105507143

Epoch: 6| Step: 2
Training loss: 3.027559280395508
Validation loss: 2.49784513186383

Epoch: 6| Step: 3
Training loss: 2.518000602722168
Validation loss: 2.491519081977106

Epoch: 6| Step: 4
Training loss: 2.1362571716308594
Validation loss: 2.4938936976976294

Epoch: 6| Step: 5
Training loss: 2.3716001510620117
Validation loss: 2.4915116140919347

Epoch: 6| Step: 6
Training loss: 2.9654836654663086
Validation loss: 2.5141651322764735

Epoch: 6| Step: 7
Training loss: 3.6528451442718506
Validation loss: 2.529384557918836

Epoch: 6| Step: 8
Training loss: 2.5866270065307617
Validation loss: 2.5393005071147794

Epoch: 6| Step: 9
Training loss: 2.830819606781006
Validation loss: 2.510316925664102

Epoch: 6| Step: 10
Training loss: 2.361069917678833
Validation loss: 2.4915805555159047

Epoch: 6| Step: 11
Training loss: 2.708599328994751
Validation loss: 2.4903065312293267

Epoch: 6| Step: 12
Training loss: 2.257294178009033
Validation loss: 2.486974775150258

Epoch: 6| Step: 13
Training loss: 2.8011040687561035
Validation loss: 2.4813304126903577

Epoch: 47| Step: 0
Training loss: 2.0722687244415283
Validation loss: 2.4805338741630636

Epoch: 6| Step: 1
Training loss: 3.3182263374328613
Validation loss: 2.477235109575333

Epoch: 6| Step: 2
Training loss: 2.538055658340454
Validation loss: 2.4715994173480618

Epoch: 6| Step: 3
Training loss: 2.454479694366455
Validation loss: 2.472528137186522

Epoch: 6| Step: 4
Training loss: 3.0674664974212646
Validation loss: 2.472966642789943

Epoch: 6| Step: 5
Training loss: 2.389122724533081
Validation loss: 2.4763589341153383

Epoch: 6| Step: 6
Training loss: 2.3120408058166504
Validation loss: 2.478381279976137

Epoch: 6| Step: 7
Training loss: 2.4939136505126953
Validation loss: 2.4807210250567366

Epoch: 6| Step: 8
Training loss: 3.0464425086975098
Validation loss: 2.4834880046947028

Epoch: 6| Step: 9
Training loss: 2.586808681488037
Validation loss: 2.4942691915778705

Epoch: 6| Step: 10
Training loss: 2.689258098602295
Validation loss: 2.509983075562344

Epoch: 6| Step: 11
Training loss: 2.8278560638427734
Validation loss: 2.497590111147973

Epoch: 6| Step: 12
Training loss: 3.0801312923431396
Validation loss: 2.490258132257769

Epoch: 6| Step: 13
Training loss: 2.631251573562622
Validation loss: 2.502150325364964

Epoch: 48| Step: 0
Training loss: 2.872527599334717
Validation loss: 2.4962463507088284

Epoch: 6| Step: 1
Training loss: 2.916853904724121
Validation loss: 2.5006148686972995

Epoch: 6| Step: 2
Training loss: 3.1409413814544678
Validation loss: 2.494503254531532

Epoch: 6| Step: 3
Training loss: 2.3939290046691895
Validation loss: 2.485007591145013

Epoch: 6| Step: 4
Training loss: 2.5211048126220703
Validation loss: 2.4792824150413595

Epoch: 6| Step: 5
Training loss: 2.2900993824005127
Validation loss: 2.4736090219149025

Epoch: 6| Step: 6
Training loss: 3.0679402351379395
Validation loss: 2.4779465788154194

Epoch: 6| Step: 7
Training loss: 2.761540412902832
Validation loss: 2.467731862939814

Epoch: 6| Step: 8
Training loss: 2.303532123565674
Validation loss: 2.4663687316320275

Epoch: 6| Step: 9
Training loss: 2.314417600631714
Validation loss: 2.4694424624084146

Epoch: 6| Step: 10
Training loss: 3.3588249683380127
Validation loss: 2.4946996486315163

Epoch: 6| Step: 11
Training loss: 2.487915515899658
Validation loss: 2.4941406634546097

Epoch: 6| Step: 12
Training loss: 2.6539814472198486
Validation loss: 2.492530486916983

Epoch: 6| Step: 13
Training loss: 2.236010789871216
Validation loss: 2.4843957167799755

Epoch: 49| Step: 0
Training loss: 2.561217784881592
Validation loss: 2.4857400514746226

Epoch: 6| Step: 1
Training loss: 2.435182571411133
Validation loss: 2.4846232296318136

Epoch: 6| Step: 2
Training loss: 2.0608949661254883
Validation loss: 2.489073843084356

Epoch: 6| Step: 3
Training loss: 2.7285096645355225
Validation loss: 2.4970526413250993

Epoch: 6| Step: 4
Training loss: 1.9706151485443115
Validation loss: 2.4987701062233216

Epoch: 6| Step: 5
Training loss: 2.551474094390869
Validation loss: 2.5127380304439093

Epoch: 6| Step: 6
Training loss: 2.1203808784484863
Validation loss: 2.4991384424189085

Epoch: 6| Step: 7
Training loss: 3.4475831985473633
Validation loss: 2.496257607654859

Epoch: 6| Step: 8
Training loss: 3.560511350631714
Validation loss: 2.494675597836894

Epoch: 6| Step: 9
Training loss: 3.082530975341797
Validation loss: 2.4936464755765853

Epoch: 6| Step: 10
Training loss: 2.217381000518799
Validation loss: 2.484661502222861

Epoch: 6| Step: 11
Training loss: 3.2168729305267334
Validation loss: 2.477271551726967

Epoch: 6| Step: 12
Training loss: 2.4243416786193848
Validation loss: 2.4703987208745812

Epoch: 6| Step: 13
Training loss: 3.2996108531951904
Validation loss: 2.4647260840221117

Epoch: 50| Step: 0
Training loss: 2.252838134765625
Validation loss: 2.462858369273524

Epoch: 6| Step: 1
Training loss: 2.57900333404541
Validation loss: 2.4589514040177867

Epoch: 6| Step: 2
Training loss: 2.288942337036133
Validation loss: 2.455324924120339

Epoch: 6| Step: 3
Training loss: 2.629971742630005
Validation loss: 2.456599130425402

Epoch: 6| Step: 4
Training loss: 2.361321210861206
Validation loss: 2.4572447474284838

Epoch: 6| Step: 5
Training loss: 2.5820577144622803
Validation loss: 2.4511521708580757

Epoch: 6| Step: 6
Training loss: 3.496248722076416
Validation loss: 2.4564320836015927

Epoch: 6| Step: 7
Training loss: 3.1205477714538574
Validation loss: 2.4566010172649095

Epoch: 6| Step: 8
Training loss: 2.845959186553955
Validation loss: 2.459098721063265

Epoch: 6| Step: 9
Training loss: 2.6096248626708984
Validation loss: 2.462420150797854

Epoch: 6| Step: 10
Training loss: 2.423952341079712
Validation loss: 2.4657828141284246

Epoch: 6| Step: 11
Training loss: 2.7194392681121826
Validation loss: 2.475766343455161

Epoch: 6| Step: 12
Training loss: 2.907444477081299
Validation loss: 2.5040227546486804

Epoch: 6| Step: 13
Training loss: 2.7148563861846924
Validation loss: 2.4682872397925264

Epoch: 51| Step: 0
Training loss: 3.254776954650879
Validation loss: 2.4609337288846254

Epoch: 6| Step: 1
Training loss: 3.1256914138793945
Validation loss: 2.4562541925778953

Epoch: 6| Step: 2
Training loss: 3.1227974891662598
Validation loss: 2.4520103136698403

Epoch: 6| Step: 3
Training loss: 2.174665927886963
Validation loss: 2.4488931048300957

Epoch: 6| Step: 4
Training loss: 3.1347579956054688
Validation loss: 2.4498708914684992

Epoch: 6| Step: 5
Training loss: 2.2865207195281982
Validation loss: 2.456912822620843

Epoch: 6| Step: 6
Training loss: 2.572505235671997
Validation loss: 2.456518485981931

Epoch: 6| Step: 7
Training loss: 2.5225462913513184
Validation loss: 2.461909911965811

Epoch: 6| Step: 8
Training loss: 1.7069114446640015
Validation loss: 2.463425446582097

Epoch: 6| Step: 9
Training loss: 2.732719659805298
Validation loss: 2.4794580141703286

Epoch: 6| Step: 10
Training loss: 1.930665135383606
Validation loss: 2.4661055995571997

Epoch: 6| Step: 11
Training loss: 2.9440507888793945
Validation loss: 2.4719299013896654

Epoch: 6| Step: 12
Training loss: 3.277722120285034
Validation loss: 2.477159776995259

Epoch: 6| Step: 13
Training loss: 2.958587408065796
Validation loss: 2.477045137395141

Epoch: 52| Step: 0
Training loss: 2.640756845474243
Validation loss: 2.4623067814816713

Epoch: 6| Step: 1
Training loss: 2.011885643005371
Validation loss: 2.4638209753139044

Epoch: 6| Step: 2
Training loss: 2.1545681953430176
Validation loss: 2.4557309612151115

Epoch: 6| Step: 3
Training loss: 2.2646291255950928
Validation loss: 2.458021802286948

Epoch: 6| Step: 4
Training loss: 2.342390298843384
Validation loss: 2.463703078608359

Epoch: 6| Step: 5
Training loss: 3.2162299156188965
Validation loss: 2.4730335076649985

Epoch: 6| Step: 6
Training loss: 3.285040855407715
Validation loss: 2.4594126875682543

Epoch: 6| Step: 7
Training loss: 2.7341771125793457
Validation loss: 2.4566216263719785

Epoch: 6| Step: 8
Training loss: 2.577934503555298
Validation loss: 2.4602001943895893

Epoch: 6| Step: 9
Training loss: 3.0586957931518555
Validation loss: 2.4581418857779553

Epoch: 6| Step: 10
Training loss: 2.7904958724975586
Validation loss: 2.460948162181403

Epoch: 6| Step: 11
Training loss: 2.8614661693573
Validation loss: 2.4611800896224154

Epoch: 6| Step: 12
Training loss: 2.8087146282196045
Validation loss: 2.4629597279333297

Epoch: 6| Step: 13
Training loss: 2.753577470779419
Validation loss: 2.4637351830800376

Epoch: 53| Step: 0
Training loss: 2.7277135848999023
Validation loss: 2.476840139717184

Epoch: 6| Step: 1
Training loss: 3.1804399490356445
Validation loss: 2.4860189371211554

Epoch: 6| Step: 2
Training loss: 2.9395406246185303
Validation loss: 2.4984916717775407

Epoch: 6| Step: 3
Training loss: 2.242861270904541
Validation loss: 2.493083917966453

Epoch: 6| Step: 4
Training loss: 2.5156874656677246
Validation loss: 2.4903680534772974

Epoch: 6| Step: 5
Training loss: 2.311589002609253
Validation loss: 2.475501037413074

Epoch: 6| Step: 6
Training loss: 2.6479663848876953
Validation loss: 2.475737751171153

Epoch: 6| Step: 7
Training loss: 2.104912519454956
Validation loss: 2.471645429570188

Epoch: 6| Step: 8
Training loss: 2.4414901733398438
Validation loss: 2.46106735608911

Epoch: 6| Step: 9
Training loss: 2.9751620292663574
Validation loss: 2.450149651496641

Epoch: 6| Step: 10
Training loss: 2.566873073577881
Validation loss: 2.4497804974996917

Epoch: 6| Step: 11
Training loss: 3.1574487686157227
Validation loss: 2.444910213511477

Epoch: 6| Step: 12
Training loss: 2.9606974124908447
Validation loss: 2.440366780886086

Epoch: 6| Step: 13
Training loss: 2.5606329441070557
Validation loss: 2.44203350364521

Epoch: 54| Step: 0
Training loss: 2.6342642307281494
Validation loss: 2.439338525136312

Epoch: 6| Step: 1
Training loss: 3.1116230487823486
Validation loss: 2.440382977967621

Epoch: 6| Step: 2
Training loss: 2.1015677452087402
Validation loss: 2.442889804481178

Epoch: 6| Step: 3
Training loss: 2.636345386505127
Validation loss: 2.4411275207355456

Epoch: 6| Step: 4
Training loss: 2.5295333862304688
Validation loss: 2.4410817956411712

Epoch: 6| Step: 5
Training loss: 2.6471757888793945
Validation loss: 2.43942621702789

Epoch: 6| Step: 6
Training loss: 2.4442334175109863
Validation loss: 2.4420401306562525

Epoch: 6| Step: 7
Training loss: 3.1887025833129883
Validation loss: 2.4430149588533627

Epoch: 6| Step: 8
Training loss: 3.1077466011047363
Validation loss: 2.4448444074200046

Epoch: 6| Step: 9
Training loss: 2.486081600189209
Validation loss: 2.443578714965492

Epoch: 6| Step: 10
Training loss: 2.626349449157715
Validation loss: 2.4441053764794463

Epoch: 6| Step: 11
Training loss: 2.7090067863464355
Validation loss: 2.4520819828074467

Epoch: 6| Step: 12
Training loss: 2.0568175315856934
Validation loss: 2.453138987223307

Epoch: 6| Step: 13
Training loss: 3.1430280208587646
Validation loss: 2.4628089166456655

Epoch: 55| Step: 0
Training loss: 3.360149621963501
Validation loss: 2.457606743740779

Epoch: 6| Step: 1
Training loss: 2.2370457649230957
Validation loss: 2.4434563344524753

Epoch: 6| Step: 2
Training loss: 2.3842124938964844
Validation loss: 2.434997404775312

Epoch: 6| Step: 3
Training loss: 2.5679826736450195
Validation loss: 2.43665192204137

Epoch: 6| Step: 4
Training loss: 3.111018657684326
Validation loss: 2.43638708514552

Epoch: 6| Step: 5
Training loss: 3.077810049057007
Validation loss: 2.4338984002349195

Epoch: 6| Step: 6
Training loss: 2.793642520904541
Validation loss: 2.4321757029461604

Epoch: 6| Step: 7
Training loss: 3.5264551639556885
Validation loss: 2.4339450405490015

Epoch: 6| Step: 8
Training loss: 2.0720787048339844
Validation loss: 2.429305427817888

Epoch: 6| Step: 9
Training loss: 2.5226895809173584
Validation loss: 2.429351809204266

Epoch: 6| Step: 10
Training loss: 2.8674657344818115
Validation loss: 2.424627011822116

Epoch: 6| Step: 11
Training loss: 1.7135403156280518
Validation loss: 2.4250412794851486

Epoch: 6| Step: 12
Training loss: 2.5601248741149902
Validation loss: 2.4247871214343655

Epoch: 6| Step: 13
Training loss: 2.3181965351104736
Validation loss: 2.427018360425067

Epoch: 56| Step: 0
Training loss: 2.6828601360321045
Validation loss: 2.4292991956075034

Epoch: 6| Step: 1
Training loss: 2.8767271041870117
Validation loss: 2.4294510656787502

Epoch: 6| Step: 2
Training loss: 2.792407512664795
Validation loss: 2.426397741481822

Epoch: 6| Step: 3
Training loss: 1.753310203552246
Validation loss: 2.425919945522021

Epoch: 6| Step: 4
Training loss: 2.770549774169922
Validation loss: 2.4211018034206924

Epoch: 6| Step: 5
Training loss: 2.097846031188965
Validation loss: 2.4241250484220442

Epoch: 6| Step: 6
Training loss: 2.546849250793457
Validation loss: 2.4262756686056814

Epoch: 6| Step: 7
Training loss: 3.7501630783081055
Validation loss: 2.430294934139457

Epoch: 6| Step: 8
Training loss: 2.228534698486328
Validation loss: 2.4394791703070364

Epoch: 6| Step: 9
Training loss: 2.6836013793945312
Validation loss: 2.444971284558696

Epoch: 6| Step: 10
Training loss: 2.761096477508545
Validation loss: 2.4362619205187728

Epoch: 6| Step: 11
Training loss: 2.8508033752441406
Validation loss: 2.4446025125442015

Epoch: 6| Step: 12
Training loss: 2.5095043182373047
Validation loss: 2.44553433310601

Epoch: 6| Step: 13
Training loss: 2.9685726165771484
Validation loss: 2.4510083429275022

Epoch: 57| Step: 0
Training loss: 3.330646276473999
Validation loss: 2.445748826508881

Epoch: 6| Step: 1
Training loss: 1.7800755500793457
Validation loss: 2.4358396709606214

Epoch: 6| Step: 2
Training loss: 2.7394356727600098
Validation loss: 2.437319358189901

Epoch: 6| Step: 3
Training loss: 2.3617124557495117
Validation loss: 2.4457822307463615

Epoch: 6| Step: 4
Training loss: 2.2224535942077637
Validation loss: 2.46086783306573

Epoch: 6| Step: 5
Training loss: 2.853609800338745
Validation loss: 2.4453792777112735

Epoch: 6| Step: 6
Training loss: 2.6978635787963867
Validation loss: 2.4266524725062872

Epoch: 6| Step: 7
Training loss: 2.2969250679016113
Validation loss: 2.422855246451593

Epoch: 6| Step: 8
Training loss: 2.876373291015625
Validation loss: 2.4186484736780964

Epoch: 6| Step: 9
Training loss: 3.0265746116638184
Validation loss: 2.420741024837699

Epoch: 6| Step: 10
Training loss: 2.1858997344970703
Validation loss: 2.420682268757974

Epoch: 6| Step: 11
Training loss: 3.040860891342163
Validation loss: 2.4242278581024497

Epoch: 6| Step: 12
Training loss: 2.896247625350952
Validation loss: 2.4282112019036406

Epoch: 6| Step: 13
Training loss: 2.8750147819519043
Validation loss: 2.430785027883386

Epoch: 58| Step: 0
Training loss: 2.5333685874938965
Validation loss: 2.4326567521659275

Epoch: 6| Step: 1
Training loss: 2.528383255004883
Validation loss: 2.43662461157768

Epoch: 6| Step: 2
Training loss: 2.1585745811462402
Validation loss: 2.4541536928505026

Epoch: 6| Step: 3
Training loss: 2.6355090141296387
Validation loss: 2.4744433767052105

Epoch: 6| Step: 4
Training loss: 2.528377056121826
Validation loss: 2.497136687719694

Epoch: 6| Step: 5
Training loss: 1.470888614654541
Validation loss: 2.5083643723559637

Epoch: 6| Step: 6
Training loss: 2.932769775390625
Validation loss: 2.520252643092986

Epoch: 6| Step: 7
Training loss: 3.5960984230041504
Validation loss: 2.5618628686474216

Epoch: 6| Step: 8
Training loss: 2.5459561347961426
Validation loss: 2.4952194972704818

Epoch: 6| Step: 9
Training loss: 3.289011001586914
Validation loss: 2.4384534333341863

Epoch: 6| Step: 10
Training loss: 2.386805772781372
Validation loss: 2.4230327811292423

Epoch: 6| Step: 11
Training loss: 2.8906450271606445
Validation loss: 2.4223585385148243

Epoch: 6| Step: 12
Training loss: 3.2945168018341064
Validation loss: 2.413208761522847

Epoch: 6| Step: 13
Training loss: 2.4568488597869873
Validation loss: 2.4088717058140743

Epoch: 59| Step: 0
Training loss: 3.018669605255127
Validation loss: 2.4112322945748605

Epoch: 6| Step: 1
Training loss: 2.467264175415039
Validation loss: 2.4151783066411174

Epoch: 6| Step: 2
Training loss: 2.5819387435913086
Validation loss: 2.4228644806851625

Epoch: 6| Step: 3
Training loss: 2.391526222229004
Validation loss: 2.4474599156328427

Epoch: 6| Step: 4
Training loss: 2.9012579917907715
Validation loss: 2.4803864417537564

Epoch: 6| Step: 5
Training loss: 2.338810443878174
Validation loss: 2.5252471559791156

Epoch: 6| Step: 6
Training loss: 3.410717010498047
Validation loss: 2.5298340884588097

Epoch: 6| Step: 7
Training loss: 3.2750089168548584
Validation loss: 2.521299321164367

Epoch: 6| Step: 8
Training loss: 2.290280818939209
Validation loss: 2.50310488670103

Epoch: 6| Step: 9
Training loss: 2.070871591567993
Validation loss: 2.469879919482816

Epoch: 6| Step: 10
Training loss: 2.988450527191162
Validation loss: 2.469269798647973

Epoch: 6| Step: 11
Training loss: 3.149221897125244
Validation loss: 2.469144698112242

Epoch: 6| Step: 12
Training loss: 2.1030874252319336
Validation loss: 2.4619554063325286

Epoch: 6| Step: 13
Training loss: 2.755383253097534
Validation loss: 2.4576348361148628

Epoch: 60| Step: 0
Training loss: 3.216031074523926
Validation loss: 2.4559141846113306

Epoch: 6| Step: 1
Training loss: 3.0055432319641113
Validation loss: 2.4397474155631116

Epoch: 6| Step: 2
Training loss: 2.1150834560394287
Validation loss: 2.4264477658015426

Epoch: 6| Step: 3
Training loss: 2.790187120437622
Validation loss: 2.4256261112869426

Epoch: 6| Step: 4
Training loss: 3.1232573986053467
Validation loss: 2.4469359126142276

Epoch: 6| Step: 5
Training loss: 2.851132869720459
Validation loss: 2.446365917882612

Epoch: 6| Step: 6
Training loss: 2.2253217697143555
Validation loss: 2.4299639142969602

Epoch: 6| Step: 7
Training loss: 2.4592089653015137
Validation loss: 2.429937231925226

Epoch: 6| Step: 8
Training loss: 2.4064557552337646
Validation loss: 2.3974121257823002

Epoch: 6| Step: 9
Training loss: 1.9959056377410889
Validation loss: 2.391329796083512

Epoch: 6| Step: 10
Training loss: 2.900510311126709
Validation loss: 2.4162702919334493

Epoch: 6| Step: 11
Training loss: 2.7904491424560547
Validation loss: 2.4544904616571244

Epoch: 6| Step: 12
Training loss: 2.4975829124450684
Validation loss: 2.5011469061656664

Epoch: 6| Step: 13
Training loss: 3.458528518676758
Validation loss: 2.4964874918742845

Epoch: 61| Step: 0
Training loss: 2.3920843601226807
Validation loss: 2.4120452762931905

Epoch: 6| Step: 1
Training loss: 2.513598918914795
Validation loss: 2.3999065814479703

Epoch: 6| Step: 2
Training loss: 2.8740365505218506
Validation loss: 2.4179378863303893

Epoch: 6| Step: 3
Training loss: 2.3109190464019775
Validation loss: 2.4917007338616157

Epoch: 6| Step: 4
Training loss: 3.189479112625122
Validation loss: 2.526253431074081

Epoch: 6| Step: 5
Training loss: 3.4790663719177246
Validation loss: 2.5234729141317387

Epoch: 6| Step: 6
Training loss: 2.207533359527588
Validation loss: 2.4837495844851256

Epoch: 6| Step: 7
Training loss: 2.1970105171203613
Validation loss: 2.436178466325165

Epoch: 6| Step: 8
Training loss: 2.8963444232940674
Validation loss: 2.417442029522311

Epoch: 6| Step: 9
Training loss: 2.483799457550049
Validation loss: 2.3902767653106363

Epoch: 6| Step: 10
Training loss: 2.5056960582733154
Validation loss: 2.387889549296389

Epoch: 6| Step: 11
Training loss: 2.7918055057525635
Validation loss: 2.3974651957070954

Epoch: 6| Step: 12
Training loss: 2.979994773864746
Validation loss: 2.4129685842862694

Epoch: 6| Step: 13
Training loss: 2.505878210067749
Validation loss: 2.4279881574774302

Epoch: 62| Step: 0
Training loss: 2.2915334701538086
Validation loss: 2.44369238679127

Epoch: 6| Step: 1
Training loss: 3.0641767978668213
Validation loss: 2.4510799454104517

Epoch: 6| Step: 2
Training loss: 2.8257133960723877
Validation loss: 2.4478650708352365

Epoch: 6| Step: 3
Training loss: 2.8654086589813232
Validation loss: 2.4323169608269968

Epoch: 6| Step: 4
Training loss: 2.7613015174865723
Validation loss: 2.430052508590042

Epoch: 6| Step: 5
Training loss: 1.811998963356018
Validation loss: 2.4120673338572183

Epoch: 6| Step: 6
Training loss: 2.9065537452697754
Validation loss: 2.401538274621451

Epoch: 6| Step: 7
Training loss: 2.485064744949341
Validation loss: 2.394945590726791

Epoch: 6| Step: 8
Training loss: 3.595170021057129
Validation loss: 2.3844075254214707

Epoch: 6| Step: 9
Training loss: 2.256979465484619
Validation loss: 2.376719456846996

Epoch: 6| Step: 10
Training loss: 1.994746446609497
Validation loss: 2.378706165539321

Epoch: 6| Step: 11
Training loss: 3.0613601207733154
Validation loss: 2.37523231967803

Epoch: 6| Step: 12
Training loss: 2.6158053874969482
Validation loss: 2.3766076154606317

Epoch: 6| Step: 13
Training loss: 2.200976848602295
Validation loss: 2.37436608858006

Epoch: 63| Step: 0
Training loss: 2.2839975357055664
Validation loss: 2.3735144343427432

Epoch: 6| Step: 1
Training loss: 2.9190926551818848
Validation loss: 2.3721067854153213

Epoch: 6| Step: 2
Training loss: 2.9484000205993652
Validation loss: 2.3740141725027435

Epoch: 6| Step: 3
Training loss: 2.964111089706421
Validation loss: 2.375076984846464

Epoch: 6| Step: 4
Training loss: 3.0152368545532227
Validation loss: 2.378430199879472

Epoch: 6| Step: 5
Training loss: 3.0738625526428223
Validation loss: 2.3818411391268492

Epoch: 6| Step: 6
Training loss: 1.8955495357513428
Validation loss: 2.3811189871962353

Epoch: 6| Step: 7
Training loss: 2.874131917953491
Validation loss: 2.374456218493882

Epoch: 6| Step: 8
Training loss: 2.565640926361084
Validation loss: 2.370498772590391

Epoch: 6| Step: 9
Training loss: 2.3630831241607666
Validation loss: 2.3718482678936375

Epoch: 6| Step: 10
Training loss: 2.2443511486053467
Validation loss: 2.367684797574115

Epoch: 6| Step: 11
Training loss: 2.77072811126709
Validation loss: 2.364735669987176

Epoch: 6| Step: 12
Training loss: 2.1835834980010986
Validation loss: 2.3652826560440885

Epoch: 6| Step: 13
Training loss: 2.791968822479248
Validation loss: 2.3631045972147295

Epoch: 64| Step: 0
Training loss: 2.809915542602539
Validation loss: 2.366402623473957

Epoch: 6| Step: 1
Training loss: 2.0980613231658936
Validation loss: 2.3662743952966507

Epoch: 6| Step: 2
Training loss: 2.7933788299560547
Validation loss: 2.3635506527398222

Epoch: 6| Step: 3
Training loss: 2.1357781887054443
Validation loss: 2.3659147395882556

Epoch: 6| Step: 4
Training loss: 2.8088040351867676
Validation loss: 2.3661719778532624

Epoch: 6| Step: 5
Training loss: 2.2580089569091797
Validation loss: 2.3634509860828357

Epoch: 6| Step: 6
Training loss: 2.6679813861846924
Validation loss: 2.3672965623999156

Epoch: 6| Step: 7
Training loss: 2.002481698989868
Validation loss: 2.3706651785040416

Epoch: 6| Step: 8
Training loss: 2.903632164001465
Validation loss: 2.36837258390201

Epoch: 6| Step: 9
Training loss: 2.5725467205047607
Validation loss: 2.3760128713423208

Epoch: 6| Step: 10
Training loss: 3.673894166946411
Validation loss: 2.371471151228874

Epoch: 6| Step: 11
Training loss: 1.894228219985962
Validation loss: 2.3691689839927097

Epoch: 6| Step: 12
Training loss: 2.7590484619140625
Validation loss: 2.3703017850076

Epoch: 6| Step: 13
Training loss: 3.8084704875946045
Validation loss: 2.366825811324581

Epoch: 65| Step: 0
Training loss: 2.3911075592041016
Validation loss: 2.369204346851636

Epoch: 6| Step: 1
Training loss: 2.0155084133148193
Validation loss: 2.3684018632417083

Epoch: 6| Step: 2
Training loss: 2.9657135009765625
Validation loss: 2.369257296285322

Epoch: 6| Step: 3
Training loss: 2.616245746612549
Validation loss: 2.360748585834298

Epoch: 6| Step: 4
Training loss: 2.010122299194336
Validation loss: 2.3584272425661803

Epoch: 6| Step: 5
Training loss: 2.470700263977051
Validation loss: 2.3534216303979196

Epoch: 6| Step: 6
Training loss: 3.0584654808044434
Validation loss: 2.352193268396521

Epoch: 6| Step: 7
Training loss: 2.5535504817962646
Validation loss: 2.353243704765074

Epoch: 6| Step: 8
Training loss: 2.9397451877593994
Validation loss: 2.3569133922617924

Epoch: 6| Step: 9
Training loss: 2.4104576110839844
Validation loss: 2.357612407335671

Epoch: 6| Step: 10
Training loss: 2.731464385986328
Validation loss: 2.357192309953833

Epoch: 6| Step: 11
Training loss: 3.1890642642974854
Validation loss: 2.3657977888661046

Epoch: 6| Step: 12
Training loss: 2.700293779373169
Validation loss: 2.3633893433437554

Epoch: 6| Step: 13
Training loss: 2.4650394916534424
Validation loss: 2.3663904384900163

Epoch: 66| Step: 0
Training loss: 1.8712236881256104
Validation loss: 2.368913814585696

Epoch: 6| Step: 1
Training loss: 2.604959726333618
Validation loss: 2.3805188055961364

Epoch: 6| Step: 2
Training loss: 3.698282241821289
Validation loss: 2.391920117921727

Epoch: 6| Step: 3
Training loss: 2.399599552154541
Validation loss: 2.3965881409183627

Epoch: 6| Step: 4
Training loss: 2.096747398376465
Validation loss: 2.3923134701226347

Epoch: 6| Step: 5
Training loss: 2.848722219467163
Validation loss: 2.3932665778744604

Epoch: 6| Step: 6
Training loss: 2.281367778778076
Validation loss: 2.3775570802791144

Epoch: 6| Step: 7
Training loss: 2.6918535232543945
Validation loss: 2.3672084141803045

Epoch: 6| Step: 8
Training loss: 2.9717330932617188
Validation loss: 2.357794838566934

Epoch: 6| Step: 9
Training loss: 2.5868477821350098
Validation loss: 2.3523209966639036

Epoch: 6| Step: 10
Training loss: 2.8077759742736816
Validation loss: 2.3525052506436586

Epoch: 6| Step: 11
Training loss: 2.0570924282073975
Validation loss: 2.352989968433175

Epoch: 6| Step: 12
Training loss: 3.111194610595703
Validation loss: 2.356352990673434

Epoch: 6| Step: 13
Training loss: 2.2642874717712402
Validation loss: 2.3511779269864483

Epoch: 67| Step: 0
Training loss: 2.5047531127929688
Validation loss: 2.3542475649105605

Epoch: 6| Step: 1
Training loss: 2.523601531982422
Validation loss: 2.3511991167581208

Epoch: 6| Step: 2
Training loss: 2.436331272125244
Validation loss: 2.352837239542315

Epoch: 6| Step: 3
Training loss: 2.362882375717163
Validation loss: 2.3518207380848546

Epoch: 6| Step: 4
Training loss: 1.8116745948791504
Validation loss: 2.354404967318299

Epoch: 6| Step: 5
Training loss: 2.6898610591888428
Validation loss: 2.360083069852603

Epoch: 6| Step: 6
Training loss: 1.9656164646148682
Validation loss: 2.35982515991375

Epoch: 6| Step: 7
Training loss: 3.450437545776367
Validation loss: 2.363480939660021

Epoch: 6| Step: 8
Training loss: 3.1353342533111572
Validation loss: 2.3598778914379817

Epoch: 6| Step: 9
Training loss: 2.5335936546325684
Validation loss: 2.359943838529689

Epoch: 6| Step: 10
Training loss: 2.806189775466919
Validation loss: 2.3594797862473356

Epoch: 6| Step: 11
Training loss: 2.7870993614196777
Validation loss: 2.367652436738373

Epoch: 6| Step: 12
Training loss: 2.8312907218933105
Validation loss: 2.3679384928877636

Epoch: 6| Step: 13
Training loss: 2.6467645168304443
Validation loss: 2.373019233826668

Epoch: 68| Step: 0
Training loss: 2.64208984375
Validation loss: 2.3753259105067097

Epoch: 6| Step: 1
Training loss: 3.3123080730438232
Validation loss: 2.363976893886443

Epoch: 6| Step: 2
Training loss: 3.025522470474243
Validation loss: 2.3613261099784606

Epoch: 6| Step: 3
Training loss: 2.540030002593994
Validation loss: 2.3676821416424167

Epoch: 6| Step: 4
Training loss: 2.364495277404785
Validation loss: 2.373192564133675

Epoch: 6| Step: 5
Training loss: 2.986548900604248
Validation loss: 2.3793662055846183

Epoch: 6| Step: 6
Training loss: 1.9202736616134644
Validation loss: 2.377744154263568

Epoch: 6| Step: 7
Training loss: 1.8464727401733398
Validation loss: 2.373261874721896

Epoch: 6| Step: 8
Training loss: 3.0309953689575195
Validation loss: 2.363125016612391

Epoch: 6| Step: 9
Training loss: 2.32204270362854
Validation loss: 2.3674137720497708

Epoch: 6| Step: 10
Training loss: 2.6692399978637695
Validation loss: 2.3659822120461413

Epoch: 6| Step: 11
Training loss: 2.459671974182129
Validation loss: 2.3714440714928413

Epoch: 6| Step: 12
Training loss: 2.946485757827759
Validation loss: 2.3677843719400387

Epoch: 6| Step: 13
Training loss: 2.058290481567383
Validation loss: 2.3550452211851716

Epoch: 69| Step: 0
Training loss: 2.9323878288269043
Validation loss: 2.3533060499416885

Epoch: 6| Step: 1
Training loss: 2.441895008087158
Validation loss: 2.346450203208513

Epoch: 6| Step: 2
Training loss: 2.130779504776001
Validation loss: 2.3412519296010337

Epoch: 6| Step: 3
Training loss: 2.5556182861328125
Validation loss: 2.3347702180185625

Epoch: 6| Step: 4
Training loss: 2.447720527648926
Validation loss: 2.331951225957563

Epoch: 6| Step: 5
Training loss: 3.3883090019226074
Validation loss: 2.3293398810971166

Epoch: 6| Step: 6
Training loss: 2.6341986656188965
Validation loss: 2.3294500099715365

Epoch: 6| Step: 7
Training loss: 1.9036816358566284
Validation loss: 2.3335936992399153

Epoch: 6| Step: 8
Training loss: 3.2560901641845703
Validation loss: 2.338887127496863

Epoch: 6| Step: 9
Training loss: 2.478079080581665
Validation loss: 2.341196411399431

Epoch: 6| Step: 10
Training loss: 2.4246416091918945
Validation loss: 2.345397415981498

Epoch: 6| Step: 11
Training loss: 3.053140163421631
Validation loss: 2.3443751822235765

Epoch: 6| Step: 12
Training loss: 2.438626289367676
Validation loss: 2.378737644482684

Epoch: 6| Step: 13
Training loss: 2.141551971435547
Validation loss: 2.4041836287385676

Epoch: 70| Step: 0
Training loss: 2.4689393043518066
Validation loss: 2.4230210217096473

Epoch: 6| Step: 1
Training loss: 2.5189812183380127
Validation loss: 2.3914066386479202

Epoch: 6| Step: 2
Training loss: 2.014767646789551
Validation loss: 2.3770824196518108

Epoch: 6| Step: 3
Training loss: 2.77243709564209
Validation loss: 2.3659267938265236

Epoch: 6| Step: 4
Training loss: 2.382554054260254
Validation loss: 2.3317957565348637

Epoch: 6| Step: 5
Training loss: 3.164954662322998
Validation loss: 2.3223201228726293

Epoch: 6| Step: 6
Training loss: 1.7213584184646606
Validation loss: 2.3198529263978362

Epoch: 6| Step: 7
Training loss: 2.409249782562256
Validation loss: 2.3196775426146803

Epoch: 6| Step: 8
Training loss: 3.142691135406494
Validation loss: 2.329750414817564

Epoch: 6| Step: 9
Training loss: 2.778075695037842
Validation loss: 2.3301662783468924

Epoch: 6| Step: 10
Training loss: 2.2507505416870117
Validation loss: 2.332558847242786

Epoch: 6| Step: 11
Training loss: 2.768838405609131
Validation loss: 2.338149311721966

Epoch: 6| Step: 12
Training loss: 3.317440986633301
Validation loss: 2.3512169699515066

Epoch: 6| Step: 13
Training loss: 2.779510259628296
Validation loss: 2.3658397530996673

Epoch: 71| Step: 0
Training loss: 2.795994520187378
Validation loss: 2.4159172683633785

Epoch: 6| Step: 1
Training loss: 3.031100034713745
Validation loss: 2.4583745541111117

Epoch: 6| Step: 2
Training loss: 2.947627544403076
Validation loss: 2.4685265889731784

Epoch: 6| Step: 3
Training loss: 2.498230457305908
Validation loss: 2.428355896344749

Epoch: 6| Step: 4
Training loss: 3.255861282348633
Validation loss: 2.4048621167418776

Epoch: 6| Step: 5
Training loss: 2.8060269355773926
Validation loss: 2.3732732188317085

Epoch: 6| Step: 6
Training loss: 2.824631690979004
Validation loss: 2.33696388429211

Epoch: 6| Step: 7
Training loss: 2.8843274116516113
Validation loss: 2.3299028796534382

Epoch: 6| Step: 8
Training loss: 2.2574431896209717
Validation loss: 2.3224865236589984

Epoch: 6| Step: 9
Training loss: 1.9450020790100098
Validation loss: 2.3216651588357906

Epoch: 6| Step: 10
Training loss: 2.7235920429229736
Validation loss: 2.3298438338823217

Epoch: 6| Step: 11
Training loss: 2.617177963256836
Validation loss: 2.334392291243358

Epoch: 6| Step: 12
Training loss: 2.111161708831787
Validation loss: 2.3427841740269817

Epoch: 6| Step: 13
Training loss: 2.3887133598327637
Validation loss: 2.327901447972944

Epoch: 72| Step: 0
Training loss: 2.9618377685546875
Validation loss: 2.3226498737130115

Epoch: 6| Step: 1
Training loss: 2.7112061977386475
Validation loss: 2.3176587063779115

Epoch: 6| Step: 2
Training loss: 3.009392261505127
Validation loss: 2.3121692826670985

Epoch: 6| Step: 3
Training loss: 3.218921184539795
Validation loss: 2.311663907061341

Epoch: 6| Step: 4
Training loss: 3.127678394317627
Validation loss: 2.3112431213419926

Epoch: 6| Step: 5
Training loss: 1.6205335855484009
Validation loss: 2.310462967042

Epoch: 6| Step: 6
Training loss: 1.969029426574707
Validation loss: 2.3110582931067354

Epoch: 6| Step: 7
Training loss: 2.586599588394165
Validation loss: 2.319052880810153

Epoch: 6| Step: 8
Training loss: 2.7388248443603516
Validation loss: 2.3376952191834808

Epoch: 6| Step: 9
Training loss: 2.380584955215454
Validation loss: 2.349091422173285

Epoch: 6| Step: 10
Training loss: 2.906418561935425
Validation loss: 2.355091269298266

Epoch: 6| Step: 11
Training loss: 2.53096079826355
Validation loss: 2.358484547625306

Epoch: 6| Step: 12
Training loss: 1.9137169122695923
Validation loss: 2.354340919884302

Epoch: 6| Step: 13
Training loss: 2.880904197692871
Validation loss: 2.3386807031528924

Epoch: 73| Step: 0
Training loss: 2.0761895179748535
Validation loss: 2.331626694689515

Epoch: 6| Step: 1
Training loss: 1.3698521852493286
Validation loss: 2.3386011277475665

Epoch: 6| Step: 2
Training loss: 2.5024805068969727
Validation loss: 2.3313411589591735

Epoch: 6| Step: 3
Training loss: 2.610668659210205
Validation loss: 2.3343403262476765

Epoch: 6| Step: 4
Training loss: 2.6920323371887207
Validation loss: 2.3380912555161344

Epoch: 6| Step: 5
Training loss: 3.696694850921631
Validation loss: 2.3449971240053893

Epoch: 6| Step: 6
Training loss: 3.508485794067383
Validation loss: 2.340465643072641

Epoch: 6| Step: 7
Training loss: 2.594278335571289
Validation loss: 2.325776328322708

Epoch: 6| Step: 8
Training loss: 2.8372368812561035
Validation loss: 2.3284101691297305

Epoch: 6| Step: 9
Training loss: 2.6168837547302246
Validation loss: 2.3285681355384087

Epoch: 6| Step: 10
Training loss: 2.2887918949127197
Validation loss: 2.3301144466605237

Epoch: 6| Step: 11
Training loss: 2.821937322616577
Validation loss: 2.332663179725729

Epoch: 6| Step: 12
Training loss: 2.2113659381866455
Validation loss: 2.321813967920119

Epoch: 6| Step: 13
Training loss: 2.2020885944366455
Validation loss: 2.3184153956751667

Epoch: 74| Step: 0
Training loss: 2.2942566871643066
Validation loss: 2.3074117911759244

Epoch: 6| Step: 1
Training loss: 3.2054340839385986
Validation loss: 2.3074097428270566

Epoch: 6| Step: 2
Training loss: 2.4819889068603516
Validation loss: 2.306939476279802

Epoch: 6| Step: 3
Training loss: 2.9861302375793457
Validation loss: 2.3096168066865657

Epoch: 6| Step: 4
Training loss: 2.4284253120422363
Validation loss: 2.307872303070561

Epoch: 6| Step: 5
Training loss: 2.672192335128784
Validation loss: 2.30275789383919

Epoch: 6| Step: 6
Training loss: 3.439591646194458
Validation loss: 2.2988225439543366

Epoch: 6| Step: 7
Training loss: 2.2480101585388184
Validation loss: 2.303369611822149

Epoch: 6| Step: 8
Training loss: 2.4554522037506104
Validation loss: 2.3009623327562885

Epoch: 6| Step: 9
Training loss: 2.196160316467285
Validation loss: 2.3007144645978044

Epoch: 6| Step: 10
Training loss: 3.3885316848754883
Validation loss: 2.300346223256921

Epoch: 6| Step: 11
Training loss: 2.09928822517395
Validation loss: 2.3163973900579635

Epoch: 6| Step: 12
Training loss: 2.0269088745117188
Validation loss: 2.3441688245342625

Epoch: 6| Step: 13
Training loss: 1.930031657218933
Validation loss: 2.353676244776736

Epoch: 75| Step: 0
Training loss: 2.264893054962158
Validation loss: 2.3519830678098943

Epoch: 6| Step: 1
Training loss: 2.6217029094696045
Validation loss: 2.3337446130732054

Epoch: 6| Step: 2
Training loss: 2.736203670501709
Validation loss: 2.326489163983253

Epoch: 6| Step: 3
Training loss: 2.993846893310547
Validation loss: 2.311322982593249

Epoch: 6| Step: 4
Training loss: 2.423304796218872
Validation loss: 2.3068937563127085

Epoch: 6| Step: 5
Training loss: 2.9860358238220215
Validation loss: 2.3064671844564457

Epoch: 6| Step: 6
Training loss: 3.4186182022094727
Validation loss: 2.296419359022571

Epoch: 6| Step: 7
Training loss: 2.962273359298706
Validation loss: 2.2866541442050727

Epoch: 6| Step: 8
Training loss: 2.3184385299682617
Validation loss: 2.28826872123185

Epoch: 6| Step: 9
Training loss: 2.0544445514678955
Validation loss: 2.2894978548890803

Epoch: 6| Step: 10
Training loss: 2.2777769565582275
Validation loss: 2.2876268125349477

Epoch: 6| Step: 11
Training loss: 2.517545700073242
Validation loss: 2.2878325241868214

Epoch: 6| Step: 12
Training loss: 2.126119613647461
Validation loss: 2.2892804709813928

Epoch: 6| Step: 13
Training loss: 2.482074022293091
Validation loss: 2.287785373708253

Epoch: 76| Step: 0
Training loss: 2.834838390350342
Validation loss: 2.287144341776448

Epoch: 6| Step: 1
Training loss: 2.9601891040802
Validation loss: 2.2826892381073325

Epoch: 6| Step: 2
Training loss: 2.053488254547119
Validation loss: 2.2890088135196316

Epoch: 6| Step: 3
Training loss: 2.469633102416992
Validation loss: 2.2929443031229

Epoch: 6| Step: 4
Training loss: 2.626589298248291
Validation loss: 2.2947053781119724

Epoch: 6| Step: 5
Training loss: 2.2261276245117188
Validation loss: 2.3025219722460677

Epoch: 6| Step: 6
Training loss: 2.8397068977355957
Validation loss: 2.312899830520794

Epoch: 6| Step: 7
Training loss: 2.457176685333252
Validation loss: 2.3255694732871106

Epoch: 6| Step: 8
Training loss: 2.441311836242676
Validation loss: 2.34815316430984

Epoch: 6| Step: 9
Training loss: 2.9554812908172607
Validation loss: 2.3989555707541843

Epoch: 6| Step: 10
Training loss: 2.41452956199646
Validation loss: 2.3935665699743454

Epoch: 6| Step: 11
Training loss: 2.616187572479248
Validation loss: 2.351921563507408

Epoch: 6| Step: 12
Training loss: 2.8051445484161377
Validation loss: 2.328274083393876

Epoch: 6| Step: 13
Training loss: 2.676194429397583
Validation loss: 2.338234455354752

Epoch: 77| Step: 0
Training loss: 2.8547720909118652
Validation loss: 2.3577533229704826

Epoch: 6| Step: 1
Training loss: 2.03369402885437
Validation loss: 2.350283177950049

Epoch: 6| Step: 2
Training loss: 2.9153671264648438
Validation loss: 2.3253643294816375

Epoch: 6| Step: 3
Training loss: 2.230544328689575
Validation loss: 2.3073793995764946

Epoch: 6| Step: 4
Training loss: 2.6538400650024414
Validation loss: 2.2984808157849055

Epoch: 6| Step: 5
Training loss: 3.0253560543060303
Validation loss: 2.299910560730965

Epoch: 6| Step: 6
Training loss: 2.56408429145813
Validation loss: 2.3044672832694104

Epoch: 6| Step: 7
Training loss: 2.0122056007385254
Validation loss: 2.3093932674777125

Epoch: 6| Step: 8
Training loss: 2.5158751010894775
Validation loss: 2.3132507698510283

Epoch: 6| Step: 9
Training loss: 2.0819690227508545
Validation loss: 2.315732744432265

Epoch: 6| Step: 10
Training loss: 2.6998205184936523
Validation loss: 2.3264776891277683

Epoch: 6| Step: 11
Training loss: 2.3504812717437744
Validation loss: 2.3399164830484698

Epoch: 6| Step: 12
Training loss: 3.087822437286377
Validation loss: 2.361878861663162

Epoch: 6| Step: 13
Training loss: 3.4828741550445557
Validation loss: 2.379638784675188

Epoch: 78| Step: 0
Training loss: 3.24627685546875
Validation loss: 2.3709665908608386

Epoch: 6| Step: 1
Training loss: 2.2612602710723877
Validation loss: 2.3706620636806695

Epoch: 6| Step: 2
Training loss: 2.1774301528930664
Validation loss: 2.3475696143283638

Epoch: 6| Step: 3
Training loss: 2.1736488342285156
Validation loss: 2.3322483442162953

Epoch: 6| Step: 4
Training loss: 2.4320602416992188
Validation loss: 2.318659560654753

Epoch: 6| Step: 5
Training loss: 2.2970733642578125
Validation loss: 2.3063275839692805

Epoch: 6| Step: 6
Training loss: 2.583557367324829
Validation loss: 2.3148485165770336

Epoch: 6| Step: 7
Training loss: 2.457364559173584
Validation loss: 2.312547940079884

Epoch: 6| Step: 8
Training loss: 2.7141201496124268
Validation loss: 2.308300397729361

Epoch: 6| Step: 9
Training loss: 2.663444995880127
Validation loss: 2.310714734497891

Epoch: 6| Step: 10
Training loss: 2.357722282409668
Validation loss: 2.312214074596282

Epoch: 6| Step: 11
Training loss: 2.128082036972046
Validation loss: 2.314539401761947

Epoch: 6| Step: 12
Training loss: 3.427403688430786
Validation loss: 2.319378545207362

Epoch: 6| Step: 13
Training loss: 3.0720529556274414
Validation loss: 2.3169007173148533

Epoch: 79| Step: 0
Training loss: 3.462348461151123
Validation loss: 2.315665824438936

Epoch: 6| Step: 1
Training loss: 1.520918846130371
Validation loss: 2.3093415434642504

Epoch: 6| Step: 2
Training loss: 3.442967414855957
Validation loss: 2.305427393605632

Epoch: 6| Step: 3
Training loss: 2.6775994300842285
Validation loss: 2.2997103685973794

Epoch: 6| Step: 4
Training loss: 2.638974189758301
Validation loss: 2.298058925136443

Epoch: 6| Step: 5
Training loss: 2.4207887649536133
Validation loss: 2.2922290320037515

Epoch: 6| Step: 6
Training loss: 2.275656223297119
Validation loss: 2.289836532326155

Epoch: 6| Step: 7
Training loss: 2.7406668663024902
Validation loss: 2.28968233190557

Epoch: 6| Step: 8
Training loss: 2.202634811401367
Validation loss: 2.2812553631362094

Epoch: 6| Step: 9
Training loss: 1.9447282552719116
Validation loss: 2.285635599526026

Epoch: 6| Step: 10
Training loss: 2.7219929695129395
Validation loss: 2.286949981925308

Epoch: 6| Step: 11
Training loss: 2.0044631958007812
Validation loss: 2.292156098991312

Epoch: 6| Step: 12
Training loss: 3.1220340728759766
Validation loss: 2.3052247237133723

Epoch: 6| Step: 13
Training loss: 2.384052276611328
Validation loss: 2.31931479771932

Epoch: 80| Step: 0
Training loss: 2.137488842010498
Validation loss: 2.333764999143539

Epoch: 6| Step: 1
Training loss: 2.458787441253662
Validation loss: 2.3301684369323072

Epoch: 6| Step: 2
Training loss: 2.437621593475342
Validation loss: 2.326169926633117

Epoch: 6| Step: 3
Training loss: 2.65423321723938
Validation loss: 2.3162702411733647

Epoch: 6| Step: 4
Training loss: 2.0427892208099365
Validation loss: 2.3023436992399153

Epoch: 6| Step: 5
Training loss: 2.7523694038391113
Validation loss: 2.301708371408524

Epoch: 6| Step: 6
Training loss: 2.1056437492370605
Validation loss: 2.303496241569519

Epoch: 6| Step: 7
Training loss: 2.9776434898376465
Validation loss: 2.2984551511785036

Epoch: 6| Step: 8
Training loss: 3.029921293258667
Validation loss: 2.298719638137407

Epoch: 6| Step: 9
Training loss: 2.210611581802368
Validation loss: 2.301165588440434

Epoch: 6| Step: 10
Training loss: 2.5857183933258057
Validation loss: 2.3028455882944088

Epoch: 6| Step: 11
Training loss: 2.869811773300171
Validation loss: 2.3098507696582424

Epoch: 6| Step: 12
Training loss: 2.530824661254883
Validation loss: 2.311680345125096

Epoch: 6| Step: 13
Training loss: 2.9752469062805176
Validation loss: 2.320374857994818

Epoch: 81| Step: 0
Training loss: 2.0798592567443848
Validation loss: 2.34881987110261

Epoch: 6| Step: 1
Training loss: 2.7235569953918457
Validation loss: 2.369143946196443

Epoch: 6| Step: 2
Training loss: 2.058727264404297
Validation loss: 2.385697900608022

Epoch: 6| Step: 3
Training loss: 2.365133762359619
Validation loss: 2.3631102449150494

Epoch: 6| Step: 4
Training loss: 2.8720479011535645
Validation loss: 2.3589781817569526

Epoch: 6| Step: 5
Training loss: 2.6668457984924316
Validation loss: 2.325595745476343

Epoch: 6| Step: 6
Training loss: 2.866549491882324
Validation loss: 2.3115986342071206

Epoch: 6| Step: 7
Training loss: 2.4737443923950195
Validation loss: 2.299974542792125

Epoch: 6| Step: 8
Training loss: 2.301718235015869
Validation loss: 2.290729712414485

Epoch: 6| Step: 9
Training loss: 2.7550272941589355
Validation loss: 2.287655640673894

Epoch: 6| Step: 10
Training loss: 3.0240793228149414
Validation loss: 2.285458546812816

Epoch: 6| Step: 11
Training loss: 1.9670908451080322
Validation loss: 2.2829642500928653

Epoch: 6| Step: 12
Training loss: 2.7845587730407715
Validation loss: 2.289293964703878

Epoch: 6| Step: 13
Training loss: 2.9865753650665283
Validation loss: 2.284565248797017

Epoch: 82| Step: 0
Training loss: 1.7574578523635864
Validation loss: 2.288608681771063

Epoch: 6| Step: 1
Training loss: 1.9047213792800903
Validation loss: 2.303280556073753

Epoch: 6| Step: 2
Training loss: 2.514991283416748
Validation loss: 2.3012079654201383

Epoch: 6| Step: 3
Training loss: 3.0467376708984375
Validation loss: 2.3006453591008342

Epoch: 6| Step: 4
Training loss: 2.8824758529663086
Validation loss: 2.3059308323808896

Epoch: 6| Step: 5
Training loss: 2.337045192718506
Validation loss: 2.3049624837854856

Epoch: 6| Step: 6
Training loss: 2.2400193214416504
Validation loss: 2.3160321507402646

Epoch: 6| Step: 7
Training loss: 2.939149856567383
Validation loss: 2.300054329697804

Epoch: 6| Step: 8
Training loss: 1.9945826530456543
Validation loss: 2.289515218427104

Epoch: 6| Step: 9
Training loss: 2.9792423248291016
Validation loss: 2.290650385682301

Epoch: 6| Step: 10
Training loss: 2.9067416191101074
Validation loss: 2.281608122651295

Epoch: 6| Step: 11
Training loss: 2.269010543823242
Validation loss: 2.2841572556444394

Epoch: 6| Step: 12
Training loss: 2.7597525119781494
Validation loss: 2.2826697723839873

Epoch: 6| Step: 13
Training loss: 3.1214163303375244
Validation loss: 2.2865805651551936

Epoch: 83| Step: 0
Training loss: 2.619344711303711
Validation loss: 2.283265621431412

Epoch: 6| Step: 1
Training loss: 2.950406074523926
Validation loss: 2.2743874237101567

Epoch: 6| Step: 2
Training loss: 2.958528518676758
Validation loss: 2.2851620515187583

Epoch: 6| Step: 3
Training loss: 2.7890212535858154
Validation loss: 2.289933560996927

Epoch: 6| Step: 4
Training loss: 1.4627364873886108
Validation loss: 2.288736286983695

Epoch: 6| Step: 5
Training loss: 2.541182518005371
Validation loss: 2.284785152763449

Epoch: 6| Step: 6
Training loss: 1.6308619976043701
Validation loss: 2.2982142907316967

Epoch: 6| Step: 7
Training loss: 2.411271572113037
Validation loss: 2.290217927707139

Epoch: 6| Step: 8
Training loss: 2.9144554138183594
Validation loss: 2.288981645337997

Epoch: 6| Step: 9
Training loss: 2.775874614715576
Validation loss: 2.2904432871008433

Epoch: 6| Step: 10
Training loss: 2.431973934173584
Validation loss: 2.2869885685623332

Epoch: 6| Step: 11
Training loss: 2.2960617542266846
Validation loss: 2.2937556543657855

Epoch: 6| Step: 12
Training loss: 2.87911319732666
Validation loss: 2.291420254656064

Epoch: 6| Step: 13
Training loss: 2.4068315029144287
Validation loss: 2.2875956540466635

Epoch: 84| Step: 0
Training loss: 2.254232406616211
Validation loss: 2.3170279392632107

Epoch: 6| Step: 1
Training loss: 2.7835021018981934
Validation loss: 2.321273165364419

Epoch: 6| Step: 2
Training loss: 2.0322160720825195
Validation loss: 2.322531718079762

Epoch: 6| Step: 3
Training loss: 2.2080273628234863
Validation loss: 2.3175749112200994

Epoch: 6| Step: 4
Training loss: 3.3338708877563477
Validation loss: 2.316784917667348

Epoch: 6| Step: 5
Training loss: 2.5816798210144043
Validation loss: 2.308923500840382

Epoch: 6| Step: 6
Training loss: 2.543452024459839
Validation loss: 2.304124834716961

Epoch: 6| Step: 7
Training loss: 2.591545581817627
Validation loss: 2.302033773032568

Epoch: 6| Step: 8
Training loss: 2.069657325744629
Validation loss: 2.30364655422908

Epoch: 6| Step: 9
Training loss: 2.431333065032959
Validation loss: 2.2963902514467955

Epoch: 6| Step: 10
Training loss: 2.5193865299224854
Validation loss: 2.2850075972977506

Epoch: 6| Step: 11
Training loss: 3.242680549621582
Validation loss: 2.278228795656594

Epoch: 6| Step: 12
Training loss: 2.5321619510650635
Validation loss: 2.2761832244934572

Epoch: 6| Step: 13
Training loss: 2.0871119499206543
Validation loss: 2.2705129705449587

Epoch: 85| Step: 0
Training loss: 2.513309955596924
Validation loss: 2.2713928171383437

Epoch: 6| Step: 1
Training loss: 2.92830753326416
Validation loss: 2.256260536050284

Epoch: 6| Step: 2
Training loss: 2.3603882789611816
Validation loss: 2.2509233297840243

Epoch: 6| Step: 3
Training loss: 2.034853458404541
Validation loss: 2.2706059948090584

Epoch: 6| Step: 4
Training loss: 2.091660261154175
Validation loss: 2.2639103089609454

Epoch: 6| Step: 5
Training loss: 2.338620185852051
Validation loss: 2.2670162493182766

Epoch: 6| Step: 6
Training loss: 1.951693058013916
Validation loss: 2.253217286961053

Epoch: 6| Step: 7
Training loss: 2.735189199447632
Validation loss: 2.2483711460585236

Epoch: 6| Step: 8
Training loss: 2.7173779010772705
Validation loss: 2.249427254481982

Epoch: 6| Step: 9
Training loss: 2.4587135314941406
Validation loss: 2.2444605724785918

Epoch: 6| Step: 10
Training loss: 3.0428378582000732
Validation loss: 2.2467726328039683

Epoch: 6| Step: 11
Training loss: 2.8800673484802246
Validation loss: 2.241847902215937

Epoch: 6| Step: 12
Training loss: 2.226719617843628
Validation loss: 2.24697861876539

Epoch: 6| Step: 13
Training loss: 3.1886723041534424
Validation loss: 2.2512619777392318

Epoch: 86| Step: 0
Training loss: 2.3749587535858154
Validation loss: 2.243970291588896

Epoch: 6| Step: 1
Training loss: 2.808037281036377
Validation loss: 2.234111742306781

Epoch: 6| Step: 2
Training loss: 2.442448377609253
Validation loss: 2.2350054851142307

Epoch: 6| Step: 3
Training loss: 2.218238115310669
Validation loss: 2.2370843861692693

Epoch: 6| Step: 4
Training loss: 2.4862356185913086
Validation loss: 2.2448383082625685

Epoch: 6| Step: 5
Training loss: 3.458890438079834
Validation loss: 2.256164936609166

Epoch: 6| Step: 6
Training loss: 2.6199772357940674
Validation loss: 2.263111452902517

Epoch: 6| Step: 7
Training loss: 2.0237457752227783
Validation loss: 2.281747712883898

Epoch: 6| Step: 8
Training loss: 2.8769073486328125
Validation loss: 2.2881146938570085

Epoch: 6| Step: 9
Training loss: 2.716603994369507
Validation loss: 2.2980315864727063

Epoch: 6| Step: 10
Training loss: 2.2218055725097656
Validation loss: 2.2793434589139876

Epoch: 6| Step: 11
Training loss: 1.8285576105117798
Validation loss: 2.2675034820392566

Epoch: 6| Step: 12
Training loss: 2.7535910606384277
Validation loss: 2.2711468973467426

Epoch: 6| Step: 13
Training loss: 2.0876986980438232
Validation loss: 2.2620495596239643

Epoch: 87| Step: 0
Training loss: 2.1642379760742188
Validation loss: 2.264628735921716

Epoch: 6| Step: 1
Training loss: 2.3152194023132324
Validation loss: 2.2744402987982637

Epoch: 6| Step: 2
Training loss: 2.9336447715759277
Validation loss: 2.2760227623806206

Epoch: 6| Step: 3
Training loss: 2.387441396713257
Validation loss: 2.2773965456152476

Epoch: 6| Step: 4
Training loss: 2.369251251220703
Validation loss: 2.285407999510406

Epoch: 6| Step: 5
Training loss: 2.8708925247192383
Validation loss: 2.3074806415906517

Epoch: 6| Step: 6
Training loss: 2.863689422607422
Validation loss: 2.3015717165444487

Epoch: 6| Step: 7
Training loss: 2.4684882164001465
Validation loss: 2.300075046477779

Epoch: 6| Step: 8
Training loss: 2.8787102699279785
Validation loss: 2.2915895959382415

Epoch: 6| Step: 9
Training loss: 2.0752079486846924
Validation loss: 2.2939815444331013

Epoch: 6| Step: 10
Training loss: 2.4364848136901855
Validation loss: 2.2727459374294487

Epoch: 6| Step: 11
Training loss: 2.442922592163086
Validation loss: 2.262976877150997

Epoch: 6| Step: 12
Training loss: 2.3216660022735596
Validation loss: 2.244433838834045

Epoch: 6| Step: 13
Training loss: 2.5085513591766357
Validation loss: 2.243814227401569

Epoch: 88| Step: 0
Training loss: 2.2615623474121094
Validation loss: 2.2422290002146075

Epoch: 6| Step: 1
Training loss: 2.227226495742798
Validation loss: 2.244915682782409

Epoch: 6| Step: 2
Training loss: 2.639535427093506
Validation loss: 2.2532666139705206

Epoch: 6| Step: 3
Training loss: 2.824146032333374
Validation loss: 2.267294499181932

Epoch: 6| Step: 4
Training loss: 2.4172544479370117
Validation loss: 2.270574290265319

Epoch: 6| Step: 5
Training loss: 2.757931709289551
Validation loss: 2.2541127948350805

Epoch: 6| Step: 6
Training loss: 2.1613929271698
Validation loss: 2.2564560738942956

Epoch: 6| Step: 7
Training loss: 2.0075457096099854
Validation loss: 2.2492020591612785

Epoch: 6| Step: 8
Training loss: 3.0215654373168945
Validation loss: 2.2298747801011607

Epoch: 6| Step: 9
Training loss: 2.25589656829834
Validation loss: 2.2222677097525647

Epoch: 6| Step: 10
Training loss: 2.4756011962890625
Validation loss: 2.224027754158102

Epoch: 6| Step: 11
Training loss: 1.688941478729248
Validation loss: 2.224542961325697

Epoch: 6| Step: 12
Training loss: 3.200601577758789
Validation loss: 2.224613061515234

Epoch: 6| Step: 13
Training loss: 3.016328811645508
Validation loss: 2.2260154395975094

Epoch: 89| Step: 0
Training loss: 1.9803671836853027
Validation loss: 2.228128694718884

Epoch: 6| Step: 1
Training loss: 2.700021743774414
Validation loss: 2.2282597864827802

Epoch: 6| Step: 2
Training loss: 2.3354053497314453
Validation loss: 2.232649451942854

Epoch: 6| Step: 3
Training loss: 1.828537940979004
Validation loss: 2.2390854563764346

Epoch: 6| Step: 4
Training loss: 2.7014966011047363
Validation loss: 2.2278865050244074

Epoch: 6| Step: 5
Training loss: 2.836841344833374
Validation loss: 2.2355208730184906

Epoch: 6| Step: 6
Training loss: 2.0288801193237305
Validation loss: 2.2324505108658985

Epoch: 6| Step: 7
Training loss: 3.20149302482605
Validation loss: 2.23108741288544

Epoch: 6| Step: 8
Training loss: 2.521723747253418
Validation loss: 2.2264591006822485

Epoch: 6| Step: 9
Training loss: 2.8259997367858887
Validation loss: 2.221994189805882

Epoch: 6| Step: 10
Training loss: 2.5447096824645996
Validation loss: 2.2367302935610534

Epoch: 6| Step: 11
Training loss: 2.685391426086426
Validation loss: 2.2710066482584965

Epoch: 6| Step: 12
Training loss: 2.0956525802612305
Validation loss: 2.318388092902399

Epoch: 6| Step: 13
Training loss: 2.199606418609619
Validation loss: 2.353655784360824

Epoch: 90| Step: 0
Training loss: 2.6583564281463623
Validation loss: 2.367171915628577

Epoch: 6| Step: 1
Training loss: 2.2922604084014893
Validation loss: 2.3604858203600814

Epoch: 6| Step: 2
Training loss: 1.828975796699524
Validation loss: 2.3625581777223976

Epoch: 6| Step: 3
Training loss: 2.680387020111084
Validation loss: 2.2934478405983216

Epoch: 6| Step: 4
Training loss: 1.8689095973968506
Validation loss: 2.2518932332274733

Epoch: 6| Step: 5
Training loss: 2.8372249603271484
Validation loss: 2.2470731376319804

Epoch: 6| Step: 6
Training loss: 2.888169765472412
Validation loss: 2.2525551242213093

Epoch: 6| Step: 7
Training loss: 2.6544041633605957
Validation loss: 2.261360422257454

Epoch: 6| Step: 8
Training loss: 2.473405122756958
Validation loss: 2.269282789640529

Epoch: 6| Step: 9
Training loss: 2.939466953277588
Validation loss: 2.278210439989644

Epoch: 6| Step: 10
Training loss: 1.9773041009902954
Validation loss: 2.262862400342059

Epoch: 6| Step: 11
Training loss: 2.881514072418213
Validation loss: 2.2437802450631255

Epoch: 6| Step: 12
Training loss: 2.4523675441741943
Validation loss: 2.240211820089689

Epoch: 6| Step: 13
Training loss: 2.4964919090270996
Validation loss: 2.2422646373830815

Epoch: 91| Step: 0
Training loss: 2.4648799896240234
Validation loss: 2.2474703993848575

Epoch: 6| Step: 1
Training loss: 2.594700813293457
Validation loss: 2.2451832191918486

Epoch: 6| Step: 2
Training loss: 1.743985652923584
Validation loss: 2.248446456847652

Epoch: 6| Step: 3
Training loss: 3.146721601486206
Validation loss: 2.2472313014409875

Epoch: 6| Step: 4
Training loss: 2.0812623500823975
Validation loss: 2.237367140349521

Epoch: 6| Step: 5
Training loss: 1.8313531875610352
Validation loss: 2.2372132937113443

Epoch: 6| Step: 6
Training loss: 2.5703530311584473
Validation loss: 2.229534820843768

Epoch: 6| Step: 7
Training loss: 2.678021192550659
Validation loss: 2.2270594399462462

Epoch: 6| Step: 8
Training loss: 2.688584327697754
Validation loss: 2.225561729041479

Epoch: 6| Step: 9
Training loss: 2.955948829650879
Validation loss: 2.216179297816369

Epoch: 6| Step: 10
Training loss: 2.129876136779785
Validation loss: 2.225210533347181

Epoch: 6| Step: 11
Training loss: 2.78049898147583
Validation loss: 2.237331723654142

Epoch: 6| Step: 12
Training loss: 2.5773119926452637
Validation loss: 2.230843033841861

Epoch: 6| Step: 13
Training loss: 2.159633159637451
Validation loss: 2.2335343873629006

Epoch: 92| Step: 0
Training loss: 2.4118828773498535
Validation loss: 2.258304542110812

Epoch: 6| Step: 1
Training loss: 2.313784122467041
Validation loss: 2.2394469232969385

Epoch: 6| Step: 2
Training loss: 3.5417420864105225
Validation loss: 2.259639355444139

Epoch: 6| Step: 3
Training loss: 2.3950448036193848
Validation loss: 2.2536403312478015

Epoch: 6| Step: 4
Training loss: 2.6258726119995117
Validation loss: 2.248381914631013

Epoch: 6| Step: 5
Training loss: 2.173180103302002
Validation loss: 2.249743664136497

Epoch: 6| Step: 6
Training loss: 2.0245277881622314
Validation loss: 2.259936405766395

Epoch: 6| Step: 7
Training loss: 3.313037157058716
Validation loss: 2.2728113128292944

Epoch: 6| Step: 8
Training loss: 2.635788917541504
Validation loss: 2.2736969993960474

Epoch: 6| Step: 9
Training loss: 2.2384140491485596
Validation loss: 2.28398452010206

Epoch: 6| Step: 10
Training loss: 2.3600268363952637
Validation loss: 2.2858834189753376

Epoch: 6| Step: 11
Training loss: 2.5651848316192627
Validation loss: 2.2833692207131335

Epoch: 6| Step: 12
Training loss: 1.606788992881775
Validation loss: 2.2499232138356855

Epoch: 6| Step: 13
Training loss: 2.15877628326416
Validation loss: 2.238626944121494

Epoch: 93| Step: 0
Training loss: 2.2401771545410156
Validation loss: 2.2326487110507105

Epoch: 6| Step: 1
Training loss: 2.083118200302124
Validation loss: 2.2297434678641697

Epoch: 6| Step: 2
Training loss: 2.5821423530578613
Validation loss: 2.2296369844867336

Epoch: 6| Step: 3
Training loss: 2.3321971893310547
Validation loss: 2.227994013858098

Epoch: 6| Step: 4
Training loss: 2.6749680042266846
Validation loss: 2.2322830666777906

Epoch: 6| Step: 5
Training loss: 2.869845390319824
Validation loss: 2.23080321153005

Epoch: 6| Step: 6
Training loss: 2.399973154067993
Validation loss: 2.219647410095379

Epoch: 6| Step: 7
Training loss: 2.3379297256469727
Validation loss: 2.2221471109697895

Epoch: 6| Step: 8
Training loss: 2.620018482208252
Validation loss: 2.205564634774321

Epoch: 6| Step: 9
Training loss: 1.9924094676971436
Validation loss: 2.1931183684256768

Epoch: 6| Step: 10
Training loss: 2.9851298332214355
Validation loss: 2.193350745785621

Epoch: 6| Step: 11
Training loss: 2.574446678161621
Validation loss: 2.1935842780656714

Epoch: 6| Step: 12
Training loss: 2.3140907287597656
Validation loss: 2.219328390654697

Epoch: 6| Step: 13
Training loss: 2.1875686645507812
Validation loss: 2.222708543141683

Epoch: 94| Step: 0
Training loss: 2.0061721801757812
Validation loss: 2.2143970330556235

Epoch: 6| Step: 1
Training loss: 2.9897871017456055
Validation loss: 2.205516754939992

Epoch: 6| Step: 2
Training loss: 2.598872661590576
Validation loss: 2.1833752444995347

Epoch: 6| Step: 3
Training loss: 2.1941275596618652
Validation loss: 2.1745459495052213

Epoch: 6| Step: 4
Training loss: 2.3682119846343994
Validation loss: 2.1767927728673464

Epoch: 6| Step: 5
Training loss: 2.615556240081787
Validation loss: 2.1810477831030406

Epoch: 6| Step: 6
Training loss: 3.218873977661133
Validation loss: 2.1850130763105167

Epoch: 6| Step: 7
Training loss: 2.6472058296203613
Validation loss: 2.187282698128813

Epoch: 6| Step: 8
Training loss: 2.8311879634857178
Validation loss: 2.1923287248098724

Epoch: 6| Step: 9
Training loss: 2.4484400749206543
Validation loss: 2.1946496399500037

Epoch: 6| Step: 10
Training loss: 1.5627166032791138
Validation loss: 2.1969701449076333

Epoch: 6| Step: 11
Training loss: 2.2885642051696777
Validation loss: 2.212661135581232

Epoch: 6| Step: 12
Training loss: 2.7019734382629395
Validation loss: 2.231319027562295

Epoch: 6| Step: 13
Training loss: 2.1543915271759033
Validation loss: 2.2462376471488708

Epoch: 95| Step: 0
Training loss: 2.469421863555908
Validation loss: 2.265376829331921

Epoch: 6| Step: 1
Training loss: 1.222816824913025
Validation loss: 2.2542093261595695

Epoch: 6| Step: 2
Training loss: 2.728024959564209
Validation loss: 2.2762468668722335

Epoch: 6| Step: 3
Training loss: 2.668403387069702
Validation loss: 2.2798296456695883

Epoch: 6| Step: 4
Training loss: 2.0136797428131104
Validation loss: 2.2244757298500306

Epoch: 6| Step: 5
Training loss: 2.3255834579467773
Validation loss: 2.2154721803562616

Epoch: 6| Step: 6
Training loss: 2.416109561920166
Validation loss: 2.201621381185388

Epoch: 6| Step: 7
Training loss: 3.218092679977417
Validation loss: 2.1920465000214113

Epoch: 6| Step: 8
Training loss: 2.9017274379730225
Validation loss: 2.1911976465614895

Epoch: 6| Step: 9
Training loss: 1.925306797027588
Validation loss: 2.1766572895870415

Epoch: 6| Step: 10
Training loss: 2.9314680099487305
Validation loss: 2.1802666623105287

Epoch: 6| Step: 11
Training loss: 3.0761566162109375
Validation loss: 2.1847390949085193

Epoch: 6| Step: 12
Training loss: 2.3143692016601562
Validation loss: 2.1788000675939743

Epoch: 6| Step: 13
Training loss: 2.469244956970215
Validation loss: 2.181267497360065

Epoch: 96| Step: 0
Training loss: 2.4209682941436768
Validation loss: 2.1696785906309723

Epoch: 6| Step: 1
Training loss: 2.0608856678009033
Validation loss: 2.1760326713644047

Epoch: 6| Step: 2
Training loss: 2.6710376739501953
Validation loss: 2.172429450096623

Epoch: 6| Step: 3
Training loss: 2.405771255493164
Validation loss: 2.181521188828253

Epoch: 6| Step: 4
Training loss: 2.7079005241394043
Validation loss: 2.1889999130720734

Epoch: 6| Step: 5
Training loss: 2.9466824531555176
Validation loss: 2.194374999692363

Epoch: 6| Step: 6
Training loss: 2.6024298667907715
Validation loss: 2.202819347381592

Epoch: 6| Step: 7
Training loss: 2.73439884185791
Validation loss: 2.1744587946963567

Epoch: 6| Step: 8
Training loss: 1.9146032333374023
Validation loss: 2.174647192801199

Epoch: 6| Step: 9
Training loss: 2.5790045261383057
Validation loss: 2.1775786389586744

Epoch: 6| Step: 10
Training loss: 2.383983612060547
Validation loss: 2.1878578739781536

Epoch: 6| Step: 11
Training loss: 2.117785692214966
Validation loss: 2.193774770664912

Epoch: 6| Step: 12
Training loss: 2.464724540710449
Validation loss: 2.198086787295598

Epoch: 6| Step: 13
Training loss: 1.6454265117645264
Validation loss: 2.2000863193183817

Epoch: 97| Step: 0
Training loss: 3.001715660095215
Validation loss: 2.2119741619274182

Epoch: 6| Step: 1
Training loss: 2.403329849243164
Validation loss: 2.2146488389661236

Epoch: 6| Step: 2
Training loss: 2.819382667541504
Validation loss: 2.2291689970160045

Epoch: 6| Step: 3
Training loss: 1.3730483055114746
Validation loss: 2.233365966427711

Epoch: 6| Step: 4
Training loss: 2.115464448928833
Validation loss: 2.233764950947095

Epoch: 6| Step: 5
Training loss: 2.481696367263794
Validation loss: 2.2263518994854343

Epoch: 6| Step: 6
Training loss: 2.7140862941741943
Validation loss: 2.216668516077021

Epoch: 6| Step: 7
Training loss: 1.9285944700241089
Validation loss: 2.188672346453513

Epoch: 6| Step: 8
Training loss: 2.5566017627716064
Validation loss: 2.181880497163342

Epoch: 6| Step: 9
Training loss: 2.3944568634033203
Validation loss: 2.186817289680563

Epoch: 6| Step: 10
Training loss: 2.368597984313965
Validation loss: 2.1983698414218042

Epoch: 6| Step: 11
Training loss: 2.706881523132324
Validation loss: 2.2142905419872654

Epoch: 6| Step: 12
Training loss: 2.8775930404663086
Validation loss: 2.21339258070915

Epoch: 6| Step: 13
Training loss: 2.375750780105591
Validation loss: 2.2036286041300785

Epoch: 98| Step: 0
Training loss: 2.4677412509918213
Validation loss: 2.200222861382269

Epoch: 6| Step: 1
Training loss: 2.449315071105957
Validation loss: 2.1970948429517847

Epoch: 6| Step: 2
Training loss: 2.575188159942627
Validation loss: 2.184872273475893

Epoch: 6| Step: 3
Training loss: 2.2890005111694336
Validation loss: 2.1770700100929505

Epoch: 6| Step: 4
Training loss: 1.7707200050354004
Validation loss: 2.1714624281852477

Epoch: 6| Step: 5
Training loss: 3.0825014114379883
Validation loss: 2.162773552761283

Epoch: 6| Step: 6
Training loss: 2.5274643898010254
Validation loss: 2.146558061722786

Epoch: 6| Step: 7
Training loss: 2.2139525413513184
Validation loss: 2.14460418557608

Epoch: 6| Step: 8
Training loss: 2.473507881164551
Validation loss: 2.1527552450856855

Epoch: 6| Step: 9
Training loss: 2.0780532360076904
Validation loss: 2.167557288241643

Epoch: 6| Step: 10
Training loss: 2.7799391746520996
Validation loss: 2.1909345452503493

Epoch: 6| Step: 11
Training loss: 2.4342007637023926
Validation loss: 2.208704194714946

Epoch: 6| Step: 12
Training loss: 2.808246374130249
Validation loss: 2.2049326563394196

Epoch: 6| Step: 13
Training loss: 1.9363468885421753
Validation loss: 2.1827479972634265

Epoch: 99| Step: 0
Training loss: 2.618194818496704
Validation loss: 2.1701783185364096

Epoch: 6| Step: 1
Training loss: 3.211857795715332
Validation loss: 2.169394721267044

Epoch: 6| Step: 2
Training loss: 1.556064248085022
Validation loss: 2.1664531359108548

Epoch: 6| Step: 3
Training loss: 2.047758102416992
Validation loss: 2.1574561980462845

Epoch: 6| Step: 4
Training loss: 2.103572130203247
Validation loss: 2.154347353084113

Epoch: 6| Step: 5
Training loss: 2.676443099975586
Validation loss: 2.1558670587437128

Epoch: 6| Step: 6
Training loss: 2.226315975189209
Validation loss: 2.1569722506307785

Epoch: 6| Step: 7
Training loss: 2.268397808074951
Validation loss: 2.1612806909827778

Epoch: 6| Step: 8
Training loss: 2.7488393783569336
Validation loss: 2.1575853568251415

Epoch: 6| Step: 9
Training loss: 2.488431692123413
Validation loss: 2.1579941036880657

Epoch: 6| Step: 10
Training loss: 2.394482135772705
Validation loss: 2.154875311800229

Epoch: 6| Step: 11
Training loss: 2.300156593322754
Validation loss: 2.1656262131147486

Epoch: 6| Step: 12
Training loss: 2.4091765880584717
Validation loss: 2.1678961605154057

Epoch: 6| Step: 13
Training loss: 3.05596661567688
Validation loss: 2.168006717517812

Epoch: 100| Step: 0
Training loss: 2.0732526779174805
Validation loss: 2.1651712309929634

Epoch: 6| Step: 1
Training loss: 2.14650821685791
Validation loss: 2.162297979477913

Epoch: 6| Step: 2
Training loss: 2.4854278564453125
Validation loss: 2.1623297942582

Epoch: 6| Step: 3
Training loss: 2.2680211067199707
Validation loss: 2.1704185598640033

Epoch: 6| Step: 4
Training loss: 1.7410526275634766
Validation loss: 2.1752214995763635

Epoch: 6| Step: 5
Training loss: 2.773550271987915
Validation loss: 2.1799731408396075

Epoch: 6| Step: 6
Training loss: 1.971872091293335
Validation loss: 2.1837564181256037

Epoch: 6| Step: 7
Training loss: 1.9652934074401855
Validation loss: 2.1921136584333194

Epoch: 6| Step: 8
Training loss: 2.234975814819336
Validation loss: 2.186415074973978

Epoch: 6| Step: 9
Training loss: 3.157139778137207
Validation loss: 2.179607122175155

Epoch: 6| Step: 10
Training loss: 2.7413837909698486
Validation loss: 2.176519017065725

Epoch: 6| Step: 11
Training loss: 2.576673746109009
Validation loss: 2.1773380515395955

Epoch: 6| Step: 12
Training loss: 3.3612046241760254
Validation loss: 2.1552484612311087

Epoch: 6| Step: 13
Training loss: 2.0470752716064453
Validation loss: 2.1445162527022825

Epoch: 101| Step: 0
Training loss: 2.369246482849121
Validation loss: 2.137416567853702

Epoch: 6| Step: 1
Training loss: 1.8980056047439575
Validation loss: 2.1279743051016204

Epoch: 6| Step: 2
Training loss: 2.1997122764587402
Validation loss: 2.125471172794219

Epoch: 6| Step: 3
Training loss: 3.0686726570129395
Validation loss: 2.124774412442279

Epoch: 6| Step: 4
Training loss: 2.7678794860839844
Validation loss: 2.12807297706604

Epoch: 6| Step: 5
Training loss: 2.1075119972229004
Validation loss: 2.139937872527748

Epoch: 6| Step: 6
Training loss: 2.2739930152893066
Validation loss: 2.139346673924436

Epoch: 6| Step: 7
Training loss: 2.518876314163208
Validation loss: 2.137583404458979

Epoch: 6| Step: 8
Training loss: 2.284653663635254
Validation loss: 2.1276700342855146

Epoch: 6| Step: 9
Training loss: 2.4749836921691895
Validation loss: 2.1193642436817126

Epoch: 6| Step: 10
Training loss: 2.3652219772338867
Validation loss: 2.1225572196386193

Epoch: 6| Step: 11
Training loss: 2.149407386779785
Validation loss: 2.137361175270491

Epoch: 6| Step: 12
Training loss: 2.1890597343444824
Validation loss: 2.148469922363117

Epoch: 6| Step: 13
Training loss: 2.95149564743042
Validation loss: 2.172582334087741

Epoch: 102| Step: 0
Training loss: 2.966647148132324
Validation loss: 2.195695415619881

Epoch: 6| Step: 1
Training loss: 1.9361286163330078
Validation loss: 2.2064582481179187

Epoch: 6| Step: 2
Training loss: 2.6962714195251465
Validation loss: 2.202981851434195

Epoch: 6| Step: 3
Training loss: 2.3880600929260254
Validation loss: 2.203264424877782

Epoch: 6| Step: 4
Training loss: 1.9882237911224365
Validation loss: 2.2523499355521253

Epoch: 6| Step: 5
Training loss: 2.5919342041015625
Validation loss: 2.292731974714546

Epoch: 6| Step: 6
Training loss: 2.4286580085754395
Validation loss: 2.3035903746081936

Epoch: 6| Step: 7
Training loss: 2.427074909210205
Validation loss: 2.25859954280238

Epoch: 6| Step: 8
Training loss: 2.484128713607788
Validation loss: 2.21903153645095

Epoch: 6| Step: 9
Training loss: 2.2411999702453613
Validation loss: 2.160381986248878

Epoch: 6| Step: 10
Training loss: 2.4889590740203857
Validation loss: 2.135739912268936

Epoch: 6| Step: 11
Training loss: 2.249673366546631
Validation loss: 2.125746762880715

Epoch: 6| Step: 12
Training loss: 2.550285816192627
Validation loss: 2.1237793686569377

Epoch: 6| Step: 13
Training loss: 3.1074211597442627
Validation loss: 2.1461080248637865

Epoch: 103| Step: 0
Training loss: 2.3057026863098145
Validation loss: 2.1551416586804133

Epoch: 6| Step: 1
Training loss: 2.5074000358581543
Validation loss: 2.2093224897179553

Epoch: 6| Step: 2
Training loss: 2.7897768020629883
Validation loss: 2.2195939581881285

Epoch: 6| Step: 3
Training loss: 2.029284954071045
Validation loss: 2.267128124031969

Epoch: 6| Step: 4
Training loss: 2.385767936706543
Validation loss: 2.252558397990401

Epoch: 6| Step: 5
Training loss: 1.9232887029647827
Validation loss: 2.2381483457421743

Epoch: 6| Step: 6
Training loss: 2.8727355003356934
Validation loss: 2.180452605729462

Epoch: 6| Step: 7
Training loss: 2.237447500228882
Validation loss: 2.1514290276394097

Epoch: 6| Step: 8
Training loss: 1.967186450958252
Validation loss: 2.1458424816849413

Epoch: 6| Step: 9
Training loss: 2.2799267768859863
Validation loss: 2.1642774407581618

Epoch: 6| Step: 10
Training loss: 2.8786354064941406
Validation loss: 2.185072639937042

Epoch: 6| Step: 11
Training loss: 2.8787169456481934
Validation loss: 2.2175682360126125

Epoch: 6| Step: 12
Training loss: 2.3408493995666504
Validation loss: 2.200296735250822

Epoch: 6| Step: 13
Training loss: 2.4401705265045166
Validation loss: 2.1875397261752876

Epoch: 104| Step: 0
Training loss: 1.7314348220825195
Validation loss: 2.176645983931839

Epoch: 6| Step: 1
Training loss: 2.8912577629089355
Validation loss: 2.1555893421173096

Epoch: 6| Step: 2
Training loss: 2.412649631500244
Validation loss: 2.17472699124326

Epoch: 6| Step: 3
Training loss: 2.266894817352295
Validation loss: 2.1674376738968717

Epoch: 6| Step: 4
Training loss: 2.973580837249756
Validation loss: 2.172583410816808

Epoch: 6| Step: 5
Training loss: 2.4985287189483643
Validation loss: 2.161378619491413

Epoch: 6| Step: 6
Training loss: 2.489574670791626
Validation loss: 2.1898653532869075

Epoch: 6| Step: 7
Training loss: 2.2513175010681152
Validation loss: 2.208807773487542

Epoch: 6| Step: 8
Training loss: 2.0654115676879883
Validation loss: 2.210622214501904

Epoch: 6| Step: 9
Training loss: 1.8646306991577148
Validation loss: 2.204998316303376

Epoch: 6| Step: 10
Training loss: 2.088931083679199
Validation loss: 2.235108721640802

Epoch: 6| Step: 11
Training loss: 2.626126766204834
Validation loss: 2.2251714660275366

Epoch: 6| Step: 12
Training loss: 2.5483739376068115
Validation loss: 2.2380394884335097

Epoch: 6| Step: 13
Training loss: 3.1643035411834717
Validation loss: 2.2685463787407003

Epoch: 105| Step: 0
Training loss: 2.087841033935547
Validation loss: 2.282200518474784

Epoch: 6| Step: 1
Training loss: 2.8004276752471924
Validation loss: 2.2603174537740727

Epoch: 6| Step: 2
Training loss: 1.7110823392868042
Validation loss: 2.1941010823813816

Epoch: 6| Step: 3
Training loss: 2.1470415592193604
Validation loss: 2.175723332230763

Epoch: 6| Step: 4
Training loss: 3.1885342597961426
Validation loss: 2.158743381500244

Epoch: 6| Step: 5
Training loss: 2.1349940299987793
Validation loss: 2.175275162983966

Epoch: 6| Step: 6
Training loss: 3.006430149078369
Validation loss: 2.2003309265259774

Epoch: 6| Step: 7
Training loss: 2.0061280727386475
Validation loss: 2.2141865017593547

Epoch: 6| Step: 8
Training loss: 2.016164779663086
Validation loss: 2.2365806564208

Epoch: 6| Step: 9
Training loss: 2.2509851455688477
Validation loss: 2.2300819632827595

Epoch: 6| Step: 10
Training loss: 2.5675928592681885
Validation loss: 2.1920733656934512

Epoch: 6| Step: 11
Training loss: 3.091939926147461
Validation loss: 2.1923506388100247

Epoch: 6| Step: 12
Training loss: 2.604010820388794
Validation loss: 2.159315650181104

Epoch: 6| Step: 13
Training loss: 2.4436023235321045
Validation loss: 2.1507311764583794

Epoch: 106| Step: 0
Training loss: 2.121757984161377
Validation loss: 2.110491427042151

Epoch: 6| Step: 1
Training loss: 2.3366284370422363
Validation loss: 2.1278156824009393

Epoch: 6| Step: 2
Training loss: 3.009105682373047
Validation loss: 2.141857977836363

Epoch: 6| Step: 3
Training loss: 2.200591802597046
Validation loss: 2.200534716729195

Epoch: 6| Step: 4
Training loss: 1.4359432458877563
Validation loss: 2.2551735242207847

Epoch: 6| Step: 5
Training loss: 2.830777645111084
Validation loss: 2.3266228834788003

Epoch: 6| Step: 6
Training loss: 2.0326409339904785
Validation loss: 2.2948686397203835

Epoch: 6| Step: 7
Training loss: 2.549875259399414
Validation loss: 2.3374627354324504

Epoch: 6| Step: 8
Training loss: 2.7992820739746094
Validation loss: 2.2725047424275386

Epoch: 6| Step: 9
Training loss: 2.493557929992676
Validation loss: 2.2099789957846365

Epoch: 6| Step: 10
Training loss: 2.8870155811309814
Validation loss: 2.1488671405341035

Epoch: 6| Step: 11
Training loss: 2.579432487487793
Validation loss: 2.1392425273054387

Epoch: 6| Step: 12
Training loss: 3.110154628753662
Validation loss: 2.1288559590616534

Epoch: 6| Step: 13
Training loss: 2.1259679794311523
Validation loss: 2.1220618909405125

Epoch: 107| Step: 0
Training loss: 2.820200204849243
Validation loss: 2.1252807596678376

Epoch: 6| Step: 1
Training loss: 2.224910259246826
Validation loss: 2.1187629161342496

Epoch: 6| Step: 2
Training loss: 2.685055732727051
Validation loss: 2.120044823615782

Epoch: 6| Step: 3
Training loss: 2.8289384841918945
Validation loss: 2.120830148778936

Epoch: 6| Step: 4
Training loss: 2.73691987991333
Validation loss: 2.1306124605158323

Epoch: 6| Step: 5
Training loss: 2.731874465942383
Validation loss: 2.129477627815739

Epoch: 6| Step: 6
Training loss: 1.6812790632247925
Validation loss: 2.1431233113811863

Epoch: 6| Step: 7
Training loss: 2.7896220684051514
Validation loss: 2.1562955405122493

Epoch: 6| Step: 8
Training loss: 1.9439890384674072
Validation loss: 2.18947607983825

Epoch: 6| Step: 9
Training loss: 1.9626548290252686
Validation loss: 2.1865173206534436

Epoch: 6| Step: 10
Training loss: 2.6034388542175293
Validation loss: 2.155991849078927

Epoch: 6| Step: 11
Training loss: 2.376603126525879
Validation loss: 2.1097863053762786

Epoch: 6| Step: 12
Training loss: 2.1729745864868164
Validation loss: 2.114247824556084

Epoch: 6| Step: 13
Training loss: 2.3023157119750977
Validation loss: 2.108582850425474

Epoch: 108| Step: 0
Training loss: 1.6475566625595093
Validation loss: 2.104104495817615

Epoch: 6| Step: 1
Training loss: 2.0657598972320557
Validation loss: 2.1358386367879887

Epoch: 6| Step: 2
Training loss: 2.3841652870178223
Validation loss: 2.152973034048593

Epoch: 6| Step: 3
Training loss: 2.375518560409546
Validation loss: 2.1732574791036625

Epoch: 6| Step: 4
Training loss: 2.5596795082092285
Validation loss: 2.1899715828639206

Epoch: 6| Step: 5
Training loss: 2.693833827972412
Validation loss: 2.2394167838558072

Epoch: 6| Step: 6
Training loss: 1.96477210521698
Validation loss: 2.191691485784387

Epoch: 6| Step: 7
Training loss: 2.2852325439453125
Validation loss: 2.163275485397667

Epoch: 6| Step: 8
Training loss: 3.175877809524536
Validation loss: 2.1421759282388995

Epoch: 6| Step: 9
Training loss: 2.0195810794830322
Validation loss: 2.1262750753792385

Epoch: 6| Step: 10
Training loss: 2.640855550765991
Validation loss: 2.116060136466898

Epoch: 6| Step: 11
Training loss: 2.4885506629943848
Validation loss: 2.1084208693555606

Epoch: 6| Step: 12
Training loss: 2.7069199085235596
Validation loss: 2.0949970637598345

Epoch: 6| Step: 13
Training loss: 2.157902956008911
Validation loss: 2.0979831116173857

Epoch: 109| Step: 0
Training loss: 2.517969846725464
Validation loss: 2.0970790258017917

Epoch: 6| Step: 1
Training loss: 2.608529806137085
Validation loss: 2.103385102364325

Epoch: 6| Step: 2
Training loss: 2.9176788330078125
Validation loss: 2.097074612494438

Epoch: 6| Step: 3
Training loss: 2.7478995323181152
Validation loss: 2.090907299390403

Epoch: 6| Step: 4
Training loss: 2.089676856994629
Validation loss: 2.084649852527085

Epoch: 6| Step: 5
Training loss: 2.6121764183044434
Validation loss: 2.0811957159349994

Epoch: 6| Step: 6
Training loss: 2.727158308029175
Validation loss: 2.0878326739034345

Epoch: 6| Step: 7
Training loss: 1.3867590427398682
Validation loss: 2.085024982370356

Epoch: 6| Step: 8
Training loss: 2.467841863632202
Validation loss: 2.0915837698085333

Epoch: 6| Step: 9
Training loss: 2.5067033767700195
Validation loss: 2.087540259925268

Epoch: 6| Step: 10
Training loss: 1.5559945106506348
Validation loss: 2.083455844592023

Epoch: 6| Step: 11
Training loss: 1.8872716426849365
Validation loss: 2.088539549099502

Epoch: 6| Step: 12
Training loss: 2.312009334564209
Validation loss: 2.107119083404541

Epoch: 6| Step: 13
Training loss: 3.047852039337158
Validation loss: 2.133300563340546

Epoch: 110| Step: 0
Training loss: 2.492802619934082
Validation loss: 2.1473785574718187

Epoch: 6| Step: 1
Training loss: 2.8338418006896973
Validation loss: 2.1561604622871644

Epoch: 6| Step: 2
Training loss: 2.634230852127075
Validation loss: 2.18505181548416

Epoch: 6| Step: 3
Training loss: 1.3600623607635498
Validation loss: 2.1971350946734027

Epoch: 6| Step: 4
Training loss: 2.6718592643737793
Validation loss: 2.1574309615678686

Epoch: 6| Step: 5
Training loss: 2.0136311054229736
Validation loss: 2.112041796407392

Epoch: 6| Step: 6
Training loss: 1.9734156131744385
Validation loss: 2.103838097664618

Epoch: 6| Step: 7
Training loss: 2.4797117710113525
Validation loss: 2.09739996669113

Epoch: 6| Step: 8
Training loss: 2.30387544631958
Validation loss: 2.0969086872634066

Epoch: 6| Step: 9
Training loss: 2.9446682929992676
Validation loss: 2.087599583851394

Epoch: 6| Step: 10
Training loss: 2.464700698852539
Validation loss: 2.0866595852759575

Epoch: 6| Step: 11
Training loss: 1.7704087495803833
Validation loss: 2.0825684865315757

Epoch: 6| Step: 12
Training loss: 2.2027158737182617
Validation loss: 2.0715963327756493

Epoch: 6| Step: 13
Training loss: 3.4117989540100098
Validation loss: 2.0685539578878753

Epoch: 111| Step: 0
Training loss: 2.4399752616882324
Validation loss: 2.067336651586717

Epoch: 6| Step: 1
Training loss: 2.767569065093994
Validation loss: 2.063066577398649

Epoch: 6| Step: 2
Training loss: 2.749671697616577
Validation loss: 2.068108707345942

Epoch: 6| Step: 3
Training loss: 2.3775711059570312
Validation loss: 2.080103019232391

Epoch: 6| Step: 4
Training loss: 2.448535203933716
Validation loss: 2.067480989681777

Epoch: 6| Step: 5
Training loss: 2.506434440612793
Validation loss: 2.062871135691161

Epoch: 6| Step: 6
Training loss: 2.2433505058288574
Validation loss: 2.0517542926214074

Epoch: 6| Step: 7
Training loss: 2.332170009613037
Validation loss: 2.048586332669822

Epoch: 6| Step: 8
Training loss: 1.95920991897583
Validation loss: 2.0543920327258367

Epoch: 6| Step: 9
Training loss: 1.8951630592346191
Validation loss: 2.0471419749721402

Epoch: 6| Step: 10
Training loss: 2.0184364318847656
Validation loss: 2.0502788917992705

Epoch: 6| Step: 11
Training loss: 2.124739646911621
Validation loss: 2.0461896927125993

Epoch: 6| Step: 12
Training loss: 2.400425910949707
Validation loss: 2.052800161864168

Epoch: 6| Step: 13
Training loss: 2.8661019802093506
Validation loss: 2.050467114294729

Epoch: 112| Step: 0
Training loss: 1.594200611114502
Validation loss: 2.0473110573266142

Epoch: 6| Step: 1
Training loss: 1.9975497722625732
Validation loss: 2.0513952585958664

Epoch: 6| Step: 2
Training loss: 2.5800533294677734
Validation loss: 2.0410030682881675

Epoch: 6| Step: 3
Training loss: 1.734344244003296
Validation loss: 2.0390665890068136

Epoch: 6| Step: 4
Training loss: 2.6965997219085693
Validation loss: 2.0458316726069294

Epoch: 6| Step: 5
Training loss: 2.508183479309082
Validation loss: 2.0548764300602738

Epoch: 6| Step: 6
Training loss: 2.651463031768799
Validation loss: 2.05998977281714

Epoch: 6| Step: 7
Training loss: 2.0453171730041504
Validation loss: 2.067179182524322

Epoch: 6| Step: 8
Training loss: 2.5138533115386963
Validation loss: 2.0621876383340485

Epoch: 6| Step: 9
Training loss: 3.025358200073242
Validation loss: 2.061350066174743

Epoch: 6| Step: 10
Training loss: 2.362961530685425
Validation loss: 2.063017378571213

Epoch: 6| Step: 11
Training loss: 1.683434009552002
Validation loss: 2.0473091345961376

Epoch: 6| Step: 12
Training loss: 2.6814839839935303
Validation loss: 2.0406348859110186

Epoch: 6| Step: 13
Training loss: 3.123732566833496
Validation loss: 2.0399448974158174

Epoch: 113| Step: 0
Training loss: 2.1157279014587402
Validation loss: 2.04192046324412

Epoch: 6| Step: 1
Training loss: 2.7783055305480957
Validation loss: 2.072417325870965

Epoch: 6| Step: 2
Training loss: 2.1264195442199707
Validation loss: 2.090280158545381

Epoch: 6| Step: 3
Training loss: 2.44846773147583
Validation loss: 2.106065511703491

Epoch: 6| Step: 4
Training loss: 2.831434726715088
Validation loss: 2.1152129942371

Epoch: 6| Step: 5
Training loss: 1.915793776512146
Validation loss: 2.131537009310979

Epoch: 6| Step: 6
Training loss: 2.2676796913146973
Validation loss: 2.117353275258054

Epoch: 6| Step: 7
Training loss: 2.4640092849731445
Validation loss: 2.1084056233847015

Epoch: 6| Step: 8
Training loss: 2.7426576614379883
Validation loss: 2.094379143048358

Epoch: 6| Step: 9
Training loss: 1.796439290046692
Validation loss: 2.0724229710076445

Epoch: 6| Step: 10
Training loss: 2.260735511779785
Validation loss: 2.0647211331193165

Epoch: 6| Step: 11
Training loss: 2.2410078048706055
Validation loss: 2.054679209186185

Epoch: 6| Step: 12
Training loss: 2.326998233795166
Validation loss: 2.070120334625244

Epoch: 6| Step: 13
Training loss: 2.543377161026001
Validation loss: 2.1089603311272076

Epoch: 114| Step: 0
Training loss: 2.61130428314209
Validation loss: 2.139648809227892

Epoch: 6| Step: 1
Training loss: 2.4208383560180664
Validation loss: 2.163318277687155

Epoch: 6| Step: 2
Training loss: 2.383291721343994
Validation loss: 2.154247223689992

Epoch: 6| Step: 3
Training loss: 1.9799344539642334
Validation loss: 2.1054974063750236

Epoch: 6| Step: 4
Training loss: 2.427889823913574
Validation loss: 2.1049060590805544

Epoch: 6| Step: 5
Training loss: 2.2242558002471924
Validation loss: 2.1003108139960998

Epoch: 6| Step: 6
Training loss: 2.286669969558716
Validation loss: 2.10157722426999

Epoch: 6| Step: 7
Training loss: 2.9587249755859375
Validation loss: 2.1011919616371073

Epoch: 6| Step: 8
Training loss: 2.8385753631591797
Validation loss: 2.099529718839994

Epoch: 6| Step: 9
Training loss: 2.2245967388153076
Validation loss: 2.0988806498947965

Epoch: 6| Step: 10
Training loss: 2.6669158935546875
Validation loss: 2.0852475384230256

Epoch: 6| Step: 11
Training loss: 1.9954520463943481
Validation loss: 2.077881516948823

Epoch: 6| Step: 12
Training loss: 1.6776200532913208
Validation loss: 2.0764050855431506

Epoch: 6| Step: 13
Training loss: 3.128171682357788
Validation loss: 2.0699012382056123

Epoch: 115| Step: 0
Training loss: 1.7530758380889893
Validation loss: 2.0652108089898222

Epoch: 6| Step: 1
Training loss: 2.8241336345672607
Validation loss: 2.0585538956426803

Epoch: 6| Step: 2
Training loss: 2.496720314025879
Validation loss: 2.054975458370742

Epoch: 6| Step: 3
Training loss: 2.1845672130584717
Validation loss: 2.0681900490996656

Epoch: 6| Step: 4
Training loss: 1.859367847442627
Validation loss: 2.0706313040948685

Epoch: 6| Step: 5
Training loss: 2.5952043533325195
Validation loss: 2.0476890763928814

Epoch: 6| Step: 6
Training loss: 3.332670211791992
Validation loss: 2.0574155956186275

Epoch: 6| Step: 7
Training loss: 2.2735705375671387
Validation loss: 2.036094537345312

Epoch: 6| Step: 8
Training loss: 2.1864686012268066
Validation loss: 2.0349813430540022

Epoch: 6| Step: 9
Training loss: 2.5798559188842773
Validation loss: 2.0386974721826534

Epoch: 6| Step: 10
Training loss: 2.3748931884765625
Validation loss: 2.030396383295777

Epoch: 6| Step: 11
Training loss: 1.9791526794433594
Validation loss: 2.033264757484518

Epoch: 6| Step: 12
Training loss: 2.046481132507324
Validation loss: 2.028689169114636

Epoch: 6| Step: 13
Training loss: 2.2541520595550537
Validation loss: 2.0218268363706526

Epoch: 116| Step: 0
Training loss: 2.700592041015625
Validation loss: 2.028006135776479

Epoch: 6| Step: 1
Training loss: 1.8453373908996582
Validation loss: 2.0195484289558987

Epoch: 6| Step: 2
Training loss: 2.2986857891082764
Validation loss: 2.0217478685481574

Epoch: 6| Step: 3
Training loss: 1.0925192832946777
Validation loss: 2.020800834060997

Epoch: 6| Step: 4
Training loss: 2.1194214820861816
Validation loss: 2.0271046007833173

Epoch: 6| Step: 5
Training loss: 2.6890599727630615
Validation loss: 2.0242256784951813

Epoch: 6| Step: 6
Training loss: 2.586092472076416
Validation loss: 2.027784207815765

Epoch: 6| Step: 7
Training loss: 2.1924991607666016
Validation loss: 2.0319599054192983

Epoch: 6| Step: 8
Training loss: 1.5985724925994873
Validation loss: 2.0337478576167936

Epoch: 6| Step: 9
Training loss: 2.504331111907959
Validation loss: 2.0323801079104022

Epoch: 6| Step: 10
Training loss: 2.778972864151001
Validation loss: 2.023475147062732

Epoch: 6| Step: 11
Training loss: 2.718991756439209
Validation loss: 2.0253591768203245

Epoch: 6| Step: 12
Training loss: 2.499951124191284
Validation loss: 2.0188115822371615

Epoch: 6| Step: 13
Training loss: 2.6344425678253174
Validation loss: 2.0203925409624652

Epoch: 117| Step: 0
Training loss: 2.0105931758880615
Validation loss: 2.019238564275926

Epoch: 6| Step: 1
Training loss: 2.7733612060546875
Validation loss: 2.021083290858935

Epoch: 6| Step: 2
Training loss: 2.59767484664917
Validation loss: 2.0230605961174093

Epoch: 6| Step: 3
Training loss: 1.6776816844940186
Validation loss: 2.0359152452920073

Epoch: 6| Step: 4
Training loss: 2.2378969192504883
Validation loss: 2.0375448593529324

Epoch: 6| Step: 5
Training loss: 1.7155427932739258
Validation loss: 2.026465044226698

Epoch: 6| Step: 6
Training loss: 2.1105151176452637
Validation loss: 2.028423333680758

Epoch: 6| Step: 7
Training loss: 2.589263677597046
Validation loss: 2.0103440976911977

Epoch: 6| Step: 8
Training loss: 2.295893669128418
Validation loss: 2.0028562571412776

Epoch: 6| Step: 9
Training loss: 2.956705093383789
Validation loss: 2.0024275190086773

Epoch: 6| Step: 10
Training loss: 2.4605491161346436
Validation loss: 2.0097210343166063

Epoch: 6| Step: 11
Training loss: 2.472811698913574
Validation loss: 2.013081160924768

Epoch: 6| Step: 12
Training loss: 1.9155360460281372
Validation loss: 2.0027183896751812

Epoch: 6| Step: 13
Training loss: 2.4170801639556885
Validation loss: 2.011048730983529

Epoch: 118| Step: 0
Training loss: 2.007277250289917
Validation loss: 2.008696790664427

Epoch: 6| Step: 1
Training loss: 2.3894171714782715
Validation loss: 2.0102835650085122

Epoch: 6| Step: 2
Training loss: 1.4980467557907104
Validation loss: 1.9972565943194973

Epoch: 6| Step: 3
Training loss: 2.027346134185791
Validation loss: 2.0096988677978516

Epoch: 6| Step: 4
Training loss: 2.1262331008911133
Validation loss: 2.0119196163710726

Epoch: 6| Step: 5
Training loss: 1.747507095336914
Validation loss: 2.013502210699102

Epoch: 6| Step: 6
Training loss: 2.4305014610290527
Validation loss: 2.010260517879199

Epoch: 6| Step: 7
Training loss: 1.9620811939239502
Validation loss: 2.0114120027070403

Epoch: 6| Step: 8
Training loss: 2.826197624206543
Validation loss: 2.0236147270407727

Epoch: 6| Step: 9
Training loss: 2.512006998062134
Validation loss: 2.0349761311725905

Epoch: 6| Step: 10
Training loss: 1.9925129413604736
Validation loss: 2.034177416114397

Epoch: 6| Step: 11
Training loss: 2.8326611518859863
Validation loss: 2.04975067287363

Epoch: 6| Step: 12
Training loss: 2.633089065551758
Validation loss: 2.0413662746388423

Epoch: 6| Step: 13
Training loss: 3.415504217147827
Validation loss: 2.035053571065267

Epoch: 119| Step: 0
Training loss: 1.9540523290634155
Validation loss: 2.0266165784610215

Epoch: 6| Step: 1
Training loss: 2.0811691284179688
Validation loss: 2.014031130780456

Epoch: 6| Step: 2
Training loss: 2.9297773838043213
Validation loss: 2.011300722757975

Epoch: 6| Step: 3
Training loss: 1.891035556793213
Validation loss: 2.008439325517224

Epoch: 6| Step: 4
Training loss: 2.0527796745300293
Validation loss: 2.0035560374618857

Epoch: 6| Step: 5
Training loss: 2.1737983226776123
Validation loss: 1.9951953964848672

Epoch: 6| Step: 6
Training loss: 2.5570011138916016
Validation loss: 1.99754076362938

Epoch: 6| Step: 7
Training loss: 2.376162052154541
Validation loss: 2.0008274137332873

Epoch: 6| Step: 8
Training loss: 2.4347963333129883
Validation loss: 2.0057063205267793

Epoch: 6| Step: 9
Training loss: 2.3119254112243652
Validation loss: 1.989162979587432

Epoch: 6| Step: 10
Training loss: 2.2207045555114746
Validation loss: 1.9947806199391682

Epoch: 6| Step: 11
Training loss: 2.4964897632598877
Validation loss: 2.0079592632991012

Epoch: 6| Step: 12
Training loss: 2.1928834915161133
Validation loss: 2.012620766957601

Epoch: 6| Step: 13
Training loss: 2.4010188579559326
Validation loss: 2.023290361127546

Epoch: 120| Step: 0
Training loss: 2.6708998680114746
Validation loss: 2.0266223389615297

Epoch: 6| Step: 1
Training loss: 3.079773426055908
Validation loss: 2.024793956869392

Epoch: 6| Step: 2
Training loss: 3.14314603805542
Validation loss: 2.0229663925786174

Epoch: 6| Step: 3
Training loss: 2.7059288024902344
Validation loss: 2.0314765296956545

Epoch: 6| Step: 4
Training loss: 2.2801096439361572
Validation loss: 2.033294347024733

Epoch: 6| Step: 5
Training loss: 2.6726620197296143
Validation loss: 2.0188786163124988

Epoch: 6| Step: 6
Training loss: 2.4377567768096924
Validation loss: 2.00063588542323

Epoch: 6| Step: 7
Training loss: 1.5832710266113281
Validation loss: 1.987050594822053

Epoch: 6| Step: 8
Training loss: 1.426445484161377
Validation loss: 1.9857182784747052

Epoch: 6| Step: 9
Training loss: 1.9572840929031372
Validation loss: 1.9829450281717445

Epoch: 6| Step: 10
Training loss: 2.000112533569336
Validation loss: 1.98692960123862

Epoch: 6| Step: 11
Training loss: 2.2274532318115234
Validation loss: 1.9924154140615975

Epoch: 6| Step: 12
Training loss: 1.5657341480255127
Validation loss: 1.9969045744147351

Epoch: 6| Step: 13
Training loss: 2.4225823879241943
Validation loss: 1.993110918229626

Epoch: 121| Step: 0
Training loss: 2.579373359680176
Validation loss: 1.9909631667598602

Epoch: 6| Step: 1
Training loss: 2.125117778778076
Validation loss: 1.9934434736928632

Epoch: 6| Step: 2
Training loss: 2.2058565616607666
Validation loss: 1.9982637936069119

Epoch: 6| Step: 3
Training loss: 1.817063570022583
Validation loss: 1.9948667557008806

Epoch: 6| Step: 4
Training loss: 2.1766631603240967
Validation loss: 2.002804660028027

Epoch: 6| Step: 5
Training loss: 1.715254306793213
Validation loss: 2.012196485714246

Epoch: 6| Step: 6
Training loss: 1.7045812606811523
Validation loss: 2.000129329260959

Epoch: 6| Step: 7
Training loss: 2.759524345397949
Validation loss: 1.9846205506273495

Epoch: 6| Step: 8
Training loss: 2.0861449241638184
Validation loss: 1.9826831689444921

Epoch: 6| Step: 9
Training loss: 2.2880349159240723
Validation loss: 1.9815493117096603

Epoch: 6| Step: 10
Training loss: 2.326310634613037
Validation loss: 1.9791097871718868

Epoch: 6| Step: 11
Training loss: 2.7654385566711426
Validation loss: 1.9828190008799236

Epoch: 6| Step: 12
Training loss: 2.5943970680236816
Validation loss: 1.9986938186871108

Epoch: 6| Step: 13
Training loss: 2.857314348220825
Validation loss: 2.0190036578844954

Epoch: 122| Step: 0
Training loss: 2.0739054679870605
Validation loss: 2.025483556973037

Epoch: 6| Step: 1
Training loss: 2.5765058994293213
Validation loss: 2.026006160243865

Epoch: 6| Step: 2
Training loss: 2.4936938285827637
Validation loss: 2.02699347337087

Epoch: 6| Step: 3
Training loss: 2.8136274814605713
Validation loss: 2.0255464789687947

Epoch: 6| Step: 4
Training loss: 1.9016153812408447
Validation loss: 2.022424787603399

Epoch: 6| Step: 5
Training loss: 2.5309853553771973
Validation loss: 2.012425507268598

Epoch: 6| Step: 6
Training loss: 2.582615375518799
Validation loss: 1.9978697325593682

Epoch: 6| Step: 7
Training loss: 2.386425018310547
Validation loss: 2.00101730644062

Epoch: 6| Step: 8
Training loss: 1.5106534957885742
Validation loss: 2.0016585934546685

Epoch: 6| Step: 9
Training loss: 2.448744058609009
Validation loss: 2.0130271885984685

Epoch: 6| Step: 10
Training loss: 2.0375218391418457
Validation loss: 2.032360202522688

Epoch: 6| Step: 11
Training loss: 2.7610201835632324
Validation loss: 2.039841772407614

Epoch: 6| Step: 12
Training loss: 1.9269131422042847
Validation loss: 2.0367035737601658

Epoch: 6| Step: 13
Training loss: 2.166468620300293
Validation loss: 2.032924429062874

Epoch: 123| Step: 0
Training loss: 2.4350550174713135
Validation loss: 2.0122771391304592

Epoch: 6| Step: 1
Training loss: 2.3481643199920654
Validation loss: 2.0057913513593775

Epoch: 6| Step: 2
Training loss: 2.2310447692871094
Validation loss: 2.006102151768182

Epoch: 6| Step: 3
Training loss: 2.233712673187256
Validation loss: 1.9979910042978102

Epoch: 6| Step: 4
Training loss: 2.219172954559326
Validation loss: 2.005858895599201

Epoch: 6| Step: 5
Training loss: 1.9800611734390259
Validation loss: 2.0045691920864965

Epoch: 6| Step: 6
Training loss: 2.703774929046631
Validation loss: 1.9994262828621814

Epoch: 6| Step: 7
Training loss: 2.2084531784057617
Validation loss: 2.0001895299521824

Epoch: 6| Step: 8
Training loss: 2.512637138366699
Validation loss: 2.0023128678721767

Epoch: 6| Step: 9
Training loss: 2.1281518936157227
Validation loss: 2.003254170058876

Epoch: 6| Step: 10
Training loss: 2.713705062866211
Validation loss: 2.007400958768783

Epoch: 6| Step: 11
Training loss: 2.197373390197754
Validation loss: 2.0098977063291814

Epoch: 6| Step: 12
Training loss: 1.7579474449157715
Validation loss: 2.022896794862645

Epoch: 6| Step: 13
Training loss: 1.6508853435516357
Validation loss: 2.019054928133565

Epoch: 124| Step: 0
Training loss: 2.3655543327331543
Validation loss: 2.0012102511621292

Epoch: 6| Step: 1
Training loss: 1.827770471572876
Validation loss: 1.9858651930286038

Epoch: 6| Step: 2
Training loss: 1.9282169342041016
Validation loss: 1.9825546818394815

Epoch: 6| Step: 3
Training loss: 2.8684587478637695
Validation loss: 1.9745165545453307

Epoch: 6| Step: 4
Training loss: 1.9144209623336792
Validation loss: 1.969909324440905

Epoch: 6| Step: 5
Training loss: 2.2993617057800293
Validation loss: 1.9589120111157816

Epoch: 6| Step: 6
Training loss: 1.9753954410552979
Validation loss: 1.9700356362968363

Epoch: 6| Step: 7
Training loss: 2.709432601928711
Validation loss: 1.9607384512501378

Epoch: 6| Step: 8
Training loss: 2.439150333404541
Validation loss: 1.9603264152362783

Epoch: 6| Step: 9
Training loss: 2.4525623321533203
Validation loss: 1.9577409118734381

Epoch: 6| Step: 10
Training loss: 2.0152878761291504
Validation loss: 1.9519118955058437

Epoch: 6| Step: 11
Training loss: 2.4273033142089844
Validation loss: 1.9569419891603532

Epoch: 6| Step: 12
Training loss: 2.2412681579589844
Validation loss: 1.9525234135248328

Epoch: 6| Step: 13
Training loss: 1.8490428924560547
Validation loss: 1.9639033373966013

Epoch: 125| Step: 0
Training loss: 2.32501482963562
Validation loss: 1.9498256714113298

Epoch: 6| Step: 1
Training loss: 2.222189426422119
Validation loss: 1.9562068088080293

Epoch: 6| Step: 2
Training loss: 2.1561970710754395
Validation loss: 1.9595105289131083

Epoch: 6| Step: 3
Training loss: 2.1218714714050293
Validation loss: 1.9593684750218545

Epoch: 6| Step: 4
Training loss: 2.707322120666504
Validation loss: 1.9665955446099723

Epoch: 6| Step: 5
Training loss: 2.0712554454803467
Validation loss: 1.9599951005751086

Epoch: 6| Step: 6
Training loss: 1.5353467464447021
Validation loss: 1.967977121312131

Epoch: 6| Step: 7
Training loss: 2.701969623565674
Validation loss: 1.9732708930969238

Epoch: 6| Step: 8
Training loss: 2.5126399993896484
Validation loss: 1.9769512427750455

Epoch: 6| Step: 9
Training loss: 2.802330732345581
Validation loss: 1.9794775491119714

Epoch: 6| Step: 10
Training loss: 2.446154832839966
Validation loss: 1.9894947954403457

Epoch: 6| Step: 11
Training loss: 2.197252035140991
Validation loss: 2.0037632526889926

Epoch: 6| Step: 12
Training loss: 2.0287506580352783
Validation loss: 1.9964707102826846

Epoch: 6| Step: 13
Training loss: 1.4031614065170288
Validation loss: 2.000376198881416

Testing loss: 2.2924569659762914

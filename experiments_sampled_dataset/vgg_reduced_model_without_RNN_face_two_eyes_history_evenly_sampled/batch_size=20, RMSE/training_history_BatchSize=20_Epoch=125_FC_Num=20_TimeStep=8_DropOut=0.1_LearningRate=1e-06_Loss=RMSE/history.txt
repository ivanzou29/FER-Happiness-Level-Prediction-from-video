Epoch: 1| Step: 0
Training loss: 4.974781909552159
Validation loss: 4.639452621583061

Epoch: 5| Step: 1
Training loss: 4.239637419834408
Validation loss: 4.631402544769733

Epoch: 5| Step: 2
Training loss: 5.104936170636653
Validation loss: 4.629754728443289

Epoch: 5| Step: 3
Training loss: 4.268126260922327
Validation loss: 4.622814117552541

Epoch: 5| Step: 4
Training loss: 4.193831709905721
Validation loss: 4.61776775448465

Epoch: 5| Step: 5
Training loss: 4.410349136337682
Validation loss: 4.610375612112441

Epoch: 5| Step: 6
Training loss: 4.0621274337125035
Validation loss: 4.604173043979335

Epoch: 5| Step: 7
Training loss: 5.535805932617977
Validation loss: 4.598761103323433

Epoch: 5| Step: 8
Training loss: 4.969306770655621
Validation loss: 4.591779957095363

Epoch: 5| Step: 9
Training loss: 5.229344378892696
Validation loss: 4.58810788519757

Epoch: 5| Step: 10
Training loss: 4.572882348745031
Validation loss: 4.582348626968241

Epoch: 2| Step: 0
Training loss: 4.560761002735428
Validation loss: 4.575076866299882

Epoch: 5| Step: 1
Training loss: 4.075938840966917
Validation loss: 4.56933515090332

Epoch: 5| Step: 2
Training loss: 4.140465153901651
Validation loss: 4.564365193942702

Epoch: 5| Step: 3
Training loss: 4.699065119514362
Validation loss: 4.560064846193734

Epoch: 5| Step: 4
Training loss: 5.36434145879054
Validation loss: 4.556418613419972

Epoch: 5| Step: 5
Training loss: 4.13410707327894
Validation loss: 4.550193584533564

Epoch: 5| Step: 6
Training loss: 4.910561386304229
Validation loss: 4.541812425125724

Epoch: 5| Step: 7
Training loss: 5.412655954558896
Validation loss: 4.537219427307941

Epoch: 5| Step: 8
Training loss: 4.35423407738201
Validation loss: 4.530824474231217

Epoch: 5| Step: 9
Training loss: 4.511629759336103
Validation loss: 4.525844493449071

Epoch: 5| Step: 10
Training loss: 4.804714097360326
Validation loss: 4.521464201417669

Epoch: 3| Step: 0
Training loss: 4.944313659557504
Validation loss: 4.516371052676395

Epoch: 5| Step: 1
Training loss: 5.024369931930831
Validation loss: 4.507502672893079

Epoch: 5| Step: 2
Training loss: 4.08774574777614
Validation loss: 4.503955847870835

Epoch: 5| Step: 3
Training loss: 4.631128117531539
Validation loss: 4.498788455178922

Epoch: 5| Step: 4
Training loss: 4.6030958952958025
Validation loss: 4.4931707095733975

Epoch: 5| Step: 5
Training loss: 4.480682660611276
Validation loss: 4.487990402119291

Epoch: 5| Step: 6
Training loss: 4.7320002778569945
Validation loss: 4.481331928291604

Epoch: 5| Step: 7
Training loss: 4.804662093428762
Validation loss: 4.477124518688975

Epoch: 5| Step: 8
Training loss: 4.095080443396762
Validation loss: 4.470496123014934

Epoch: 5| Step: 9
Training loss: 4.566245227125145
Validation loss: 4.4657071400902755

Epoch: 5| Step: 10
Training loss: 4.461715817404591
Validation loss: 4.458891622698446

Epoch: 4| Step: 0
Training loss: 4.5266515089033135
Validation loss: 4.455633827246655

Epoch: 5| Step: 1
Training loss: 4.229867936546023
Validation loss: 4.449851971982849

Epoch: 5| Step: 2
Training loss: 4.051730862203995
Validation loss: 4.442521873246367

Epoch: 5| Step: 3
Training loss: 4.979506647268916
Validation loss: 4.438354830432649

Epoch: 5| Step: 4
Training loss: 4.3140065975359825
Validation loss: 4.431836220575267

Epoch: 5| Step: 5
Training loss: 4.0690663016995625
Validation loss: 4.426251415092104

Epoch: 5| Step: 6
Training loss: 5.660482435390749
Validation loss: 4.42038532220105

Epoch: 5| Step: 7
Training loss: 4.5347423906771915
Validation loss: 4.41510688085172

Epoch: 5| Step: 8
Training loss: 5.3210997146127195
Validation loss: 4.411169931998646

Epoch: 5| Step: 9
Training loss: 4.541911873920385
Validation loss: 4.403952625768055

Epoch: 5| Step: 10
Training loss: 2.8659532265423517
Validation loss: 4.399202597657963

Epoch: 5| Step: 0
Training loss: 4.2431812562583175
Validation loss: 4.391267871529061

Epoch: 5| Step: 1
Training loss: 4.68478294464144
Validation loss: 4.385093114465305

Epoch: 5| Step: 2
Training loss: 4.892277739967892
Validation loss: 4.376927372272053

Epoch: 5| Step: 3
Training loss: 3.603715827983647
Validation loss: 4.375841913504837

Epoch: 5| Step: 4
Training loss: 4.61010943235315
Validation loss: 4.367611272662428

Epoch: 5| Step: 5
Training loss: 4.752143376277822
Validation loss: 4.359959266555486

Epoch: 5| Step: 6
Training loss: 3.588136352032058
Validation loss: 4.355253132433249

Epoch: 5| Step: 7
Training loss: 4.619031405619433
Validation loss: 4.350403944436707

Epoch: 5| Step: 8
Training loss: 4.220713066855482
Validation loss: 4.344463080635555

Epoch: 5| Step: 9
Training loss: 4.89888273463751
Validation loss: 4.3355922222637

Epoch: 5| Step: 10
Training loss: 4.907904862169641
Validation loss: 4.330313982284752

Epoch: 6| Step: 0
Training loss: 4.909905887797277
Validation loss: 4.32248192489093

Epoch: 5| Step: 1
Training loss: 5.023254486119952
Validation loss: 4.317454258519422

Epoch: 5| Step: 2
Training loss: 4.201050881204574
Validation loss: 4.3103321292422825

Epoch: 5| Step: 3
Training loss: 4.742392621953788
Validation loss: 4.303609198676652

Epoch: 5| Step: 4
Training loss: 4.604659846662596
Validation loss: 4.294774418966214

Epoch: 5| Step: 5
Training loss: 4.556263253568151
Validation loss: 4.290221686167239

Epoch: 5| Step: 6
Training loss: 4.025893800429165
Validation loss: 4.283689652447098

Epoch: 5| Step: 7
Training loss: 3.6917135443771625
Validation loss: 4.2768916086400335

Epoch: 5| Step: 8
Training loss: 4.2550743327837
Validation loss: 4.270457560017233

Epoch: 5| Step: 9
Training loss: 3.9673175779925205
Validation loss: 4.262891887043381

Epoch: 5| Step: 10
Training loss: 4.31847023616744
Validation loss: 4.253820794981138

Epoch: 7| Step: 0
Training loss: 3.4870441972947113
Validation loss: 4.249886874986152

Epoch: 5| Step: 1
Training loss: 5.164061576623878
Validation loss: 4.2426256580401045

Epoch: 5| Step: 2
Training loss: 4.452194812731435
Validation loss: 4.232879645867509

Epoch: 5| Step: 3
Training loss: 4.152958534641792
Validation loss: 4.226997857567984

Epoch: 5| Step: 4
Training loss: 4.646389344932487
Validation loss: 4.222968031224333

Epoch: 5| Step: 5
Training loss: 3.8886570376961416
Validation loss: 4.211263285846035

Epoch: 5| Step: 6
Training loss: 4.5800079535536025
Validation loss: 4.206154264728184

Epoch: 5| Step: 7
Training loss: 4.268617355944396
Validation loss: 4.197416643349166

Epoch: 5| Step: 8
Training loss: 4.150417509104106
Validation loss: 4.19124111682877

Epoch: 5| Step: 9
Training loss: 4.323734190824555
Validation loss: 4.184639823430915

Epoch: 5| Step: 10
Training loss: 4.323391195089943
Validation loss: 4.1746200472142485

Epoch: 8| Step: 0
Training loss: 4.789523875677881
Validation loss: 4.1660622783926815

Epoch: 5| Step: 1
Training loss: 4.1953320493899735
Validation loss: 4.1586860884089

Epoch: 5| Step: 2
Training loss: 3.396755038812951
Validation loss: 4.1512787534107

Epoch: 5| Step: 3
Training loss: 5.1639668373561864
Validation loss: 4.146811915763514

Epoch: 5| Step: 4
Training loss: 3.9061400131000066
Validation loss: 4.134840158267039

Epoch: 5| Step: 5
Training loss: 3.687802835734175
Validation loss: 4.127814097752981

Epoch: 5| Step: 6
Training loss: 4.63124775957267
Validation loss: 4.1189656548939855

Epoch: 5| Step: 7
Training loss: 4.274768684776944
Validation loss: 4.109453187839138

Epoch: 5| Step: 8
Training loss: 4.457658995971454
Validation loss: 4.102108628595676

Epoch: 5| Step: 9
Training loss: 3.9143916617923122
Validation loss: 4.094370725017255

Epoch: 5| Step: 10
Training loss: 4.010065527401967
Validation loss: 4.085821165253226

Epoch: 9| Step: 0
Training loss: 4.345330918049109
Validation loss: 4.076930930712318

Epoch: 5| Step: 1
Training loss: 3.4460820659888216
Validation loss: 4.067246554858681

Epoch: 5| Step: 2
Training loss: 3.738221681253775
Validation loss: 4.058161650620582

Epoch: 5| Step: 3
Training loss: 3.8852440571561258
Validation loss: 4.045419839100119

Epoch: 5| Step: 4
Training loss: 4.150118327464653
Validation loss: 4.038749859332917

Epoch: 5| Step: 5
Training loss: 4.098615245040681
Validation loss: 4.0339366103356165

Epoch: 5| Step: 6
Training loss: 4.52548608856204
Validation loss: 4.02172174999099

Epoch: 5| Step: 7
Training loss: 4.58165971659474
Validation loss: 4.013547970267197

Epoch: 5| Step: 8
Training loss: 4.065342304977605
Validation loss: 4.005592400212311

Epoch: 5| Step: 9
Training loss: 4.305472134564765
Validation loss: 3.9967272839513868

Epoch: 5| Step: 10
Training loss: 4.495281712887709
Validation loss: 3.984385786337491

Epoch: 10| Step: 0
Training loss: 3.7768094810245634
Validation loss: 3.976323940371572

Epoch: 5| Step: 1
Training loss: 4.589085293051797
Validation loss: 3.961502366623459

Epoch: 5| Step: 2
Training loss: 4.26823865025576
Validation loss: 3.9537537270272014

Epoch: 5| Step: 3
Training loss: 4.2576342064161095
Validation loss: 3.94416378797726

Epoch: 5| Step: 4
Training loss: 4.609111403347374
Validation loss: 3.9341556131382305

Epoch: 5| Step: 5
Training loss: 3.841912799709448
Validation loss: 3.9189287162743693

Epoch: 5| Step: 6
Training loss: 3.245735304743292
Validation loss: 3.9094208426981396

Epoch: 5| Step: 7
Training loss: 3.9329922016298298
Validation loss: 3.9011352222041884

Epoch: 5| Step: 8
Training loss: 3.6696282188592946
Validation loss: 3.8880979783082306

Epoch: 5| Step: 9
Training loss: 3.5923422004802887
Validation loss: 3.874697883517497

Epoch: 5| Step: 10
Training loss: 4.6515155250399305
Validation loss: 3.8622412302626055

Epoch: 11| Step: 0
Training loss: 3.9981929750494616
Validation loss: 3.8608629124238276

Epoch: 5| Step: 1
Training loss: 4.287885660782177
Validation loss: 3.8440015797053673

Epoch: 5| Step: 2
Training loss: 3.161516686560331
Validation loss: 3.8302084628425717

Epoch: 5| Step: 3
Training loss: 3.305009109322926
Validation loss: 3.8257341772368734

Epoch: 5| Step: 4
Training loss: 3.928421696373256
Validation loss: 3.8131937746982576

Epoch: 5| Step: 5
Training loss: 4.819207805255886
Validation loss: 3.803381748080228

Epoch: 5| Step: 6
Training loss: 3.8828992008837457
Validation loss: 3.788517934039544

Epoch: 5| Step: 7
Training loss: 3.8100654458259706
Validation loss: 3.7780287182936103

Epoch: 5| Step: 8
Training loss: 4.596829582316741
Validation loss: 3.764058189447684

Epoch: 5| Step: 9
Training loss: 3.7065290426415536
Validation loss: 3.7531776692221466

Epoch: 5| Step: 10
Training loss: 3.4305548125217746
Validation loss: 3.7427983127712965

Epoch: 12| Step: 0
Training loss: 3.2370981740314693
Validation loss: 3.7288211523351587

Epoch: 5| Step: 1
Training loss: 4.451098385815569
Validation loss: 3.712484374251161

Epoch: 5| Step: 2
Training loss: 2.841158010501564
Validation loss: 3.7041581170323603

Epoch: 5| Step: 3
Training loss: 3.8311489073218286
Validation loss: 3.687181784760739

Epoch: 5| Step: 4
Training loss: 3.6588253504296007
Validation loss: 3.6759358066557732

Epoch: 5| Step: 5
Training loss: 3.868574906979712
Validation loss: 3.664020702776894

Epoch: 5| Step: 6
Training loss: 3.931120290875491
Validation loss: 3.6541056831232543

Epoch: 5| Step: 7
Training loss: 4.009596994368758
Validation loss: 3.6409447846548155

Epoch: 5| Step: 8
Training loss: 3.842811283763759
Validation loss: 3.6281899690479302

Epoch: 5| Step: 9
Training loss: 4.400349013618169
Validation loss: 3.6129309533046983

Epoch: 5| Step: 10
Training loss: 3.580933780257103
Validation loss: 3.597857755217697

Epoch: 13| Step: 0
Training loss: 3.7796227602955357
Validation loss: 3.584478992622547

Epoch: 5| Step: 1
Training loss: 3.7077574264866615
Validation loss: 3.574473177105397

Epoch: 5| Step: 2
Training loss: 3.38912142978928
Validation loss: 3.5573855151850946

Epoch: 5| Step: 3
Training loss: 3.9227090989659
Validation loss: 3.5433528691151883

Epoch: 5| Step: 4
Training loss: 4.09473622796528
Validation loss: 3.5254354067367513

Epoch: 5| Step: 5
Training loss: 4.169080848662048
Validation loss: 3.518017233831515

Epoch: 5| Step: 6
Training loss: 3.257275287785028
Validation loss: 3.501386567034964

Epoch: 5| Step: 7
Training loss: 3.6876485762324953
Validation loss: 3.4877528662574004

Epoch: 5| Step: 8
Training loss: 3.9205872142593967
Validation loss: 3.4690304160300727

Epoch: 5| Step: 9
Training loss: 2.851281389985978
Validation loss: 3.4543446606832235

Epoch: 5| Step: 10
Training loss: 3.455697874700347
Validation loss: 3.4386839363181934

Epoch: 14| Step: 0
Training loss: 3.7597843952453536
Validation loss: 3.426557508346529

Epoch: 5| Step: 1
Training loss: 3.5150067273180032
Validation loss: 3.415707136568513

Epoch: 5| Step: 2
Training loss: 2.7895928741436267
Validation loss: 3.3955138238802705

Epoch: 5| Step: 3
Training loss: 3.315816694321596
Validation loss: 3.379901603523262

Epoch: 5| Step: 4
Training loss: 3.7611126594659225
Validation loss: 3.36796870749554

Epoch: 5| Step: 5
Training loss: 3.616419931131634
Validation loss: 3.3556823857284783

Epoch: 5| Step: 6
Training loss: 3.7078630099129555
Validation loss: 3.3369329203261384

Epoch: 5| Step: 7
Training loss: 4.189066807290423
Validation loss: 3.3195236405874726

Epoch: 5| Step: 8
Training loss: 3.3635405508074774
Validation loss: 3.306646515729114

Epoch: 5| Step: 9
Training loss: 3.81822275370993
Validation loss: 3.2920690531729266

Epoch: 5| Step: 10
Training loss: 2.6553813636005708
Validation loss: 3.280134509322649

Epoch: 15| Step: 0
Training loss: 3.1749288956497344
Validation loss: 3.2662497890961752

Epoch: 5| Step: 1
Training loss: 2.7062797879524454
Validation loss: 3.2442316179581234

Epoch: 5| Step: 2
Training loss: 2.8353334734668723
Validation loss: 3.232420758312025

Epoch: 5| Step: 3
Training loss: 3.5678431278947946
Validation loss: 3.2146887194052387

Epoch: 5| Step: 4
Training loss: 3.2228401640229314
Validation loss: 3.1993516018981167

Epoch: 5| Step: 5
Training loss: 3.497836670558156
Validation loss: 3.187474253303382

Epoch: 5| Step: 6
Training loss: 3.381829310517258
Validation loss: 3.1730431446932053

Epoch: 5| Step: 7
Training loss: 3.975936270757616
Validation loss: 3.15804699588904

Epoch: 5| Step: 8
Training loss: 3.560370092978342
Validation loss: 3.1410579939008136

Epoch: 5| Step: 9
Training loss: 3.9000749531903782
Validation loss: 3.123956726829012

Epoch: 5| Step: 10
Training loss: 3.0551243930596135
Validation loss: 3.1141605505232253

Epoch: 16| Step: 0
Training loss: 3.064084013183018
Validation loss: 3.0951805937939807

Epoch: 5| Step: 1
Training loss: 3.877042908673857
Validation loss: 3.0718934127076696

Epoch: 5| Step: 2
Training loss: 3.2832789369779305
Validation loss: 3.0686776170889987

Epoch: 5| Step: 3
Training loss: 3.3937707889826405
Validation loss: 3.046282006029442

Epoch: 5| Step: 4
Training loss: 3.689454675919473
Validation loss: 3.023603988387523

Epoch: 5| Step: 5
Training loss: 3.1887203199320506
Validation loss: 3.0096317197136786

Epoch: 5| Step: 6
Training loss: 2.8755184203745046
Validation loss: 2.9916333120392893

Epoch: 5| Step: 7
Training loss: 3.2122779146012204
Validation loss: 2.9788001778985436

Epoch: 5| Step: 8
Training loss: 2.7111142675280653
Validation loss: 2.968347680421761

Epoch: 5| Step: 9
Training loss: 3.1769870097249275
Validation loss: 2.956408688465267

Epoch: 5| Step: 10
Training loss: 3.0571034431463335
Validation loss: 2.930457630947173

Epoch: 17| Step: 0
Training loss: 3.2175477384332125
Validation loss: 2.9157398409604687

Epoch: 5| Step: 1
Training loss: 2.698894829090684
Validation loss: 2.9016855354961

Epoch: 5| Step: 2
Training loss: 2.9100169718139304
Validation loss: 2.887433126479851

Epoch: 5| Step: 3
Training loss: 2.789746027089276
Validation loss: 2.8785583870982268

Epoch: 5| Step: 4
Training loss: 3.117786321271068
Validation loss: 2.8684430639780003

Epoch: 5| Step: 5
Training loss: 2.9618778396338468
Validation loss: 2.851084611051417

Epoch: 5| Step: 6
Training loss: 3.2013048908930193
Validation loss: 2.8382277728439336

Epoch: 5| Step: 7
Training loss: 3.4920823595574917
Validation loss: 2.8319267953542275

Epoch: 5| Step: 8
Training loss: 3.705120595186873
Validation loss: 2.8038986833032373

Epoch: 5| Step: 9
Training loss: 2.8975871604731864
Validation loss: 2.7958071282093075

Epoch: 5| Step: 10
Training loss: 3.013657160072225
Validation loss: 2.7862093848290104

Epoch: 18| Step: 0
Training loss: 3.0136482994358964
Validation loss: 2.772430942550871

Epoch: 5| Step: 1
Training loss: 2.297246370979188
Validation loss: 2.7640503973776407

Epoch: 5| Step: 2
Training loss: 3.3039148748668166
Validation loss: 2.748528371098724

Epoch: 5| Step: 3
Training loss: 2.7668688719366856
Validation loss: 2.73682316985471

Epoch: 5| Step: 4
Training loss: 2.6108628103096136
Validation loss: 2.734454676438528

Epoch: 5| Step: 5
Training loss: 3.156256949539843
Validation loss: 2.720699708321901

Epoch: 5| Step: 6
Training loss: 3.0896074708667514
Validation loss: 2.7197191121027373

Epoch: 5| Step: 7
Training loss: 2.95974780013522
Validation loss: 2.6949669829160006

Epoch: 5| Step: 8
Training loss: 3.0632933542702707
Validation loss: 2.699354491707219

Epoch: 5| Step: 9
Training loss: 3.5129601445292233
Validation loss: 2.6800510955430927

Epoch: 5| Step: 10
Training loss: 3.3279946171827066
Validation loss: 2.661865542927931

Epoch: 19| Step: 0
Training loss: 3.0714197855725525
Validation loss: 2.650582871442814

Epoch: 5| Step: 1
Training loss: 2.8783639218446906
Validation loss: 2.653407280755607

Epoch: 5| Step: 2
Training loss: 3.035502174724767
Validation loss: 2.6456074487835153

Epoch: 5| Step: 3
Training loss: 3.1761304750138213
Validation loss: 2.6328096469783238

Epoch: 5| Step: 4
Training loss: 2.435355098610744
Validation loss: 2.6382082200723422

Epoch: 5| Step: 5
Training loss: 2.7133284257411705
Validation loss: 2.628936201319085

Epoch: 5| Step: 6
Training loss: 3.110316761726427
Validation loss: 2.626360692451091

Epoch: 5| Step: 7
Training loss: 2.825961724029526
Validation loss: 2.6202317129660426

Epoch: 5| Step: 8
Training loss: 3.0975132412508137
Validation loss: 2.6079099149173746

Epoch: 5| Step: 9
Training loss: 2.7252519403584934
Validation loss: 2.6040090487782446

Epoch: 5| Step: 10
Training loss: 3.4117271414501364
Validation loss: 2.597721587241521

Epoch: 20| Step: 0
Training loss: 2.9588414324487697
Validation loss: 2.6002739881057333

Epoch: 5| Step: 1
Training loss: 2.950747547154133
Validation loss: 2.586122599444204

Epoch: 5| Step: 2
Training loss: 2.4076738974888543
Validation loss: 2.5975194050718597

Epoch: 5| Step: 3
Training loss: 2.6672278250901362
Validation loss: 2.5943036630690877

Epoch: 5| Step: 4
Training loss: 2.940702275589494
Validation loss: 2.5878760805377197

Epoch: 5| Step: 5
Training loss: 3.1963459013335336
Validation loss: 2.580309964017146

Epoch: 5| Step: 6
Training loss: 2.7421563736325494
Validation loss: 2.566560183317751

Epoch: 5| Step: 7
Training loss: 3.0025536476951746
Validation loss: 2.577201034825552

Epoch: 5| Step: 8
Training loss: 3.170200150004549
Validation loss: 2.5647690234200358

Epoch: 5| Step: 9
Training loss: 3.226924552647837
Validation loss: 2.5633304879896177

Epoch: 5| Step: 10
Training loss: 2.8218203077793156
Validation loss: 2.572177009601842

Epoch: 21| Step: 0
Training loss: 3.631526990296231
Validation loss: 2.567460723588359

Epoch: 5| Step: 1
Training loss: 2.6220733131093166
Validation loss: 2.560647804813695

Epoch: 5| Step: 2
Training loss: 1.996295956039059
Validation loss: 2.5608917179580364

Epoch: 5| Step: 3
Training loss: 2.937267213079238
Validation loss: 2.546999345174006

Epoch: 5| Step: 4
Training loss: 2.388841350092267
Validation loss: 2.5441335759324617

Epoch: 5| Step: 5
Training loss: 3.4776944150131803
Validation loss: 2.5532539939066803

Epoch: 5| Step: 6
Training loss: 3.1591229346718417
Validation loss: 2.5422269673278928

Epoch: 5| Step: 7
Training loss: 2.794854995705767
Validation loss: 2.553385355756342

Epoch: 5| Step: 8
Training loss: 2.820250534264516
Validation loss: 2.5380338618745903

Epoch: 5| Step: 9
Training loss: 3.041338933419715
Validation loss: 2.5496565336338324

Epoch: 5| Step: 10
Training loss: 2.960863097370777
Validation loss: 2.5360952532143295

Epoch: 22| Step: 0
Training loss: 3.033059435484997
Validation loss: 2.5287703533952364

Epoch: 5| Step: 1
Training loss: 2.192211200479955
Validation loss: 2.5407657051898256

Epoch: 5| Step: 2
Training loss: 2.870630011443073
Validation loss: 2.535020856074721

Epoch: 5| Step: 3
Training loss: 3.3944191513603723
Validation loss: 2.5308233276492684

Epoch: 5| Step: 4
Training loss: 3.165618287933253
Validation loss: 2.5437374814008087

Epoch: 5| Step: 5
Training loss: 2.7224897774328407
Validation loss: 2.5430184926164094

Epoch: 5| Step: 6
Training loss: 3.393555249550313
Validation loss: 2.532254833771684

Epoch: 5| Step: 7
Training loss: 2.699401495967557
Validation loss: 2.5372087394021245

Epoch: 5| Step: 8
Training loss: 3.1184986387796934
Validation loss: 2.533288387888637

Epoch: 5| Step: 9
Training loss: 2.3902880861320024
Validation loss: 2.5400703715484316

Epoch: 5| Step: 10
Training loss: 2.7049624862467043
Validation loss: 2.520078696786382

Epoch: 23| Step: 0
Training loss: 3.1943848995980093
Validation loss: 2.525306278141114

Epoch: 5| Step: 1
Training loss: 3.217984599245535
Validation loss: 2.5278144085376395

Epoch: 5| Step: 2
Training loss: 3.2864013924882682
Validation loss: 2.5357578295142353

Epoch: 5| Step: 3
Training loss: 2.9411891740636804
Validation loss: 2.532158776980691

Epoch: 5| Step: 4
Training loss: 2.757439261188592
Validation loss: 2.542298700499685

Epoch: 5| Step: 5
Training loss: 2.3187695648930844
Validation loss: 2.5335729693988864

Epoch: 5| Step: 6
Training loss: 2.4036628389153027
Validation loss: 2.54587445239398

Epoch: 5| Step: 7
Training loss: 3.0547674222281227
Validation loss: 2.5388635287010035

Epoch: 5| Step: 8
Training loss: 3.067324516667866
Validation loss: 2.545236297516035

Epoch: 5| Step: 9
Training loss: 2.6262358526459777
Validation loss: 2.5303569166683006

Epoch: 5| Step: 10
Training loss: 2.97349041377027
Validation loss: 2.528542934994849

Epoch: 24| Step: 0
Training loss: 3.212172519007987
Validation loss: 2.530585876020909

Epoch: 5| Step: 1
Training loss: 2.8655848376539113
Validation loss: 2.5421200510443134

Epoch: 5| Step: 2
Training loss: 2.5464467817850385
Validation loss: 2.5301323533295186

Epoch: 5| Step: 3
Training loss: 3.266182883763245
Validation loss: 2.5428743853550917

Epoch: 5| Step: 4
Training loss: 3.3574350832800834
Validation loss: 2.536414042669814

Epoch: 5| Step: 5
Training loss: 2.7553511488208007
Validation loss: 2.530456837998574

Epoch: 5| Step: 6
Training loss: 2.3058577330575534
Validation loss: 2.5336063137614837

Epoch: 5| Step: 7
Training loss: 3.240840328644193
Validation loss: 2.530102506034166

Epoch: 5| Step: 8
Training loss: 2.3631256808946097
Validation loss: 2.547613912836777

Epoch: 5| Step: 9
Training loss: 2.491147676425076
Validation loss: 2.529099626982692

Epoch: 5| Step: 10
Training loss: 3.2816920391423854
Validation loss: 2.5484232265545774

Epoch: 25| Step: 0
Training loss: 3.0742662128046723
Validation loss: 2.5229583491562235

Epoch: 5| Step: 1
Training loss: 3.4526301931352736
Validation loss: 2.536601241944638

Epoch: 5| Step: 2
Training loss: 2.8111669136141186
Validation loss: 2.528609954656159

Epoch: 5| Step: 3
Training loss: 2.7869813992605366
Validation loss: 2.53993509120363

Epoch: 5| Step: 4
Training loss: 2.6229717956841605
Validation loss: 2.547527021156715

Epoch: 5| Step: 5
Training loss: 2.747525055122852
Validation loss: 2.5399537012197024

Epoch: 5| Step: 6
Training loss: 2.7150875283985294
Validation loss: 2.548985689553327

Epoch: 5| Step: 7
Training loss: 2.451147266279607
Validation loss: 2.5467075947413407

Epoch: 5| Step: 8
Training loss: 2.9018005437786423
Validation loss: 2.5427112850055744

Epoch: 5| Step: 9
Training loss: 3.640953212891085
Validation loss: 2.539647508344194

Epoch: 5| Step: 10
Training loss: 2.346769803428624
Validation loss: 2.5448920742325547

Epoch: 26| Step: 0
Training loss: 3.1727770194575213
Validation loss: 2.537212618897943

Epoch: 5| Step: 1
Training loss: 2.97880153597017
Validation loss: 2.534363889704633

Epoch: 5| Step: 2
Training loss: 2.535113647291063
Validation loss: 2.5311775531374807

Epoch: 5| Step: 3
Training loss: 2.8315210343202204
Validation loss: 2.5324417462655298

Epoch: 5| Step: 4
Training loss: 3.1000883889671886
Validation loss: 2.531879608554068

Epoch: 5| Step: 5
Training loss: 2.5443314096403635
Validation loss: 2.527259940020197

Epoch: 5| Step: 6
Training loss: 2.5295424176482757
Validation loss: 2.5241896813984286

Epoch: 5| Step: 7
Training loss: 3.035553070403696
Validation loss: 2.5321405318632224

Epoch: 5| Step: 8
Training loss: 2.8033059630567916
Validation loss: 2.531196883340714

Epoch: 5| Step: 9
Training loss: 3.0502747959104286
Validation loss: 2.5232500607158856

Epoch: 5| Step: 10
Training loss: 3.265375301178097
Validation loss: 2.532843995990192

Epoch: 27| Step: 0
Training loss: 3.014284774945422
Validation loss: 2.52732427418083

Epoch: 5| Step: 1
Training loss: 2.774490199643877
Validation loss: 2.5244126184032964

Epoch: 5| Step: 2
Training loss: 2.9510616784315484
Validation loss: 2.5256893790602764

Epoch: 5| Step: 3
Training loss: 3.213614357363684
Validation loss: 2.532210026823519

Epoch: 5| Step: 4
Training loss: 2.5028097575698136
Validation loss: 2.5306747407743346

Epoch: 5| Step: 5
Training loss: 2.4733158816658434
Validation loss: 2.5325811914878558

Epoch: 5| Step: 6
Training loss: 2.487167037072559
Validation loss: 2.526951096611524

Epoch: 5| Step: 7
Training loss: 3.2509159851389025
Validation loss: 2.5275415787935898

Epoch: 5| Step: 8
Training loss: 2.675593326401625
Validation loss: 2.5151575291469133

Epoch: 5| Step: 9
Training loss: 3.358700706764756
Validation loss: 2.5245937526633497

Epoch: 5| Step: 10
Training loss: 2.9935847672833873
Validation loss: 2.518058582656858

Epoch: 28| Step: 0
Training loss: 2.5260019410670322
Validation loss: 2.5256685100374816

Epoch: 5| Step: 1
Training loss: 3.4731494394581692
Validation loss: 2.5403040241163346

Epoch: 5| Step: 2
Training loss: 2.5484027609921998
Validation loss: 2.5220163795308688

Epoch: 5| Step: 3
Training loss: 3.1277054329051537
Validation loss: 2.52529189553634

Epoch: 5| Step: 4
Training loss: 2.691498111214667
Validation loss: 2.5198671819548215

Epoch: 5| Step: 5
Training loss: 3.433531811603607
Validation loss: 2.5184969433208138

Epoch: 5| Step: 6
Training loss: 2.7312565923475933
Validation loss: 2.524873220005691

Epoch: 5| Step: 7
Training loss: 2.633550432969835
Validation loss: 2.528644873565238

Epoch: 5| Step: 8
Training loss: 3.148798237999369
Validation loss: 2.529531498391248

Epoch: 5| Step: 9
Training loss: 2.7085462241986207
Validation loss: 2.52457098376856

Epoch: 5| Step: 10
Training loss: 2.3810001672763117
Validation loss: 2.5227049799320787

Epoch: 29| Step: 0
Training loss: 2.3479980626138888
Validation loss: 2.5308975517212957

Epoch: 5| Step: 1
Training loss: 2.709331201465524
Validation loss: 2.5218542240843034

Epoch: 5| Step: 2
Training loss: 3.236699397834677
Validation loss: 2.522018024742632

Epoch: 5| Step: 3
Training loss: 2.362274409427705
Validation loss: 2.521195923503528

Epoch: 5| Step: 4
Training loss: 3.242889548896597
Validation loss: 2.5313459140436514

Epoch: 5| Step: 5
Training loss: 2.7867254301503896
Validation loss: 2.5195632951610705

Epoch: 5| Step: 6
Training loss: 3.1078754432100095
Validation loss: 2.5226168613594697

Epoch: 5| Step: 7
Training loss: 2.6247291425200885
Validation loss: 2.524379566439552

Epoch: 5| Step: 8
Training loss: 3.2391563861966097
Validation loss: 2.5266133172299234

Epoch: 5| Step: 9
Training loss: 2.364885879408504
Validation loss: 2.527360081677355

Epoch: 5| Step: 10
Training loss: 3.5117738780955166
Validation loss: 2.533501456664161

Epoch: 30| Step: 0
Training loss: 2.5890948333286747
Validation loss: 2.5274196028276563

Epoch: 5| Step: 1
Training loss: 2.9451415318415726
Validation loss: 2.5232187471747167

Epoch: 5| Step: 2
Training loss: 2.8229917865813414
Validation loss: 2.5331508801769846

Epoch: 5| Step: 3
Training loss: 3.107608005552553
Validation loss: 2.5308073115765928

Epoch: 5| Step: 4
Training loss: 2.384556566677301
Validation loss: 2.5289732035088304

Epoch: 5| Step: 5
Training loss: 2.915405618442131
Validation loss: 2.5142593891093807

Epoch: 5| Step: 6
Training loss: 2.6652090340630816
Validation loss: 2.5222142450152494

Epoch: 5| Step: 7
Training loss: 3.6862352432881793
Validation loss: 2.5217094116034366

Epoch: 5| Step: 8
Training loss: 2.9158749913760076
Validation loss: 2.5144475404046

Epoch: 5| Step: 9
Training loss: 3.0657700188387653
Validation loss: 2.520614764722207

Epoch: 5| Step: 10
Training loss: 2.193135767312632
Validation loss: 2.520865847375273

Epoch: 31| Step: 0
Training loss: 2.8391800935332765
Validation loss: 2.5232701806294404

Epoch: 5| Step: 1
Training loss: 3.2658594950758597
Validation loss: 2.5178492358809024

Epoch: 5| Step: 2
Training loss: 3.0810149128276554
Validation loss: 2.526845915149496

Epoch: 5| Step: 3
Training loss: 2.540227065412413
Validation loss: 2.5155047377303927

Epoch: 5| Step: 4
Training loss: 2.6009125685261765
Validation loss: 2.527133609142541

Epoch: 5| Step: 5
Training loss: 2.862200565585284
Validation loss: 2.5135055435839435

Epoch: 5| Step: 6
Training loss: 2.844626658368456
Validation loss: 2.517354446268411

Epoch: 5| Step: 7
Training loss: 2.332015607771817
Validation loss: 2.5170863681371887

Epoch: 5| Step: 8
Training loss: 2.9559716123836126
Validation loss: 2.524342307564937

Epoch: 5| Step: 9
Training loss: 2.992614237581514
Validation loss: 2.513678937650713

Epoch: 5| Step: 10
Training loss: 3.2234698014298164
Validation loss: 2.5152271445047254

Epoch: 32| Step: 0
Training loss: 2.952031977782687
Validation loss: 2.528327131438225

Epoch: 5| Step: 1
Training loss: 2.999688768137201
Validation loss: 2.50684786168629

Epoch: 5| Step: 2
Training loss: 2.5348727827485225
Validation loss: 2.5036361287805216

Epoch: 5| Step: 3
Training loss: 3.18583875262689
Validation loss: 2.5181127339051734

Epoch: 5| Step: 4
Training loss: 2.559561649731382
Validation loss: 2.516825610667365

Epoch: 5| Step: 5
Training loss: 2.772655709987576
Validation loss: 2.5061051926375155

Epoch: 5| Step: 6
Training loss: 2.733152454097888
Validation loss: 2.515932666978229

Epoch: 5| Step: 7
Training loss: 2.591124895225286
Validation loss: 2.5133101856805062

Epoch: 5| Step: 8
Training loss: 2.578965853349303
Validation loss: 2.522768595949489

Epoch: 5| Step: 9
Training loss: 2.934889424725192
Validation loss: 2.5199215517270805

Epoch: 5| Step: 10
Training loss: 3.635123225541077
Validation loss: 2.5227934541423336

Epoch: 33| Step: 0
Training loss: 3.2629889110922723
Validation loss: 2.524170912501848

Epoch: 5| Step: 1
Training loss: 2.974838278085335
Validation loss: 2.5278966921091572

Epoch: 5| Step: 2
Training loss: 3.167149523680036
Validation loss: 2.5163956153982814

Epoch: 5| Step: 3
Training loss: 2.149407296211238
Validation loss: 2.5176034723053418

Epoch: 5| Step: 4
Training loss: 2.9394229519838437
Validation loss: 2.509496037431813

Epoch: 5| Step: 5
Training loss: 2.1196313245962073
Validation loss: 2.521829958448409

Epoch: 5| Step: 6
Training loss: 2.7116663060327753
Validation loss: 2.506692301822119

Epoch: 5| Step: 7
Training loss: 2.7950212526281124
Validation loss: 2.5214845641096946

Epoch: 5| Step: 8
Training loss: 2.791645382093072
Validation loss: 2.5209992477122283

Epoch: 5| Step: 9
Training loss: 3.446714777607615
Validation loss: 2.5234854891185026

Epoch: 5| Step: 10
Training loss: 2.9740586851239224
Validation loss: 2.509627931671433

Epoch: 34| Step: 0
Training loss: 2.625354016364779
Validation loss: 2.5249912887239407

Epoch: 5| Step: 1
Training loss: 3.2517397698873283
Validation loss: 2.525737351783157

Epoch: 5| Step: 2
Training loss: 3.0214034145484154
Validation loss: 2.5166565478107272

Epoch: 5| Step: 3
Training loss: 2.588318066114934
Validation loss: 2.5212942963177962

Epoch: 5| Step: 4
Training loss: 3.083799447234161
Validation loss: 2.5139185919697353

Epoch: 5| Step: 5
Training loss: 2.5843063695944157
Validation loss: 2.5182337797097447

Epoch: 5| Step: 6
Training loss: 2.5789547596425018
Validation loss: 2.5120614437345306

Epoch: 5| Step: 7
Training loss: 3.299512630655248
Validation loss: 2.52350909983215

Epoch: 5| Step: 8
Training loss: 2.825598330980458
Validation loss: 2.5082518101714153

Epoch: 5| Step: 9
Training loss: 2.7861006367533956
Validation loss: 2.515116038395606

Epoch: 5| Step: 10
Training loss: 2.764376859757007
Validation loss: 2.5252952177298953

Epoch: 35| Step: 0
Training loss: 2.3286335024972558
Validation loss: 2.519165598613622

Epoch: 5| Step: 1
Training loss: 3.1732869129899552
Validation loss: 2.518486676509801

Epoch: 5| Step: 2
Training loss: 2.5916921886238207
Validation loss: 2.520880633033911

Epoch: 5| Step: 3
Training loss: 3.282955344141308
Validation loss: 2.520206180853605

Epoch: 5| Step: 4
Training loss: 2.376008171125291
Validation loss: 2.51251522543856

Epoch: 5| Step: 5
Training loss: 2.862892363864341
Validation loss: 2.510874962761285

Epoch: 5| Step: 6
Training loss: 3.1522732600351477
Validation loss: 2.51478167118694

Epoch: 5| Step: 7
Training loss: 3.1095413853586673
Validation loss: 2.523394858946325

Epoch: 5| Step: 8
Training loss: 2.75860100299289
Validation loss: 2.5139219378614257

Epoch: 5| Step: 9
Training loss: 2.854509059296273
Validation loss: 2.5222594856689935

Epoch: 5| Step: 10
Training loss: 2.8543328192768067
Validation loss: 2.525343535990471

Epoch: 36| Step: 0
Training loss: 2.6812574053041858
Validation loss: 2.5124366160657305

Epoch: 5| Step: 1
Training loss: 2.5155725888307248
Validation loss: 2.5140771115708986

Epoch: 5| Step: 2
Training loss: 2.995573433826774
Validation loss: 2.513393397988359

Epoch: 5| Step: 3
Training loss: 2.841222625076927
Validation loss: 2.5220368828486364

Epoch: 5| Step: 4
Training loss: 2.4423849600775243
Validation loss: 2.512556194129431

Epoch: 5| Step: 5
Training loss: 3.1379440783641175
Validation loss: 2.5047121021906205

Epoch: 5| Step: 6
Training loss: 2.2111415162330905
Validation loss: 2.525125144229853

Epoch: 5| Step: 7
Training loss: 2.8944749093074464
Validation loss: 2.5164275762007793

Epoch: 5| Step: 8
Training loss: 3.2746349684561395
Validation loss: 2.515561050461301

Epoch: 5| Step: 9
Training loss: 3.2391462286705193
Validation loss: 2.5127798643415313

Epoch: 5| Step: 10
Training loss: 3.064215977320147
Validation loss: 2.514546631933485

Epoch: 37| Step: 0
Training loss: 3.301976889129113
Validation loss: 2.508181389733042

Epoch: 5| Step: 1
Training loss: 2.6531907359899645
Validation loss: 2.5102625313495315

Epoch: 5| Step: 2
Training loss: 3.053934848482668
Validation loss: 2.513344862230305

Epoch: 5| Step: 3
Training loss: 2.5231481805529836
Validation loss: 2.5168102266921353

Epoch: 5| Step: 4
Training loss: 3.117085008203058
Validation loss: 2.5220069173823214

Epoch: 5| Step: 5
Training loss: 2.574118720789836
Validation loss: 2.5074165406562

Epoch: 5| Step: 6
Training loss: 2.7322772017074355
Validation loss: 2.5134905340096334

Epoch: 5| Step: 7
Training loss: 2.094219553662039
Validation loss: 2.5249548836235736

Epoch: 5| Step: 8
Training loss: 2.9804268330317494
Validation loss: 2.5208642395478904

Epoch: 5| Step: 9
Training loss: 3.004949936537005
Validation loss: 2.518895578987788

Epoch: 5| Step: 10
Training loss: 3.194553574250606
Validation loss: 2.517387666846265

Epoch: 38| Step: 0
Training loss: 2.548696603706801
Validation loss: 2.513649923033026

Epoch: 5| Step: 1
Training loss: 3.7357177873984218
Validation loss: 2.508399894331894

Epoch: 5| Step: 2
Training loss: 2.9882351341927764
Validation loss: 2.5193647925693496

Epoch: 5| Step: 3
Training loss: 2.789393643080361
Validation loss: 2.515428709059598

Epoch: 5| Step: 4
Training loss: 2.4282081737044234
Validation loss: 2.516551013501116

Epoch: 5| Step: 5
Training loss: 2.489314321457944
Validation loss: 2.5132172637141257

Epoch: 5| Step: 6
Training loss: 2.215074530525158
Validation loss: 2.504461270228704

Epoch: 5| Step: 7
Training loss: 2.685502130309066
Validation loss: 2.5103260674610284

Epoch: 5| Step: 8
Training loss: 2.828755345358457
Validation loss: 2.5160284282553955

Epoch: 5| Step: 9
Training loss: 3.2464265985526364
Validation loss: 2.5017931506312103

Epoch: 5| Step: 10
Training loss: 3.058827748123125
Validation loss: 2.5122719585459783

Epoch: 39| Step: 0
Training loss: 2.6869548865769963
Validation loss: 2.51462506473048

Epoch: 5| Step: 1
Training loss: 2.7273623979160986
Validation loss: 2.516902401811833

Epoch: 5| Step: 2
Training loss: 2.9962075421981855
Validation loss: 2.511586853278453

Epoch: 5| Step: 3
Training loss: 3.402886506157898
Validation loss: 2.5180974158243727

Epoch: 5| Step: 4
Training loss: 2.8682170963027604
Validation loss: 2.5104661396352923

Epoch: 5| Step: 5
Training loss: 3.0039081548677316
Validation loss: 2.5142007203512824

Epoch: 5| Step: 6
Training loss: 2.626300761963728
Validation loss: 2.507774642297218

Epoch: 5| Step: 7
Training loss: 2.2682148830403506
Validation loss: 2.5137055858475446

Epoch: 5| Step: 8
Training loss: 2.2892372663526417
Validation loss: 2.506220184812483

Epoch: 5| Step: 9
Training loss: 3.2746390456837533
Validation loss: 2.5078846031692565

Epoch: 5| Step: 10
Training loss: 2.922275291399348
Validation loss: 2.5167473534053726

Epoch: 40| Step: 0
Training loss: 3.1334582493278513
Validation loss: 2.5130193993519008

Epoch: 5| Step: 1
Training loss: 2.653014872032263
Validation loss: 2.509327273550362

Epoch: 5| Step: 2
Training loss: 2.495643921438129
Validation loss: 2.523348660271804

Epoch: 5| Step: 3
Training loss: 2.6871775943854344
Validation loss: 2.5063885605650884

Epoch: 5| Step: 4
Training loss: 2.7427710105224077
Validation loss: 2.502350885234182

Epoch: 5| Step: 5
Training loss: 2.8050639715396746
Validation loss: 2.5066676347528203

Epoch: 5| Step: 6
Training loss: 2.9341570723496937
Validation loss: 2.497479100960536

Epoch: 5| Step: 7
Training loss: 2.888108926615662
Validation loss: 2.505270828335007

Epoch: 5| Step: 8
Training loss: 2.4140432412573722
Validation loss: 2.505010325895003

Epoch: 5| Step: 9
Training loss: 3.4402246947426827
Validation loss: 2.5095710812742764

Epoch: 5| Step: 10
Training loss: 2.9868320117322624
Validation loss: 2.5056405987207193

Epoch: 41| Step: 0
Training loss: 1.8019322091242214
Validation loss: 2.5033781761708296

Epoch: 5| Step: 1
Training loss: 2.5606928710285284
Validation loss: 2.513236681563635

Epoch: 5| Step: 2
Training loss: 3.2482920340242907
Validation loss: 2.5049328570158322

Epoch: 5| Step: 3
Training loss: 2.897047671014387
Validation loss: 2.5002202008801664

Epoch: 5| Step: 4
Training loss: 3.2984386912117403
Validation loss: 2.5070563907864822

Epoch: 5| Step: 5
Training loss: 3.2652180249265403
Validation loss: 2.4852876736509013

Epoch: 5| Step: 6
Training loss: 2.672940582699619
Validation loss: 2.5086894236950297

Epoch: 5| Step: 7
Training loss: 3.1986745294609706
Validation loss: 2.509189630751135

Epoch: 5| Step: 8
Training loss: 2.872104181519952
Validation loss: 2.5082571720112212

Epoch: 5| Step: 9
Training loss: 2.5296246997372944
Validation loss: 2.4922551753363997

Epoch: 5| Step: 10
Training loss: 2.5060299631688463
Validation loss: 2.5049553090732024

Epoch: 42| Step: 0
Training loss: 2.842153079855745
Validation loss: 2.499772849069273

Epoch: 5| Step: 1
Training loss: 2.372637527386926
Validation loss: 2.5006124136032493

Epoch: 5| Step: 2
Training loss: 2.4882814416331365
Validation loss: 2.4847539178208673

Epoch: 5| Step: 3
Training loss: 2.854958714388749
Validation loss: 2.513153782334591

Epoch: 5| Step: 4
Training loss: 3.3428771403812605
Validation loss: 2.506797441217104

Epoch: 5| Step: 5
Training loss: 2.5667323075194957
Validation loss: 2.493755488050789

Epoch: 5| Step: 6
Training loss: 2.700083967951826
Validation loss: 2.5052284214504716

Epoch: 5| Step: 7
Training loss: 3.1032316775391457
Validation loss: 2.4980734929124067

Epoch: 5| Step: 8
Training loss: 3.2586810740273617
Validation loss: 2.5027513848180925

Epoch: 5| Step: 9
Training loss: 2.8537677755316038
Validation loss: 2.4964144764483462

Epoch: 5| Step: 10
Training loss: 2.4862790278930915
Validation loss: 2.5049554001582655

Epoch: 43| Step: 0
Training loss: 3.2872054051443684
Validation loss: 2.49703438327018

Epoch: 5| Step: 1
Training loss: 2.89502476077123
Validation loss: 2.507043731338009

Epoch: 5| Step: 2
Training loss: 2.9405096341568644
Validation loss: 2.5137251855389073

Epoch: 5| Step: 3
Training loss: 2.684187422250416
Validation loss: 2.4958071424337263

Epoch: 5| Step: 4
Training loss: 2.7835275133053528
Validation loss: 2.5005746642408

Epoch: 5| Step: 5
Training loss: 2.7229856622929955
Validation loss: 2.5023454800039717

Epoch: 5| Step: 6
Training loss: 2.6406137138892967
Validation loss: 2.504852233455717

Epoch: 5| Step: 7
Training loss: 2.768138110055926
Validation loss: 2.5015079267775513

Epoch: 5| Step: 8
Training loss: 2.907188346081114
Validation loss: 2.51085858256654

Epoch: 5| Step: 9
Training loss: 2.3923889832763914
Validation loss: 2.5039349546756955

Epoch: 5| Step: 10
Training loss: 3.1086796121946074
Validation loss: 2.5069743875124972

Epoch: 44| Step: 0
Training loss: 3.0479887662865788
Validation loss: 2.5114720210810106

Epoch: 5| Step: 1
Training loss: 2.671982255534112
Validation loss: 2.5211545440743546

Epoch: 5| Step: 2
Training loss: 2.7377030528110775
Validation loss: 2.4960540835605096

Epoch: 5| Step: 3
Training loss: 2.7524855391757432
Validation loss: 2.4991674595664133

Epoch: 5| Step: 4
Training loss: 2.730097274724028
Validation loss: 2.4984384346105313

Epoch: 5| Step: 5
Training loss: 3.609534982545836
Validation loss: 2.498089176997342

Epoch: 5| Step: 6
Training loss: 3.1923350146505864
Validation loss: 2.5077570692919786

Epoch: 5| Step: 7
Training loss: 2.713775468493294
Validation loss: 2.4941971949817243

Epoch: 5| Step: 8
Training loss: 2.8705901450331983
Validation loss: 2.4833078319360347

Epoch: 5| Step: 9
Training loss: 2.091280491453288
Validation loss: 2.504703320312977

Epoch: 5| Step: 10
Training loss: 2.3787704454065546
Validation loss: 2.4892990506744943

Epoch: 45| Step: 0
Training loss: 2.5778642927190765
Validation loss: 2.4960516935503514

Epoch: 5| Step: 1
Training loss: 2.9709901087958035
Validation loss: 2.4954510629011324

Epoch: 5| Step: 2
Training loss: 3.0377155054824563
Validation loss: 2.5017932766719224

Epoch: 5| Step: 3
Training loss: 3.001828907746382
Validation loss: 2.5112230342509383

Epoch: 5| Step: 4
Training loss: 2.4004418761056314
Validation loss: 2.506510481443539

Epoch: 5| Step: 5
Training loss: 2.2718113067807684
Validation loss: 2.4992920549978193

Epoch: 5| Step: 6
Training loss: 3.057263003230643
Validation loss: 2.503682109593906

Epoch: 5| Step: 7
Training loss: 2.618100954893808
Validation loss: 2.510349590549322

Epoch: 5| Step: 8
Training loss: 2.6765839423875675
Validation loss: 2.5055120583902064

Epoch: 5| Step: 9
Training loss: 3.0880787048390257
Validation loss: 2.5070047617185036

Epoch: 5| Step: 10
Training loss: 3.3793326930139984
Validation loss: 2.517974048223677

Epoch: 46| Step: 0
Training loss: 2.2757200338273775
Validation loss: 2.5103183050181928

Epoch: 5| Step: 1
Training loss: 2.779393122729541
Validation loss: 2.506063970155459

Epoch: 5| Step: 2
Training loss: 2.4645100169644802
Validation loss: 2.4940428669256733

Epoch: 5| Step: 3
Training loss: 2.964209688560981
Validation loss: 2.5128265327848287

Epoch: 5| Step: 4
Training loss: 2.992595116960799
Validation loss: 2.4968398486625505

Epoch: 5| Step: 5
Training loss: 2.591346914390081
Validation loss: 2.4946086976669344

Epoch: 5| Step: 6
Training loss: 3.0086153618515024
Validation loss: 2.501941842490187

Epoch: 5| Step: 7
Training loss: 2.710300689569491
Validation loss: 2.494929903759495

Epoch: 5| Step: 8
Training loss: 3.2563206758352394
Validation loss: 2.5024946031135964

Epoch: 5| Step: 9
Training loss: 2.9652528744400537
Validation loss: 2.500581170282208

Epoch: 5| Step: 10
Training loss: 2.90714668469162
Validation loss: 2.492727237611546

Epoch: 47| Step: 0
Training loss: 2.8737624448794667
Validation loss: 2.487327506876972

Epoch: 5| Step: 1
Training loss: 2.9498917575956516
Validation loss: 2.4929310008512306

Epoch: 5| Step: 2
Training loss: 2.7451133660147717
Validation loss: 2.5068020995002396

Epoch: 5| Step: 3
Training loss: 2.6425331940817185
Validation loss: 2.508210184488361

Epoch: 5| Step: 4
Training loss: 2.3110763704101522
Validation loss: 2.5022101246633044

Epoch: 5| Step: 5
Training loss: 2.0807490023613218
Validation loss: 2.503085195450651

Epoch: 5| Step: 6
Training loss: 2.926755205725399
Validation loss: 2.5004763467289477

Epoch: 5| Step: 7
Training loss: 2.6560965998616526
Validation loss: 2.4997074520029545

Epoch: 5| Step: 8
Training loss: 3.468469556864314
Validation loss: 2.505776063918508

Epoch: 5| Step: 9
Training loss: 2.98030795842285
Validation loss: 2.508661943514638

Epoch: 5| Step: 10
Training loss: 3.2013953027962083
Validation loss: 2.4962771835505433

Epoch: 48| Step: 0
Training loss: 2.9242628815256255
Validation loss: 2.4970517216829116

Epoch: 5| Step: 1
Training loss: 2.9451554557560007
Validation loss: 2.506501233334839

Epoch: 5| Step: 2
Training loss: 3.035534534421898
Validation loss: 2.510844224920937

Epoch: 5| Step: 3
Training loss: 1.981676564792118
Validation loss: 2.495909172090686

Epoch: 5| Step: 4
Training loss: 2.3420414100979876
Validation loss: 2.493829694863299

Epoch: 5| Step: 5
Training loss: 3.0922296044103197
Validation loss: 2.5021060040892165

Epoch: 5| Step: 6
Training loss: 2.523560134222602
Validation loss: 2.496968597231204

Epoch: 5| Step: 7
Training loss: 3.0296812422374453
Validation loss: 2.5034210608857177

Epoch: 5| Step: 8
Training loss: 3.22410922501081
Validation loss: 2.5080719125758177

Epoch: 5| Step: 9
Training loss: 2.807006387364235
Validation loss: 2.498803345068035

Epoch: 5| Step: 10
Training loss: 2.870718047803607
Validation loss: 2.4873984620519956

Epoch: 49| Step: 0
Training loss: 3.1108591189585897
Validation loss: 2.5061396906394307

Epoch: 5| Step: 1
Training loss: 2.459237907495499
Validation loss: 2.511310407238004

Epoch: 5| Step: 2
Training loss: 3.1572381918185917
Validation loss: 2.500311686456987

Epoch: 5| Step: 3
Training loss: 2.7446026287575247
Validation loss: 2.5065617893144547

Epoch: 5| Step: 4
Training loss: 2.5587353849156678
Validation loss: 2.4893829502588636

Epoch: 5| Step: 5
Training loss: 2.844539658456228
Validation loss: 2.504737067938453

Epoch: 5| Step: 6
Training loss: 3.2612085000406377
Validation loss: 2.508422648533232

Epoch: 5| Step: 7
Training loss: 2.0708114310730332
Validation loss: 2.4841239226007232

Epoch: 5| Step: 8
Training loss: 3.032777539011929
Validation loss: 2.4966809934504064

Epoch: 5| Step: 9
Training loss: 2.4151978029865937
Validation loss: 2.4914269904696638

Epoch: 5| Step: 10
Training loss: 3.1280636837615643
Validation loss: 2.482418611610399

Epoch: 50| Step: 0
Training loss: 3.2730814665047605
Validation loss: 2.494230023525268

Epoch: 5| Step: 1
Training loss: 2.918142127180626
Validation loss: 2.487428878449292

Epoch: 5| Step: 2
Training loss: 2.8601793502379085
Validation loss: 2.498840868297149

Epoch: 5| Step: 3
Training loss: 2.6809954328999646
Validation loss: 2.514084002791375

Epoch: 5| Step: 4
Training loss: 2.8306308835727143
Validation loss: 2.5048960457681164

Epoch: 5| Step: 5
Training loss: 2.679881275166274
Validation loss: 2.4906544416040015

Epoch: 5| Step: 6
Training loss: 3.0699065039497806
Validation loss: 2.486481786409946

Epoch: 5| Step: 7
Training loss: 3.0565317347816854
Validation loss: 2.4878854899039258

Epoch: 5| Step: 8
Training loss: 2.2881763813217226
Validation loss: 2.4860368349256317

Epoch: 5| Step: 9
Training loss: 2.3670126469559913
Validation loss: 2.4877558003183142

Epoch: 5| Step: 10
Training loss: 2.7444709334680297
Validation loss: 2.498888877346571

Epoch: 51| Step: 0
Training loss: 2.8764512089755994
Validation loss: 2.4810088494897293

Epoch: 5| Step: 1
Training loss: 3.153791017390352
Validation loss: 2.486653090489443

Epoch: 5| Step: 2
Training loss: 2.3912536193533116
Validation loss: 2.4963187863891343

Epoch: 5| Step: 3
Training loss: 3.2405020503794137
Validation loss: 2.4975970861661776

Epoch: 5| Step: 4
Training loss: 2.6579718899126674
Validation loss: 2.488612795492195

Epoch: 5| Step: 5
Training loss: 3.2186382978000854
Validation loss: 2.485717076600522

Epoch: 5| Step: 6
Training loss: 2.2342723476064434
Validation loss: 2.492023119148944

Epoch: 5| Step: 7
Training loss: 2.839005085391347
Validation loss: 2.500395891161047

Epoch: 5| Step: 8
Training loss: 2.117870262948983
Validation loss: 2.482880320016684

Epoch: 5| Step: 9
Training loss: 3.2898294818598663
Validation loss: 2.496981334453707

Epoch: 5| Step: 10
Training loss: 2.449563618157952
Validation loss: 2.4983819842624704

Epoch: 52| Step: 0
Training loss: 3.149936215193895
Validation loss: 2.4882285142666674

Epoch: 5| Step: 1
Training loss: 2.5402964249303017
Validation loss: 2.497035793920054

Epoch: 5| Step: 2
Training loss: 2.1318815665925372
Validation loss: 2.4995538867260123

Epoch: 5| Step: 3
Training loss: 2.4834928085871226
Validation loss: 2.499648738610539

Epoch: 5| Step: 4
Training loss: 2.7488972446890645
Validation loss: 2.4973163844666977

Epoch: 5| Step: 5
Training loss: 2.8387575026647744
Validation loss: 2.490958868484487

Epoch: 5| Step: 6
Training loss: 3.3333474317888294
Validation loss: 2.5141191723986633

Epoch: 5| Step: 7
Training loss: 2.713452845978915
Validation loss: 2.49150468070171

Epoch: 5| Step: 8
Training loss: 3.089863194767356
Validation loss: 2.5086413814677035

Epoch: 5| Step: 9
Training loss: 3.0507971925599766
Validation loss: 2.5139204336893943

Epoch: 5| Step: 10
Training loss: 2.5676607407394987
Validation loss: 2.486925959370933

Epoch: 53| Step: 0
Training loss: 3.0132001704216083
Validation loss: 2.4971954714605658

Epoch: 5| Step: 1
Training loss: 2.94681257907942
Validation loss: 2.4831673066635687

Epoch: 5| Step: 2
Training loss: 2.8317604935226743
Validation loss: 2.4908249554180846

Epoch: 5| Step: 3
Training loss: 2.954872542729113
Validation loss: 2.501284794198575

Epoch: 5| Step: 4
Training loss: 2.8200705469384046
Validation loss: 2.508299906875219

Epoch: 5| Step: 5
Training loss: 2.700765119862521
Validation loss: 2.482726281848014

Epoch: 5| Step: 6
Training loss: 2.321984918182311
Validation loss: 2.4962111413431396

Epoch: 5| Step: 7
Training loss: 2.9255736090162663
Validation loss: 2.5003756353728646

Epoch: 5| Step: 8
Training loss: 2.7076927796943626
Validation loss: 2.4947710019463067

Epoch: 5| Step: 9
Training loss: 2.8530888065244375
Validation loss: 2.5032715741029308

Epoch: 5| Step: 10
Training loss: 2.6300935827220546
Validation loss: 2.503588469833435

Epoch: 54| Step: 0
Training loss: 2.7566584693144107
Validation loss: 2.485099338619062

Epoch: 5| Step: 1
Training loss: 2.934434955540978
Validation loss: 2.4977743824765533

Epoch: 5| Step: 2
Training loss: 2.5653300590903996
Validation loss: 2.5014677896961266

Epoch: 5| Step: 3
Training loss: 2.762228135069969
Validation loss: 2.502646857038792

Epoch: 5| Step: 4
Training loss: 3.0752832297789054
Validation loss: 2.5073228184895933

Epoch: 5| Step: 5
Training loss: 2.9912791653784256
Validation loss: 2.503814119651404

Epoch: 5| Step: 6
Training loss: 2.8866662212955876
Validation loss: 2.5094693455551362

Epoch: 5| Step: 7
Training loss: 2.4199291657098607
Validation loss: 2.494850759307119

Epoch: 5| Step: 8
Training loss: 2.521648327781149
Validation loss: 2.5018604575172136

Epoch: 5| Step: 9
Training loss: 3.2146276352689838
Validation loss: 2.5037568730749147

Epoch: 5| Step: 10
Training loss: 2.411524948177728
Validation loss: 2.498422373059725

Epoch: 55| Step: 0
Training loss: 2.53777038561822
Validation loss: 2.485500436528145

Epoch: 5| Step: 1
Training loss: 2.7679399187921274
Validation loss: 2.5017126718277094

Epoch: 5| Step: 2
Training loss: 3.09841567131352
Validation loss: 2.497077829688597

Epoch: 5| Step: 3
Training loss: 3.1797429860215622
Validation loss: 2.507291810624191

Epoch: 5| Step: 4
Training loss: 2.703821610672133
Validation loss: 2.490393133923777

Epoch: 5| Step: 5
Training loss: 2.5178975331043825
Validation loss: 2.4879024324987413

Epoch: 5| Step: 6
Training loss: 2.699566389516333
Validation loss: 2.4936587820086427

Epoch: 5| Step: 7
Training loss: 2.877721742095083
Validation loss: 2.5008920908342964

Epoch: 5| Step: 8
Training loss: 3.0761542038182914
Validation loss: 2.4995719943218186

Epoch: 5| Step: 9
Training loss: 2.661068394432472
Validation loss: 2.491372701666322

Epoch: 5| Step: 10
Training loss: 2.571799917504048
Validation loss: 2.487284555976997

Epoch: 56| Step: 0
Training loss: 2.815253372308249
Validation loss: 2.491054729446044

Epoch: 5| Step: 1
Training loss: 2.683678504904786
Validation loss: 2.488194900255653

Epoch: 5| Step: 2
Training loss: 3.28508672289127
Validation loss: 2.483558416878286

Epoch: 5| Step: 3
Training loss: 2.63706405074066
Validation loss: 2.4983368355845528

Epoch: 5| Step: 4
Training loss: 3.5482398276657947
Validation loss: 2.4984465274258936

Epoch: 5| Step: 5
Training loss: 2.6274299273960597
Validation loss: 2.474145615402136

Epoch: 5| Step: 6
Training loss: 2.2622968761668543
Validation loss: 2.4868489363687356

Epoch: 5| Step: 7
Training loss: 3.0089036423837823
Validation loss: 2.486197483886496

Epoch: 5| Step: 8
Training loss: 2.374411459831386
Validation loss: 2.485763296054877

Epoch: 5| Step: 9
Training loss: 2.587832675500945
Validation loss: 2.4930681746721213

Epoch: 5| Step: 10
Training loss: 2.6743657509968566
Validation loss: 2.4954894856082257

Epoch: 57| Step: 0
Training loss: 2.8894372097730714
Validation loss: 2.482961918241542

Epoch: 5| Step: 1
Training loss: 2.8225014769530845
Validation loss: 2.496134350419662

Epoch: 5| Step: 2
Training loss: 2.871646085795642
Validation loss: 2.5052528407241037

Epoch: 5| Step: 3
Training loss: 2.6537152931074877
Validation loss: 2.491531369535639

Epoch: 5| Step: 4
Training loss: 3.093388083510551
Validation loss: 2.4953704031866613

Epoch: 5| Step: 5
Training loss: 3.0360739166348547
Validation loss: 2.4901630095571363

Epoch: 5| Step: 6
Training loss: 2.952617944853598
Validation loss: 2.491823442225324

Epoch: 5| Step: 7
Training loss: 2.544727566869861
Validation loss: 2.4979850319263073

Epoch: 5| Step: 8
Training loss: 2.438242285752333
Validation loss: 2.4999317713625535

Epoch: 5| Step: 9
Training loss: 2.8346442013792643
Validation loss: 2.5015378887648616

Epoch: 5| Step: 10
Training loss: 2.420481126894121
Validation loss: 2.510017249551379

Epoch: 58| Step: 0
Training loss: 2.5890165593106516
Validation loss: 2.4865455992135157

Epoch: 5| Step: 1
Training loss: 2.956299382577602
Validation loss: 2.497261528699752

Epoch: 5| Step: 2
Training loss: 2.6133799626932817
Validation loss: 2.508426548531929

Epoch: 5| Step: 3
Training loss: 2.764055571862784
Validation loss: 2.4974897600237256

Epoch: 5| Step: 4
Training loss: 3.542944928039746
Validation loss: 2.4861113236879926

Epoch: 5| Step: 5
Training loss: 2.6171214536617424
Validation loss: 2.5071102039773825

Epoch: 5| Step: 6
Training loss: 2.929201457077929
Validation loss: 2.495381798110297

Epoch: 5| Step: 7
Training loss: 2.4296033053514132
Validation loss: 2.506285962252442

Epoch: 5| Step: 8
Training loss: 1.9343066201888397
Validation loss: 2.48899223617222

Epoch: 5| Step: 9
Training loss: 2.51305850829245
Validation loss: 2.4942597723012585

Epoch: 5| Step: 10
Training loss: 3.5155544697786247
Validation loss: 2.4904855981128615

Epoch: 59| Step: 0
Training loss: 2.7234938002351745
Validation loss: 2.4849823095728945

Epoch: 5| Step: 1
Training loss: 2.6959771705079674
Validation loss: 2.4820697646012113

Epoch: 5| Step: 2
Training loss: 2.7607079886014088
Validation loss: 2.4866593009865814

Epoch: 5| Step: 3
Training loss: 2.821366385314372
Validation loss: 2.4962507652340573

Epoch: 5| Step: 4
Training loss: 3.1241035701566346
Validation loss: 2.5040123090464084

Epoch: 5| Step: 5
Training loss: 2.4265871536761225
Validation loss: 2.486944208391772

Epoch: 5| Step: 6
Training loss: 3.3907199266215686
Validation loss: 2.497269944594633

Epoch: 5| Step: 7
Training loss: 3.2923484933067755
Validation loss: 2.4829599410167864

Epoch: 5| Step: 8
Training loss: 2.179648887811843
Validation loss: 2.5057357271499203

Epoch: 5| Step: 9
Training loss: 2.187861821360123
Validation loss: 2.4894639872731683

Epoch: 5| Step: 10
Training loss: 2.8125839220877697
Validation loss: 2.4986765075705883

Epoch: 60| Step: 0
Training loss: 2.545333864088421
Validation loss: 2.4892328150900784

Epoch: 5| Step: 1
Training loss: 2.304533872494895
Validation loss: 2.489854857969629

Epoch: 5| Step: 2
Training loss: 2.397491964539556
Validation loss: 2.4952600531699947

Epoch: 5| Step: 3
Training loss: 3.148568351809378
Validation loss: 2.499088339550743

Epoch: 5| Step: 4
Training loss: 3.0155435659847547
Validation loss: 2.489152328769219

Epoch: 5| Step: 5
Training loss: 3.0311708734940774
Validation loss: 2.4911756605903284

Epoch: 5| Step: 6
Training loss: 2.690283730869625
Validation loss: 2.469923403849548

Epoch: 5| Step: 7
Training loss: 2.6460722767826126
Validation loss: 2.4926106109235997

Epoch: 5| Step: 8
Training loss: 3.3307928576828396
Validation loss: 2.4798549852321505

Epoch: 5| Step: 9
Training loss: 2.1293758889239274
Validation loss: 2.4815512571016063

Epoch: 5| Step: 10
Training loss: 3.1365983495728247
Validation loss: 2.49931063120194

Epoch: 61| Step: 0
Training loss: 2.5383562719753434
Validation loss: 2.4827250509993224

Epoch: 5| Step: 1
Training loss: 2.3613844644721227
Validation loss: 2.4956515970096347

Epoch: 5| Step: 2
Training loss: 3.4065019706833475
Validation loss: 2.4810397058732585

Epoch: 5| Step: 3
Training loss: 2.8005172626363106
Validation loss: 2.4908622766912085

Epoch: 5| Step: 4
Training loss: 2.3826439969877655
Validation loss: 2.4795052026745528

Epoch: 5| Step: 5
Training loss: 2.1548853439666087
Validation loss: 2.493074730123645

Epoch: 5| Step: 6
Training loss: 3.384475968563983
Validation loss: 2.4753143000357225

Epoch: 5| Step: 7
Training loss: 2.9570676934882756
Validation loss: 2.482045195742398

Epoch: 5| Step: 8
Training loss: 2.41253297615244
Validation loss: 2.486055524611791

Epoch: 5| Step: 9
Training loss: 2.8838892634132764
Validation loss: 2.485830752845137

Epoch: 5| Step: 10
Training loss: 2.992408047110246
Validation loss: 2.484645817136282

Epoch: 62| Step: 0
Training loss: 2.3334891971119838
Validation loss: 2.4717851836046143

Epoch: 5| Step: 1
Training loss: 2.805677743859133
Validation loss: 2.487364033935334

Epoch: 5| Step: 2
Training loss: 2.855697518302672
Validation loss: 2.4839061125093607

Epoch: 5| Step: 3
Training loss: 2.7067272901871005
Validation loss: 2.4780572834163186

Epoch: 5| Step: 4
Training loss: 2.746412017307497
Validation loss: 2.478277398689667

Epoch: 5| Step: 5
Training loss: 2.9823619184481722
Validation loss: 2.495997424194941

Epoch: 5| Step: 6
Training loss: 2.90585390078376
Validation loss: 2.482383392547677

Epoch: 5| Step: 7
Training loss: 2.1994293816692436
Validation loss: 2.48511688925112

Epoch: 5| Step: 8
Training loss: 3.064767115148029
Validation loss: 2.490159440253272

Epoch: 5| Step: 9
Training loss: 2.927894959426075
Validation loss: 2.4947296600031366

Epoch: 5| Step: 10
Training loss: 2.9284310456765454
Validation loss: 2.4718484890626926

Epoch: 63| Step: 0
Training loss: 2.8908780373835157
Validation loss: 2.4893795878624405

Epoch: 5| Step: 1
Training loss: 2.777596635210399
Validation loss: 2.487243133917947

Epoch: 5| Step: 2
Training loss: 3.1859087992783683
Validation loss: 2.480594803437543

Epoch: 5| Step: 3
Training loss: 3.233743062227561
Validation loss: 2.482321776833399

Epoch: 5| Step: 4
Training loss: 2.463472737698212
Validation loss: 2.4812720784243694

Epoch: 5| Step: 5
Training loss: 2.754881081619449
Validation loss: 2.487128018859315

Epoch: 5| Step: 6
Training loss: 2.7648243580332417
Validation loss: 2.4923637400599326

Epoch: 5| Step: 7
Training loss: 2.8612236342375668
Validation loss: 2.4925114012306544

Epoch: 5| Step: 8
Training loss: 2.356652919004378
Validation loss: 2.491843525761871

Epoch: 5| Step: 9
Training loss: 2.895423000180731
Validation loss: 2.4884355253523225

Epoch: 5| Step: 10
Training loss: 2.127548372680274
Validation loss: 2.4894023001698553

Epoch: 64| Step: 0
Training loss: 2.2697058395491467
Validation loss: 2.4831541279464266

Epoch: 5| Step: 1
Training loss: 2.6502644137057216
Validation loss: 2.4869258696872265

Epoch: 5| Step: 2
Training loss: 3.0662112423064216
Validation loss: 2.4962617853949727

Epoch: 5| Step: 3
Training loss: 2.463440993162701
Validation loss: 2.4885754997179292

Epoch: 5| Step: 4
Training loss: 3.333495056679658
Validation loss: 2.4910256447896124

Epoch: 5| Step: 5
Training loss: 2.7340892533451737
Validation loss: 2.491820357817636

Epoch: 5| Step: 6
Training loss: 2.7262099073593684
Validation loss: 2.4924730488414046

Epoch: 5| Step: 7
Training loss: 2.726022224128569
Validation loss: 2.488308975864502

Epoch: 5| Step: 8
Training loss: 2.873688606047776
Validation loss: 2.4795624984075495

Epoch: 5| Step: 9
Training loss: 2.745599173101554
Validation loss: 2.5015396668370395

Epoch: 5| Step: 10
Training loss: 2.6849111463453554
Validation loss: 2.4849002040746595

Epoch: 65| Step: 0
Training loss: 3.1660237579900166
Validation loss: 2.4921213998205665

Epoch: 5| Step: 1
Training loss: 2.5249805751846517
Validation loss: 2.482804601601984

Epoch: 5| Step: 2
Training loss: 2.6367909061307477
Validation loss: 2.4930515715643713

Epoch: 5| Step: 3
Training loss: 2.45769720952978
Validation loss: 2.483619994687432

Epoch: 5| Step: 4
Training loss: 2.5628034830649926
Validation loss: 2.5005621888129164

Epoch: 5| Step: 5
Training loss: 3.849598227010114
Validation loss: 2.504081771754988

Epoch: 5| Step: 6
Training loss: 2.4633557260895262
Validation loss: 2.5038526013935054

Epoch: 5| Step: 7
Training loss: 2.076625770770312
Validation loss: 2.486412582623706

Epoch: 5| Step: 8
Training loss: 2.821975682554633
Validation loss: 2.4927721083990697

Epoch: 5| Step: 9
Training loss: 2.4961885007446853
Validation loss: 2.4949070819884924

Epoch: 5| Step: 10
Training loss: 3.0868915097246385
Validation loss: 2.471581983889309

Epoch: 66| Step: 0
Training loss: 2.9114143673074198
Validation loss: 2.469687399021713

Epoch: 5| Step: 1
Training loss: 2.960662748528184
Validation loss: 2.476444246607934

Epoch: 5| Step: 2
Training loss: 3.1252062920191013
Validation loss: 2.4899485261830834

Epoch: 5| Step: 3
Training loss: 2.6143959758445092
Validation loss: 2.4851587841399874

Epoch: 5| Step: 4
Training loss: 3.1535884096990348
Validation loss: 2.4945028955167374

Epoch: 5| Step: 5
Training loss: 2.7392750459016724
Validation loss: 2.503150142739892

Epoch: 5| Step: 6
Training loss: 2.5180773421355753
Validation loss: 2.488202578199426

Epoch: 5| Step: 7
Training loss: 2.60909694629245
Validation loss: 2.4890294031349356

Epoch: 5| Step: 8
Training loss: 2.611259282644818
Validation loss: 2.481998538307052

Epoch: 5| Step: 9
Training loss: 2.8061920635810154
Validation loss: 2.4811129776552208

Epoch: 5| Step: 10
Training loss: 2.194913345920484
Validation loss: 2.4812043068087237

Epoch: 67| Step: 0
Training loss: 2.6225887304665063
Validation loss: 2.479415622191676

Epoch: 5| Step: 1
Training loss: 2.8621909028795347
Validation loss: 2.4741541928198365

Epoch: 5| Step: 2
Training loss: 2.5029174947274226
Validation loss: 2.487599164033593

Epoch: 5| Step: 3
Training loss: 2.7257080488142167
Validation loss: 2.480272927474393

Epoch: 5| Step: 4
Training loss: 2.831513961377652
Validation loss: 2.481361780954744

Epoch: 5| Step: 5
Training loss: 3.4705356435128376
Validation loss: 2.481701339438913

Epoch: 5| Step: 6
Training loss: 2.496903409068182
Validation loss: 2.4795424032813607

Epoch: 5| Step: 7
Training loss: 2.8108479204052874
Validation loss: 2.4915523464826923

Epoch: 5| Step: 8
Training loss: 2.5889144311808217
Validation loss: 2.500069979231737

Epoch: 5| Step: 9
Training loss: 2.549218158914753
Validation loss: 2.5004687977215285

Epoch: 5| Step: 10
Training loss: 2.875819379724214
Validation loss: 2.4996666019409064

Epoch: 68| Step: 0
Training loss: 2.657088113842004
Validation loss: 2.4912623917023393

Epoch: 5| Step: 1
Training loss: 2.8703705376336743
Validation loss: 2.4951605059922652

Epoch: 5| Step: 2
Training loss: 2.283973009300757
Validation loss: 2.478354144496795

Epoch: 5| Step: 3
Training loss: 3.5058410134007327
Validation loss: 2.4908291114597314

Epoch: 5| Step: 4
Training loss: 2.351404431882489
Validation loss: 2.4764945914795757

Epoch: 5| Step: 5
Training loss: 2.978305736713464
Validation loss: 2.481660332441884

Epoch: 5| Step: 6
Training loss: 2.047292069903199
Validation loss: 2.488616921233352

Epoch: 5| Step: 7
Training loss: 3.011521151105316
Validation loss: 2.4928226387471515

Epoch: 5| Step: 8
Training loss: 2.505790584627935
Validation loss: 2.4838960742800253

Epoch: 5| Step: 9
Training loss: 3.0547794416010814
Validation loss: 2.4783273049661827

Epoch: 5| Step: 10
Training loss: 2.842104592983174
Validation loss: 2.4893535432964984

Epoch: 69| Step: 0
Training loss: 3.192177874176704
Validation loss: 2.4855942162341385

Epoch: 5| Step: 1
Training loss: 2.6634521680188783
Validation loss: 2.481043742945213

Epoch: 5| Step: 2
Training loss: 2.3358959589808066
Validation loss: 2.482478282424404

Epoch: 5| Step: 3
Training loss: 3.1106008289050315
Validation loss: 2.467338304809359

Epoch: 5| Step: 4
Training loss: 2.5995959848335652
Validation loss: 2.474956313147811

Epoch: 5| Step: 5
Training loss: 3.0485129624201135
Validation loss: 2.465234590304532

Epoch: 5| Step: 6
Training loss: 2.854477320190927
Validation loss: 2.4859444517354747

Epoch: 5| Step: 7
Training loss: 2.0844868963080883
Validation loss: 2.4794360192281166

Epoch: 5| Step: 8
Training loss: 2.8482290353833006
Validation loss: 2.4744589940939172

Epoch: 5| Step: 9
Training loss: 2.3839388827227967
Validation loss: 2.4922011926000773

Epoch: 5| Step: 10
Training loss: 3.014340616370596
Validation loss: 2.4828426118491955

Epoch: 70| Step: 0
Training loss: 2.5934849799011563
Validation loss: 2.4868419294735067

Epoch: 5| Step: 1
Training loss: 2.617837671398191
Validation loss: 2.4877014040901058

Epoch: 5| Step: 2
Training loss: 2.964533330978655
Validation loss: 2.487310957180755

Epoch: 5| Step: 3
Training loss: 2.0003481800274447
Validation loss: 2.489571606376778

Epoch: 5| Step: 4
Training loss: 2.53767587211994
Validation loss: 2.482792431839385

Epoch: 5| Step: 5
Training loss: 2.0558840672428773
Validation loss: 2.476052362615628

Epoch: 5| Step: 6
Training loss: 2.774385875758789
Validation loss: 2.4901204359838207

Epoch: 5| Step: 7
Training loss: 2.9169936587186616
Validation loss: 2.4894022903865487

Epoch: 5| Step: 8
Training loss: 3.699476803053779
Validation loss: 2.4827961449346536

Epoch: 5| Step: 9
Training loss: 2.810056175675002
Validation loss: 2.4866317495194954

Epoch: 5| Step: 10
Training loss: 3.0424939080132685
Validation loss: 2.4861296699477275

Epoch: 71| Step: 0
Training loss: 2.8865933733027607
Validation loss: 2.4720195653057635

Epoch: 5| Step: 1
Training loss: 2.5657989851843515
Validation loss: 2.491554095668318

Epoch: 5| Step: 2
Training loss: 3.0154050440448095
Validation loss: 2.4839919477250385

Epoch: 5| Step: 3
Training loss: 3.231954359837067
Validation loss: 2.486465895073199

Epoch: 5| Step: 4
Training loss: 2.8765365806626093
Validation loss: 2.4941488273517147

Epoch: 5| Step: 5
Training loss: 2.858305183768796
Validation loss: 2.490084139824003

Epoch: 5| Step: 6
Training loss: 2.7125501425894436
Validation loss: 2.5024282660330655

Epoch: 5| Step: 7
Training loss: 2.730136660135151
Validation loss: 2.5037934921948435

Epoch: 5| Step: 8
Training loss: 2.188899981873487
Validation loss: 2.4861065492969883

Epoch: 5| Step: 9
Training loss: 2.5020601367325144
Validation loss: 2.495502450223371

Epoch: 5| Step: 10
Training loss: 2.5931233947557697
Validation loss: 2.487340742328894

Epoch: 72| Step: 0
Training loss: 2.8384119593298465
Validation loss: 2.4894014799166957

Epoch: 5| Step: 1
Training loss: 2.6481021454981573
Validation loss: 2.498641858304097

Epoch: 5| Step: 2
Training loss: 2.5997412552831842
Validation loss: 2.497881568098933

Epoch: 5| Step: 3
Training loss: 2.833092305272257
Validation loss: 2.48865303791636

Epoch: 5| Step: 4
Training loss: 2.6785446710612475
Validation loss: 2.4925444952230995

Epoch: 5| Step: 5
Training loss: 2.5471080837194418
Validation loss: 2.4902231969065745

Epoch: 5| Step: 6
Training loss: 2.637325414872633
Validation loss: 2.485251299689617

Epoch: 5| Step: 7
Training loss: 2.96774580184155
Validation loss: 2.503552115491027

Epoch: 5| Step: 8
Training loss: 2.805847268209211
Validation loss: 2.4997818708113817

Epoch: 5| Step: 9
Training loss: 2.321025905689726
Validation loss: 2.482229258475233

Epoch: 5| Step: 10
Training loss: 3.49883182649373
Validation loss: 2.5021635522623407

Epoch: 73| Step: 0
Training loss: 2.8701037302444803
Validation loss: 2.4995968750009085

Epoch: 5| Step: 1
Training loss: 2.870637154116358
Validation loss: 2.5010339741936782

Epoch: 5| Step: 2
Training loss: 2.5922588060354355
Validation loss: 2.486200742315946

Epoch: 5| Step: 3
Training loss: 2.9171049787785073
Validation loss: 2.484596361764668

Epoch: 5| Step: 4
Training loss: 3.005676938197371
Validation loss: 2.4916191182003504

Epoch: 5| Step: 5
Training loss: 2.655311777806821
Validation loss: 2.490859121621007

Epoch: 5| Step: 6
Training loss: 2.5481160890312538
Validation loss: 2.4910493758611714

Epoch: 5| Step: 7
Training loss: 2.761690433686164
Validation loss: 2.5078767299237557

Epoch: 5| Step: 8
Training loss: 2.6050119071000064
Validation loss: 2.489750799895932

Epoch: 5| Step: 9
Training loss: 2.605567301154577
Validation loss: 2.4790180941126985

Epoch: 5| Step: 10
Training loss: 2.66859430935486
Validation loss: 2.483059837940572

Epoch: 74| Step: 0
Training loss: 2.9131194933430242
Validation loss: 2.4757186348920315

Epoch: 5| Step: 1
Training loss: 3.1626615460414937
Validation loss: 2.4854478924369126

Epoch: 5| Step: 2
Training loss: 2.622433815876455
Validation loss: 2.4819277004138334

Epoch: 5| Step: 3
Training loss: 2.814378577256984
Validation loss: 2.4835036050988064

Epoch: 5| Step: 4
Training loss: 2.508488834246002
Validation loss: 2.477296811907284

Epoch: 5| Step: 5
Training loss: 1.9709588027705267
Validation loss: 2.478723118365478

Epoch: 5| Step: 6
Training loss: 2.395749452753491
Validation loss: 2.4882195330595063

Epoch: 5| Step: 7
Training loss: 3.198224296228583
Validation loss: 2.498428923697655

Epoch: 5| Step: 8
Training loss: 2.3377933338543775
Validation loss: 2.482713850971006

Epoch: 5| Step: 9
Training loss: 3.1923185839745623
Validation loss: 2.4847053591443014

Epoch: 5| Step: 10
Training loss: 2.897946380382456
Validation loss: 2.48184140541298

Epoch: 75| Step: 0
Training loss: 2.5672903178469784
Validation loss: 2.4733388715659403

Epoch: 5| Step: 1
Training loss: 2.663005003697747
Validation loss: 2.4707821661955847

Epoch: 5| Step: 2
Training loss: 2.7216825782916563
Validation loss: 2.486121887124403

Epoch: 5| Step: 3
Training loss: 2.612139031692673
Validation loss: 2.488700079985632

Epoch: 5| Step: 4
Training loss: 2.40753506120957
Validation loss: 2.4854036166125075

Epoch: 5| Step: 5
Training loss: 3.010984972117818
Validation loss: 2.466314482830027

Epoch: 5| Step: 6
Training loss: 2.5402410500955086
Validation loss: 2.479481546189585

Epoch: 5| Step: 7
Training loss: 3.3813766709608037
Validation loss: 2.482900737118986

Epoch: 5| Step: 8
Training loss: 2.9231836988661684
Validation loss: 2.475675036273604

Epoch: 5| Step: 9
Training loss: 2.0983052590542113
Validation loss: 2.474710141483208

Epoch: 5| Step: 10
Training loss: 3.153404388299258
Validation loss: 2.4835997218165082

Epoch: 76| Step: 0
Training loss: 2.9893638893605265
Validation loss: 2.487488308493271

Epoch: 5| Step: 1
Training loss: 2.731817826314455
Validation loss: 2.485431389018565

Epoch: 5| Step: 2
Training loss: 2.3541277204234614
Validation loss: 2.4767249362600645

Epoch: 5| Step: 3
Training loss: 3.4420700212501436
Validation loss: 2.4996938774386357

Epoch: 5| Step: 4
Training loss: 2.6274642276960245
Validation loss: 2.4933173200953918

Epoch: 5| Step: 5
Training loss: 2.83953427566017
Validation loss: 2.4895856166243373

Epoch: 5| Step: 6
Training loss: 2.6617366000809413
Validation loss: 2.48440878023418

Epoch: 5| Step: 7
Training loss: 2.4003426505898027
Validation loss: 2.478984461729573

Epoch: 5| Step: 8
Training loss: 2.494275500923187
Validation loss: 2.473737040770778

Epoch: 5| Step: 9
Training loss: 2.665292047812824
Validation loss: 2.486386329634589

Epoch: 5| Step: 10
Training loss: 2.8442767986259967
Validation loss: 2.4833862768413755

Epoch: 77| Step: 0
Training loss: 2.3109433889152804
Validation loss: 2.4835598898913935

Epoch: 5| Step: 1
Training loss: 2.667914416540451
Validation loss: 2.4877039005377557

Epoch: 5| Step: 2
Training loss: 3.049809221648487
Validation loss: 2.4823527454737686

Epoch: 5| Step: 3
Training loss: 2.8108255276051843
Validation loss: 2.4804256172877426

Epoch: 5| Step: 4
Training loss: 2.444027694183125
Validation loss: 2.482617199643803

Epoch: 5| Step: 5
Training loss: 3.0514774871250734
Validation loss: 2.475844014826015

Epoch: 5| Step: 6
Training loss: 2.6500555176587497
Validation loss: 2.4867971527097876

Epoch: 5| Step: 7
Training loss: 3.073017666121356
Validation loss: 2.4774168375813326

Epoch: 5| Step: 8
Training loss: 2.8028556770897497
Validation loss: 2.4689395801116008

Epoch: 5| Step: 9
Training loss: 2.3138038464941815
Validation loss: 2.488426055535114

Epoch: 5| Step: 10
Training loss: 2.8392936247161047
Validation loss: 2.4965634136923684

Epoch: 78| Step: 0
Training loss: 2.4676196250789357
Validation loss: 2.4801133553396992

Epoch: 5| Step: 1
Training loss: 2.5102612670244397
Validation loss: 2.4795017720846495

Epoch: 5| Step: 2
Training loss: 3.1046164440668846
Validation loss: 2.4860997145804737

Epoch: 5| Step: 3
Training loss: 2.9012621665084795
Validation loss: 2.482887194571041

Epoch: 5| Step: 4
Training loss: 3.048179621032067
Validation loss: 2.482268633574731

Epoch: 5| Step: 5
Training loss: 2.3402486460375878
Validation loss: 2.4876767594572913

Epoch: 5| Step: 6
Training loss: 2.754781034875524
Validation loss: 2.481673951930801

Epoch: 5| Step: 7
Training loss: 3.2282349913951385
Validation loss: 2.4860560092795585

Epoch: 5| Step: 8
Training loss: 2.8923219390899106
Validation loss: 2.4974280930591397

Epoch: 5| Step: 9
Training loss: 2.475931273682101
Validation loss: 2.4730605498442553

Epoch: 5| Step: 10
Training loss: 2.2361607384818374
Validation loss: 2.485700006692507

Epoch: 79| Step: 0
Training loss: 1.9536814392907682
Validation loss: 2.489711485464978

Epoch: 5| Step: 1
Training loss: 2.875818219059951
Validation loss: 2.47767692706603

Epoch: 5| Step: 2
Training loss: 3.0708364568631468
Validation loss: 2.487375600038217

Epoch: 5| Step: 3
Training loss: 2.9600912611012267
Validation loss: 2.482146770651815

Epoch: 5| Step: 4
Training loss: 2.760282712361298
Validation loss: 2.4819923533319144

Epoch: 5| Step: 5
Training loss: 2.6113173513825063
Validation loss: 2.483563378874807

Epoch: 5| Step: 6
Training loss: 2.5872288732334208
Validation loss: 2.4841166551759386

Epoch: 5| Step: 7
Training loss: 2.544891895928566
Validation loss: 2.4855392513155747

Epoch: 5| Step: 8
Training loss: 2.721329353377581
Validation loss: 2.4982775271300457

Epoch: 5| Step: 9
Training loss: 2.9027349072403124
Validation loss: 2.5004687187761996

Epoch: 5| Step: 10
Training loss: 2.9409474373445765
Validation loss: 2.488136889389496

Epoch: 80| Step: 0
Training loss: 3.1503340165597606
Validation loss: 2.484624446543811

Epoch: 5| Step: 1
Training loss: 2.6640725233499123
Validation loss: 2.4854273941433878

Epoch: 5| Step: 2
Training loss: 2.4394219475151893
Validation loss: 2.5000023431664427

Epoch: 5| Step: 3
Training loss: 2.4549353655913135
Validation loss: 2.477977709214266

Epoch: 5| Step: 4
Training loss: 2.4706946331375805
Validation loss: 2.488274882826113

Epoch: 5| Step: 5
Training loss: 2.5698884654797967
Validation loss: 2.48857874061144

Epoch: 5| Step: 6
Training loss: 2.7716053243406744
Validation loss: 2.4845563776291515

Epoch: 5| Step: 7
Training loss: 2.7506177381797996
Validation loss: 2.4833649676727085

Epoch: 5| Step: 8
Training loss: 2.7410672499747752
Validation loss: 2.494057648164434

Epoch: 5| Step: 9
Training loss: 2.602614548037557
Validation loss: 2.4738361867770964

Epoch: 5| Step: 10
Training loss: 3.446165917678011
Validation loss: 2.483090572820415

Epoch: 81| Step: 0
Training loss: 2.6750104030513797
Validation loss: 2.4903078189115653

Epoch: 5| Step: 1
Training loss: 2.9169143753222215
Validation loss: 2.4858306481680468

Epoch: 5| Step: 2
Training loss: 2.292785325260412
Validation loss: 2.4767431734964234

Epoch: 5| Step: 3
Training loss: 2.4844420801859286
Validation loss: 2.4875779084733036

Epoch: 5| Step: 4
Training loss: 2.87985085631007
Validation loss: 2.4909891992114006

Epoch: 5| Step: 5
Training loss: 2.6711715753848617
Validation loss: 2.4854758448517527

Epoch: 5| Step: 6
Training loss: 2.6083075801402695
Validation loss: 2.4928326502863136

Epoch: 5| Step: 7
Training loss: 3.025948676794153
Validation loss: 2.4804084789242364

Epoch: 5| Step: 8
Training loss: 2.784986835907484
Validation loss: 2.4873759700454325

Epoch: 5| Step: 9
Training loss: 2.6631983812665845
Validation loss: 2.474275425708668

Epoch: 5| Step: 10
Training loss: 3.060168721901841
Validation loss: 2.4859950332077196

Epoch: 82| Step: 0
Training loss: 2.839413197029766
Validation loss: 2.499199639663945

Epoch: 5| Step: 1
Training loss: 2.5873810118807143
Validation loss: 2.4802158031985186

Epoch: 5| Step: 2
Training loss: 2.4360187627072913
Validation loss: 2.490793438028618

Epoch: 5| Step: 3
Training loss: 3.1098963621479063
Validation loss: 2.4869436826635076

Epoch: 5| Step: 4
Training loss: 2.6703263641229653
Validation loss: 2.4835696765806836

Epoch: 5| Step: 5
Training loss: 2.997256295905428
Validation loss: 2.494593133519105

Epoch: 5| Step: 6
Training loss: 2.492786299658798
Validation loss: 2.48512883718588

Epoch: 5| Step: 7
Training loss: 2.6206189790124776
Validation loss: 2.4743054180267965

Epoch: 5| Step: 8
Training loss: 3.227533891590033
Validation loss: 2.4819597569774863

Epoch: 5| Step: 9
Training loss: 2.8040781023192363
Validation loss: 2.4680131451473786

Epoch: 5| Step: 10
Training loss: 1.7431596857438052
Validation loss: 2.4922600449272156

Epoch: 83| Step: 0
Training loss: 3.117249299395234
Validation loss: 2.4793155081218754

Epoch: 5| Step: 1
Training loss: 2.7121269848556913
Validation loss: 2.4932579796384795

Epoch: 5| Step: 2
Training loss: 3.453278274931838
Validation loss: 2.484841600434975

Epoch: 5| Step: 3
Training loss: 2.062044844834407
Validation loss: 2.4878070405381796

Epoch: 5| Step: 4
Training loss: 2.6127038611492988
Validation loss: 2.487573836660153

Epoch: 5| Step: 5
Training loss: 2.5924398031575593
Validation loss: 2.494479450120556

Epoch: 5| Step: 6
Training loss: 2.9546861508354767
Validation loss: 2.4837066190897072

Epoch: 5| Step: 7
Training loss: 2.9286090137316143
Validation loss: 2.485861910842197

Epoch: 5| Step: 8
Training loss: 2.7413574005671064
Validation loss: 2.4841944182009636

Epoch: 5| Step: 9
Training loss: 2.474133862074192
Validation loss: 2.4693797979931853

Epoch: 5| Step: 10
Training loss: 1.9255047148887265
Validation loss: 2.487893778826789

Epoch: 84| Step: 0
Training loss: 2.8401009759279328
Validation loss: 2.4799122584111166

Epoch: 5| Step: 1
Training loss: 2.3003554152496752
Validation loss: 2.4872369666226986

Epoch: 5| Step: 2
Training loss: 2.013679452580311
Validation loss: 2.4801354408229357

Epoch: 5| Step: 3
Training loss: 2.558185014764019
Validation loss: 2.483989893915251

Epoch: 5| Step: 4
Training loss: 2.9588406266650975
Validation loss: 2.485784369118089

Epoch: 5| Step: 5
Training loss: 2.5926892053172614
Validation loss: 2.482897929699936

Epoch: 5| Step: 6
Training loss: 2.663868290151176
Validation loss: 2.475871753643102

Epoch: 5| Step: 7
Training loss: 3.321140033594142
Validation loss: 2.4801749039473693

Epoch: 5| Step: 8
Training loss: 3.1396707631885072
Validation loss: 2.492922419138412

Epoch: 5| Step: 9
Training loss: 2.5469783457751767
Validation loss: 2.4857692664132935

Epoch: 5| Step: 10
Training loss: 2.829844252543299
Validation loss: 2.4731572206524723

Epoch: 85| Step: 0
Training loss: 2.9204865282842256
Validation loss: 2.5022399650309737

Epoch: 5| Step: 1
Training loss: 1.9913355782241522
Validation loss: 2.4792879515634647

Epoch: 5| Step: 2
Training loss: 2.943332158135611
Validation loss: 2.466806662721012

Epoch: 5| Step: 3
Training loss: 3.070727914754519
Validation loss: 2.4836536618419194

Epoch: 5| Step: 4
Training loss: 2.8183896597796814
Validation loss: 2.4777266374676503

Epoch: 5| Step: 5
Training loss: 2.2209025941775806
Validation loss: 2.477917208909252

Epoch: 5| Step: 6
Training loss: 3.083088547208659
Validation loss: 2.479212491556488

Epoch: 5| Step: 7
Training loss: 2.4615970757044003
Validation loss: 2.48263808978823

Epoch: 5| Step: 8
Training loss: 2.7915054009009688
Validation loss: 2.47796113017949

Epoch: 5| Step: 9
Training loss: 2.551511884795835
Validation loss: 2.4876555055905545

Epoch: 5| Step: 10
Training loss: 2.8667723643607457
Validation loss: 2.4725408498150316

Epoch: 86| Step: 0
Training loss: 3.3719386592367413
Validation loss: 2.474490203592103

Epoch: 5| Step: 1
Training loss: 2.7897040647021547
Validation loss: 2.485476478161126

Epoch: 5| Step: 2
Training loss: 2.0700392056866543
Validation loss: 2.4768102512957872

Epoch: 5| Step: 3
Training loss: 2.9928215291362146
Validation loss: 2.487158547823549

Epoch: 5| Step: 4
Training loss: 2.7167669496183913
Validation loss: 2.481807589200925

Epoch: 5| Step: 5
Training loss: 2.517493081561586
Validation loss: 2.490574107246694

Epoch: 5| Step: 6
Training loss: 2.729722080546649
Validation loss: 2.4930127482831845

Epoch: 5| Step: 7
Training loss: 2.658707233868751
Validation loss: 2.4814666112923836

Epoch: 5| Step: 8
Training loss: 2.9079140441973075
Validation loss: 2.4756274841890784

Epoch: 5| Step: 9
Training loss: 2.3969949863844517
Validation loss: 2.4724817925148415

Epoch: 5| Step: 10
Training loss: 2.517313609643354
Validation loss: 2.498628172270631

Epoch: 87| Step: 0
Training loss: 2.6983157909624973
Validation loss: 2.483001362255791

Epoch: 5| Step: 1
Training loss: 2.654774244663869
Validation loss: 2.478322868318729

Epoch: 5| Step: 2
Training loss: 3.04581233346556
Validation loss: 2.48619210953087

Epoch: 5| Step: 3
Training loss: 2.790547891642962
Validation loss: 2.4727653175193334

Epoch: 5| Step: 4
Training loss: 2.763091225166355
Validation loss: 2.4744240160710715

Epoch: 5| Step: 5
Training loss: 2.5444981067760617
Validation loss: 2.482399077139052

Epoch: 5| Step: 6
Training loss: 2.6110049884504503
Validation loss: 2.481914820861371

Epoch: 5| Step: 7
Training loss: 2.3253739251143166
Validation loss: 2.4926843129028757

Epoch: 5| Step: 8
Training loss: 3.1490267445944924
Validation loss: 2.4813702177147525

Epoch: 5| Step: 9
Training loss: 2.704118579109739
Validation loss: 2.4927559620019744

Epoch: 5| Step: 10
Training loss: 2.4611873439332603
Validation loss: 2.4884815753275453

Epoch: 88| Step: 0
Training loss: 3.1497597012370817
Validation loss: 2.477804000943566

Epoch: 5| Step: 1
Training loss: 2.977681908163398
Validation loss: 2.4790540259246465

Epoch: 5| Step: 2
Training loss: 2.591111921283398
Validation loss: 2.4822090403453823

Epoch: 5| Step: 3
Training loss: 2.5256367362131327
Validation loss: 2.477963539707252

Epoch: 5| Step: 4
Training loss: 2.466686207032346
Validation loss: 2.4807904744813083

Epoch: 5| Step: 5
Training loss: 2.725083789062213
Validation loss: 2.5015525416816202

Epoch: 5| Step: 6
Training loss: 2.9138487872998873
Validation loss: 2.4882579927366435

Epoch: 5| Step: 7
Training loss: 2.5588641540676145
Validation loss: 2.4729237268822946

Epoch: 5| Step: 8
Training loss: 2.550054613164363
Validation loss: 2.487317487609006

Epoch: 5| Step: 9
Training loss: 2.667430370962951
Validation loss: 2.4971628743933065

Epoch: 5| Step: 10
Training loss: 2.664772185273623
Validation loss: 2.4760697382038175

Epoch: 89| Step: 0
Training loss: 2.3527056681354055
Validation loss: 2.4786696745460786

Epoch: 5| Step: 1
Training loss: 2.812197011945746
Validation loss: 2.4840582838359095

Epoch: 5| Step: 2
Training loss: 2.221709198808117
Validation loss: 2.4876687459958906

Epoch: 5| Step: 3
Training loss: 2.7088135415842243
Validation loss: 2.4841115879870315

Epoch: 5| Step: 4
Training loss: 2.581907329961758
Validation loss: 2.4770014500587387

Epoch: 5| Step: 5
Training loss: 3.3467805622928304
Validation loss: 2.4771547658378754

Epoch: 5| Step: 6
Training loss: 3.1265852912556364
Validation loss: 2.480044780139025

Epoch: 5| Step: 7
Training loss: 2.650489035053525
Validation loss: 2.488981193609763

Epoch: 5| Step: 8
Training loss: 2.8803712179103425
Validation loss: 2.481911062031592

Epoch: 5| Step: 9
Training loss: 1.990997078464727
Validation loss: 2.485324692806025

Epoch: 5| Step: 10
Training loss: 3.039679535739364
Validation loss: 2.4707433187968677

Epoch: 90| Step: 0
Training loss: 2.6364376116033372
Validation loss: 2.4823902715763815

Epoch: 5| Step: 1
Training loss: 2.656361476578534
Validation loss: 2.4672866810601586

Epoch: 5| Step: 2
Training loss: 2.6045994716842644
Validation loss: 2.4953635774057448

Epoch: 5| Step: 3
Training loss: 2.567872440028051
Validation loss: 2.4890852498689813

Epoch: 5| Step: 4
Training loss: 2.7240590842136867
Validation loss: 2.4770630356260868

Epoch: 5| Step: 5
Training loss: 2.7073660859499684
Validation loss: 2.477037810619512

Epoch: 5| Step: 6
Training loss: 3.0654964190630016
Validation loss: 2.4601906789959442

Epoch: 5| Step: 7
Training loss: 2.7139518754778367
Validation loss: 2.4883508365849796

Epoch: 5| Step: 8
Training loss: 2.57546573241833
Validation loss: 2.4705271722962703

Epoch: 5| Step: 9
Training loss: 3.308104567861962
Validation loss: 2.4907967316155064

Epoch: 5| Step: 10
Training loss: 1.7821260272371808
Validation loss: 2.476519343766582

Epoch: 91| Step: 0
Training loss: 2.8660034728471753
Validation loss: 2.488967905596094

Epoch: 5| Step: 1
Training loss: 2.649212029104614
Validation loss: 2.4798625215243715

Epoch: 5| Step: 2
Training loss: 2.574573822308483
Validation loss: 2.4683649955285802

Epoch: 5| Step: 3
Training loss: 2.8410704008602186
Validation loss: 2.4921274444434753

Epoch: 5| Step: 4
Training loss: 2.210992819158038
Validation loss: 2.487209868375853

Epoch: 5| Step: 5
Training loss: 2.5404723962651716
Validation loss: 2.467789409730925

Epoch: 5| Step: 6
Training loss: 2.947529168603437
Validation loss: 2.497511053378153

Epoch: 5| Step: 7
Training loss: 2.540161270668825
Validation loss: 2.482536190586097

Epoch: 5| Step: 8
Training loss: 2.64925378693586
Validation loss: 2.4943439531398925

Epoch: 5| Step: 9
Training loss: 3.3482644097345595
Validation loss: 2.4886741761311404

Epoch: 5| Step: 10
Training loss: 2.5002154257464255
Validation loss: 2.4893028652895506

Epoch: 92| Step: 0
Training loss: 2.563117255324984
Validation loss: 2.4896406930054873

Epoch: 5| Step: 1
Training loss: 2.6502514593910047
Validation loss: 2.4852152821146247

Epoch: 5| Step: 2
Training loss: 2.640670324534742
Validation loss: 2.4780062662769247

Epoch: 5| Step: 3
Training loss: 2.6851263875357927
Validation loss: 2.489734025361594

Epoch: 5| Step: 4
Training loss: 3.2697602149704066
Validation loss: 2.476849946461821

Epoch: 5| Step: 5
Training loss: 2.716897969452665
Validation loss: 2.476039624374569

Epoch: 5| Step: 6
Training loss: 2.880864175048967
Validation loss: 2.4885896654613635

Epoch: 5| Step: 7
Training loss: 2.9216156706754277
Validation loss: 2.4790353951393715

Epoch: 5| Step: 8
Training loss: 2.2183996649306135
Validation loss: 2.485869677463256

Epoch: 5| Step: 9
Training loss: 2.577343631592904
Validation loss: 2.4946451612743594

Epoch: 5| Step: 10
Training loss: 2.534671025797015
Validation loss: 2.474082480113008

Epoch: 93| Step: 0
Training loss: 2.8203826641960412
Validation loss: 2.475351994980536

Epoch: 5| Step: 1
Training loss: 3.1397454847462356
Validation loss: 2.486017161311435

Epoch: 5| Step: 2
Training loss: 2.702595300537983
Validation loss: 2.4992264699433138

Epoch: 5| Step: 3
Training loss: 3.040371257548661
Validation loss: 2.483231931437038

Epoch: 5| Step: 4
Training loss: 2.0715504525882076
Validation loss: 2.4863068621823987

Epoch: 5| Step: 5
Training loss: 2.9448256355829994
Validation loss: 2.477051162114086

Epoch: 5| Step: 6
Training loss: 2.7132255287061566
Validation loss: 2.487292446992733

Epoch: 5| Step: 7
Training loss: 2.276931646890037
Validation loss: 2.480773859473878

Epoch: 5| Step: 8
Training loss: 2.389557743572937
Validation loss: 2.4918530463777824

Epoch: 5| Step: 9
Training loss: 2.6942679772373244
Validation loss: 2.4830171766484703

Epoch: 5| Step: 10
Training loss: 2.7343363731925736
Validation loss: 2.4800005789336654

Epoch: 94| Step: 0
Training loss: 2.480612346667838
Validation loss: 2.4816798236594875

Epoch: 5| Step: 1
Training loss: 2.559405994344054
Validation loss: 2.483114556272735

Epoch: 5| Step: 2
Training loss: 2.957590269515293
Validation loss: 2.4624151166194066

Epoch: 5| Step: 3
Training loss: 2.286203727942184
Validation loss: 2.4968897083722035

Epoch: 5| Step: 4
Training loss: 2.79020278658577
Validation loss: 2.4808567070904552

Epoch: 5| Step: 5
Training loss: 2.9254554393962433
Validation loss: 2.498724158633062

Epoch: 5| Step: 6
Training loss: 2.7634867360138555
Validation loss: 2.496520842470104

Epoch: 5| Step: 7
Training loss: 2.1556529448598307
Validation loss: 2.4864643021190957

Epoch: 5| Step: 8
Training loss: 2.8918182384624727
Validation loss: 2.4875289906233466

Epoch: 5| Step: 9
Training loss: 2.83392413842341
Validation loss: 2.468726393105333

Epoch: 5| Step: 10
Training loss: 2.8267612435905125
Validation loss: 2.485169065878732

Epoch: 95| Step: 0
Training loss: 2.314132732636625
Validation loss: 2.4857667943215587

Epoch: 5| Step: 1
Training loss: 2.5284470474464604
Validation loss: 2.4855043353623025

Epoch: 5| Step: 2
Training loss: 2.5406402850189806
Validation loss: 2.4959768204703914

Epoch: 5| Step: 3
Training loss: 2.505573354499491
Validation loss: 2.4881388831072306

Epoch: 5| Step: 4
Training loss: 2.645244012116174
Validation loss: 2.490011880012503

Epoch: 5| Step: 5
Training loss: 3.4914885980058536
Validation loss: 2.48667887879686

Epoch: 5| Step: 6
Training loss: 3.2517174804356546
Validation loss: 2.485010726305391

Epoch: 5| Step: 7
Training loss: 2.4327642946851373
Validation loss: 2.468361851169062

Epoch: 5| Step: 8
Training loss: 2.620474138091147
Validation loss: 2.480818076353029

Epoch: 5| Step: 9
Training loss: 2.2887353240146258
Validation loss: 2.4826071685945346

Epoch: 5| Step: 10
Training loss: 2.823476269048905
Validation loss: 2.479672292919156

Epoch: 96| Step: 0
Training loss: 3.0124563063622514
Validation loss: 2.475651198212998

Epoch: 5| Step: 1
Training loss: 2.6485105071231176
Validation loss: 2.4708230217110803

Epoch: 5| Step: 2
Training loss: 2.500015544843029
Validation loss: 2.4852827120024386

Epoch: 5| Step: 3
Training loss: 2.690609131521468
Validation loss: 2.4753273030000895

Epoch: 5| Step: 4
Training loss: 2.9234311452905613
Validation loss: 2.4784234346873943

Epoch: 5| Step: 5
Training loss: 2.480680585811733
Validation loss: 2.4976304879323648

Epoch: 5| Step: 6
Training loss: 2.3601634748058484
Validation loss: 2.4787561006519403

Epoch: 5| Step: 7
Training loss: 3.0595636108987407
Validation loss: 2.5013927835668124

Epoch: 5| Step: 8
Training loss: 2.5383872674867343
Validation loss: 2.4914365373728384

Epoch: 5| Step: 9
Training loss: 2.71676458014594
Validation loss: 2.4857127191491935

Epoch: 5| Step: 10
Training loss: 2.685553799706981
Validation loss: 2.49954907237062

Epoch: 97| Step: 0
Training loss: 2.4852540961979033
Validation loss: 2.482646164923002

Epoch: 5| Step: 1
Training loss: 1.821587766641275
Validation loss: 2.4902322645699204

Epoch: 5| Step: 2
Training loss: 2.6327029242196742
Validation loss: 2.492840733533588

Epoch: 5| Step: 3
Training loss: 2.8450901101696853
Validation loss: 2.4886430002918507

Epoch: 5| Step: 4
Training loss: 2.9180481681586983
Validation loss: 2.478932306951443

Epoch: 5| Step: 5
Training loss: 3.1330045798152857
Validation loss: 2.4838220330867475

Epoch: 5| Step: 6
Training loss: 2.6447178649421272
Validation loss: 2.4694704204289573

Epoch: 5| Step: 7
Training loss: 2.502173242110289
Validation loss: 2.487287318247631

Epoch: 5| Step: 8
Training loss: 2.781462843390833
Validation loss: 2.487037780067579

Epoch: 5| Step: 9
Training loss: 3.0382100849051783
Validation loss: 2.483461541897023

Epoch: 5| Step: 10
Training loss: 2.5747189303578395
Validation loss: 2.4987064696518693

Epoch: 98| Step: 0
Training loss: 2.5430977549843123
Validation loss: 2.479454676987725

Epoch: 5| Step: 1
Training loss: 2.3416920082467754
Validation loss: 2.4742047956633275

Epoch: 5| Step: 2
Training loss: 2.7471055490652483
Validation loss: 2.4914906400871644

Epoch: 5| Step: 3
Training loss: 2.1689949974610894
Validation loss: 2.4814837614603977

Epoch: 5| Step: 4
Training loss: 2.587130821538862
Validation loss: 2.4964129350273128

Epoch: 5| Step: 5
Training loss: 2.579842475103877
Validation loss: 2.4854542451923027

Epoch: 5| Step: 6
Training loss: 2.6994431839863857
Validation loss: 2.493146875970676

Epoch: 5| Step: 7
Training loss: 2.585871116500655
Validation loss: 2.4864650465288425

Epoch: 5| Step: 8
Training loss: 3.2739598260371006
Validation loss: 2.4638403082549987

Epoch: 5| Step: 9
Training loss: 2.6643268532354627
Validation loss: 2.4839019593336467

Epoch: 5| Step: 10
Training loss: 3.233478219105955
Validation loss: 2.472624041715881

Epoch: 99| Step: 0
Training loss: 2.494909539885792
Validation loss: 2.4676425111741422

Epoch: 5| Step: 1
Training loss: 2.915742855000141
Validation loss: 2.493197719024391

Epoch: 5| Step: 2
Training loss: 2.7444519952351856
Validation loss: 2.4544718242836914

Epoch: 5| Step: 3
Training loss: 3.0512860572869194
Validation loss: 2.492689415116626

Epoch: 5| Step: 4
Training loss: 2.860544934676472
Validation loss: 2.480538609491252

Epoch: 5| Step: 5
Training loss: 2.454971201896742
Validation loss: 2.4577147336612972

Epoch: 5| Step: 6
Training loss: 2.3096727150923972
Validation loss: 2.46643851145741

Epoch: 5| Step: 7
Training loss: 2.4281104756317324
Validation loss: 2.4742833996578626

Epoch: 5| Step: 8
Training loss: 2.6028626088753035
Validation loss: 2.4826610780376375

Epoch: 5| Step: 9
Training loss: 3.143639176161517
Validation loss: 2.4817156890419643

Epoch: 5| Step: 10
Training loss: 2.401406702412281
Validation loss: 2.4894030957070306

Epoch: 100| Step: 0
Training loss: 2.542412622351309
Validation loss: 2.493327043808462

Epoch: 5| Step: 1
Training loss: 2.4801952782011893
Validation loss: 2.4737551829288393

Epoch: 5| Step: 2
Training loss: 2.3926672097573345
Validation loss: 2.488285979006954

Epoch: 5| Step: 3
Training loss: 2.7695340047588557
Validation loss: 2.4824383168416553

Epoch: 5| Step: 4
Training loss: 2.867780660448576
Validation loss: 2.484968025257822

Epoch: 5| Step: 5
Training loss: 2.8738737802947987
Validation loss: 2.478864649507829

Epoch: 5| Step: 6
Training loss: 2.5696940968060122
Validation loss: 2.490706292254422

Epoch: 5| Step: 7
Training loss: 3.1262154313608663
Validation loss: 2.481373238658827

Epoch: 5| Step: 8
Training loss: 2.7173152075354214
Validation loss: 2.487606674288138

Epoch: 5| Step: 9
Training loss: 2.553205240919402
Validation loss: 2.4865557762418136

Epoch: 5| Step: 10
Training loss: 2.589373745854668
Validation loss: 2.4910015800439678

Epoch: 101| Step: 0
Training loss: 2.250310028757127
Validation loss: 2.487407025719497

Epoch: 5| Step: 1
Training loss: 2.5638955200796545
Validation loss: 2.501512852657038

Epoch: 5| Step: 2
Training loss: 2.4863502759164002
Validation loss: 2.4879868995356027

Epoch: 5| Step: 3
Training loss: 2.6665013579946852
Validation loss: 2.4811178060855017

Epoch: 5| Step: 4
Training loss: 2.868089082002127
Validation loss: 2.4857374312116334

Epoch: 5| Step: 5
Training loss: 2.8865816447692842
Validation loss: 2.4858214871008015

Epoch: 5| Step: 6
Training loss: 3.0055814320814944
Validation loss: 2.4824435846933954

Epoch: 5| Step: 7
Training loss: 2.7864897158748003
Validation loss: 2.482798400048758

Epoch: 5| Step: 8
Training loss: 2.6158116610154756
Validation loss: 2.4868335370488084

Epoch: 5| Step: 9
Training loss: 2.4623211558503777
Validation loss: 2.4812883264130465

Epoch: 5| Step: 10
Training loss: 3.0061740921176106
Validation loss: 2.486172264941731

Epoch: 102| Step: 0
Training loss: 2.6886530332225167
Validation loss: 2.4728450252403786

Epoch: 5| Step: 1
Training loss: 2.6497308954319845
Validation loss: 2.4829521250267517

Epoch: 5| Step: 2
Training loss: 2.599549760715316
Validation loss: 2.483036431106029

Epoch: 5| Step: 3
Training loss: 2.733391023430236
Validation loss: 2.4918997713543214

Epoch: 5| Step: 4
Training loss: 2.953934170102446
Validation loss: 2.4768595588666487

Epoch: 5| Step: 5
Training loss: 2.488512444647265
Validation loss: 2.4814002585877346

Epoch: 5| Step: 6
Training loss: 2.5769578545957623
Validation loss: 2.4752096746938963

Epoch: 5| Step: 7
Training loss: 2.75294796117893
Validation loss: 2.4790727020708676

Epoch: 5| Step: 8
Training loss: 2.8495831034065695
Validation loss: 2.4787436917516343

Epoch: 5| Step: 9
Training loss: 2.684538606891484
Validation loss: 2.4803580998009647

Epoch: 5| Step: 10
Training loss: 2.4921405273602795
Validation loss: 2.4801674492295507

Epoch: 103| Step: 0
Training loss: 2.7740648876376297
Validation loss: 2.4672698421162966

Epoch: 5| Step: 1
Training loss: 2.1456242678624635
Validation loss: 2.47922966817496

Epoch: 5| Step: 2
Training loss: 1.9658798610500783
Validation loss: 2.467963233900065

Epoch: 5| Step: 3
Training loss: 3.0054902066287146
Validation loss: 2.480848546561337

Epoch: 5| Step: 4
Training loss: 2.68852719925411
Validation loss: 2.4873904744910442

Epoch: 5| Step: 5
Training loss: 3.1407778878267756
Validation loss: 2.4972244729657493

Epoch: 5| Step: 6
Training loss: 2.0371776068315994
Validation loss: 2.4844555760790588

Epoch: 5| Step: 7
Training loss: 2.8887778489055105
Validation loss: 2.5002124193659414

Epoch: 5| Step: 8
Training loss: 2.4113950343437542
Validation loss: 2.482002861996284

Epoch: 5| Step: 9
Training loss: 3.2247306297235654
Validation loss: 2.4974665366750943

Epoch: 5| Step: 10
Training loss: 2.900971079069752
Validation loss: 2.481471264438448

Epoch: 104| Step: 0
Training loss: 2.752170053329083
Validation loss: 2.4721589991936495

Epoch: 5| Step: 1
Training loss: 2.798953940630978
Validation loss: 2.4853194475652622

Epoch: 5| Step: 2
Training loss: 3.0701312528504388
Validation loss: 2.483036846155698

Epoch: 5| Step: 3
Training loss: 2.17187192628492
Validation loss: 2.493504394925929

Epoch: 5| Step: 4
Training loss: 2.4397914581607103
Validation loss: 2.4645354232463417

Epoch: 5| Step: 5
Training loss: 3.000220449613137
Validation loss: 2.4812572985101338

Epoch: 5| Step: 6
Training loss: 3.007403774481996
Validation loss: 2.4852232653196027

Epoch: 5| Step: 7
Training loss: 2.934012107631714
Validation loss: 2.4937339045922973

Epoch: 5| Step: 8
Training loss: 2.4680482856997865
Validation loss: 2.5002602523783373

Epoch: 5| Step: 9
Training loss: 1.83545610325591
Validation loss: 2.485983531837135

Epoch: 5| Step: 10
Training loss: 2.612038446566971
Validation loss: 2.48381394835704

Epoch: 105| Step: 0
Training loss: 3.525627588988978
Validation loss: 2.4899919726891455

Epoch: 5| Step: 1
Training loss: 2.266270302262345
Validation loss: 2.4913483593512975

Epoch: 5| Step: 2
Training loss: 2.530469515059268
Validation loss: 2.490054166652485

Epoch: 5| Step: 3
Training loss: 2.63413139836967
Validation loss: 2.4783754724402285

Epoch: 5| Step: 4
Training loss: 2.6262944980575473
Validation loss: 2.4760280736540725

Epoch: 5| Step: 5
Training loss: 3.050755304087012
Validation loss: 2.458800085053008

Epoch: 5| Step: 6
Training loss: 3.016045259434614
Validation loss: 2.475651540977355

Epoch: 5| Step: 7
Training loss: 2.051904686705197
Validation loss: 2.488387127119882

Epoch: 5| Step: 8
Training loss: 2.5567875900750376
Validation loss: 2.459225651341686

Epoch: 5| Step: 9
Training loss: 2.389543375902309
Validation loss: 2.4834063646069526

Epoch: 5| Step: 10
Training loss: 2.258385819767139
Validation loss: 2.4736477799542875

Epoch: 106| Step: 0
Training loss: 2.8868936736053574
Validation loss: 2.4811701722003328

Epoch: 5| Step: 1
Training loss: 2.5947934545150675
Validation loss: 2.4940860550335295

Epoch: 5| Step: 2
Training loss: 2.642517675595098
Validation loss: 2.481238970467645

Epoch: 5| Step: 3
Training loss: 2.8980074252394807
Validation loss: 2.487118012187697

Epoch: 5| Step: 4
Training loss: 2.0882274616476986
Validation loss: 2.4865910083776144

Epoch: 5| Step: 5
Training loss: 2.696782869288088
Validation loss: 2.4835082203635506

Epoch: 5| Step: 6
Training loss: 3.028650958952608
Validation loss: 2.4638653988890415

Epoch: 5| Step: 7
Training loss: 2.9360513564563244
Validation loss: 2.4742962153235637

Epoch: 5| Step: 8
Training loss: 2.707606310954314
Validation loss: 2.4873711125368536

Epoch: 5| Step: 9
Training loss: 1.9365034924748232
Validation loss: 2.4831095056226506

Epoch: 5| Step: 10
Training loss: 2.7320609628066843
Validation loss: 2.488035717708417

Epoch: 107| Step: 0
Training loss: 2.9618070025264718
Validation loss: 2.4720696782865392

Epoch: 5| Step: 1
Training loss: 2.5161228990361737
Validation loss: 2.4913572407840374

Epoch: 5| Step: 2
Training loss: 3.205158909088179
Validation loss: 2.494551509608689

Epoch: 5| Step: 3
Training loss: 2.166598270021387
Validation loss: 2.479370587232247

Epoch: 5| Step: 4
Training loss: 2.560602555589772
Validation loss: 2.4854149597567545

Epoch: 5| Step: 5
Training loss: 2.704070350408979
Validation loss: 2.4728526777555495

Epoch: 5| Step: 6
Training loss: 2.280786231988665
Validation loss: 2.4889438024146675

Epoch: 5| Step: 7
Training loss: 2.9778844746252107
Validation loss: 2.4933526273672624

Epoch: 5| Step: 8
Training loss: 2.6188544131929135
Validation loss: 2.4784807787010172

Epoch: 5| Step: 9
Training loss: 1.923143232009349
Validation loss: 2.4724887975943735

Epoch: 5| Step: 10
Training loss: 3.097601140844119
Validation loss: 2.490673036941056

Epoch: 108| Step: 0
Training loss: 2.604335006359186
Validation loss: 2.4766976841817248

Epoch: 5| Step: 1
Training loss: 2.5565333799245527
Validation loss: 2.4984537572727543

Epoch: 5| Step: 2
Training loss: 2.87632206912312
Validation loss: 2.4855048923381116

Epoch: 5| Step: 3
Training loss: 2.7817407239408047
Validation loss: 2.478700241494531

Epoch: 5| Step: 4
Training loss: 2.816238546470924
Validation loss: 2.4744148790806606

Epoch: 5| Step: 5
Training loss: 1.8592566444490601
Validation loss: 2.480940462824716

Epoch: 5| Step: 6
Training loss: 3.089566880274714
Validation loss: 2.47816436855896

Epoch: 5| Step: 7
Training loss: 1.9422938814129782
Validation loss: 2.4864327768834733

Epoch: 5| Step: 8
Training loss: 2.9184287243613207
Validation loss: 2.4839747328246378

Epoch: 5| Step: 9
Training loss: 2.7899642042372683
Validation loss: 2.469016912612967

Epoch: 5| Step: 10
Training loss: 2.6875220009546403
Validation loss: 2.4740631678874823

Epoch: 109| Step: 0
Training loss: 2.66717696274395
Validation loss: 2.4662269429687336

Epoch: 5| Step: 1
Training loss: 2.6383889566880927
Validation loss: 2.491775493554752

Epoch: 5| Step: 2
Training loss: 2.5789648364281663
Validation loss: 2.4741699228438114

Epoch: 5| Step: 3
Training loss: 2.5106547283666028
Validation loss: 2.4754120033944176

Epoch: 5| Step: 4
Training loss: 3.1230148113386877
Validation loss: 2.4674779372543156

Epoch: 5| Step: 5
Training loss: 2.6616051043049582
Validation loss: 2.489697585588729

Epoch: 5| Step: 6
Training loss: 2.0920378103686863
Validation loss: 2.48215055287975

Epoch: 5| Step: 7
Training loss: 2.6826202981359377
Validation loss: 2.464840544463918

Epoch: 5| Step: 8
Training loss: 2.2172432739899035
Validation loss: 2.47540438312886

Epoch: 5| Step: 9
Training loss: 2.7531013340533628
Validation loss: 2.4821685602130668

Epoch: 5| Step: 10
Training loss: 3.3692197397866854
Validation loss: 2.472679120581888

Epoch: 110| Step: 0
Training loss: 2.781126298189407
Validation loss: 2.468221742648238

Epoch: 5| Step: 1
Training loss: 2.797228657617104
Validation loss: 2.485796823314497

Epoch: 5| Step: 2
Training loss: 2.473234232624862
Validation loss: 2.4599120978434157

Epoch: 5| Step: 3
Training loss: 2.5331102293888867
Validation loss: 2.497837614277732

Epoch: 5| Step: 4
Training loss: 2.4146150123376597
Validation loss: 2.4753156673956633

Epoch: 5| Step: 5
Training loss: 2.923900860714353
Validation loss: 2.4695521648433942

Epoch: 5| Step: 6
Training loss: 2.983883003303713
Validation loss: 2.4933247056789525

Epoch: 5| Step: 7
Training loss: 1.588540857867259
Validation loss: 2.491722528260359

Epoch: 5| Step: 8
Training loss: 3.136745809055921
Validation loss: 2.4847739728634264

Epoch: 5| Step: 9
Training loss: 2.7355608303395553
Validation loss: 2.478988170187042

Epoch: 5| Step: 10
Training loss: 2.6216196365487106
Validation loss: 2.4875384545802537

Epoch: 111| Step: 0
Training loss: 2.529993098404107
Validation loss: 2.481293900463247

Epoch: 5| Step: 1
Training loss: 2.7347962845106832
Validation loss: 2.477846703364

Epoch: 5| Step: 2
Training loss: 2.981046888042195
Validation loss: 2.490966524529563

Epoch: 5| Step: 3
Training loss: 2.5489538402904905
Validation loss: 2.4838644761958353

Epoch: 5| Step: 4
Training loss: 2.3630859294285687
Validation loss: 2.4941162583058514

Epoch: 5| Step: 5
Training loss: 2.1294123896615567
Validation loss: 2.462815924201352

Epoch: 5| Step: 6
Training loss: 2.6421662296497015
Validation loss: 2.4822336003529095

Epoch: 5| Step: 7
Training loss: 1.668684651670465
Validation loss: 2.4782604120169194

Epoch: 5| Step: 8
Training loss: 2.733108401541442
Validation loss: 2.495822634299026

Epoch: 5| Step: 9
Training loss: 3.013649248791035
Validation loss: 2.4674828308078656

Epoch: 5| Step: 10
Training loss: 3.463729799749309
Validation loss: 2.4681117706226385

Epoch: 112| Step: 0
Training loss: 2.6321743607784707
Validation loss: 2.494326095371947

Epoch: 5| Step: 1
Training loss: 2.302899821439363
Validation loss: 2.4860931489864293

Epoch: 5| Step: 2
Training loss: 2.2942674848262903
Validation loss: 2.476307410798122

Epoch: 5| Step: 3
Training loss: 2.624470702804501
Validation loss: 2.4917926792545115

Epoch: 5| Step: 4
Training loss: 3.142000570859317
Validation loss: 2.5014757466541653

Epoch: 5| Step: 5
Training loss: 2.195576716369109
Validation loss: 2.48513113866801

Epoch: 5| Step: 6
Training loss: 3.289758893949869
Validation loss: 2.4909113374928817

Epoch: 5| Step: 7
Training loss: 2.4304719989280357
Validation loss: 2.4730299536174765

Epoch: 5| Step: 8
Training loss: 2.4506326637060676
Validation loss: 2.4875134326780106

Epoch: 5| Step: 9
Training loss: 2.8166444548082823
Validation loss: 2.490148864088649

Epoch: 5| Step: 10
Training loss: 2.7709842631217714
Validation loss: 2.483233710228289

Epoch: 113| Step: 0
Training loss: 2.6129215831180095
Validation loss: 2.4875537660588085

Epoch: 5| Step: 1
Training loss: 2.3732949209676555
Validation loss: 2.501532538143497

Epoch: 5| Step: 2
Training loss: 2.3115151860066074
Validation loss: 2.4696122524203075

Epoch: 5| Step: 3
Training loss: 3.081978027503071
Validation loss: 2.4740130647176115

Epoch: 5| Step: 4
Training loss: 3.6060015614316314
Validation loss: 2.488523660815614

Epoch: 5| Step: 5
Training loss: 2.299402727445677
Validation loss: 2.4699554999640134

Epoch: 5| Step: 6
Training loss: 2.6214153700778646
Validation loss: 2.4724881371103686

Epoch: 5| Step: 7
Training loss: 2.543261720624904
Validation loss: 2.486128758902361

Epoch: 5| Step: 8
Training loss: 2.366957650183296
Validation loss: 2.4882608605620935

Epoch: 5| Step: 9
Training loss: 2.3047995329960007
Validation loss: 2.4761494104752826

Epoch: 5| Step: 10
Training loss: 2.8094152700149744
Validation loss: 2.485565646303753

Epoch: 114| Step: 0
Training loss: 2.803625304144407
Validation loss: 2.484588923408366

Epoch: 5| Step: 1
Training loss: 2.30916427043165
Validation loss: 2.4892793564984212

Epoch: 5| Step: 2
Training loss: 2.5519220625596906
Validation loss: 2.4988398772467075

Epoch: 5| Step: 3
Training loss: 2.859304625265289
Validation loss: 2.4779798745676858

Epoch: 5| Step: 4
Training loss: 2.8295464921317786
Validation loss: 2.479364507372505

Epoch: 5| Step: 5
Training loss: 2.8597818720939245
Validation loss: 2.493870022720959

Epoch: 5| Step: 6
Training loss: 2.972199533995868
Validation loss: 2.483708618428065

Epoch: 5| Step: 7
Training loss: 2.811778081854594
Validation loss: 2.4704623552710645

Epoch: 5| Step: 8
Training loss: 2.642589853828831
Validation loss: 2.4769561005543244

Epoch: 5| Step: 9
Training loss: 2.4382086481776617
Validation loss: 2.4817345827488237

Epoch: 5| Step: 10
Training loss: 1.8298497988861722
Validation loss: 2.468774610827429

Epoch: 115| Step: 0
Training loss: 2.211536339851306
Validation loss: 2.4761534638029334

Epoch: 5| Step: 1
Training loss: 2.2067814778465937
Validation loss: 2.4744430493736402

Epoch: 5| Step: 2
Training loss: 2.3803263717753986
Validation loss: 2.4829919852862212

Epoch: 5| Step: 3
Training loss: 2.589221540134974
Validation loss: 2.4830817423502

Epoch: 5| Step: 4
Training loss: 3.293154963503458
Validation loss: 2.4895439609938217

Epoch: 5| Step: 5
Training loss: 2.347779434196697
Validation loss: 2.4790284695725355

Epoch: 5| Step: 6
Training loss: 2.486720962835029
Validation loss: 2.4742212641338144

Epoch: 5| Step: 7
Training loss: 2.6005561417216776
Validation loss: 2.4768713541172165

Epoch: 5| Step: 8
Training loss: 2.916094278755461
Validation loss: 2.4827884161970113

Epoch: 5| Step: 9
Training loss: 3.171151214853341
Validation loss: 2.4633303711245174

Epoch: 5| Step: 10
Training loss: 2.635709080412211
Validation loss: 2.473059464495846

Epoch: 116| Step: 0
Training loss: 2.96367524733728
Validation loss: 2.467742312389283

Epoch: 5| Step: 1
Training loss: 2.468481701142326
Validation loss: 2.4860407754561487

Epoch: 5| Step: 2
Training loss: 2.413265355910856
Validation loss: 2.486860383224106

Epoch: 5| Step: 3
Training loss: 2.3916694503750255
Validation loss: 2.475872105695971

Epoch: 5| Step: 4
Training loss: 2.733406898237397
Validation loss: 2.492415327835712

Epoch: 5| Step: 5
Training loss: 2.191082772436433
Validation loss: 2.483522920817505

Epoch: 5| Step: 6
Training loss: 3.0995890683408542
Validation loss: 2.4951264798582304

Epoch: 5| Step: 7
Training loss: 2.601083155738941
Validation loss: 2.4724930487430865

Epoch: 5| Step: 8
Training loss: 2.3496060264354255
Validation loss: 2.488701505145631

Epoch: 5| Step: 9
Training loss: 2.861668735059799
Validation loss: 2.4751917353253687

Epoch: 5| Step: 10
Training loss: 2.863520551103875
Validation loss: 2.4888517022361007

Epoch: 117| Step: 0
Training loss: 2.8685469146184777
Validation loss: 2.4771581334437562

Epoch: 5| Step: 1
Training loss: 2.729047544563539
Validation loss: 2.4955497252965446

Epoch: 5| Step: 2
Training loss: 2.5543834260335356
Validation loss: 2.4733332412368565

Epoch: 5| Step: 3
Training loss: 3.121314049347377
Validation loss: 2.478317372926603

Epoch: 5| Step: 4
Training loss: 2.488692364947386
Validation loss: 2.492144067080879

Epoch: 5| Step: 5
Training loss: 2.252035068367656
Validation loss: 2.477564586617368

Epoch: 5| Step: 6
Training loss: 2.545769949049577
Validation loss: 2.4848498458627564

Epoch: 5| Step: 7
Training loss: 2.3897871157272763
Validation loss: 2.4785295958606723

Epoch: 5| Step: 8
Training loss: 2.566018271187173
Validation loss: 2.501073863007985

Epoch: 5| Step: 9
Training loss: 2.705593767838432
Validation loss: 2.487992983054696

Epoch: 5| Step: 10
Training loss: 2.5501559602503416
Validation loss: 2.491489632222045

Epoch: 118| Step: 0
Training loss: 2.279480339191425
Validation loss: 2.470972765884212

Epoch: 5| Step: 1
Training loss: 3.073627885314091
Validation loss: 2.4791799206964096

Epoch: 5| Step: 2
Training loss: 2.8810397851349214
Validation loss: 2.4724024748833466

Epoch: 5| Step: 3
Training loss: 1.8293020257708288
Validation loss: 2.4869251913896586

Epoch: 5| Step: 4
Training loss: 2.827540047359433
Validation loss: 2.491863169836968

Epoch: 5| Step: 5
Training loss: 2.213985646918964
Validation loss: 2.486597890199959

Epoch: 5| Step: 6
Training loss: 3.1488321591781934
Validation loss: 2.4689640945597353

Epoch: 5| Step: 7
Training loss: 2.916976984863715
Validation loss: 2.505331473404447

Epoch: 5| Step: 8
Training loss: 1.80364544000345
Validation loss: 2.4899609070422355

Epoch: 5| Step: 9
Training loss: 2.6140638243129772
Validation loss: 2.4768564206332746

Epoch: 5| Step: 10
Training loss: 3.022792540371645
Validation loss: 2.501748024573999

Epoch: 119| Step: 0
Training loss: 2.6274302903645412
Validation loss: 2.4948743800099207

Epoch: 5| Step: 1
Training loss: 2.4836768362925623
Validation loss: 2.472120634905086

Epoch: 5| Step: 2
Training loss: 3.4877859620471585
Validation loss: 2.490385440601136

Epoch: 5| Step: 3
Training loss: 2.2565219451087386
Validation loss: 2.4787436979571345

Epoch: 5| Step: 4
Training loss: 2.3344108160046804
Validation loss: 2.49598573781612

Epoch: 5| Step: 5
Training loss: 2.8146889539469018
Validation loss: 2.4734927242618654

Epoch: 5| Step: 6
Training loss: 2.5602687332545218
Validation loss: 2.478499077490925

Epoch: 5| Step: 7
Training loss: 2.9275092813945727
Validation loss: 2.4863820455312338

Epoch: 5| Step: 8
Training loss: 2.500014018973145
Validation loss: 2.484202674032655

Epoch: 5| Step: 9
Training loss: 2.1410588848028818
Validation loss: 2.4691727736920925

Epoch: 5| Step: 10
Training loss: 2.776313944925641
Validation loss: 2.4643150370859233

Epoch: 120| Step: 0
Training loss: 3.248731952659673
Validation loss: 2.4741856092492562

Epoch: 5| Step: 1
Training loss: 2.6257204247931702
Validation loss: 2.483685106231016

Epoch: 5| Step: 2
Training loss: 2.7693998792559995
Validation loss: 2.4833622681404237

Epoch: 5| Step: 3
Training loss: 2.792221545591963
Validation loss: 2.464008068529366

Epoch: 5| Step: 4
Training loss: 2.5218475340301203
Validation loss: 2.461533105631315

Epoch: 5| Step: 5
Training loss: 2.4290201429754488
Validation loss: 2.469733008633678

Epoch: 5| Step: 6
Training loss: 2.346564472241855
Validation loss: 2.482169089792974

Epoch: 5| Step: 7
Training loss: 2.2809517025247352
Validation loss: 2.48603397948725

Epoch: 5| Step: 8
Training loss: 2.7579226377285857
Validation loss: 2.493597069651786

Epoch: 5| Step: 9
Training loss: 2.181796458526319
Validation loss: 2.4935124215050157

Epoch: 5| Step: 10
Training loss: 2.973279048127617
Validation loss: 2.486687726390726

Epoch: 121| Step: 0
Training loss: 2.3255172564289284
Validation loss: 2.480593104915551

Epoch: 5| Step: 1
Training loss: 2.289297462877019
Validation loss: 2.473049622737905

Epoch: 5| Step: 2
Training loss: 3.019966123782508
Validation loss: 2.4798218717953024

Epoch: 5| Step: 3
Training loss: 2.749390794726436
Validation loss: 2.5013055110132663

Epoch: 5| Step: 4
Training loss: 2.1884207831189175
Validation loss: 2.4848628690846604

Epoch: 5| Step: 5
Training loss: 2.6924359437766263
Validation loss: 2.474773555993858

Epoch: 5| Step: 6
Training loss: 2.4787178167503625
Validation loss: 2.5052706727935847

Epoch: 5| Step: 7
Training loss: 2.544835122094361
Validation loss: 2.5020878933154127

Epoch: 5| Step: 8
Training loss: 2.5900413012065204
Validation loss: 2.479298138736615

Epoch: 5| Step: 9
Training loss: 3.149467508025197
Validation loss: 2.4860789040522797

Epoch: 5| Step: 10
Training loss: 2.7384558171721083
Validation loss: 2.5020904947748606

Epoch: 122| Step: 0
Training loss: 2.8416732079513602
Validation loss: 2.4895657815793784

Epoch: 5| Step: 1
Training loss: 3.0439543982459845
Validation loss: 2.486727174193589

Epoch: 5| Step: 2
Training loss: 2.321613088547758
Validation loss: 2.487786904063497

Epoch: 5| Step: 3
Training loss: 2.4781154252356803
Validation loss: 2.4852472013706084

Epoch: 5| Step: 4
Training loss: 2.570612742039795
Validation loss: 2.466630828054409

Epoch: 5| Step: 5
Training loss: 2.1472232230796746
Validation loss: 2.49390276254056

Epoch: 5| Step: 6
Training loss: 1.9080503748155766
Validation loss: 2.465301873089677

Epoch: 5| Step: 7
Training loss: 2.60062141695076
Validation loss: 2.4924103608332926

Epoch: 5| Step: 8
Training loss: 2.5983821456986362
Validation loss: 2.4835977389061448

Epoch: 5| Step: 9
Training loss: 2.819444179273905
Validation loss: 2.4743931517849433

Epoch: 5| Step: 10
Training loss: 3.328364547203631
Validation loss: 2.4648306095637658

Epoch: 123| Step: 0
Training loss: 2.341772440448003
Validation loss: 2.4849701030189957

Epoch: 5| Step: 1
Training loss: 3.1294874396827086
Validation loss: 2.4833015170484862

Epoch: 5| Step: 2
Training loss: 2.687013094064817
Validation loss: 2.4919202256657313

Epoch: 5| Step: 3
Training loss: 2.571935633967817
Validation loss: 2.483096533086137

Epoch: 5| Step: 4
Training loss: 1.9440855270221404
Validation loss: 2.48390660275703

Epoch: 5| Step: 5
Training loss: 3.203943808502195
Validation loss: 2.472546493353764

Epoch: 5| Step: 6
Training loss: 2.7300543081729582
Validation loss: 2.496165646318344

Epoch: 5| Step: 7
Training loss: 3.014170716015542
Validation loss: 2.5032901694213243

Epoch: 5| Step: 8
Training loss: 2.744079285068436
Validation loss: 2.4875280950341407

Epoch: 5| Step: 9
Training loss: 2.427344956165968
Validation loss: 2.4907357120650793

Epoch: 5| Step: 10
Training loss: 1.5691688130519865
Validation loss: 2.475744192274459

Epoch: 124| Step: 0
Training loss: 2.3290566238130603
Validation loss: 2.4939146406287693

Epoch: 5| Step: 1
Training loss: 2.591265948145766
Validation loss: 2.4793548395376206

Epoch: 5| Step: 2
Training loss: 2.6467877766583343
Validation loss: 2.4921422637887987

Epoch: 5| Step: 3
Training loss: 2.236790345245612
Validation loss: 2.4825963743802264

Epoch: 5| Step: 4
Training loss: 2.3373001870008165
Validation loss: 2.4911154715544526

Epoch: 5| Step: 5
Training loss: 2.876787542173592
Validation loss: 2.4858766726610018

Epoch: 5| Step: 6
Training loss: 2.343238673064029
Validation loss: 2.49440792144185

Epoch: 5| Step: 7
Training loss: 2.874478831944645
Validation loss: 2.484417481129201

Epoch: 5| Step: 8
Training loss: 2.370517918456047
Validation loss: 2.5068954832118693

Epoch: 5| Step: 9
Training loss: 2.674839004235186
Validation loss: 2.493470752308351

Epoch: 5| Step: 10
Training loss: 3.4577196028581825
Validation loss: 2.48883629779705

Epoch: 125| Step: 0
Training loss: 2.773520283068
Validation loss: 2.46991429691758

Epoch: 5| Step: 1
Training loss: 2.8166516497360288
Validation loss: 2.507875900890624

Epoch: 5| Step: 2
Training loss: 2.4156428722593923
Validation loss: 2.46913054734111

Epoch: 5| Step: 3
Training loss: 2.7741017580407616
Validation loss: 2.4980452334997603

Epoch: 5| Step: 4
Training loss: 2.57835821773042
Validation loss: 2.4890262338966584

Epoch: 5| Step: 5
Training loss: 2.632176625241647
Validation loss: 2.468068622949058

Epoch: 5| Step: 6
Training loss: 2.3265840692077533
Validation loss: 2.497568599135701

Epoch: 5| Step: 7
Training loss: 2.4262012849031556
Validation loss: 2.4766401172173786

Epoch: 5| Step: 8
Training loss: 2.9442750434208818
Validation loss: 2.485217090432975

Epoch: 5| Step: 9
Training loss: 2.2630210908656236
Validation loss: 2.4662441003610986

Epoch: 5| Step: 10
Training loss: 2.8281584679746654
Validation loss: 2.475680251207088

Testing loss: 2.5059083421917383

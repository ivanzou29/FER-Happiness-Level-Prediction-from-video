Epoch: 1| Step: 0
Training loss: 4.277166366577148
Validation loss: 4.770531469775785

Epoch: 6| Step: 1
Training loss: 4.811233997344971
Validation loss: 4.764813971775834

Epoch: 6| Step: 2
Training loss: 4.743266582489014
Validation loss: 4.761290391286214

Epoch: 6| Step: 3
Training loss: 5.754011154174805
Validation loss: 4.758573003994521

Epoch: 6| Step: 4
Training loss: 5.26784086227417
Validation loss: 4.751555740192372

Epoch: 6| Step: 5
Training loss: 3.2886602878570557
Validation loss: 4.749509065381942

Epoch: 6| Step: 6
Training loss: 5.8477678298950195
Validation loss: 4.748203877479799

Epoch: 6| Step: 7
Training loss: 4.411466598510742
Validation loss: 4.745090107763967

Epoch: 6| Step: 8
Training loss: 3.5521657466888428
Validation loss: 4.7357632216586865

Epoch: 6| Step: 9
Training loss: 3.3855528831481934
Validation loss: 4.730709768110706

Epoch: 6| Step: 10
Training loss: 5.034436225891113
Validation loss: 4.72890547783144

Epoch: 6| Step: 11
Training loss: 3.9173429012298584
Validation loss: 4.722523679015457

Epoch: 6| Step: 12
Training loss: 4.859184265136719
Validation loss: 4.718455022381198

Epoch: 6| Step: 13
Training loss: 4.169574737548828
Validation loss: 4.7178262587516535

Epoch: 2| Step: 0
Training loss: 4.339848518371582
Validation loss: 4.712223242687923

Epoch: 6| Step: 1
Training loss: 5.7119975090026855
Validation loss: 4.708375505221787

Epoch: 6| Step: 2
Training loss: 4.759389877319336
Validation loss: 4.70291438153995

Epoch: 6| Step: 3
Training loss: 4.427735328674316
Validation loss: 4.697064189500706

Epoch: 6| Step: 4
Training loss: 4.40927791595459
Validation loss: 4.6941484020602315

Epoch: 6| Step: 5
Training loss: 3.907176971435547
Validation loss: 4.689229411463583

Epoch: 6| Step: 6
Training loss: 3.438849687576294
Validation loss: 4.687091483864733

Epoch: 6| Step: 7
Training loss: 4.843759536743164
Validation loss: 4.678870319038309

Epoch: 6| Step: 8
Training loss: 3.7015609741210938
Validation loss: 4.679096314214891

Epoch: 6| Step: 9
Training loss: 6.064113616943359
Validation loss: 4.672929394629694

Epoch: 6| Step: 10
Training loss: 4.88995361328125
Validation loss: 4.665949472817042

Epoch: 6| Step: 11
Training loss: 3.7831685543060303
Validation loss: 4.660883272847822

Epoch: 6| Step: 12
Training loss: 4.422023773193359
Validation loss: 4.65337465142691

Epoch: 6| Step: 13
Training loss: 3.4798879623413086
Validation loss: 4.651878223624281

Epoch: 3| Step: 0
Training loss: 3.6473746299743652
Validation loss: 4.645755578112858

Epoch: 6| Step: 1
Training loss: 4.043885707855225
Validation loss: 4.640311482132122

Epoch: 6| Step: 2
Training loss: 4.949858665466309
Validation loss: 4.63545746444374

Epoch: 6| Step: 3
Training loss: 2.881718635559082
Validation loss: 4.630386726830595

Epoch: 6| Step: 4
Training loss: 5.257094383239746
Validation loss: 4.630160003580073

Epoch: 6| Step: 5
Training loss: 4.6047515869140625
Validation loss: 4.621092324615807

Epoch: 6| Step: 6
Training loss: 3.851062774658203
Validation loss: 4.612005879802089

Epoch: 6| Step: 7
Training loss: 3.844076633453369
Validation loss: 4.613649224722257

Epoch: 6| Step: 8
Training loss: 5.347238540649414
Validation loss: 4.605357405959919

Epoch: 6| Step: 9
Training loss: 5.301712512969971
Validation loss: 4.598719114898353

Epoch: 6| Step: 10
Training loss: 3.358055591583252
Validation loss: 4.5905676503335275

Epoch: 6| Step: 11
Training loss: 5.275299072265625
Validation loss: 4.589845667603195

Epoch: 6| Step: 12
Training loss: 5.603950500488281
Validation loss: 4.5851353470997145

Epoch: 6| Step: 13
Training loss: 3.1300485134124756
Validation loss: 4.574333011463124

Epoch: 4| Step: 0
Training loss: 4.981292724609375
Validation loss: 4.577005540170977

Epoch: 6| Step: 1
Training loss: 3.217390298843384
Validation loss: 4.566629014989381

Epoch: 6| Step: 2
Training loss: 4.614548683166504
Validation loss: 4.561122925050797

Epoch: 6| Step: 3
Training loss: 5.216650009155273
Validation loss: 4.554629228448355

Epoch: 6| Step: 4
Training loss: 4.59166145324707
Validation loss: 4.5434694033797065

Epoch: 6| Step: 5
Training loss: 4.120329856872559
Validation loss: 4.543627113424321

Epoch: 6| Step: 6
Training loss: 4.846966743469238
Validation loss: 4.533343192069761

Epoch: 6| Step: 7
Training loss: 5.065877914428711
Validation loss: 4.53340950832572

Epoch: 6| Step: 8
Training loss: 4.247135162353516
Validation loss: 4.52361511927779

Epoch: 6| Step: 9
Training loss: 3.3569560050964355
Validation loss: 4.5189654493844635

Epoch: 6| Step: 10
Training loss: 3.0831925868988037
Validation loss: 4.510771500167026

Epoch: 6| Step: 11
Training loss: 3.427304744720459
Validation loss: 4.504556153410224

Epoch: 6| Step: 12
Training loss: 5.2307233810424805
Validation loss: 4.501411053442186

Epoch: 6| Step: 13
Training loss: 4.773610591888428
Validation loss: 4.492319548001853

Epoch: 5| Step: 0
Training loss: 5.010428428649902
Validation loss: 4.482223233868999

Epoch: 6| Step: 1
Training loss: 4.684755802154541
Validation loss: 4.477958135707404

Epoch: 6| Step: 2
Training loss: 4.855906963348389
Validation loss: 4.473551914256106

Epoch: 6| Step: 3
Training loss: 4.329717636108398
Validation loss: 4.472290095462594

Epoch: 6| Step: 4
Training loss: 5.077780723571777
Validation loss: 4.4589451615528395

Epoch: 6| Step: 5
Training loss: 3.6906259059906006
Validation loss: 4.457761364598428

Epoch: 6| Step: 6
Training loss: 4.454015731811523
Validation loss: 4.4514499120814826

Epoch: 6| Step: 7
Training loss: 3.360064744949341
Validation loss: 4.438391295812464

Epoch: 6| Step: 8
Training loss: 4.706456184387207
Validation loss: 4.43566842232981

Epoch: 6| Step: 9
Training loss: 3.109267473220825
Validation loss: 4.430642215154505

Epoch: 6| Step: 10
Training loss: 4.168307304382324
Validation loss: 4.419753325882779

Epoch: 6| Step: 11
Training loss: 4.034992218017578
Validation loss: 4.412698940564227

Epoch: 6| Step: 12
Training loss: 4.043668270111084
Validation loss: 4.413317931595669

Epoch: 6| Step: 13
Training loss: 3.4001190662384033
Validation loss: 4.401214825209751

Epoch: 6| Step: 0
Training loss: 4.161256790161133
Validation loss: 4.390143835416404

Epoch: 6| Step: 1
Training loss: 2.947418689727783
Validation loss: 4.384825262972104

Epoch: 6| Step: 2
Training loss: 4.038538455963135
Validation loss: 4.3772156623102

Epoch: 6| Step: 3
Training loss: 4.0905303955078125
Validation loss: 4.367559068946428

Epoch: 6| Step: 4
Training loss: 3.8118200302124023
Validation loss: 4.3669314692097325

Epoch: 6| Step: 5
Training loss: 5.2137770652771
Validation loss: 4.356370028629098

Epoch: 6| Step: 6
Training loss: 3.8473901748657227
Validation loss: 4.345374132997247

Epoch: 6| Step: 7
Training loss: 3.4658827781677246
Validation loss: 4.337872546206238

Epoch: 6| Step: 8
Training loss: 4.4895243644714355
Validation loss: 4.336236687116726

Epoch: 6| Step: 9
Training loss: 5.539452075958252
Validation loss: 4.3242738887827885

Epoch: 6| Step: 10
Training loss: 3.9306344985961914
Validation loss: 4.3194366911406155

Epoch: 6| Step: 11
Training loss: 3.800530195236206
Validation loss: 4.3097607140899985

Epoch: 6| Step: 12
Training loss: 4.553286075592041
Validation loss: 4.302147998604723

Epoch: 6| Step: 13
Training loss: 4.03319787979126
Validation loss: 4.29753496826336

Epoch: 7| Step: 0
Training loss: 4.091387748718262
Validation loss: 4.289388795052806

Epoch: 6| Step: 1
Training loss: 4.963586807250977
Validation loss: 4.2771744599906345

Epoch: 6| Step: 2
Training loss: 4.8710784912109375
Validation loss: 4.270549248623592

Epoch: 6| Step: 3
Training loss: 4.540981292724609
Validation loss: 4.261896251350321

Epoch: 6| Step: 4
Training loss: 2.9885201454162598
Validation loss: 4.253667972421133

Epoch: 6| Step: 5
Training loss: 4.260004997253418
Validation loss: 4.244481999387023

Epoch: 6| Step: 6
Training loss: 4.3617024421691895
Validation loss: 4.235499607619419

Epoch: 6| Step: 7
Training loss: 2.908423900604248
Validation loss: 4.22995154575635

Epoch: 6| Step: 8
Training loss: 4.706155776977539
Validation loss: 4.215879588998774

Epoch: 6| Step: 9
Training loss: 4.0746965408325195
Validation loss: 4.206621508444509

Epoch: 6| Step: 10
Training loss: 1.9055814743041992
Validation loss: 4.199696492123348

Epoch: 6| Step: 11
Training loss: 4.398365020751953
Validation loss: 4.193230049584502

Epoch: 6| Step: 12
Training loss: 4.647183895111084
Validation loss: 4.175728356966409

Epoch: 6| Step: 13
Training loss: 3.653394937515259
Validation loss: 4.170753181621593

Epoch: 8| Step: 0
Training loss: 3.3880088329315186
Validation loss: 4.158214312727734

Epoch: 6| Step: 1
Training loss: 3.962155818939209
Validation loss: 4.147672104579146

Epoch: 6| Step: 2
Training loss: 3.590406894683838
Validation loss: 4.1461031667647825

Epoch: 6| Step: 3
Training loss: 3.5535664558410645
Validation loss: 4.136245117392591

Epoch: 6| Step: 4
Training loss: 4.1055779457092285
Validation loss: 4.123487298206617

Epoch: 6| Step: 5
Training loss: 4.06411075592041
Validation loss: 4.113937826566799

Epoch: 6| Step: 6
Training loss: 4.160763263702393
Validation loss: 4.1075755908925045

Epoch: 6| Step: 7
Training loss: 4.379114627838135
Validation loss: 4.09547060279436

Epoch: 6| Step: 8
Training loss: 3.2459843158721924
Validation loss: 4.085474055300477

Epoch: 6| Step: 9
Training loss: 3.793339967727661
Validation loss: 4.0736957186011855

Epoch: 6| Step: 10
Training loss: 4.121828079223633
Validation loss: 4.067618693074873

Epoch: 6| Step: 11
Training loss: 3.532890796661377
Validation loss: 4.055024849471225

Epoch: 6| Step: 12
Training loss: 4.478796005249023
Validation loss: 4.043820883638116

Epoch: 6| Step: 13
Training loss: 4.931467056274414
Validation loss: 4.035246418368432

Epoch: 9| Step: 0
Training loss: 4.70924186706543
Validation loss: 4.024524775884485

Epoch: 6| Step: 1
Training loss: 2.663588047027588
Validation loss: 4.01252063628166

Epoch: 6| Step: 2
Training loss: 3.0602433681488037
Validation loss: 4.004884381448069

Epoch: 6| Step: 3
Training loss: 2.96425199508667
Validation loss: 3.988276066318635

Epoch: 6| Step: 4
Training loss: 4.6098785400390625
Validation loss: 3.981180467913228

Epoch: 6| Step: 5
Training loss: 3.8497633934020996
Validation loss: 3.966430507680421

Epoch: 6| Step: 6
Training loss: 4.298500061035156
Validation loss: 3.955913389882734

Epoch: 6| Step: 7
Training loss: 4.138935089111328
Validation loss: 3.9474121678260063

Epoch: 6| Step: 8
Training loss: 3.696049928665161
Validation loss: 3.9280248970113774

Epoch: 6| Step: 9
Training loss: 3.745997428894043
Validation loss: 3.9183968908043316

Epoch: 6| Step: 10
Training loss: 4.552788734436035
Validation loss: 3.911895736571281

Epoch: 6| Step: 11
Training loss: 2.9552419185638428
Validation loss: 3.9020903366868214

Epoch: 6| Step: 12
Training loss: 3.8536739349365234
Validation loss: 3.8984191033147995

Epoch: 6| Step: 13
Training loss: 4.187485218048096
Validation loss: 3.8762999093660744

Epoch: 10| Step: 0
Training loss: 3.717381715774536
Validation loss: 3.86707547403151

Epoch: 6| Step: 1
Training loss: 3.9282374382019043
Validation loss: 3.849168954357024

Epoch: 6| Step: 2
Training loss: 3.3173258304595947
Validation loss: 3.8363210924210085

Epoch: 6| Step: 3
Training loss: 3.3680579662323
Validation loss: 3.83524259444206

Epoch: 6| Step: 4
Training loss: 3.253206491470337
Validation loss: 3.8135907419266237

Epoch: 6| Step: 5
Training loss: 4.367603302001953
Validation loss: 3.81811963101869

Epoch: 6| Step: 6
Training loss: 3.5702645778656006
Validation loss: 3.8021802594584804

Epoch: 6| Step: 7
Training loss: 3.329770565032959
Validation loss: 3.77736597676431

Epoch: 6| Step: 8
Training loss: 3.886603355407715
Validation loss: 3.7723987922873548

Epoch: 6| Step: 9
Training loss: 3.4696860313415527
Validation loss: 3.762492815653483

Epoch: 6| Step: 10
Training loss: 3.5190281867980957
Validation loss: 3.7389951572623303

Epoch: 6| Step: 11
Training loss: 4.284058570861816
Validation loss: 3.74360962324245

Epoch: 6| Step: 12
Training loss: 3.4360883235931396
Validation loss: 3.7174887246983026

Epoch: 6| Step: 13
Training loss: 3.5633435249328613
Validation loss: 3.706592318832233

Epoch: 11| Step: 0
Training loss: 2.8778114318847656
Validation loss: 3.697197865414363

Epoch: 6| Step: 1
Training loss: 3.436790943145752
Validation loss: 3.670710212440901

Epoch: 6| Step: 2
Training loss: 3.75410532951355
Validation loss: 3.6575871129189768

Epoch: 6| Step: 3
Training loss: 3.904266357421875
Validation loss: 3.656294830383793

Epoch: 6| Step: 4
Training loss: 3.062911033630371
Validation loss: 3.6302606162204536

Epoch: 6| Step: 5
Training loss: 4.474076271057129
Validation loss: 3.619262167202529

Epoch: 6| Step: 6
Training loss: 1.8335068225860596
Validation loss: 3.6038267843184935

Epoch: 6| Step: 7
Training loss: 2.7495622634887695
Validation loss: 3.589759724114531

Epoch: 6| Step: 8
Training loss: 4.7218122482299805
Validation loss: 3.5624304433022775

Epoch: 6| Step: 9
Training loss: 2.9523355960845947
Validation loss: 3.5619109497275403

Epoch: 6| Step: 10
Training loss: 3.6987414360046387
Validation loss: 3.5508750177198842

Epoch: 6| Step: 11
Training loss: 4.025175094604492
Validation loss: 3.533790737070063

Epoch: 6| Step: 12
Training loss: 3.004271984100342
Validation loss: 3.509639375953264

Epoch: 6| Step: 13
Training loss: 5.03029203414917
Validation loss: 3.502246928471391

Epoch: 12| Step: 0
Training loss: 3.845813751220703
Validation loss: 3.4794526202704317

Epoch: 6| Step: 1
Training loss: 2.3810532093048096
Validation loss: 3.467775034648116

Epoch: 6| Step: 2
Training loss: 4.236777305603027
Validation loss: 3.4467327005119732

Epoch: 6| Step: 3
Training loss: 3.133256196975708
Validation loss: 3.442054940808204

Epoch: 6| Step: 4
Training loss: 3.681258201599121
Validation loss: 3.425176215428178

Epoch: 6| Step: 5
Training loss: 3.4856653213500977
Validation loss: 3.4116901736105643

Epoch: 6| Step: 6
Training loss: 2.8812708854675293
Validation loss: 3.3857920785104074

Epoch: 6| Step: 7
Training loss: 4.4725518226623535
Validation loss: 3.3643322221694456

Epoch: 6| Step: 8
Training loss: 2.6811587810516357
Validation loss: 3.3592769920185046

Epoch: 6| Step: 9
Training loss: 2.752274990081787
Validation loss: 3.335717301214895

Epoch: 6| Step: 10
Training loss: 3.857889413833618
Validation loss: 3.305576983318534

Epoch: 6| Step: 11
Training loss: 2.699336051940918
Validation loss: 3.29373477607645

Epoch: 6| Step: 12
Training loss: 2.95629620552063
Validation loss: 3.28554009109415

Epoch: 6| Step: 13
Training loss: 2.844294548034668
Validation loss: 3.2657198829035603

Epoch: 13| Step: 0
Training loss: 1.9469785690307617
Validation loss: 3.229472452594388

Epoch: 6| Step: 1
Training loss: 2.7545676231384277
Validation loss: 3.2148169343189528

Epoch: 6| Step: 2
Training loss: 3.9531071186065674
Validation loss: 3.2002370588241087

Epoch: 6| Step: 3
Training loss: 3.5055229663848877
Validation loss: 3.207824230194092

Epoch: 6| Step: 4
Training loss: 2.7970824241638184
Validation loss: 3.166007493131904

Epoch: 6| Step: 5
Training loss: 3.130002021789551
Validation loss: 3.159950799839471

Epoch: 6| Step: 6
Training loss: 2.1392931938171387
Validation loss: 3.1347004803278113

Epoch: 6| Step: 7
Training loss: 3.8879590034484863
Validation loss: 3.137961459416215

Epoch: 6| Step: 8
Training loss: 2.913435935974121
Validation loss: 3.1218729275529102

Epoch: 6| Step: 9
Training loss: 3.116748332977295
Validation loss: 3.084855492397021

Epoch: 6| Step: 10
Training loss: 3.8351011276245117
Validation loss: 3.0782727682462303

Epoch: 6| Step: 11
Training loss: 3.218991756439209
Validation loss: 3.061993616883473

Epoch: 6| Step: 12
Training loss: 2.529980421066284
Validation loss: 3.027593502434351

Epoch: 6| Step: 13
Training loss: 4.089268207550049
Validation loss: 3.013535899500693

Epoch: 14| Step: 0
Training loss: 2.046700954437256
Validation loss: 3.0114164403689805

Epoch: 6| Step: 1
Training loss: 3.192948818206787
Validation loss: 2.9831478031732703

Epoch: 6| Step: 2
Training loss: 3.1009159088134766
Validation loss: 2.9792741421730287

Epoch: 6| Step: 3
Training loss: 3.117157459259033
Validation loss: 2.948715712434502

Epoch: 6| Step: 4
Training loss: 2.6902432441711426
Validation loss: 2.921874210398684

Epoch: 6| Step: 5
Training loss: 2.553656578063965
Validation loss: 2.895207994727678

Epoch: 6| Step: 6
Training loss: 3.074913501739502
Validation loss: 2.8877673790019047

Epoch: 6| Step: 7
Training loss: 3.1486499309539795
Validation loss: 2.8701499969728532

Epoch: 6| Step: 8
Training loss: 3.37632155418396
Validation loss: 2.861264923567413

Epoch: 6| Step: 9
Training loss: 3.519097328186035
Validation loss: 2.8472007679682907

Epoch: 6| Step: 10
Training loss: 3.131108283996582
Validation loss: 2.8218514893644597

Epoch: 6| Step: 11
Training loss: 3.1132023334503174
Validation loss: 2.7953725578964397

Epoch: 6| Step: 12
Training loss: 2.682095766067505
Validation loss: 2.7797025121668333

Epoch: 6| Step: 13
Training loss: 1.935915470123291
Validation loss: 2.7651311377043366

Epoch: 15| Step: 0
Training loss: 2.5614476203918457
Validation loss: 2.7366948666111117

Epoch: 6| Step: 1
Training loss: 3.3678884506225586
Validation loss: 2.7025963926828034

Epoch: 6| Step: 2
Training loss: 3.0035908222198486
Validation loss: 2.710895040983795

Epoch: 6| Step: 3
Training loss: 3.309455394744873
Validation loss: 2.6994662054123415

Epoch: 6| Step: 4
Training loss: 2.7780842781066895
Validation loss: 2.680535683067896

Epoch: 6| Step: 5
Training loss: 2.434584856033325
Validation loss: 2.6710578292928715

Epoch: 6| Step: 6
Training loss: 2.561479091644287
Validation loss: 2.6334565711277786

Epoch: 6| Step: 7
Training loss: 2.5427708625793457
Validation loss: 2.6403198754915627

Epoch: 6| Step: 8
Training loss: 2.97489595413208
Validation loss: 2.6177339784560667

Epoch: 6| Step: 9
Training loss: 2.2664010524749756
Validation loss: 2.6221993687332317

Epoch: 6| Step: 10
Training loss: 2.765500068664551
Validation loss: 2.5716670302934546

Epoch: 6| Step: 11
Training loss: 3.2028748989105225
Validation loss: 2.5495672251588557

Epoch: 6| Step: 12
Training loss: 2.639293670654297
Validation loss: 2.555358697009343

Epoch: 6| Step: 13
Training loss: 2.321108818054199
Validation loss: 2.5703365648946455

Epoch: 16| Step: 0
Training loss: 3.146610736846924
Validation loss: 2.5324999850283385

Epoch: 6| Step: 1
Training loss: 3.1690893173217773
Validation loss: 2.514891214268182

Epoch: 6| Step: 2
Training loss: 2.283296585083008
Validation loss: 2.488544520511422

Epoch: 6| Step: 3
Training loss: 2.679187774658203
Validation loss: 2.4630908376427105

Epoch: 6| Step: 4
Training loss: 3.1893489360809326
Validation loss: 2.4497317242366012

Epoch: 6| Step: 5
Training loss: 2.3665506839752197
Validation loss: 2.4411792985854612

Epoch: 6| Step: 6
Training loss: 1.4861195087432861
Validation loss: 2.424632713358889

Epoch: 6| Step: 7
Training loss: 2.4360713958740234
Validation loss: 2.436658554179694

Epoch: 6| Step: 8
Training loss: 3.2991654872894287
Validation loss: 2.386518568120977

Epoch: 6| Step: 9
Training loss: 2.1966917514801025
Validation loss: 2.401840727816346

Epoch: 6| Step: 10
Training loss: 2.75425386428833
Validation loss: 2.3991229021421043

Epoch: 6| Step: 11
Training loss: 2.026883363723755
Validation loss: 2.377387644142233

Epoch: 6| Step: 12
Training loss: 3.283895254135132
Validation loss: 2.370797090632941

Epoch: 6| Step: 13
Training loss: 2.5978097915649414
Validation loss: 2.3486541137900403

Epoch: 17| Step: 0
Training loss: 2.437964916229248
Validation loss: 2.320698099751626

Epoch: 6| Step: 1
Training loss: 2.4742178916931152
Validation loss: 2.316009021574451

Epoch: 6| Step: 2
Training loss: 1.8813238143920898
Validation loss: 2.294241794975855

Epoch: 6| Step: 3
Training loss: 2.6642212867736816
Validation loss: 2.2999104428034958

Epoch: 6| Step: 4
Training loss: 2.7015695571899414
Validation loss: 2.294614056105255

Epoch: 6| Step: 5
Training loss: 2.421870708465576
Validation loss: 2.2720931217234623

Epoch: 6| Step: 6
Training loss: 2.153326988220215
Validation loss: 2.271524824121947

Epoch: 6| Step: 7
Training loss: 2.96657657623291
Validation loss: 2.2345405419667563

Epoch: 6| Step: 8
Training loss: 2.0903117656707764
Validation loss: 2.2305377144967355

Epoch: 6| Step: 9
Training loss: 3.0900347232818604
Validation loss: 2.241211273336923

Epoch: 6| Step: 10
Training loss: 2.8463311195373535
Validation loss: 2.210593622217896

Epoch: 6| Step: 11
Training loss: 2.5869016647338867
Validation loss: 2.225685396502095

Epoch: 6| Step: 12
Training loss: 2.6919217109680176
Validation loss: 2.236538916505793

Epoch: 6| Step: 13
Training loss: 2.7198479175567627
Validation loss: 2.2183348171172605

Epoch: 18| Step: 0
Training loss: 1.9000415802001953
Validation loss: 2.2071793899741223

Epoch: 6| Step: 1
Training loss: 2.9128634929656982
Validation loss: 2.1887740627411874

Epoch: 6| Step: 2
Training loss: 2.5067927837371826
Validation loss: 2.193836986377675

Epoch: 6| Step: 3
Training loss: 2.4047751426696777
Validation loss: 2.19436808811721

Epoch: 6| Step: 4
Training loss: 2.6816139221191406
Validation loss: 2.1900027695522515

Epoch: 6| Step: 5
Training loss: 2.1887636184692383
Validation loss: 2.180910579619869

Epoch: 6| Step: 6
Training loss: 3.039536952972412
Validation loss: 2.1927801844894246

Epoch: 6| Step: 7
Training loss: 2.3187692165374756
Validation loss: 2.190921777038164

Epoch: 6| Step: 8
Training loss: 2.203677177429199
Validation loss: 2.179532681742022

Epoch: 6| Step: 9
Training loss: 2.7584004402160645
Validation loss: 2.1539355042160198

Epoch: 6| Step: 10
Training loss: 3.0289697647094727
Validation loss: 2.156557898367605

Epoch: 6| Step: 11
Training loss: 2.671217918395996
Validation loss: 2.1543199759657665

Epoch: 6| Step: 12
Training loss: 2.0732243061065674
Validation loss: 2.157487569316741

Epoch: 6| Step: 13
Training loss: 2.0787837505340576
Validation loss: 2.1493964759252404

Epoch: 19| Step: 0
Training loss: 2.029114246368408
Validation loss: 2.1316001979253625

Epoch: 6| Step: 1
Training loss: 2.0853567123413086
Validation loss: 2.122530867976527

Epoch: 6| Step: 2
Training loss: 2.038038730621338
Validation loss: 2.1496247783783944

Epoch: 6| Step: 3
Training loss: 3.063663959503174
Validation loss: 2.1446704300501014

Epoch: 6| Step: 4
Training loss: 2.279216766357422
Validation loss: 2.1579184839802403

Epoch: 6| Step: 5
Training loss: 2.535736560821533
Validation loss: 2.1263809691193285

Epoch: 6| Step: 6
Training loss: 2.313103675842285
Validation loss: 2.117686492140575

Epoch: 6| Step: 7
Training loss: 2.5461437702178955
Validation loss: 2.14147452897923

Epoch: 6| Step: 8
Training loss: 2.5321731567382812
Validation loss: 2.115518393055085

Epoch: 6| Step: 9
Training loss: 2.8455967903137207
Validation loss: 2.072689615270143

Epoch: 6| Step: 10
Training loss: 2.483884811401367
Validation loss: 2.1205385782385386

Epoch: 6| Step: 11
Training loss: 2.374929904937744
Validation loss: 2.126090488126201

Epoch: 6| Step: 12
Training loss: 2.882573127746582
Validation loss: 2.1019263011153027

Epoch: 6| Step: 13
Training loss: 2.72967267036438
Validation loss: 2.1618160483657674

Epoch: 20| Step: 0
Training loss: 3.084026336669922
Validation loss: 2.1009651640410065

Epoch: 6| Step: 1
Training loss: 2.465981960296631
Validation loss: 2.138878248071158

Epoch: 6| Step: 2
Training loss: 2.278153419494629
Validation loss: 2.137874739144438

Epoch: 6| Step: 3
Training loss: 2.3965749740600586
Validation loss: 2.1541399443021385

Epoch: 6| Step: 4
Training loss: 2.74066162109375
Validation loss: 2.126706515589068

Epoch: 6| Step: 5
Training loss: 1.659786343574524
Validation loss: 2.144385025065432

Epoch: 6| Step: 6
Training loss: 3.1840977668762207
Validation loss: 2.13049574564862

Epoch: 6| Step: 7
Training loss: 2.762678384780884
Validation loss: 2.1014853472350747

Epoch: 6| Step: 8
Training loss: 2.1822400093078613
Validation loss: 2.105288714490911

Epoch: 6| Step: 9
Training loss: 2.2697739601135254
Validation loss: 2.1059435259911323

Epoch: 6| Step: 10
Training loss: 1.7703958749771118
Validation loss: 2.1221062188507407

Epoch: 6| Step: 11
Training loss: 2.5235400199890137
Validation loss: 2.106626784929665

Epoch: 6| Step: 12
Training loss: 3.096217155456543
Validation loss: 2.1189367848057903

Epoch: 6| Step: 13
Training loss: 2.049055576324463
Validation loss: 2.088883440981629

Epoch: 21| Step: 0
Training loss: 2.7814338207244873
Validation loss: 2.1071857572883688

Epoch: 6| Step: 1
Training loss: 2.1347265243530273
Validation loss: 2.1118678303175074

Epoch: 6| Step: 2
Training loss: 2.275944709777832
Validation loss: 2.111116788720572

Epoch: 6| Step: 3
Training loss: 1.9904369115829468
Validation loss: 2.120374818002024

Epoch: 6| Step: 4
Training loss: 2.482089042663574
Validation loss: 2.1185662310610534

Epoch: 6| Step: 5
Training loss: 2.448986291885376
Validation loss: 2.124429965531954

Epoch: 6| Step: 6
Training loss: 2.371359348297119
Validation loss: 2.0777183732678814

Epoch: 6| Step: 7
Training loss: 2.9105420112609863
Validation loss: 2.1159670404208604

Epoch: 6| Step: 8
Training loss: 2.620030641555786
Validation loss: 2.111875218729819

Epoch: 6| Step: 9
Training loss: 2.0480570793151855
Validation loss: 2.1382901514730146

Epoch: 6| Step: 10
Training loss: 2.328096866607666
Validation loss: 2.134739932193551

Epoch: 6| Step: 11
Training loss: 2.737761974334717
Validation loss: 2.085012025730584

Epoch: 6| Step: 12
Training loss: 2.593343734741211
Validation loss: 2.1355732910094725

Epoch: 6| Step: 13
Training loss: 2.7414097785949707
Validation loss: 2.127839598604428

Epoch: 22| Step: 0
Training loss: 2.6988840103149414
Validation loss: 2.100572914205572

Epoch: 6| Step: 1
Training loss: 2.678280830383301
Validation loss: 2.09708773064357

Epoch: 6| Step: 2
Training loss: 1.8299827575683594
Validation loss: 2.1218425202113327

Epoch: 6| Step: 3
Training loss: 2.5890326499938965
Validation loss: 2.1197186567450084

Epoch: 6| Step: 4
Training loss: 2.019317388534546
Validation loss: 2.1053527696158296

Epoch: 6| Step: 5
Training loss: 2.5029845237731934
Validation loss: 2.088026623572073

Epoch: 6| Step: 6
Training loss: 2.641097068786621
Validation loss: 2.1028690261225544

Epoch: 6| Step: 7
Training loss: 3.0848774909973145
Validation loss: 2.095501842037324

Epoch: 6| Step: 8
Training loss: 2.1218528747558594
Validation loss: 2.1112959320827196

Epoch: 6| Step: 9
Training loss: 2.1223154067993164
Validation loss: 2.083233982004145

Epoch: 6| Step: 10
Training loss: 2.640981674194336
Validation loss: 2.0705337691050705

Epoch: 6| Step: 11
Training loss: 2.4066455364227295
Validation loss: 2.0666605862238074

Epoch: 6| Step: 12
Training loss: 2.7477614879608154
Validation loss: 2.0909811117315806

Epoch: 6| Step: 13
Training loss: 2.058927536010742
Validation loss: 2.0790273938127743

Epoch: 23| Step: 0
Training loss: 2.3458328247070312
Validation loss: 2.096117978454918

Epoch: 6| Step: 1
Training loss: 2.252044677734375
Validation loss: 2.08058677693849

Epoch: 6| Step: 2
Training loss: 2.589939832687378
Validation loss: 2.098802025600146

Epoch: 6| Step: 3
Training loss: 2.1384408473968506
Validation loss: 2.1187681715975524

Epoch: 6| Step: 4
Training loss: 2.253756523132324
Validation loss: 2.075081745783488

Epoch: 6| Step: 5
Training loss: 2.90726637840271
Validation loss: 2.089153899941393

Epoch: 6| Step: 6
Training loss: 2.4642200469970703
Validation loss: 2.0915101138494347

Epoch: 6| Step: 7
Training loss: 3.0857372283935547
Validation loss: 2.079891848307784

Epoch: 6| Step: 8
Training loss: 3.4423511028289795
Validation loss: 2.082516429244831

Epoch: 6| Step: 9
Training loss: 2.5432848930358887
Validation loss: 2.067353706206045

Epoch: 6| Step: 10
Training loss: 1.9474551677703857
Validation loss: 2.089524510086224

Epoch: 6| Step: 11
Training loss: 2.2899391651153564
Validation loss: 2.057192430701307

Epoch: 6| Step: 12
Training loss: 2.0476231575012207
Validation loss: 2.092372635359405

Epoch: 6| Step: 13
Training loss: 1.4415993690490723
Validation loss: 2.0820135583159742

Epoch: 24| Step: 0
Training loss: 2.1126370429992676
Validation loss: 2.1228526458945325

Epoch: 6| Step: 1
Training loss: 2.4267544746398926
Validation loss: 2.0796034079726025

Epoch: 6| Step: 2
Training loss: 3.1849870681762695
Validation loss: 2.074508382428077

Epoch: 6| Step: 3
Training loss: 2.4428272247314453
Validation loss: 2.059145709519745

Epoch: 6| Step: 4
Training loss: 2.9255623817443848
Validation loss: 2.0742804901574248

Epoch: 6| Step: 5
Training loss: 2.5718443393707275
Validation loss: 2.1054782995613675

Epoch: 6| Step: 6
Training loss: 2.5635643005371094
Validation loss: 2.085684053359493

Epoch: 6| Step: 7
Training loss: 1.6562645435333252
Validation loss: 2.070539436032695

Epoch: 6| Step: 8
Training loss: 2.4688029289245605
Validation loss: 2.0687397026246592

Epoch: 6| Step: 9
Training loss: 2.057058811187744
Validation loss: 2.0999670003050115

Epoch: 6| Step: 10
Training loss: 2.7463152408599854
Validation loss: 2.0898438371637815

Epoch: 6| Step: 11
Training loss: 2.066221237182617
Validation loss: 2.0873766483799105

Epoch: 6| Step: 12
Training loss: 2.3528785705566406
Validation loss: 2.1097863822854976

Epoch: 6| Step: 13
Training loss: 3.2699759006500244
Validation loss: 2.1014358100070747

Epoch: 25| Step: 0
Training loss: 2.4515089988708496
Validation loss: 2.0908861416642384

Epoch: 6| Step: 1
Training loss: 2.6795973777770996
Validation loss: 2.068638860538442

Epoch: 6| Step: 2
Training loss: 2.3907320499420166
Validation loss: 2.0869836563705118

Epoch: 6| Step: 3
Training loss: 2.727363348007202
Validation loss: 2.1128685294940905

Epoch: 6| Step: 4
Training loss: 1.9661892652511597
Validation loss: 2.0695917208989463

Epoch: 6| Step: 5
Training loss: 2.814730167388916
Validation loss: 2.097977169098393

Epoch: 6| Step: 6
Training loss: 2.6897268295288086
Validation loss: 2.0618043586771977

Epoch: 6| Step: 7
Training loss: 2.1658360958099365
Validation loss: 2.1070023787918912

Epoch: 6| Step: 8
Training loss: 2.2396130561828613
Validation loss: 2.082653913446652

Epoch: 6| Step: 9
Training loss: 1.8198087215423584
Validation loss: 2.081906185355238

Epoch: 6| Step: 10
Training loss: 2.0107240676879883
Validation loss: 2.075059313927927

Epoch: 6| Step: 11
Training loss: 3.026759624481201
Validation loss: 2.062790366911119

Epoch: 6| Step: 12
Training loss: 2.81223726272583
Validation loss: 2.0933475122656873

Epoch: 6| Step: 13
Training loss: 2.4189388751983643
Validation loss: 2.061567193718367

Epoch: 26| Step: 0
Training loss: 2.6699137687683105
Validation loss: 2.103376519295477

Epoch: 6| Step: 1
Training loss: 2.0999631881713867
Validation loss: 2.08716243825933

Epoch: 6| Step: 2
Training loss: 1.8491734266281128
Validation loss: 2.0833420676569783

Epoch: 6| Step: 3
Training loss: 2.7949490547180176
Validation loss: 2.0460587316943752

Epoch: 6| Step: 4
Training loss: 3.0405149459838867
Validation loss: 2.0909575646923435

Epoch: 6| Step: 5
Training loss: 3.0793228149414062
Validation loss: 2.0693626480717815

Epoch: 6| Step: 6
Training loss: 1.889485478401184
Validation loss: 2.105442839284097

Epoch: 6| Step: 7
Training loss: 2.129915714263916
Validation loss: 2.0952149155319377

Epoch: 6| Step: 8
Training loss: 2.5246970653533936
Validation loss: 2.085025297698154

Epoch: 6| Step: 9
Training loss: 2.47635555267334
Validation loss: 2.0809589368040844

Epoch: 6| Step: 10
Training loss: 1.5846619606018066
Validation loss: 2.0837855108322634

Epoch: 6| Step: 11
Training loss: 3.0991530418395996
Validation loss: 2.085523618164883

Epoch: 6| Step: 12
Training loss: 2.2583303451538086
Validation loss: 2.1123616541585615

Epoch: 6| Step: 13
Training loss: 2.9595258235931396
Validation loss: 2.101288221215689

Epoch: 27| Step: 0
Training loss: 2.778256416320801
Validation loss: 2.1026163280651136

Epoch: 6| Step: 1
Training loss: 2.2210896015167236
Validation loss: 2.0896702774109377

Epoch: 6| Step: 2
Training loss: 3.2428462505340576
Validation loss: 2.105837401523385

Epoch: 6| Step: 3
Training loss: 2.412220001220703
Validation loss: 2.0990473198634323

Epoch: 6| Step: 4
Training loss: 2.635756015777588
Validation loss: 2.131559548839446

Epoch: 6| Step: 5
Training loss: 1.9606270790100098
Validation loss: 2.064086929444344

Epoch: 6| Step: 6
Training loss: 1.4487930536270142
Validation loss: 2.0897897763918807

Epoch: 6| Step: 7
Training loss: 2.7955846786499023
Validation loss: 2.117529461460729

Epoch: 6| Step: 8
Training loss: 1.9360426664352417
Validation loss: 2.0722681117314163

Epoch: 6| Step: 9
Training loss: 2.311222553253174
Validation loss: 2.083314129101333

Epoch: 6| Step: 10
Training loss: 3.271296501159668
Validation loss: 2.0756438034836964

Epoch: 6| Step: 11
Training loss: 2.5031630992889404
Validation loss: 2.081211636143346

Epoch: 6| Step: 12
Training loss: 2.5651488304138184
Validation loss: 2.104712493958012

Epoch: 6| Step: 13
Training loss: 1.8461804389953613
Validation loss: 2.0887411102171867

Epoch: 28| Step: 0
Training loss: 2.484135627746582
Validation loss: 2.0333891491736136

Epoch: 6| Step: 1
Training loss: 1.8675479888916016
Validation loss: 2.0940950429567726

Epoch: 6| Step: 2
Training loss: 2.8544790744781494
Validation loss: 2.0723057049576954

Epoch: 6| Step: 3
Training loss: 2.283740520477295
Validation loss: 2.098515413140738

Epoch: 6| Step: 4
Training loss: 1.9183405637741089
Validation loss: 2.0634830997836207

Epoch: 6| Step: 5
Training loss: 2.536914110183716
Validation loss: 2.0987874231030865

Epoch: 6| Step: 6
Training loss: 2.913148880004883
Validation loss: 2.0819942515383483

Epoch: 6| Step: 7
Training loss: 3.0078587532043457
Validation loss: 2.102702627899826

Epoch: 6| Step: 8
Training loss: 3.185786247253418
Validation loss: 2.102202110393073

Epoch: 6| Step: 9
Training loss: 1.6459108591079712
Validation loss: 2.0647961862625612

Epoch: 6| Step: 10
Training loss: 1.9044806957244873
Validation loss: 2.0897964328847904

Epoch: 6| Step: 11
Training loss: 2.6191506385803223
Validation loss: 2.072053583719397

Epoch: 6| Step: 12
Training loss: 2.1000380516052246
Validation loss: 2.071592007913897

Epoch: 6| Step: 13
Training loss: 2.747725009918213
Validation loss: 2.093563800217003

Epoch: 29| Step: 0
Training loss: 1.910753846168518
Validation loss: 2.080850370468632

Epoch: 6| Step: 1
Training loss: 2.480347156524658
Validation loss: 2.0832012827678392

Epoch: 6| Step: 2
Training loss: 2.7629477977752686
Validation loss: 2.0425934048109156

Epoch: 6| Step: 3
Training loss: 2.1895878314971924
Validation loss: 2.0907311747151036

Epoch: 6| Step: 4
Training loss: 1.8698444366455078
Validation loss: 2.071507025790471

Epoch: 6| Step: 5
Training loss: 2.8945975303649902
Validation loss: 2.1051856548555437

Epoch: 6| Step: 6
Training loss: 2.491715431213379
Validation loss: 2.091988416128261

Epoch: 6| Step: 7
Training loss: 2.803750514984131
Validation loss: 2.0725197343416113

Epoch: 6| Step: 8
Training loss: 3.3351047039031982
Validation loss: 2.0915817060778217

Epoch: 6| Step: 9
Training loss: 2.30895733833313
Validation loss: 2.0776324118337324

Epoch: 6| Step: 10
Training loss: 2.3400652408599854
Validation loss: 2.0766763943497852

Epoch: 6| Step: 11
Training loss: 1.924415111541748
Validation loss: 2.082781513532003

Epoch: 6| Step: 12
Training loss: 2.4494590759277344
Validation loss: 2.0778839870165755

Epoch: 6| Step: 13
Training loss: 2.44781494140625
Validation loss: 2.105954647064209

Epoch: 30| Step: 0
Training loss: 2.4182655811309814
Validation loss: 2.0429689012547976

Epoch: 6| Step: 1
Training loss: 2.408555507659912
Validation loss: 2.0711795078810824

Epoch: 6| Step: 2
Training loss: 2.9013843536376953
Validation loss: 2.064938288862987

Epoch: 6| Step: 3
Training loss: 2.5458498001098633
Validation loss: 2.063467835867277

Epoch: 6| Step: 4
Training loss: 2.1682939529418945
Validation loss: 2.062482431370725

Epoch: 6| Step: 5
Training loss: 2.105740785598755
Validation loss: 2.0697393724995274

Epoch: 6| Step: 6
Training loss: 2.2106571197509766
Validation loss: 2.1055326423337384

Epoch: 6| Step: 7
Training loss: 2.2592577934265137
Validation loss: 2.09350993299997

Epoch: 6| Step: 8
Training loss: 2.077547788619995
Validation loss: 2.091770648956299

Epoch: 6| Step: 9
Training loss: 2.5482077598571777
Validation loss: 2.04878391117178

Epoch: 6| Step: 10
Training loss: 3.6012165546417236
Validation loss: 2.0774751337625648

Epoch: 6| Step: 11
Training loss: 1.8644382953643799
Validation loss: 2.0499045938573857

Epoch: 6| Step: 12
Training loss: 1.8629438877105713
Validation loss: 2.105122129122416

Epoch: 6| Step: 13
Training loss: 2.8805084228515625
Validation loss: 2.071983152820218

Epoch: 31| Step: 0
Training loss: 2.6204729080200195
Validation loss: 2.1429545366635887

Epoch: 6| Step: 1
Training loss: 1.475254774093628
Validation loss: 2.078762695353518

Epoch: 6| Step: 2
Training loss: 3.5739455223083496
Validation loss: 2.0755494192082393

Epoch: 6| Step: 3
Training loss: 2.147139072418213
Validation loss: 2.061001705866988

Epoch: 6| Step: 4
Training loss: 2.158535957336426
Validation loss: 2.0749195916678316

Epoch: 6| Step: 5
Training loss: 3.027899742126465
Validation loss: 2.0697235843186736

Epoch: 6| Step: 6
Training loss: 1.8928812742233276
Validation loss: 2.0918402902541624

Epoch: 6| Step: 7
Training loss: 2.6397337913513184
Validation loss: 2.052017729769471

Epoch: 6| Step: 8
Training loss: 2.8447914123535156
Validation loss: 2.0647623846607823

Epoch: 6| Step: 9
Training loss: 2.623187303543091
Validation loss: 2.0723242785341

Epoch: 6| Step: 10
Training loss: 2.058229923248291
Validation loss: 2.075361451795024

Epoch: 6| Step: 11
Training loss: 2.2373318672180176
Validation loss: 2.0738077189332698

Epoch: 6| Step: 12
Training loss: 2.1090457439422607
Validation loss: 2.12901964751623

Epoch: 6| Step: 13
Training loss: 2.342921495437622
Validation loss: 2.0814632061989076

Epoch: 32| Step: 0
Training loss: 2.6607561111450195
Validation loss: 2.092662275478404

Epoch: 6| Step: 1
Training loss: 1.6904457807540894
Validation loss: 2.099011910858975

Epoch: 6| Step: 2
Training loss: 2.540193557739258
Validation loss: 2.050244192923269

Epoch: 6| Step: 3
Training loss: 2.3891119956970215
Validation loss: 2.086054148212556

Epoch: 6| Step: 4
Training loss: 1.7708323001861572
Validation loss: 2.0863805740110335

Epoch: 6| Step: 5
Training loss: 2.3394463062286377
Validation loss: 2.037726965001834

Epoch: 6| Step: 6
Training loss: 3.023160457611084
Validation loss: 2.059123618628389

Epoch: 6| Step: 7
Training loss: 2.3943705558776855
Validation loss: 2.0254506244454333

Epoch: 6| Step: 8
Training loss: 2.679727554321289
Validation loss: 2.089502542249618

Epoch: 6| Step: 9
Training loss: 2.642239570617676
Validation loss: 2.100238561630249

Epoch: 6| Step: 10
Training loss: 2.0946903228759766
Validation loss: 2.0744909009625836

Epoch: 6| Step: 11
Training loss: 2.8933544158935547
Validation loss: 2.1096011951405513

Epoch: 6| Step: 12
Training loss: 2.4024102687835693
Validation loss: 2.055091440036733

Epoch: 6| Step: 13
Training loss: 2.255429267883301
Validation loss: 2.0656546764476325

Epoch: 33| Step: 0
Training loss: 2.324981689453125
Validation loss: 2.049231849690919

Epoch: 6| Step: 1
Training loss: 2.9371705055236816
Validation loss: 2.0690693970649474

Epoch: 6| Step: 2
Training loss: 1.7655972242355347
Validation loss: 2.0646182042296215

Epoch: 6| Step: 3
Training loss: 2.40254545211792
Validation loss: 2.060851276561778

Epoch: 6| Step: 4
Training loss: 3.072115182876587
Validation loss: 2.074617789637658

Epoch: 6| Step: 5
Training loss: 2.4143295288085938
Validation loss: 2.088992057308074

Epoch: 6| Step: 6
Training loss: 1.5425639152526855
Validation loss: 2.079977625159807

Epoch: 6| Step: 7
Training loss: 1.2519925832748413
Validation loss: 2.0597292941103698

Epoch: 6| Step: 8
Training loss: 2.6390128135681152
Validation loss: 2.0685422523047334

Epoch: 6| Step: 9
Training loss: 3.2900805473327637
Validation loss: 2.0708551047950663

Epoch: 6| Step: 10
Training loss: 2.5665693283081055
Validation loss: 2.056862178669181

Epoch: 6| Step: 11
Training loss: 2.668612480163574
Validation loss: 2.0894711158608876

Epoch: 6| Step: 12
Training loss: 2.247960090637207
Validation loss: 2.057824644991147

Epoch: 6| Step: 13
Training loss: 2.8383772373199463
Validation loss: 2.087639275417533

Epoch: 34| Step: 0
Training loss: 2.4367785453796387
Validation loss: 2.0412479677507953

Epoch: 6| Step: 1
Training loss: 2.5024662017822266
Validation loss: 2.0913757995892595

Epoch: 6| Step: 2
Training loss: 2.549124240875244
Validation loss: 2.0648004624151413

Epoch: 6| Step: 3
Training loss: 2.0873589515686035
Validation loss: 2.0708873451396985

Epoch: 6| Step: 4
Training loss: 2.1161842346191406
Validation loss: 2.055303294171569

Epoch: 6| Step: 5
Training loss: 1.708606243133545
Validation loss: 2.070200648359073

Epoch: 6| Step: 6
Training loss: 1.7189505100250244
Validation loss: 2.0396867144492363

Epoch: 6| Step: 7
Training loss: 2.606407642364502
Validation loss: 2.0504216071098083

Epoch: 6| Step: 8
Training loss: 2.353386402130127
Validation loss: 2.0783018655674432

Epoch: 6| Step: 9
Training loss: 3.075608253479004
Validation loss: 2.078295889721122

Epoch: 6| Step: 10
Training loss: 2.426145076751709
Validation loss: 2.0878693365281626

Epoch: 6| Step: 11
Training loss: 3.289130449295044
Validation loss: 2.0919473837780695

Epoch: 6| Step: 12
Training loss: 2.3866214752197266
Validation loss: 2.0706127151366203

Epoch: 6| Step: 13
Training loss: 2.1029646396636963
Validation loss: 2.0870737901297947

Epoch: 35| Step: 0
Training loss: 2.7129759788513184
Validation loss: 2.0693989684504848

Epoch: 6| Step: 1
Training loss: 2.0909509658813477
Validation loss: 2.10043732325236

Epoch: 6| Step: 2
Training loss: 2.3500053882598877
Validation loss: 2.0639463547737367

Epoch: 6| Step: 3
Training loss: 2.940997838973999
Validation loss: 2.062176542897378

Epoch: 6| Step: 4
Training loss: 2.3495559692382812
Validation loss: 2.0523386360496603

Epoch: 6| Step: 5
Training loss: 2.1270194053649902
Validation loss: 2.055948561237704

Epoch: 6| Step: 6
Training loss: 2.9161789417266846
Validation loss: 2.0520862020472044

Epoch: 6| Step: 7
Training loss: 2.0424420833587646
Validation loss: 2.073510012319011

Epoch: 6| Step: 8
Training loss: 2.1974589824676514
Validation loss: 2.068286621442405

Epoch: 6| Step: 9
Training loss: 2.282331943511963
Validation loss: 2.059116821135244

Epoch: 6| Step: 10
Training loss: 2.9478769302368164
Validation loss: 2.063798980046344

Epoch: 6| Step: 11
Training loss: 2.389902114868164
Validation loss: 2.083673800191572

Epoch: 6| Step: 12
Training loss: 2.089982032775879
Validation loss: 2.056182546000327

Epoch: 6| Step: 13
Training loss: 2.25551700592041
Validation loss: 2.0497748339047996

Epoch: 36| Step: 0
Training loss: 2.428382396697998
Validation loss: 2.0782411713753977

Epoch: 6| Step: 1
Training loss: 2.5800487995147705
Validation loss: 2.0526663218775103

Epoch: 6| Step: 2
Training loss: 2.267479181289673
Validation loss: 2.0673323164704027

Epoch: 6| Step: 3
Training loss: 1.947380542755127
Validation loss: 2.0506035127947406

Epoch: 6| Step: 4
Training loss: 1.8008464574813843
Validation loss: 2.080743989636821

Epoch: 6| Step: 5
Training loss: 2.4402849674224854
Validation loss: 2.028288152910048

Epoch: 6| Step: 6
Training loss: 2.651031017303467
Validation loss: 2.0370256311150006

Epoch: 6| Step: 7
Training loss: 2.610720157623291
Validation loss: 2.0521717738079768

Epoch: 6| Step: 8
Training loss: 2.5141448974609375
Validation loss: 2.0457362257024294

Epoch: 6| Step: 9
Training loss: 2.567800283432007
Validation loss: 2.053106291319734

Epoch: 6| Step: 10
Training loss: 2.786393880844116
Validation loss: 2.048105255250008

Epoch: 6| Step: 11
Training loss: 1.911536455154419
Validation loss: 2.0519306634062078

Epoch: 6| Step: 12
Training loss: 2.9046177864074707
Validation loss: 2.047419512143699

Epoch: 6| Step: 13
Training loss: 2.1228935718536377
Validation loss: 2.0495298806057183

Epoch: 37| Step: 0
Training loss: 3.3689260482788086
Validation loss: 2.0647624538790796

Epoch: 6| Step: 1
Training loss: 3.0279788970947266
Validation loss: 2.0485444799546273

Epoch: 6| Step: 2
Training loss: 1.8429431915283203
Validation loss: 2.0471874924116236

Epoch: 6| Step: 3
Training loss: 2.242640972137451
Validation loss: 2.0635842610430974

Epoch: 6| Step: 4
Training loss: 2.6387293338775635
Validation loss: 2.0479769988726546

Epoch: 6| Step: 5
Training loss: 2.3639376163482666
Validation loss: 2.028756049371535

Epoch: 6| Step: 6
Training loss: 2.8875107765197754
Validation loss: 2.0301010403581845

Epoch: 6| Step: 7
Training loss: 2.3240902423858643
Validation loss: 2.07017772172087

Epoch: 6| Step: 8
Training loss: 2.2356154918670654
Validation loss: 2.0510149796803794

Epoch: 6| Step: 9
Training loss: 1.9731680154800415
Validation loss: 2.096317175895937

Epoch: 6| Step: 10
Training loss: 2.0641183853149414
Validation loss: 2.06042500593329

Epoch: 6| Step: 11
Training loss: 1.7093727588653564
Validation loss: 2.0812772127889816

Epoch: 6| Step: 12
Training loss: 1.910983681678772
Validation loss: 2.052073376153105

Epoch: 6| Step: 13
Training loss: 3.141209363937378
Validation loss: 2.0411453682889222

Epoch: 38| Step: 0
Training loss: 2.606889247894287
Validation loss: 2.0587991693968415

Epoch: 6| Step: 1
Training loss: 2.6619129180908203
Validation loss: 2.042381371221235

Epoch: 6| Step: 2
Training loss: 1.8439191579818726
Validation loss: 2.0595283726210236

Epoch: 6| Step: 3
Training loss: 3.19118595123291
Validation loss: 2.049959780067526

Epoch: 6| Step: 4
Training loss: 2.346245765686035
Validation loss: 2.0299396181619294

Epoch: 6| Step: 5
Training loss: 1.8892942667007446
Validation loss: 2.030398271417105

Epoch: 6| Step: 6
Training loss: 2.1562294960021973
Validation loss: 2.069439700854722

Epoch: 6| Step: 7
Training loss: 2.2398433685302734
Validation loss: 2.0398740127522457

Epoch: 6| Step: 8
Training loss: 2.79754900932312
Validation loss: 2.0593844408630044

Epoch: 6| Step: 9
Training loss: 2.5774378776550293
Validation loss: 2.051828827909244

Epoch: 6| Step: 10
Training loss: 1.547170877456665
Validation loss: 2.037454430774976

Epoch: 6| Step: 11
Training loss: 2.2178661823272705
Validation loss: 2.0912229014981176

Epoch: 6| Step: 12
Training loss: 3.0329904556274414
Validation loss: 2.067812991398637

Epoch: 6| Step: 13
Training loss: 2.32684326171875
Validation loss: 2.0349861729529595

Epoch: 39| Step: 0
Training loss: 2.220017910003662
Validation loss: 2.0705576673630746

Epoch: 6| Step: 1
Training loss: 1.8948036432266235
Validation loss: 2.0616382040003294

Epoch: 6| Step: 2
Training loss: 2.4221558570861816
Validation loss: 2.092797665185826

Epoch: 6| Step: 3
Training loss: 1.9243124723434448
Validation loss: 2.0539352304192

Epoch: 6| Step: 4
Training loss: 2.659208297729492
Validation loss: 2.056490191849329

Epoch: 6| Step: 5
Training loss: 2.7115254402160645
Validation loss: 2.071931750543656

Epoch: 6| Step: 6
Training loss: 2.724597454071045
Validation loss: 2.0272592036954817

Epoch: 6| Step: 7
Training loss: 2.774778366088867
Validation loss: 2.0415324805885233

Epoch: 6| Step: 8
Training loss: 2.726588249206543
Validation loss: 2.069952522554705

Epoch: 6| Step: 9
Training loss: 2.1786129474639893
Validation loss: 2.037730755344514

Epoch: 6| Step: 10
Training loss: 2.030423641204834
Validation loss: 2.073721924135762

Epoch: 6| Step: 11
Training loss: 2.821162223815918
Validation loss: 2.0738536709098407

Epoch: 6| Step: 12
Training loss: 2.4287030696868896
Validation loss: 2.0763226401421333

Epoch: 6| Step: 13
Training loss: 1.6297401189804077
Validation loss: 2.061245472200455

Epoch: 40| Step: 0
Training loss: 2.136587142944336
Validation loss: 2.0562116740852274

Epoch: 6| Step: 1
Training loss: 3.172183036804199
Validation loss: 2.093929426644438

Epoch: 6| Step: 2
Training loss: 1.6262552738189697
Validation loss: 2.0881773412868543

Epoch: 6| Step: 3
Training loss: 2.103048086166382
Validation loss: 2.054339455020043

Epoch: 6| Step: 4
Training loss: 1.9306281805038452
Validation loss: 2.0500090878496886

Epoch: 6| Step: 5
Training loss: 2.465991973876953
Validation loss: 2.032635640072566

Epoch: 6| Step: 6
Training loss: 2.2002925872802734
Validation loss: 2.0922944161199752

Epoch: 6| Step: 7
Training loss: 2.1685612201690674
Validation loss: 2.0451988340705953

Epoch: 6| Step: 8
Training loss: 2.201448917388916
Validation loss: 2.0368044555828138

Epoch: 6| Step: 9
Training loss: 2.284601926803589
Validation loss: 2.030164808355352

Epoch: 6| Step: 10
Training loss: 2.8336658477783203
Validation loss: 2.066919205009296

Epoch: 6| Step: 11
Training loss: 2.5226244926452637
Validation loss: 2.0797671182181245

Epoch: 6| Step: 12
Training loss: 2.8763530254364014
Validation loss: 2.0213968253904775

Epoch: 6| Step: 13
Training loss: 3.1269547939300537
Validation loss: 2.0618172589168755

Epoch: 41| Step: 0
Training loss: 2.20890736579895
Validation loss: 2.046748807353358

Epoch: 6| Step: 1
Training loss: 2.6344642639160156
Validation loss: 2.0793084764993317

Epoch: 6| Step: 2
Training loss: 2.147219181060791
Validation loss: 2.0066755253781556

Epoch: 6| Step: 3
Training loss: 2.8079380989074707
Validation loss: 2.038579156321864

Epoch: 6| Step: 4
Training loss: 2.719836950302124
Validation loss: 2.0789432038543043

Epoch: 6| Step: 5
Training loss: 2.1016430854797363
Validation loss: 2.0405735200451267

Epoch: 6| Step: 6
Training loss: 2.507580041885376
Validation loss: 2.0534025853680027

Epoch: 6| Step: 7
Training loss: 2.1378896236419678
Validation loss: 2.0685510263648084

Epoch: 6| Step: 8
Training loss: 2.3545374870300293
Validation loss: 2.0489669179403656

Epoch: 6| Step: 9
Training loss: 1.912258267402649
Validation loss: 2.0593822156229327

Epoch: 6| Step: 10
Training loss: 2.5946826934814453
Validation loss: 2.0403516600208897

Epoch: 6| Step: 11
Training loss: 2.414835214614868
Validation loss: 2.04738615405175

Epoch: 6| Step: 12
Training loss: 1.8278993368148804
Validation loss: 2.0888666375990836

Epoch: 6| Step: 13
Training loss: 2.8885815143585205
Validation loss: 2.0151406744475007

Epoch: 42| Step: 0
Training loss: 2.196606159210205
Validation loss: 2.03984825329114

Epoch: 6| Step: 1
Training loss: 2.2417705059051514
Validation loss: 2.031402340499304

Epoch: 6| Step: 2
Training loss: 1.9730406999588013
Validation loss: 2.019401104219498

Epoch: 6| Step: 3
Training loss: 2.8704159259796143
Validation loss: 2.0725293133848455

Epoch: 6| Step: 4
Training loss: 2.8184854984283447
Validation loss: 2.028835754240713

Epoch: 6| Step: 5
Training loss: 2.417943239212036
Validation loss: 2.0280479679825487

Epoch: 6| Step: 6
Training loss: 1.8377912044525146
Validation loss: 2.0415945232555432

Epoch: 6| Step: 7
Training loss: 1.8294392824172974
Validation loss: 2.0298498933033278

Epoch: 6| Step: 8
Training loss: 3.0072922706604004
Validation loss: 2.0548338979803105

Epoch: 6| Step: 9
Training loss: 2.9462685585021973
Validation loss: 2.0429596388211815

Epoch: 6| Step: 10
Training loss: 2.2523741722106934
Validation loss: 2.064390610623103

Epoch: 6| Step: 11
Training loss: 2.567457675933838
Validation loss: 2.0034785629600607

Epoch: 6| Step: 12
Training loss: 2.052790641784668
Validation loss: 2.0654924723409835

Epoch: 6| Step: 13
Training loss: 2.000655174255371
Validation loss: 2.0854633315916984

Epoch: 43| Step: 0
Training loss: 2.3899731636047363
Validation loss: 2.0425011983481784

Epoch: 6| Step: 1
Training loss: 2.423135280609131
Validation loss: 2.0407891235043927

Epoch: 6| Step: 2
Training loss: 1.9116305112838745
Validation loss: 2.0395849558614914

Epoch: 6| Step: 3
Training loss: 2.14426326751709
Validation loss: 2.0189530734092958

Epoch: 6| Step: 4
Training loss: 1.8038139343261719
Validation loss: 2.0601485416453373

Epoch: 6| Step: 5
Training loss: 2.8137989044189453
Validation loss: 2.0195248562802552

Epoch: 6| Step: 6
Training loss: 3.288957118988037
Validation loss: 2.00608612773239

Epoch: 6| Step: 7
Training loss: 2.3347296714782715
Validation loss: 2.029635875455795

Epoch: 6| Step: 8
Training loss: 1.814985752105713
Validation loss: 2.0546037561150006

Epoch: 6| Step: 9
Training loss: 2.1163854598999023
Validation loss: 2.03193752227291

Epoch: 6| Step: 10
Training loss: 2.6262874603271484
Validation loss: 2.03731829761177

Epoch: 6| Step: 11
Training loss: 1.9126369953155518
Validation loss: 2.0271179137691373

Epoch: 6| Step: 12
Training loss: 2.7963342666625977
Validation loss: 2.044496064545006

Epoch: 6| Step: 13
Training loss: 2.9815378189086914
Validation loss: 2.024969441916353

Epoch: 44| Step: 0
Training loss: 1.235809564590454
Validation loss: 2.0751455342897804

Epoch: 6| Step: 1
Training loss: 1.949635624885559
Validation loss: 2.0219940870038924

Epoch: 6| Step: 2
Training loss: 1.9655332565307617
Validation loss: 2.0509245293114775

Epoch: 6| Step: 3
Training loss: 2.425562858581543
Validation loss: 2.028651337469778

Epoch: 6| Step: 4
Training loss: 2.9111499786376953
Validation loss: 2.023913910312037

Epoch: 6| Step: 5
Training loss: 2.4977965354919434
Validation loss: 2.0306482673973165

Epoch: 6| Step: 6
Training loss: 2.0597190856933594
Validation loss: 2.0199951305184314

Epoch: 6| Step: 7
Training loss: 2.140408515930176
Validation loss: 2.0200373998252292

Epoch: 6| Step: 8
Training loss: 2.156911849975586
Validation loss: 2.038373110114887

Epoch: 6| Step: 9
Training loss: 3.2744898796081543
Validation loss: 2.0337430841179303

Epoch: 6| Step: 10
Training loss: 2.9808971881866455
Validation loss: 2.03710469507402

Epoch: 6| Step: 11
Training loss: 2.5987741947174072
Validation loss: 2.0449369645887807

Epoch: 6| Step: 12
Training loss: 2.4798614978790283
Validation loss: 2.0186474964182866

Epoch: 6| Step: 13
Training loss: 2.7043368816375732
Validation loss: 2.011263462804979

Epoch: 45| Step: 0
Training loss: 1.7945114374160767
Validation loss: 2.0142679804114887

Epoch: 6| Step: 1
Training loss: 2.883876085281372
Validation loss: 2.042072155142343

Epoch: 6| Step: 2
Training loss: 2.746997356414795
Validation loss: 2.0158599576642438

Epoch: 6| Step: 3
Training loss: 1.9834954738616943
Validation loss: 2.0080875465946812

Epoch: 6| Step: 4
Training loss: 2.1396050453186035
Validation loss: 2.012237503964414

Epoch: 6| Step: 5
Training loss: 2.181723117828369
Validation loss: 2.021005666384133

Epoch: 6| Step: 6
Training loss: 2.317577838897705
Validation loss: 2.049305640241151

Epoch: 6| Step: 7
Training loss: 2.5537261962890625
Validation loss: 2.0350675557249334

Epoch: 6| Step: 8
Training loss: 2.8658528327941895
Validation loss: 2.0454902110561246

Epoch: 6| Step: 9
Training loss: 2.0921833515167236
Validation loss: 1.998194940628544

Epoch: 6| Step: 10
Training loss: 2.5470259189605713
Validation loss: 2.042892894437236

Epoch: 6| Step: 11
Training loss: 2.8857688903808594
Validation loss: 2.034387042445521

Epoch: 6| Step: 12
Training loss: 2.13193941116333
Validation loss: 2.035965186293407

Epoch: 6| Step: 13
Training loss: 1.586552381515503
Validation loss: 2.0306610702186503

Epoch: 46| Step: 0
Training loss: 2.62595534324646
Validation loss: 2.0534114632555234

Epoch: 6| Step: 1
Training loss: 2.0371978282928467
Validation loss: 2.0333817876795286

Epoch: 6| Step: 2
Training loss: 2.303351402282715
Validation loss: 2.059548347227035

Epoch: 6| Step: 3
Training loss: 2.0293891429901123
Validation loss: 2.0593478128474247

Epoch: 6| Step: 4
Training loss: 2.5133254528045654
Validation loss: 2.0287159296774093

Epoch: 6| Step: 5
Training loss: 2.9391236305236816
Validation loss: 2.044286253631756

Epoch: 6| Step: 6
Training loss: 2.198005199432373
Validation loss: 2.0346847657234437

Epoch: 6| Step: 7
Training loss: 2.3779306411743164
Validation loss: 2.055854762754133

Epoch: 6| Step: 8
Training loss: 2.2703936100006104
Validation loss: 2.055922649239981

Epoch: 6| Step: 9
Training loss: 2.4686179161071777
Validation loss: 2.055464311312604

Epoch: 6| Step: 10
Training loss: 2.2002010345458984
Validation loss: 2.0630024017826205

Epoch: 6| Step: 11
Training loss: 2.58272123336792
Validation loss: 2.0604687941971647

Epoch: 6| Step: 12
Training loss: 2.406806468963623
Validation loss: 2.0213510810687976

Epoch: 6| Step: 13
Training loss: 2.135864019393921
Validation loss: 2.038355645313058

Epoch: 47| Step: 0
Training loss: 1.823554515838623
Validation loss: 2.06444275763727

Epoch: 6| Step: 1
Training loss: 2.948869228363037
Validation loss: 2.0477103289737495

Epoch: 6| Step: 2
Training loss: 2.5143330097198486
Validation loss: 2.0828082215401436

Epoch: 6| Step: 3
Training loss: 2.251465320587158
Validation loss: 2.0621713874160603

Epoch: 6| Step: 4
Training loss: 2.677799701690674
Validation loss: 2.065645649868955

Epoch: 6| Step: 5
Training loss: 2.3183484077453613
Validation loss: 2.03546045672509

Epoch: 6| Step: 6
Training loss: 2.913276195526123
Validation loss: 2.0699911450826995

Epoch: 6| Step: 7
Training loss: 2.858652353286743
Validation loss: 2.05103370066612

Epoch: 6| Step: 8
Training loss: 1.842743992805481
Validation loss: 2.0535485244566396

Epoch: 6| Step: 9
Training loss: 2.0698142051696777
Validation loss: 2.0103731052849882

Epoch: 6| Step: 10
Training loss: 2.630572557449341
Validation loss: 2.0615349687555784

Epoch: 6| Step: 11
Training loss: 1.9394347667694092
Validation loss: 2.0281135933373564

Epoch: 6| Step: 12
Training loss: 2.0210163593292236
Validation loss: 2.0703387388619046

Epoch: 6| Step: 13
Training loss: 2.105881690979004
Validation loss: 2.02798701870826

Epoch: 48| Step: 0
Training loss: 2.409893274307251
Validation loss: 2.0156123151061354

Epoch: 6| Step: 1
Training loss: 2.4636363983154297
Validation loss: 2.0632065291045816

Epoch: 6| Step: 2
Training loss: 2.363614559173584
Validation loss: 2.061563085484248

Epoch: 6| Step: 3
Training loss: 2.79240083694458
Validation loss: 2.046882539667109

Epoch: 6| Step: 4
Training loss: 1.667568325996399
Validation loss: 2.050005707689511

Epoch: 6| Step: 5
Training loss: 1.80552077293396
Validation loss: 2.0837935504092964

Epoch: 6| Step: 6
Training loss: 2.3689661026000977
Validation loss: 2.030871195177878

Epoch: 6| Step: 7
Training loss: 2.364272356033325
Validation loss: 2.077987540152765

Epoch: 6| Step: 8
Training loss: 2.693694829940796
Validation loss: 2.0411163850497176

Epoch: 6| Step: 9
Training loss: 3.0618181228637695
Validation loss: 2.0610841474225445

Epoch: 6| Step: 10
Training loss: 2.325578451156616
Validation loss: 2.0349760542633715

Epoch: 6| Step: 11
Training loss: 2.058757781982422
Validation loss: 2.0466962822021975

Epoch: 6| Step: 12
Training loss: 2.3125686645507812
Validation loss: 2.0677701555272585

Epoch: 6| Step: 13
Training loss: 2.0250723361968994
Validation loss: 2.0756199462439424

Epoch: 49| Step: 0
Training loss: 2.379160165786743
Validation loss: 2.0336368737682218

Epoch: 6| Step: 1
Training loss: 2.5543105602264404
Validation loss: 2.0501357227243404

Epoch: 6| Step: 2
Training loss: 2.3496971130371094
Validation loss: 2.023744565184398

Epoch: 6| Step: 3
Training loss: 2.2404918670654297
Validation loss: 2.099977449704242

Epoch: 6| Step: 4
Training loss: 2.4654366970062256
Validation loss: 2.033799836712499

Epoch: 6| Step: 5
Training loss: 2.347586154937744
Validation loss: 2.058798031140399

Epoch: 6| Step: 6
Training loss: 2.0299267768859863
Validation loss: 2.054935932159424

Epoch: 6| Step: 7
Training loss: 2.5855813026428223
Validation loss: 2.0571148908266457

Epoch: 6| Step: 8
Training loss: 2.1935343742370605
Validation loss: 2.036697651750298

Epoch: 6| Step: 9
Training loss: 2.1587421894073486
Validation loss: 2.045554740454561

Epoch: 6| Step: 10
Training loss: 2.2394771575927734
Validation loss: 2.040637772570374

Epoch: 6| Step: 11
Training loss: 2.488182306289673
Validation loss: 2.071716530348665

Epoch: 6| Step: 12
Training loss: 2.4514174461364746
Validation loss: 2.056585870763307

Epoch: 6| Step: 13
Training loss: 2.2098143100738525
Validation loss: 2.0659092574991207

Epoch: 50| Step: 0
Training loss: 2.0035290718078613
Validation loss: 2.0513345297946723

Epoch: 6| Step: 1
Training loss: 2.9046850204467773
Validation loss: 2.0362652373570267

Epoch: 6| Step: 2
Training loss: 2.4300971031188965
Validation loss: 2.0278329131423787

Epoch: 6| Step: 3
Training loss: 2.165158271789551
Validation loss: 2.0562759125104515

Epoch: 6| Step: 4
Training loss: 2.32037091255188
Validation loss: 2.050707394076932

Epoch: 6| Step: 5
Training loss: 2.64571475982666
Validation loss: 2.0326716130779636

Epoch: 6| Step: 6
Training loss: 2.4562692642211914
Validation loss: 2.0139286928279425

Epoch: 6| Step: 7
Training loss: 2.7920570373535156
Validation loss: 2.041895467747924

Epoch: 6| Step: 8
Training loss: 2.68390154838562
Validation loss: 2.064326242734027

Epoch: 6| Step: 9
Training loss: 2.2276434898376465
Validation loss: 2.044526743632491

Epoch: 6| Step: 10
Training loss: 2.0499682426452637
Validation loss: 2.058386178426845

Epoch: 6| Step: 11
Training loss: 1.5127007961273193
Validation loss: 2.0419436475282073

Epoch: 6| Step: 12
Training loss: 2.015209674835205
Validation loss: 2.069582389247033

Epoch: 6| Step: 13
Training loss: 2.913381814956665
Validation loss: 2.037003368459722

Epoch: 51| Step: 0
Training loss: 2.105797052383423
Validation loss: 2.039514098116147

Epoch: 6| Step: 1
Training loss: 1.771669626235962
Validation loss: 2.051818678455968

Epoch: 6| Step: 2
Training loss: 2.4116482734680176
Validation loss: 2.003529328171925

Epoch: 6| Step: 3
Training loss: 2.1694462299346924
Validation loss: 2.041701638570396

Epoch: 6| Step: 4
Training loss: 2.528136968612671
Validation loss: 2.0884155252928376

Epoch: 6| Step: 5
Training loss: 2.3178584575653076
Validation loss: 2.01568010545546

Epoch: 6| Step: 6
Training loss: 2.3937551975250244
Validation loss: 2.0477045518095776

Epoch: 6| Step: 7
Training loss: 3.2171716690063477
Validation loss: 2.050809832029445

Epoch: 6| Step: 8
Training loss: 2.0224664211273193
Validation loss: 2.052017142695765

Epoch: 6| Step: 9
Training loss: 2.502565383911133
Validation loss: 2.040473472687506

Epoch: 6| Step: 10
Training loss: 1.5289547443389893
Validation loss: 2.0533666405626523

Epoch: 6| Step: 11
Training loss: 2.726531505584717
Validation loss: 2.0305378411405828

Epoch: 6| Step: 12
Training loss: 3.143169641494751
Validation loss: 2.051234247863934

Epoch: 6| Step: 13
Training loss: 1.6834999322891235
Validation loss: 2.0644280231127174

Epoch: 52| Step: 0
Training loss: 2.530785083770752
Validation loss: 2.0233536433148127

Epoch: 6| Step: 1
Training loss: 2.6752777099609375
Validation loss: 2.039840800787813

Epoch: 6| Step: 2
Training loss: 1.8995440006256104
Validation loss: 2.034996171151438

Epoch: 6| Step: 3
Training loss: 1.9048454761505127
Validation loss: 2.0591983538801952

Epoch: 6| Step: 4
Training loss: 2.5209951400756836
Validation loss: 2.030076346089763

Epoch: 6| Step: 5
Training loss: 1.9716981649398804
Validation loss: 2.0494421297504055

Epoch: 6| Step: 6
Training loss: 2.7241694927215576
Validation loss: 2.063428178910286

Epoch: 6| Step: 7
Training loss: 1.9744808673858643
Validation loss: 2.0437527420700237

Epoch: 6| Step: 8
Training loss: 2.219062566757202
Validation loss: 2.0635255305997786

Epoch: 6| Step: 9
Training loss: 1.8490378856658936
Validation loss: 2.045587739636821

Epoch: 6| Step: 10
Training loss: 2.301581382751465
Validation loss: 2.0651092478024062

Epoch: 6| Step: 11
Training loss: 2.5451345443725586
Validation loss: 2.04635989409621

Epoch: 6| Step: 12
Training loss: 2.883371353149414
Validation loss: 2.0434618688398793

Epoch: 6| Step: 13
Training loss: 3.2825679779052734
Validation loss: 2.0820016835325506

Epoch: 53| Step: 0
Training loss: 2.1114001274108887
Validation loss: 2.0316268192824496

Epoch: 6| Step: 1
Training loss: 2.392439603805542
Validation loss: 2.0683960299338064

Epoch: 6| Step: 2
Training loss: 2.913140058517456
Validation loss: 2.0519173978477396

Epoch: 6| Step: 3
Training loss: 2.8013217449188232
Validation loss: 2.062021702848455

Epoch: 6| Step: 4
Training loss: 2.3999595642089844
Validation loss: 2.0646892606571154

Epoch: 6| Step: 5
Training loss: 2.144339084625244
Validation loss: 2.0591659263897966

Epoch: 6| Step: 6
Training loss: 2.5253477096557617
Validation loss: 2.0699498166320143

Epoch: 6| Step: 7
Training loss: 1.946149468421936
Validation loss: 2.0224445199453704

Epoch: 6| Step: 8
Training loss: 2.5057857036590576
Validation loss: 2.048258621205566

Epoch: 6| Step: 9
Training loss: 1.861323356628418
Validation loss: 2.053894841542808

Epoch: 6| Step: 10
Training loss: 2.6845169067382812
Validation loss: 2.0439087780573035

Epoch: 6| Step: 11
Training loss: 1.6201750040054321
Validation loss: 2.0486401614322456

Epoch: 6| Step: 12
Training loss: 2.2976155281066895
Validation loss: 2.0369584944940384

Epoch: 6| Step: 13
Training loss: 2.7717556953430176
Validation loss: 2.0138535050935644

Epoch: 54| Step: 0
Training loss: 2.370968818664551
Validation loss: 2.0591431215245235

Epoch: 6| Step: 1
Training loss: 2.557676315307617
Validation loss: 2.078082829393366

Epoch: 6| Step: 2
Training loss: 2.6261720657348633
Validation loss: 2.032666124323363

Epoch: 6| Step: 3
Training loss: 2.4296226501464844
Validation loss: 2.0548649308502034

Epoch: 6| Step: 4
Training loss: 2.1914796829223633
Validation loss: 2.086858598134851

Epoch: 6| Step: 5
Training loss: 1.9644280672073364
Validation loss: 2.0484509634715256

Epoch: 6| Step: 6
Training loss: 2.976408004760742
Validation loss: 2.029266684286056

Epoch: 6| Step: 7
Training loss: 1.884208083152771
Validation loss: 2.0674311704533075

Epoch: 6| Step: 8
Training loss: 2.655608654022217
Validation loss: 2.0548646860225226

Epoch: 6| Step: 9
Training loss: 2.4871878623962402
Validation loss: 2.0349255120882423

Epoch: 6| Step: 10
Training loss: 1.6155880689620972
Validation loss: 2.068573174938079

Epoch: 6| Step: 11
Training loss: 2.548142194747925
Validation loss: 2.0554393593982985

Epoch: 6| Step: 12
Training loss: 2.1053669452667236
Validation loss: 2.0332799162915958

Epoch: 6| Step: 13
Training loss: 2.1709556579589844
Validation loss: 2.0585011102819957

Epoch: 55| Step: 0
Training loss: 2.2484817504882812
Validation loss: 2.016744823865993

Epoch: 6| Step: 1
Training loss: 1.8117432594299316
Validation loss: 2.05185172506558

Epoch: 6| Step: 2
Training loss: 2.4635555744171143
Validation loss: 2.0169679605832664

Epoch: 6| Step: 3
Training loss: 2.3815929889678955
Validation loss: 2.0561220543358916

Epoch: 6| Step: 4
Training loss: 2.091212749481201
Validation loss: 2.0118120908737183

Epoch: 6| Step: 5
Training loss: 2.201070547103882
Validation loss: 2.0567809099792154

Epoch: 6| Step: 6
Training loss: 1.8799960613250732
Validation loss: 2.0409973000967376

Epoch: 6| Step: 7
Training loss: 2.374337911605835
Validation loss: 2.066630627519341

Epoch: 6| Step: 8
Training loss: 2.653292179107666
Validation loss: 2.031257233312053

Epoch: 6| Step: 9
Training loss: 2.6641180515289307
Validation loss: 2.0480822952844764

Epoch: 6| Step: 10
Training loss: 1.9493553638458252
Validation loss: 1.9965911449924592

Epoch: 6| Step: 11
Training loss: 3.313215732574463
Validation loss: 2.016153088179968

Epoch: 6| Step: 12
Training loss: 2.1444716453552246
Validation loss: 2.0437933398831274

Epoch: 6| Step: 13
Training loss: 2.6049232482910156
Validation loss: 1.9890690772764144

Epoch: 56| Step: 0
Training loss: 2.260390043258667
Validation loss: 2.03370528323676

Epoch: 6| Step: 1
Training loss: 1.873452067375183
Validation loss: 2.011582730918802

Epoch: 6| Step: 2
Training loss: 2.2947959899902344
Validation loss: 2.022040731163435

Epoch: 6| Step: 3
Training loss: 1.824571967124939
Validation loss: 2.048874016731016

Epoch: 6| Step: 4
Training loss: 2.388256072998047
Validation loss: 2.0396257344112603

Epoch: 6| Step: 5
Training loss: 2.354215621948242
Validation loss: 2.094293625124039

Epoch: 6| Step: 6
Training loss: 2.046356201171875
Validation loss: 2.0465603772030083

Epoch: 6| Step: 7
Training loss: 2.3990519046783447
Validation loss: 2.037990762341407

Epoch: 6| Step: 8
Training loss: 1.8720852136611938
Validation loss: 2.005082645723897

Epoch: 6| Step: 9
Training loss: 2.5651283264160156
Validation loss: 2.0489652041466004

Epoch: 6| Step: 10
Training loss: 1.7969053983688354
Validation loss: 2.0412425353962886

Epoch: 6| Step: 11
Training loss: 3.0119664669036865
Validation loss: 2.0645881955341627

Epoch: 6| Step: 12
Training loss: 3.2571730613708496
Validation loss: 2.010914018077235

Epoch: 6| Step: 13
Training loss: 3.142570734024048
Validation loss: 2.042789525883172

Epoch: 57| Step: 0
Training loss: 2.1580026149749756
Validation loss: 2.060196412506924

Epoch: 6| Step: 1
Training loss: 2.5450925827026367
Validation loss: 2.022616595350286

Epoch: 6| Step: 2
Training loss: 2.3323616981506348
Validation loss: 2.0230002582714124

Epoch: 6| Step: 3
Training loss: 2.612685441970825
Validation loss: 2.025634084978411

Epoch: 6| Step: 4
Training loss: 2.364738941192627
Validation loss: 2.0210775893221617

Epoch: 6| Step: 5
Training loss: 2.175729751586914
Validation loss: 2.0188588096249487

Epoch: 6| Step: 6
Training loss: 1.6910640001296997
Validation loss: 2.0267384359913487

Epoch: 6| Step: 7
Training loss: 2.6842386722564697
Validation loss: 2.0174110730489097

Epoch: 6| Step: 8
Training loss: 1.8230133056640625
Validation loss: 2.010223816799861

Epoch: 6| Step: 9
Training loss: 2.1214494705200195
Validation loss: 2.0451322242777836

Epoch: 6| Step: 10
Training loss: 2.247530460357666
Validation loss: 2.037151159778718

Epoch: 6| Step: 11
Training loss: 2.593804121017456
Validation loss: 2.035118915701425

Epoch: 6| Step: 12
Training loss: 2.8984689712524414
Validation loss: 2.056833005720569

Epoch: 6| Step: 13
Training loss: 2.197300434112549
Validation loss: 1.994678702405704

Epoch: 58| Step: 0
Training loss: 2.051187753677368
Validation loss: 2.0789408183866933

Epoch: 6| Step: 1
Training loss: 2.297548770904541
Validation loss: 2.040388398273017

Epoch: 6| Step: 2
Training loss: 2.2861642837524414
Validation loss: 2.0628165122001403

Epoch: 6| Step: 3
Training loss: 3.051530599594116
Validation loss: 1.991254952646071

Epoch: 6| Step: 4
Training loss: 2.1571602821350098
Validation loss: 2.045473590973885

Epoch: 6| Step: 5
Training loss: 2.146949529647827
Validation loss: 2.0399485044581915

Epoch: 6| Step: 6
Training loss: 2.6389389038085938
Validation loss: 2.0258172071108254

Epoch: 6| Step: 7
Training loss: 1.7207112312316895
Validation loss: 2.0378137147554787

Epoch: 6| Step: 8
Training loss: 1.70182204246521
Validation loss: 2.0301113872117895

Epoch: 6| Step: 9
Training loss: 2.533557415008545
Validation loss: 2.022177155299853

Epoch: 6| Step: 10
Training loss: 2.411895513534546
Validation loss: 2.049396530274422

Epoch: 6| Step: 11
Training loss: 2.124539375305176
Validation loss: 2.0205247299645537

Epoch: 6| Step: 12
Training loss: 3.1637237071990967
Validation loss: 2.032558660353384

Epoch: 6| Step: 13
Training loss: 2.2536168098449707
Validation loss: 2.0251854876036286

Epoch: 59| Step: 0
Training loss: 1.6036895513534546
Validation loss: 2.0463515802096297

Epoch: 6| Step: 1
Training loss: 2.051494598388672
Validation loss: 2.040181703464959

Epoch: 6| Step: 2
Training loss: 3.1343960762023926
Validation loss: 2.044464156191836

Epoch: 6| Step: 3
Training loss: 1.5594993829727173
Validation loss: 2.0478878713423208

Epoch: 6| Step: 4
Training loss: 1.9717469215393066
Validation loss: 2.0559040756635767

Epoch: 6| Step: 5
Training loss: 1.806662917137146
Validation loss: 2.022715881306638

Epoch: 6| Step: 6
Training loss: 2.250148296356201
Validation loss: 2.0347520484719226

Epoch: 6| Step: 7
Training loss: 2.762092113494873
Validation loss: 2.0234164525103826

Epoch: 6| Step: 8
Training loss: 2.3380496501922607
Validation loss: 1.9918860184249056

Epoch: 6| Step: 9
Training loss: 2.5329337120056152
Validation loss: 2.0510792193874234

Epoch: 6| Step: 10
Training loss: 2.712799549102783
Validation loss: 2.0185318569983206

Epoch: 6| Step: 11
Training loss: 1.5273150205612183
Validation loss: 2.0393438211051365

Epoch: 6| Step: 12
Training loss: 3.414943218231201
Validation loss: 2.002790476686211

Epoch: 6| Step: 13
Training loss: 3.434112071990967
Validation loss: 2.019419591913941

Epoch: 60| Step: 0
Training loss: 2.5130324363708496
Validation loss: 1.9962011306516585

Epoch: 6| Step: 1
Training loss: 1.7066991329193115
Validation loss: 2.009064619259168

Epoch: 6| Step: 2
Training loss: 2.774446487426758
Validation loss: 2.0291612763558664

Epoch: 6| Step: 3
Training loss: 3.261162757873535
Validation loss: 2.0313027058878252

Epoch: 6| Step: 4
Training loss: 2.4633822441101074
Validation loss: 2.0432837765703917

Epoch: 6| Step: 5
Training loss: 2.4014101028442383
Validation loss: 2.0232228053513395

Epoch: 6| Step: 6
Training loss: 2.1610028743743896
Validation loss: 2.0101976215198474

Epoch: 6| Step: 7
Training loss: 1.2944672107696533
Validation loss: 2.018831040269585

Epoch: 6| Step: 8
Training loss: 1.8701121807098389
Validation loss: 2.057202499399903

Epoch: 6| Step: 9
Training loss: 2.275848865509033
Validation loss: 2.000501125089584

Epoch: 6| Step: 10
Training loss: 2.5577778816223145
Validation loss: 2.0052115455750497

Epoch: 6| Step: 11
Training loss: 1.8575665950775146
Validation loss: 2.0289073208326935

Epoch: 6| Step: 12
Training loss: 2.9849653244018555
Validation loss: 2.0162558119784117

Epoch: 6| Step: 13
Training loss: 2.2683427333831787
Validation loss: 2.0371164352663103

Epoch: 61| Step: 0
Training loss: 3.0564804077148438
Validation loss: 2.0195450090592906

Epoch: 6| Step: 1
Training loss: 2.3994626998901367
Validation loss: 2.0171467616993892

Epoch: 6| Step: 2
Training loss: 2.4061343669891357
Validation loss: 2.012077059797061

Epoch: 6| Step: 3
Training loss: 2.218968152999878
Validation loss: 1.9885907250065957

Epoch: 6| Step: 4
Training loss: 1.4721245765686035
Validation loss: 2.0487239758173623

Epoch: 6| Step: 5
Training loss: 2.853468179702759
Validation loss: 2.0204868508923437

Epoch: 6| Step: 6
Training loss: 3.170177698135376
Validation loss: 2.015580733617147

Epoch: 6| Step: 7
Training loss: 1.7266299724578857
Validation loss: 2.0346014294573056

Epoch: 6| Step: 8
Training loss: 2.1252529621124268
Validation loss: 2.030880608866292

Epoch: 6| Step: 9
Training loss: 2.6468591690063477
Validation loss: 2.017645489784979

Epoch: 6| Step: 10
Training loss: 1.4950547218322754
Validation loss: 2.000304978380921

Epoch: 6| Step: 11
Training loss: 2.025243043899536
Validation loss: 2.015892950437402

Epoch: 6| Step: 12
Training loss: 2.531468629837036
Validation loss: 2.0346890444396646

Epoch: 6| Step: 13
Training loss: 2.3399832248687744
Validation loss: 2.011372165013385

Epoch: 62| Step: 0
Training loss: 1.803061842918396
Validation loss: 2.0289994952499226

Epoch: 6| Step: 1
Training loss: 2.2548716068267822
Validation loss: 1.993634658475076

Epoch: 6| Step: 2
Training loss: 1.6987357139587402
Validation loss: 2.017101913370112

Epoch: 6| Step: 3
Training loss: 1.682368516921997
Validation loss: 2.020498343693313

Epoch: 6| Step: 4
Training loss: 2.6257801055908203
Validation loss: 1.9795112097135155

Epoch: 6| Step: 5
Training loss: 2.128037452697754
Validation loss: 2.032687644804678

Epoch: 6| Step: 6
Training loss: 2.2198104858398438
Validation loss: 2.045291764761812

Epoch: 6| Step: 7
Training loss: 3.159912109375
Validation loss: 2.0144931565048876

Epoch: 6| Step: 8
Training loss: 2.6934168338775635
Validation loss: 2.0014671587174937

Epoch: 6| Step: 9
Training loss: 1.8746964931488037
Validation loss: 2.05022459132697

Epoch: 6| Step: 10
Training loss: 2.632658004760742
Validation loss: 2.02850438446127

Epoch: 6| Step: 11
Training loss: 2.4388580322265625
Validation loss: 2.0499098326570246

Epoch: 6| Step: 12
Training loss: 2.640063762664795
Validation loss: 2.0619669755299888

Epoch: 6| Step: 13
Training loss: 2.630343437194824
Validation loss: 2.016652686621553

Epoch: 63| Step: 0
Training loss: 2.0641915798187256
Validation loss: 2.066685656065582

Epoch: 6| Step: 1
Training loss: 2.7473368644714355
Validation loss: 2.05015040469426

Epoch: 6| Step: 2
Training loss: 2.7725322246551514
Validation loss: 2.051614712643367

Epoch: 6| Step: 3
Training loss: 2.051520347595215
Validation loss: 2.0219497142299527

Epoch: 6| Step: 4
Training loss: 2.363107204437256
Validation loss: 2.0676729768835087

Epoch: 6| Step: 5
Training loss: 2.0483484268188477
Validation loss: 2.0262646047017907

Epoch: 6| Step: 6
Training loss: 2.209549903869629
Validation loss: 2.019148052379649

Epoch: 6| Step: 7
Training loss: 1.9405341148376465
Validation loss: 2.0236155999604093

Epoch: 6| Step: 8
Training loss: 2.5453600883483887
Validation loss: 1.9969976050879366

Epoch: 6| Step: 9
Training loss: 1.7241995334625244
Validation loss: 2.036713223303518

Epoch: 6| Step: 10
Training loss: 2.0878241062164307
Validation loss: 2.033339451718074

Epoch: 6| Step: 11
Training loss: 2.551100730895996
Validation loss: 2.045764741077218

Epoch: 6| Step: 12
Training loss: 2.703624725341797
Validation loss: 2.0159111484404533

Epoch: 6| Step: 13
Training loss: 2.292915105819702
Validation loss: 2.0180983261395524

Epoch: 64| Step: 0
Training loss: 1.7382934093475342
Validation loss: 2.0300393437826507

Epoch: 6| Step: 1
Training loss: 1.7009623050689697
Validation loss: 1.9857091224321755

Epoch: 6| Step: 2
Training loss: 2.1299681663513184
Validation loss: 2.020463205152942

Epoch: 6| Step: 3
Training loss: 2.321951150894165
Validation loss: 2.004511389681088

Epoch: 6| Step: 4
Training loss: 2.1019513607025146
Validation loss: 2.05553505497594

Epoch: 6| Step: 5
Training loss: 2.1037261486053467
Validation loss: 2.0535078894707466

Epoch: 6| Step: 6
Training loss: 2.305365800857544
Validation loss: 2.0447417497634888

Epoch: 6| Step: 7
Training loss: 3.2731285095214844
Validation loss: 2.0394072109653103

Epoch: 6| Step: 8
Training loss: 2.2224159240722656
Validation loss: 2.025259515290619

Epoch: 6| Step: 9
Training loss: 1.8555786609649658
Validation loss: 2.0577441774388796

Epoch: 6| Step: 10
Training loss: 2.901982545852661
Validation loss: 2.030627394235262

Epoch: 6| Step: 11
Training loss: 2.291060447692871
Validation loss: 2.0364947011393886

Epoch: 6| Step: 12
Training loss: 2.368351697921753
Validation loss: 2.0267377002264864

Epoch: 6| Step: 13
Training loss: 3.0210330486297607
Validation loss: 2.0416382141010736

Epoch: 65| Step: 0
Training loss: 2.8156795501708984
Validation loss: 2.0285341790927354

Epoch: 6| Step: 1
Training loss: 2.598130226135254
Validation loss: 2.0885309301396853

Epoch: 6| Step: 2
Training loss: 2.42278790473938
Validation loss: 2.043586181056115

Epoch: 6| Step: 3
Training loss: 2.2057793140411377
Validation loss: 2.02206059937836

Epoch: 6| Step: 4
Training loss: 2.8016762733459473
Validation loss: 1.9924749815335838

Epoch: 6| Step: 5
Training loss: 2.271446704864502
Validation loss: 2.05205274140963

Epoch: 6| Step: 6
Training loss: 1.354660987854004
Validation loss: 2.054912931175642

Epoch: 6| Step: 7
Training loss: 1.9006187915802002
Validation loss: 2.0115052846170243

Epoch: 6| Step: 8
Training loss: 2.519758462905884
Validation loss: 2.0433468434118454

Epoch: 6| Step: 9
Training loss: 2.4072041511535645
Validation loss: 2.0436078104921567

Epoch: 6| Step: 10
Training loss: 1.8886258602142334
Validation loss: 2.015866971785022

Epoch: 6| Step: 11
Training loss: 2.0883278846740723
Validation loss: 2.0172045128319853

Epoch: 6| Step: 12
Training loss: 2.5372934341430664
Validation loss: 2.004139801507355

Epoch: 6| Step: 13
Training loss: 2.96132493019104
Validation loss: 2.0439129747370237

Epoch: 66| Step: 0
Training loss: 2.4029574394226074
Validation loss: 2.0609928459249516

Epoch: 6| Step: 1
Training loss: 2.1504809856414795
Validation loss: 2.0510685290059736

Epoch: 6| Step: 2
Training loss: 2.274721622467041
Validation loss: 2.021311649712183

Epoch: 6| Step: 3
Training loss: 2.5940358638763428
Validation loss: 2.040899174187773

Epoch: 6| Step: 4
Training loss: 2.354018449783325
Validation loss: 2.0319817502011537

Epoch: 6| Step: 5
Training loss: 2.36038875579834
Validation loss: 2.0397389268362396

Epoch: 6| Step: 6
Training loss: 2.344305992126465
Validation loss: 1.994978153577415

Epoch: 6| Step: 7
Training loss: 1.6449400186538696
Validation loss: 2.0536347025184223

Epoch: 6| Step: 8
Training loss: 2.783810615539551
Validation loss: 2.0434698853441464

Epoch: 6| Step: 9
Training loss: 2.636256694793701
Validation loss: 2.0123196955650084

Epoch: 6| Step: 10
Training loss: 1.697379469871521
Validation loss: 2.0234760827915643

Epoch: 6| Step: 11
Training loss: 2.361734390258789
Validation loss: 2.0575651661042245

Epoch: 6| Step: 12
Training loss: 2.14983868598938
Validation loss: 2.051034140330489

Epoch: 6| Step: 13
Training loss: 2.231382131576538
Validation loss: 2.0259840872979935

Epoch: 67| Step: 0
Training loss: 2.902122974395752
Validation loss: 2.0168830886963875

Epoch: 6| Step: 1
Training loss: 1.9106433391571045
Validation loss: 2.034134334133517

Epoch: 6| Step: 2
Training loss: 2.3716375827789307
Validation loss: 2.022260312111147

Epoch: 6| Step: 3
Training loss: 2.5350871086120605
Validation loss: 2.0602685225907194

Epoch: 6| Step: 4
Training loss: 1.906245231628418
Validation loss: 2.073196530342102

Epoch: 6| Step: 5
Training loss: 2.787616729736328
Validation loss: 2.0421331031348116

Epoch: 6| Step: 6
Training loss: 2.247789144515991
Validation loss: 2.05234125737221

Epoch: 6| Step: 7
Training loss: 2.1315338611602783
Validation loss: 2.063278108514765

Epoch: 6| Step: 8
Training loss: 2.662487268447876
Validation loss: 2.0691520796027234

Epoch: 6| Step: 9
Training loss: 1.1761844158172607
Validation loss: 2.048868433121712

Epoch: 6| Step: 10
Training loss: 2.156496524810791
Validation loss: 2.0463027005554526

Epoch: 6| Step: 11
Training loss: 2.079531669616699
Validation loss: 2.0385626516034527

Epoch: 6| Step: 12
Training loss: 2.5730202198028564
Validation loss: 2.0423214448395597

Epoch: 6| Step: 13
Training loss: 3.0984458923339844
Validation loss: 2.0392691627625497

Epoch: 68| Step: 0
Training loss: 2.9638683795928955
Validation loss: 2.0446915639344083

Epoch: 6| Step: 1
Training loss: 2.102574348449707
Validation loss: 2.080241508381341

Epoch: 6| Step: 2
Training loss: 2.6342086791992188
Validation loss: 2.0359199123997844

Epoch: 6| Step: 3
Training loss: 1.9341167211532593
Validation loss: 2.0337979614093737

Epoch: 6| Step: 4
Training loss: 2.4357447624206543
Validation loss: 2.0279222150002756

Epoch: 6| Step: 5
Training loss: 1.9266924858093262
Validation loss: 2.0260241493102042

Epoch: 6| Step: 6
Training loss: 2.95560359954834
Validation loss: 2.0062917009476693

Epoch: 6| Step: 7
Training loss: 2.199957847595215
Validation loss: 2.034206067362139

Epoch: 6| Step: 8
Training loss: 2.3411498069763184
Validation loss: 2.0342548560070735

Epoch: 6| Step: 9
Training loss: 2.1923725605010986
Validation loss: 2.0512421951499036

Epoch: 6| Step: 10
Training loss: 2.485046625137329
Validation loss: 2.0164407325047318

Epoch: 6| Step: 11
Training loss: 1.982279896736145
Validation loss: 2.0293103007860083

Epoch: 6| Step: 12
Training loss: 1.8595328330993652
Validation loss: 2.0572242813725627

Epoch: 6| Step: 13
Training loss: 2.121734619140625
Validation loss: 2.0340357262601136

Epoch: 69| Step: 0
Training loss: 1.9860639572143555
Validation loss: 2.028255495973813

Epoch: 6| Step: 1
Training loss: 1.8877568244934082
Validation loss: 2.0019372329917005

Epoch: 6| Step: 2
Training loss: 3.0514962673187256
Validation loss: 2.040815181629632

Epoch: 6| Step: 3
Training loss: 2.7148914337158203
Validation loss: 2.0190810772680465

Epoch: 6| Step: 4
Training loss: 1.9101667404174805
Validation loss: 2.0247476882832025

Epoch: 6| Step: 5
Training loss: 2.3214056491851807
Validation loss: 1.995820330035302

Epoch: 6| Step: 6
Training loss: 2.2639641761779785
Validation loss: 2.054882537934088

Epoch: 6| Step: 7
Training loss: 2.3178091049194336
Validation loss: 2.0037127476866528

Epoch: 6| Step: 8
Training loss: 1.504317283630371
Validation loss: 2.008030076180735

Epoch: 6| Step: 9
Training loss: 2.44767427444458
Validation loss: 2.02494139312416

Epoch: 6| Step: 10
Training loss: 2.530811309814453
Validation loss: 2.036833224758025

Epoch: 6| Step: 11
Training loss: 2.3496580123901367
Validation loss: 2.0108412042740853

Epoch: 6| Step: 12
Training loss: 2.858031749725342
Validation loss: 2.0155575288239347

Epoch: 6| Step: 13
Training loss: 2.175100088119507
Validation loss: 2.0157334009806314

Epoch: 70| Step: 0
Training loss: 2.193680763244629
Validation loss: 1.987497059247827

Epoch: 6| Step: 1
Training loss: 2.1799886226654053
Validation loss: 2.0221532954964587

Epoch: 6| Step: 2
Training loss: 2.70511794090271
Validation loss: 2.0167482668353665

Epoch: 6| Step: 3
Training loss: 2.1832480430603027
Validation loss: 2.0055680877418927

Epoch: 6| Step: 4
Training loss: 2.6836915016174316
Validation loss: 2.032564501608572

Epoch: 6| Step: 5
Training loss: 2.7880780696868896
Validation loss: 2.0198172343674528

Epoch: 6| Step: 6
Training loss: 2.0939955711364746
Validation loss: 1.9955771123209307

Epoch: 6| Step: 7
Training loss: 2.3687705993652344
Validation loss: 2.004246311803018

Epoch: 6| Step: 8
Training loss: 1.662581443786621
Validation loss: 2.0251604357073383

Epoch: 6| Step: 9
Training loss: 2.8453903198242188
Validation loss: 2.019827499184557

Epoch: 6| Step: 10
Training loss: 1.3970876932144165
Validation loss: 2.02036456395221

Epoch: 6| Step: 11
Training loss: 1.990020751953125
Validation loss: 1.9853008639427923

Epoch: 6| Step: 12
Training loss: 3.025393486022949
Validation loss: 2.029822818694576

Epoch: 6| Step: 13
Training loss: 1.9290504455566406
Validation loss: 2.037285525311706

Epoch: 71| Step: 0
Training loss: 2.2240993976593018
Validation loss: 1.9930699102340206

Epoch: 6| Step: 1
Training loss: 1.6431115865707397
Validation loss: 2.0050757905488372

Epoch: 6| Step: 2
Training loss: 2.3534984588623047
Validation loss: 2.054146380834682

Epoch: 6| Step: 3
Training loss: 1.9003140926361084
Validation loss: 1.9985958209601782

Epoch: 6| Step: 4
Training loss: 2.3706657886505127
Validation loss: 2.0101195907080047

Epoch: 6| Step: 5
Training loss: 2.6904215812683105
Validation loss: 2.0250513143436883

Epoch: 6| Step: 6
Training loss: 2.284156560897827
Validation loss: 2.030410248746154

Epoch: 6| Step: 7
Training loss: 2.1052350997924805
Validation loss: 2.037715394009826

Epoch: 6| Step: 8
Training loss: 2.625957489013672
Validation loss: 1.9699735923479962

Epoch: 6| Step: 9
Training loss: 1.8917818069458008
Validation loss: 1.9972179782006048

Epoch: 6| Step: 10
Training loss: 2.4251322746276855
Validation loss: 2.0281848663924844

Epoch: 6| Step: 11
Training loss: 2.2651236057281494
Validation loss: 2.0216738716248543

Epoch: 6| Step: 12
Training loss: 2.2099528312683105
Validation loss: 2.002565742820822

Epoch: 6| Step: 13
Training loss: 3.394674301147461
Validation loss: 2.0279694936608754

Epoch: 72| Step: 0
Training loss: 1.9898709058761597
Validation loss: 2.0550051632747857

Epoch: 6| Step: 1
Training loss: 3.0462470054626465
Validation loss: 2.0271697236645605

Epoch: 6| Step: 2
Training loss: 2.057661533355713
Validation loss: 1.9957956434578024

Epoch: 6| Step: 3
Training loss: 2.5066659450531006
Validation loss: 2.054032178335292

Epoch: 6| Step: 4
Training loss: 1.2464611530303955
Validation loss: 1.9993824061527048

Epoch: 6| Step: 5
Training loss: 3.1278128623962402
Validation loss: 2.049584340023738

Epoch: 6| Step: 6
Training loss: 1.8344248533248901
Validation loss: 2.040110203527635

Epoch: 6| Step: 7
Training loss: 1.589187502861023
Validation loss: 2.064219400446902

Epoch: 6| Step: 8
Training loss: 2.6112327575683594
Validation loss: 2.0488579016859814

Epoch: 6| Step: 9
Training loss: 2.1422202587127686
Validation loss: 2.022093367832963

Epoch: 6| Step: 10
Training loss: 2.617191791534424
Validation loss: 2.035269659052613

Epoch: 6| Step: 11
Training loss: 2.2027430534362793
Validation loss: 2.0311993142609954

Epoch: 6| Step: 12
Training loss: 2.0515100955963135
Validation loss: 2.017555508562314

Epoch: 6| Step: 13
Training loss: 3.5591697692871094
Validation loss: 2.0239302560847294

Epoch: 73| Step: 0
Training loss: 2.030531644821167
Validation loss: 2.034254143314977

Epoch: 6| Step: 1
Training loss: 2.958500862121582
Validation loss: 2.003735815325091

Epoch: 6| Step: 2
Training loss: 1.9001283645629883
Validation loss: 2.0256044992836575

Epoch: 6| Step: 3
Training loss: 2.4577951431274414
Validation loss: 2.017653936980873

Epoch: 6| Step: 4
Training loss: 2.57558012008667
Validation loss: 2.034189760044057

Epoch: 6| Step: 5
Training loss: 2.3621060848236084
Validation loss: 2.004139631025253

Epoch: 6| Step: 6
Training loss: 2.228945255279541
Validation loss: 2.0026152723579

Epoch: 6| Step: 7
Training loss: 2.1183366775512695
Validation loss: 1.9985837321127615

Epoch: 6| Step: 8
Training loss: 2.2484569549560547
Validation loss: 2.0188300225042526

Epoch: 6| Step: 9
Training loss: 2.157843589782715
Validation loss: 2.0120478586484025

Epoch: 6| Step: 10
Training loss: 2.1791181564331055
Validation loss: 1.9982157663632465

Epoch: 6| Step: 11
Training loss: 2.2552342414855957
Validation loss: 2.016791779507873

Epoch: 6| Step: 12
Training loss: 2.6261658668518066
Validation loss: 2.014292614434355

Epoch: 6| Step: 13
Training loss: 2.173081159591675
Validation loss: 2.043836908955728

Epoch: 74| Step: 0
Training loss: 2.0102713108062744
Validation loss: 2.0265489996120496

Epoch: 6| Step: 1
Training loss: 1.9659234285354614
Validation loss: 2.019025400120725

Epoch: 6| Step: 2
Training loss: 2.5500450134277344
Validation loss: 2.0257862088500813

Epoch: 6| Step: 3
Training loss: 1.9347527027130127
Validation loss: 2.0339514799015497

Epoch: 6| Step: 4
Training loss: 2.371151924133301
Validation loss: 2.033826915166711

Epoch: 6| Step: 5
Training loss: 1.8247193098068237
Validation loss: 2.0467886617106776

Epoch: 6| Step: 6
Training loss: 2.3857293128967285
Validation loss: 2.044461375923567

Epoch: 6| Step: 7
Training loss: 2.2211358547210693
Validation loss: 2.000134675733505

Epoch: 6| Step: 8
Training loss: 2.2966320514678955
Validation loss: 2.0233172960178827

Epoch: 6| Step: 9
Training loss: 1.9403964281082153
Validation loss: 2.02290323088246

Epoch: 6| Step: 10
Training loss: 3.0669493675231934
Validation loss: 2.0048338559366043

Epoch: 6| Step: 11
Training loss: 2.280142307281494
Validation loss: 2.0144696133111113

Epoch: 6| Step: 12
Training loss: 3.149152994155884
Validation loss: 2.012341394219347

Epoch: 6| Step: 13
Training loss: 1.8414350748062134
Validation loss: 2.0389199218442364

Epoch: 75| Step: 0
Training loss: 2.6547927856445312
Validation loss: 2.0241379814763225

Epoch: 6| Step: 1
Training loss: 2.3625130653381348
Validation loss: 2.045860669946158

Epoch: 6| Step: 2
Training loss: 2.103703022003174
Validation loss: 2.024631510498703

Epoch: 6| Step: 3
Training loss: 2.7975575923919678
Validation loss: 2.0095308967815932

Epoch: 6| Step: 4
Training loss: 2.237936496734619
Validation loss: 2.038107675890769

Epoch: 6| Step: 5
Training loss: 2.1669890880584717
Validation loss: 2.0274416938904793

Epoch: 6| Step: 6
Training loss: 2.3562099933624268
Validation loss: 2.0299321759131645

Epoch: 6| Step: 7
Training loss: 1.8565871715545654
Validation loss: 1.9987897514015116

Epoch: 6| Step: 8
Training loss: 1.9134052991867065
Validation loss: 2.027566263752599

Epoch: 6| Step: 9
Training loss: 2.1656012535095215
Validation loss: 2.0323469485006025

Epoch: 6| Step: 10
Training loss: 2.2470779418945312
Validation loss: 2.044518765582833

Epoch: 6| Step: 11
Training loss: 1.9338793754577637
Validation loss: 2.0042865404518704

Epoch: 6| Step: 12
Training loss: 2.6888585090637207
Validation loss: 2.0258729932128743

Epoch: 6| Step: 13
Training loss: 2.3093981742858887
Validation loss: 2.0465415677716656

Epoch: 76| Step: 0
Training loss: 2.4120192527770996
Validation loss: 2.013055532209335

Epoch: 6| Step: 1
Training loss: 2.387449264526367
Validation loss: 2.0459600546026744

Epoch: 6| Step: 2
Training loss: 2.7632288932800293
Validation loss: 2.016362064628191

Epoch: 6| Step: 3
Training loss: 2.426628589630127
Validation loss: 2.0274318238740325

Epoch: 6| Step: 4
Training loss: 1.8527560234069824
Validation loss: 2.001399919550906

Epoch: 6| Step: 5
Training loss: 2.5875964164733887
Validation loss: 2.0416722246395644

Epoch: 6| Step: 6
Training loss: 2.0976181030273438
Validation loss: 2.013179261197326

Epoch: 6| Step: 7
Training loss: 1.4706876277923584
Validation loss: 2.037767361569148

Epoch: 6| Step: 8
Training loss: 2.2824015617370605
Validation loss: 2.039321868650375

Epoch: 6| Step: 9
Training loss: 2.9502809047698975
Validation loss: 2.037088865874916

Epoch: 6| Step: 10
Training loss: 2.5898633003234863
Validation loss: 2.016835710053803

Epoch: 6| Step: 11
Training loss: 1.8934953212738037
Validation loss: 2.030098904845535

Epoch: 6| Step: 12
Training loss: 1.636565089225769
Validation loss: 2.020073295921408

Epoch: 6| Step: 13
Training loss: 2.761291027069092
Validation loss: 2.0029644991761897

Epoch: 77| Step: 0
Training loss: 2.3889265060424805
Validation loss: 1.999563080008312

Epoch: 6| Step: 1
Training loss: 1.9844253063201904
Validation loss: 2.0107463290614467

Epoch: 6| Step: 2
Training loss: 1.6428033113479614
Validation loss: 2.0129982258683894

Epoch: 6| Step: 3
Training loss: 2.0707485675811768
Validation loss: 1.9974709428766722

Epoch: 6| Step: 4
Training loss: 2.708021402359009
Validation loss: 2.001697905601994

Epoch: 6| Step: 5
Training loss: 1.666730523109436
Validation loss: 2.0388676786935456

Epoch: 6| Step: 6
Training loss: 2.4073829650878906
Validation loss: 2.0095475771093882

Epoch: 6| Step: 7
Training loss: 2.2543106079101562
Validation loss: 2.0095671812693277

Epoch: 6| Step: 8
Training loss: 3.1383116245269775
Validation loss: 1.9957440847991614

Epoch: 6| Step: 9
Training loss: 1.6730984449386597
Validation loss: 2.0385921808981124

Epoch: 6| Step: 10
Training loss: 2.4978792667388916
Validation loss: 1.9768809400578982

Epoch: 6| Step: 11
Training loss: 2.0976924896240234
Validation loss: 2.0302384066325363

Epoch: 6| Step: 12
Training loss: 2.6813087463378906
Validation loss: 2.024068460669569

Epoch: 6| Step: 13
Training loss: 2.6507439613342285
Validation loss: 1.9885113957107707

Epoch: 78| Step: 0
Training loss: 2.251565933227539
Validation loss: 2.002997129194198

Epoch: 6| Step: 1
Training loss: 2.373899459838867
Validation loss: 2.023342422259751

Epoch: 6| Step: 2
Training loss: 2.3425655364990234
Validation loss: 2.012136618296305

Epoch: 6| Step: 3
Training loss: 2.1751856803894043
Validation loss: 2.028140655127905

Epoch: 6| Step: 4
Training loss: 2.041367769241333
Validation loss: 2.0177784401883363

Epoch: 6| Step: 5
Training loss: 2.4898040294647217
Validation loss: 2.0740768268544185

Epoch: 6| Step: 6
Training loss: 2.955996513366699
Validation loss: 2.0178636274030133

Epoch: 6| Step: 7
Training loss: 2.033723831176758
Validation loss: 1.9982852089789607

Epoch: 6| Step: 8
Training loss: 1.8689465522766113
Validation loss: 1.9972985316348333

Epoch: 6| Step: 9
Training loss: 2.6737284660339355
Validation loss: 2.009384866683714

Epoch: 6| Step: 10
Training loss: 1.8886970281600952
Validation loss: 2.0119448349040043

Epoch: 6| Step: 11
Training loss: 1.7623472213745117
Validation loss: 1.9451754195715791

Epoch: 6| Step: 12
Training loss: 2.6240622997283936
Validation loss: 2.027507285917959

Epoch: 6| Step: 13
Training loss: 2.252270221710205
Validation loss: 2.044878702009878

Epoch: 79| Step: 0
Training loss: 2.0691134929656982
Validation loss: 1.9913964809909943

Epoch: 6| Step: 1
Training loss: 2.60591459274292
Validation loss: 1.9913276113489622

Epoch: 6| Step: 2
Training loss: 1.6368962526321411
Validation loss: 2.017647063860329

Epoch: 6| Step: 3
Training loss: 2.6562347412109375
Validation loss: 2.047070792926255

Epoch: 6| Step: 4
Training loss: 2.6515369415283203
Validation loss: 2.0253638977645547

Epoch: 6| Step: 5
Training loss: 1.8882042169570923
Validation loss: 2.015631598810996

Epoch: 6| Step: 6
Training loss: 2.411510944366455
Validation loss: 2.0174817295484644

Epoch: 6| Step: 7
Training loss: 1.4493516683578491
Validation loss: 2.0463418255570116

Epoch: 6| Step: 8
Training loss: 2.659489631652832
Validation loss: 2.0570395685011342

Epoch: 6| Step: 9
Training loss: 2.357576370239258
Validation loss: 2.0452119945197977

Epoch: 6| Step: 10
Training loss: 2.5355093479156494
Validation loss: 2.0376560393200127

Epoch: 6| Step: 11
Training loss: 1.7817952632904053
Validation loss: 2.0099778316354238

Epoch: 6| Step: 12
Training loss: 2.703563928604126
Validation loss: 2.020838181177775

Epoch: 6| Step: 13
Training loss: 2.455683946609497
Validation loss: 2.022468747631196

Epoch: 80| Step: 0
Training loss: 2.6318867206573486
Validation loss: 2.021713679836642

Epoch: 6| Step: 1
Training loss: 2.313220977783203
Validation loss: 2.03146110298813

Epoch: 6| Step: 2
Training loss: 1.7362372875213623
Validation loss: 2.009788713147563

Epoch: 6| Step: 3
Training loss: 2.983307123184204
Validation loss: 2.011053257091071

Epoch: 6| Step: 4
Training loss: 1.8100112676620483
Validation loss: 2.045466160261503

Epoch: 6| Step: 5
Training loss: 2.9111123085021973
Validation loss: 2.049081353731053

Epoch: 6| Step: 6
Training loss: 2.6278462409973145
Validation loss: 2.0347585319190897

Epoch: 6| Step: 7
Training loss: 2.801572799682617
Validation loss: 2.0507357030786495

Epoch: 6| Step: 8
Training loss: 1.7945793867111206
Validation loss: 2.049619787482805

Epoch: 6| Step: 9
Training loss: 2.2046048641204834
Validation loss: 2.064047392978463

Epoch: 6| Step: 10
Training loss: 1.9242372512817383
Validation loss: 2.084901991710868

Epoch: 6| Step: 11
Training loss: 1.7201204299926758
Validation loss: 2.0508218247403383

Epoch: 6| Step: 12
Training loss: 2.4949421882629395
Validation loss: 2.075141909301922

Epoch: 6| Step: 13
Training loss: 1.7960540056228638
Validation loss: 2.06200651455951

Epoch: 81| Step: 0
Training loss: 2.194835662841797
Validation loss: 2.076404845842751

Epoch: 6| Step: 1
Training loss: 2.9611268043518066
Validation loss: 2.077062573484195

Epoch: 6| Step: 2
Training loss: 2.717005729675293
Validation loss: 2.075519751476985

Epoch: 6| Step: 3
Training loss: 2.3680875301361084
Validation loss: 2.0333851178487143

Epoch: 6| Step: 4
Training loss: 2.0458266735076904
Validation loss: 2.069090195881423

Epoch: 6| Step: 5
Training loss: 2.833599090576172
Validation loss: 2.0491745395045124

Epoch: 6| Step: 6
Training loss: 1.7618067264556885
Validation loss: 2.052647457327894

Epoch: 6| Step: 7
Training loss: 2.656360149383545
Validation loss: 2.065710965023246

Epoch: 6| Step: 8
Training loss: 1.8191275596618652
Validation loss: 2.0941947954957203

Epoch: 6| Step: 9
Training loss: 2.193673610687256
Validation loss: 2.0587524367916967

Epoch: 6| Step: 10
Training loss: 2.305422067642212
Validation loss: 2.066783816583695

Epoch: 6| Step: 11
Training loss: 2.063748836517334
Validation loss: 2.047619791441066

Epoch: 6| Step: 12
Training loss: 1.8654987812042236
Validation loss: 2.0710343263482534

Epoch: 6| Step: 13
Training loss: 2.1910083293914795
Validation loss: 2.039512389449663

Epoch: 82| Step: 0
Training loss: 2.1148674488067627
Validation loss: 2.06369105205741

Epoch: 6| Step: 1
Training loss: 1.4046564102172852
Validation loss: 2.0518694218768867

Epoch: 6| Step: 2
Training loss: 2.047433853149414
Validation loss: 2.0356535424468336

Epoch: 6| Step: 3
Training loss: 2.7950632572174072
Validation loss: 2.042084638790418

Epoch: 6| Step: 4
Training loss: 2.2780914306640625
Validation loss: 2.0461956275406705

Epoch: 6| Step: 5
Training loss: 2.3694419860839844
Validation loss: 2.0545838622636694

Epoch: 6| Step: 6
Training loss: 2.6426968574523926
Validation loss: 2.0611405270074004

Epoch: 6| Step: 7
Training loss: 1.8936954736709595
Validation loss: 2.0485440377266175

Epoch: 6| Step: 8
Training loss: 1.7688108682632446
Validation loss: 2.0526338495234007

Epoch: 6| Step: 9
Training loss: 2.4023594856262207
Validation loss: 2.0403531802597867

Epoch: 6| Step: 10
Training loss: 2.4723730087280273
Validation loss: 2.041073101823048

Epoch: 6| Step: 11
Training loss: 1.8945705890655518
Validation loss: 2.0188946441937516

Epoch: 6| Step: 12
Training loss: 3.016366481781006
Validation loss: 2.005483568355601

Epoch: 6| Step: 13
Training loss: 2.7785122394561768
Validation loss: 2.0511818137220157

Epoch: 83| Step: 0
Training loss: 1.8536032438278198
Validation loss: 2.0378434568323116

Epoch: 6| Step: 1
Training loss: 2.0593886375427246
Validation loss: 2.033223139342441

Epoch: 6| Step: 2
Training loss: 2.4616615772247314
Validation loss: 2.045649422112332

Epoch: 6| Step: 3
Training loss: 2.52213716506958
Validation loss: 2.024609734935145

Epoch: 6| Step: 4
Training loss: 2.275383234024048
Validation loss: 2.0090532418220275

Epoch: 6| Step: 5
Training loss: 1.6819826364517212
Validation loss: 2.0121397792652087

Epoch: 6| Step: 6
Training loss: 2.479959487915039
Validation loss: 2.035418051545338

Epoch: 6| Step: 7
Training loss: 3.199033260345459
Validation loss: 2.0219485618734874

Epoch: 6| Step: 8
Training loss: 2.4729456901550293
Validation loss: 1.9911497767253588

Epoch: 6| Step: 9
Training loss: 2.1844236850738525
Validation loss: 2.0500073048376266

Epoch: 6| Step: 10
Training loss: 1.5249640941619873
Validation loss: 2.062860219709335

Epoch: 6| Step: 11
Training loss: 3.0892558097839355
Validation loss: 2.036124305058551

Epoch: 6| Step: 12
Training loss: 1.692651629447937
Validation loss: 1.9910923665569675

Epoch: 6| Step: 13
Training loss: 2.2617602348327637
Validation loss: 2.0196629878013366

Epoch: 84| Step: 0
Training loss: 2.6579627990722656
Validation loss: 2.0199060465699885

Epoch: 6| Step: 1
Training loss: 1.7576717138290405
Validation loss: 2.0301506878227316

Epoch: 6| Step: 2
Training loss: 1.5347923040390015
Validation loss: 2.037684525212934

Epoch: 6| Step: 3
Training loss: 3.0598645210266113
Validation loss: 2.0280999624600975

Epoch: 6| Step: 4
Training loss: 2.094637870788574
Validation loss: 2.0111018867902857

Epoch: 6| Step: 5
Training loss: 2.257338523864746
Validation loss: 2.0066594000785583

Epoch: 6| Step: 6
Training loss: 2.3301842212677
Validation loss: 2.0004369648553992

Epoch: 6| Step: 7
Training loss: 2.0651745796203613
Validation loss: 2.0112390672006915

Epoch: 6| Step: 8
Training loss: 2.3782505989074707
Validation loss: 2.000237459777504

Epoch: 6| Step: 9
Training loss: 2.4200427532196045
Validation loss: 2.023561246933476

Epoch: 6| Step: 10
Training loss: 2.026047945022583
Validation loss: 2.001762454227735

Epoch: 6| Step: 11
Training loss: 2.0008926391601562
Validation loss: 2.028177176752398

Epoch: 6| Step: 12
Training loss: 2.116537570953369
Validation loss: 2.018450347326135

Epoch: 6| Step: 13
Training loss: 3.1248769760131836
Validation loss: 1.9928527801267562

Epoch: 85| Step: 0
Training loss: 2.2174692153930664
Validation loss: 1.9982988680562666

Epoch: 6| Step: 1
Training loss: 1.8733807802200317
Validation loss: 1.9845778147379558

Epoch: 6| Step: 2
Training loss: 2.523111343383789
Validation loss: 2.012657739782846

Epoch: 6| Step: 3
Training loss: 2.492797374725342
Validation loss: 1.9947833066345544

Epoch: 6| Step: 4
Training loss: 1.8161333799362183
Validation loss: 1.9780309764287805

Epoch: 6| Step: 5
Training loss: 1.9058246612548828
Validation loss: 1.995440351065769

Epoch: 6| Step: 6
Training loss: 2.6227715015411377
Validation loss: 2.0027724299379575

Epoch: 6| Step: 7
Training loss: 1.8461123704910278
Validation loss: 2.0135544833316597

Epoch: 6| Step: 8
Training loss: 2.9834136962890625
Validation loss: 2.050502448953608

Epoch: 6| Step: 9
Training loss: 3.0394625663757324
Validation loss: 2.0257942279179892

Epoch: 6| Step: 10
Training loss: 1.8337178230285645
Validation loss: 1.9980930025859545

Epoch: 6| Step: 11
Training loss: 1.902774691581726
Validation loss: 2.0057697475597425

Epoch: 6| Step: 12
Training loss: 2.1359968185424805
Validation loss: 2.043366232225972

Epoch: 6| Step: 13
Training loss: 2.8023736476898193
Validation loss: 2.0261308352152505

Epoch: 86| Step: 0
Training loss: 2.613847017288208
Validation loss: 2.031823132627754

Epoch: 6| Step: 1
Training loss: 1.5990967750549316
Validation loss: 2.045624461225284

Epoch: 6| Step: 2
Training loss: 2.110363245010376
Validation loss: 2.0493097920571604

Epoch: 6| Step: 3
Training loss: 1.67367684841156
Validation loss: 2.024980414298273

Epoch: 6| Step: 4
Training loss: 2.217369556427002
Validation loss: 2.047316356371808

Epoch: 6| Step: 5
Training loss: 2.1818184852600098
Validation loss: 2.0184351052007368

Epoch: 6| Step: 6
Training loss: 2.202324151992798
Validation loss: 2.0381440975332774

Epoch: 6| Step: 7
Training loss: 2.0053327083587646
Validation loss: 2.0336882939902683

Epoch: 6| Step: 8
Training loss: 2.119905471801758
Validation loss: 2.0161051519455446

Epoch: 6| Step: 9
Training loss: 2.9296822547912598
Validation loss: 2.014794171497386

Epoch: 6| Step: 10
Training loss: 2.4062628746032715
Validation loss: 2.0558530143512193

Epoch: 6| Step: 11
Training loss: 2.938711166381836
Validation loss: 2.017552373229816

Epoch: 6| Step: 12
Training loss: 2.4405338764190674
Validation loss: 2.0162629978631132

Epoch: 6| Step: 13
Training loss: 2.2223715782165527
Validation loss: 2.057558908257433

Epoch: 87| Step: 0
Training loss: 2.1829793453216553
Validation loss: 2.0501596414914696

Epoch: 6| Step: 1
Training loss: 2.364227533340454
Validation loss: 2.034110934503617

Epoch: 6| Step: 2
Training loss: 2.896275043487549
Validation loss: 2.05900542454053

Epoch: 6| Step: 3
Training loss: 1.9349132776260376
Validation loss: 2.015977019904762

Epoch: 6| Step: 4
Training loss: 1.4473059177398682
Validation loss: 2.0107600688934326

Epoch: 6| Step: 5
Training loss: 2.1268246173858643
Validation loss: 2.0450103795656593

Epoch: 6| Step: 6
Training loss: 2.285334587097168
Validation loss: 2.0195377257562455

Epoch: 6| Step: 7
Training loss: 2.3382461071014404
Validation loss: 2.0403907504133

Epoch: 6| Step: 8
Training loss: 2.2114803791046143
Validation loss: 1.9839539412529237

Epoch: 6| Step: 9
Training loss: 1.9874389171600342
Validation loss: 2.0126366230749313

Epoch: 6| Step: 10
Training loss: 2.184356212615967
Validation loss: 2.0026449952074277

Epoch: 6| Step: 11
Training loss: 2.359485149383545
Validation loss: 1.99410843336454

Epoch: 6| Step: 12
Training loss: 2.778136730194092
Validation loss: 2.0147861537112983

Epoch: 6| Step: 13
Training loss: 2.5797510147094727
Validation loss: 2.0411409895907164

Epoch: 88| Step: 0
Training loss: 2.4202849864959717
Validation loss: 2.064079189813265

Epoch: 6| Step: 1
Training loss: 2.8429951667785645
Validation loss: 2.01397418719466

Epoch: 6| Step: 2
Training loss: 2.547980308532715
Validation loss: 2.007309462434502

Epoch: 6| Step: 3
Training loss: 2.019801378250122
Validation loss: 2.0455740895322574

Epoch: 6| Step: 4
Training loss: 2.5094194412231445
Validation loss: 2.0128850770253006

Epoch: 6| Step: 5
Training loss: 2.5249338150024414
Validation loss: 1.9804079160895398

Epoch: 6| Step: 6
Training loss: 1.812728762626648
Validation loss: 2.0263443531528598

Epoch: 6| Step: 7
Training loss: 2.052394390106201
Validation loss: 2.0094764386453936

Epoch: 6| Step: 8
Training loss: 1.7923177480697632
Validation loss: 1.9989347073339647

Epoch: 6| Step: 9
Training loss: 1.8430664539337158
Validation loss: 1.9999261633042367

Epoch: 6| Step: 10
Training loss: 2.3479180335998535
Validation loss: 1.9777149538840018

Epoch: 6| Step: 11
Training loss: 2.026048183441162
Validation loss: 1.9740927065572431

Epoch: 6| Step: 12
Training loss: 2.4741740226745605
Validation loss: 2.0220424270117157

Epoch: 6| Step: 13
Training loss: 2.4665210247039795
Validation loss: 2.0269176460081533

Epoch: 89| Step: 0
Training loss: 3.1354925632476807
Validation loss: 2.0444178222328104

Epoch: 6| Step: 1
Training loss: 1.8978984355926514
Validation loss: 2.019036319948012

Epoch: 6| Step: 2
Training loss: 1.07328200340271
Validation loss: 2.0341795772634526

Epoch: 6| Step: 3
Training loss: 1.9071989059448242
Validation loss: 2.017351455585931

Epoch: 6| Step: 4
Training loss: 2.0742995738983154
Validation loss: 2.08350549333839

Epoch: 6| Step: 5
Training loss: 1.850632905960083
Validation loss: 2.0403916887057725

Epoch: 6| Step: 6
Training loss: 2.578068733215332
Validation loss: 2.0551737316193117

Epoch: 6| Step: 7
Training loss: 2.201401472091675
Validation loss: 2.062010795839371

Epoch: 6| Step: 8
Training loss: 2.5096638202667236
Validation loss: 2.0636610407983103

Epoch: 6| Step: 9
Training loss: 2.637180805206299
Validation loss: 2.0454466996654386

Epoch: 6| Step: 10
Training loss: 2.9012794494628906
Validation loss: 1.9980422194286058

Epoch: 6| Step: 11
Training loss: 2.6512627601623535
Validation loss: 2.0509988492535007

Epoch: 6| Step: 12
Training loss: 2.06913423538208
Validation loss: 2.0454210414681384

Epoch: 6| Step: 13
Training loss: 2.165550947189331
Validation loss: 2.0233035805404826

Epoch: 90| Step: 0
Training loss: 2.484757900238037
Validation loss: 2.0585239574473393

Epoch: 6| Step: 1
Training loss: 2.2079522609710693
Validation loss: 2.0399750817206597

Epoch: 6| Step: 2
Training loss: 2.0112667083740234
Validation loss: 2.034711064830903

Epoch: 6| Step: 3
Training loss: 2.2716803550720215
Validation loss: 2.034438649813334

Epoch: 6| Step: 4
Training loss: 2.4452598094940186
Validation loss: 2.0267112844733783

Epoch: 6| Step: 5
Training loss: 2.6373682022094727
Validation loss: 2.023540445553359

Epoch: 6| Step: 6
Training loss: 2.2431869506835938
Validation loss: 2.0242530709953717

Epoch: 6| Step: 7
Training loss: 1.83871328830719
Validation loss: 2.0232814447854155

Epoch: 6| Step: 8
Training loss: 2.428645133972168
Validation loss: 2.052090567927207

Epoch: 6| Step: 9
Training loss: 1.6638742685317993
Validation loss: 2.026643555651429

Epoch: 6| Step: 10
Training loss: 1.9159486293792725
Validation loss: 2.0127339004188456

Epoch: 6| Step: 11
Training loss: 1.9947922229766846
Validation loss: 2.0464224635913806

Epoch: 6| Step: 12
Training loss: 2.469252347946167
Validation loss: 2.0386653946292017

Epoch: 6| Step: 13
Training loss: 3.02748441696167
Validation loss: 2.0332137077085433

Epoch: 91| Step: 0
Training loss: 2.1185784339904785
Validation loss: 2.0068139055723786

Epoch: 6| Step: 1
Training loss: 2.3653156757354736
Validation loss: 2.0453086053171465

Epoch: 6| Step: 2
Training loss: 1.6850818395614624
Validation loss: 2.0556584814543366

Epoch: 6| Step: 3
Training loss: 2.1137847900390625
Validation loss: 2.009319551529423

Epoch: 6| Step: 4
Training loss: 2.5194783210754395
Validation loss: 2.00389297803243

Epoch: 6| Step: 5
Training loss: 2.3488097190856934
Validation loss: 2.0144354579269246

Epoch: 6| Step: 6
Training loss: 2.2660341262817383
Validation loss: 2.0128270592740787

Epoch: 6| Step: 7
Training loss: 2.836712598800659
Validation loss: 2.0226207497299358

Epoch: 6| Step: 8
Training loss: 2.397035598754883
Validation loss: 2.0361140043504777

Epoch: 6| Step: 9
Training loss: 1.5518896579742432
Validation loss: 2.0316384428290912

Epoch: 6| Step: 10
Training loss: 2.3789401054382324
Validation loss: 2.039551258087158

Epoch: 6| Step: 11
Training loss: 2.141563653945923
Validation loss: 2.055856235565678

Epoch: 6| Step: 12
Training loss: 2.1656928062438965
Validation loss: 2.0179738793321835

Epoch: 6| Step: 13
Training loss: 2.3886427879333496
Validation loss: 2.027049531218826

Epoch: 92| Step: 0
Training loss: 2.539961338043213
Validation loss: 2.0589373034815632

Epoch: 6| Step: 1
Training loss: 1.6332324743270874
Validation loss: 2.024182032513362

Epoch: 6| Step: 2
Training loss: 1.7944841384887695
Validation loss: 2.0425008753294587

Epoch: 6| Step: 3
Training loss: 1.5142481327056885
Validation loss: 2.0260942725725073

Epoch: 6| Step: 4
Training loss: 2.345067024230957
Validation loss: 2.005335360445002

Epoch: 6| Step: 5
Training loss: 2.95749831199646
Validation loss: 2.032782631535684

Epoch: 6| Step: 6
Training loss: 2.509636402130127
Validation loss: 2.0203477439060005

Epoch: 6| Step: 7
Training loss: 2.256908893585205
Validation loss: 2.0269019872911516

Epoch: 6| Step: 8
Training loss: 2.900428056716919
Validation loss: 2.0140544881102858

Epoch: 6| Step: 9
Training loss: 2.2571747303009033
Validation loss: 2.0259287908513057

Epoch: 6| Step: 10
Training loss: 2.367572546005249
Validation loss: 2.0309860334601453

Epoch: 6| Step: 11
Training loss: 1.7963881492614746
Validation loss: 2.0100435569722164

Epoch: 6| Step: 12
Training loss: 2.324141502380371
Validation loss: 2.043818571234262

Epoch: 6| Step: 13
Training loss: 2.4109883308410645
Validation loss: 2.0622945357394475

Epoch: 93| Step: 0
Training loss: 1.8780765533447266
Validation loss: 2.0515985309436755

Epoch: 6| Step: 1
Training loss: 2.8217339515686035
Validation loss: 1.997732006093507

Epoch: 6| Step: 2
Training loss: 2.6779141426086426
Validation loss: 2.007936680188743

Epoch: 6| Step: 3
Training loss: 2.2303433418273926
Validation loss: 2.0190524542203514

Epoch: 6| Step: 4
Training loss: 2.3392691612243652
Validation loss: 2.0092933985494796

Epoch: 6| Step: 5
Training loss: 2.0883378982543945
Validation loss: 2.035177593590111

Epoch: 6| Step: 6
Training loss: 2.567068576812744
Validation loss: 2.025204486744378

Epoch: 6| Step: 7
Training loss: 1.967703104019165
Validation loss: 2.0197108676356654

Epoch: 6| Step: 8
Training loss: 2.115914821624756
Validation loss: 2.0353336987956876

Epoch: 6| Step: 9
Training loss: 2.098931312561035
Validation loss: 2.018218796740296

Epoch: 6| Step: 10
Training loss: 2.623839855194092
Validation loss: 2.038077392885762

Epoch: 6| Step: 11
Training loss: 1.8712657690048218
Validation loss: 1.9945817634623537

Epoch: 6| Step: 12
Training loss: 1.9346387386322021
Validation loss: 2.062579293404856

Epoch: 6| Step: 13
Training loss: 1.812817931175232
Validation loss: 2.018419647729525

Epoch: 94| Step: 0
Training loss: 2.400221824645996
Validation loss: 2.036335592628807

Epoch: 6| Step: 1
Training loss: 3.005976915359497
Validation loss: 2.013437589009603

Epoch: 6| Step: 2
Training loss: 2.225179672241211
Validation loss: 2.0134138637973416

Epoch: 6| Step: 3
Training loss: 2.3545119762420654
Validation loss: 2.030954017434069

Epoch: 6| Step: 4
Training loss: 1.6269205808639526
Validation loss: 2.0363108586239558

Epoch: 6| Step: 5
Training loss: 2.010775566101074
Validation loss: 2.005589844078146

Epoch: 6| Step: 6
Training loss: 1.5200648307800293
Validation loss: 2.006468738279035

Epoch: 6| Step: 7
Training loss: 2.3081324100494385
Validation loss: 2.0149883518936815

Epoch: 6| Step: 8
Training loss: 2.3624274730682373
Validation loss: 2.0229494699867825

Epoch: 6| Step: 9
Training loss: 2.640310764312744
Validation loss: 2.0237151179262387

Epoch: 6| Step: 10
Training loss: 2.8999557495117188
Validation loss: 2.039236896781511

Epoch: 6| Step: 11
Training loss: 1.923035740852356
Validation loss: 2.031271166698907

Epoch: 6| Step: 12
Training loss: 1.8714886903762817
Validation loss: 2.0132014495070263

Epoch: 6| Step: 13
Training loss: 1.7842284440994263
Validation loss: 2.025955820596346

Epoch: 95| Step: 0
Training loss: 2.2482051849365234
Validation loss: 2.0154519568207445

Epoch: 6| Step: 1
Training loss: 2.3988723754882812
Validation loss: 2.0104978481928506

Epoch: 6| Step: 2
Training loss: 2.401859760284424
Validation loss: 1.9860313143781436

Epoch: 6| Step: 3
Training loss: 2.286879301071167
Validation loss: 1.9916264651924052

Epoch: 6| Step: 4
Training loss: 2.025679588317871
Validation loss: 2.033677198553598

Epoch: 6| Step: 5
Training loss: 1.5160717964172363
Validation loss: 2.0273824327735492

Epoch: 6| Step: 6
Training loss: 2.2471718788146973
Validation loss: 2.0288905635956795

Epoch: 6| Step: 7
Training loss: 2.499424934387207
Validation loss: 2.0199917875310427

Epoch: 6| Step: 8
Training loss: 2.0632100105285645
Validation loss: 1.991028372959424

Epoch: 6| Step: 9
Training loss: 2.8701794147491455
Validation loss: 2.0190138586105837

Epoch: 6| Step: 10
Training loss: 2.479098320007324
Validation loss: 2.02205382111252

Epoch: 6| Step: 11
Training loss: 2.641646385192871
Validation loss: 2.0198944178960656

Epoch: 6| Step: 12
Training loss: 1.5192698240280151
Validation loss: 1.9958743805526404

Epoch: 6| Step: 13
Training loss: 2.125096082687378
Validation loss: 2.009913267627839

Epoch: 96| Step: 0
Training loss: 2.1416544914245605
Validation loss: 2.032664886084936

Epoch: 6| Step: 1
Training loss: 2.8115391731262207
Validation loss: 1.9942728370748541

Epoch: 6| Step: 2
Training loss: 2.615386962890625
Validation loss: 2.021238532117618

Epoch: 6| Step: 3
Training loss: 2.676210880279541
Validation loss: 2.0456067413412113

Epoch: 6| Step: 4
Training loss: 1.8536911010742188
Validation loss: 2.0300876914813952

Epoch: 6| Step: 5
Training loss: 2.1221585273742676
Validation loss: 2.0119046242006364

Epoch: 6| Step: 6
Training loss: 2.401705265045166
Validation loss: 2.0262757373112503

Epoch: 6| Step: 7
Training loss: 2.015725612640381
Validation loss: 2.0099554625890588

Epoch: 6| Step: 8
Training loss: 1.9418072700500488
Validation loss: 2.0301666541766097

Epoch: 6| Step: 9
Training loss: 2.277096748352051
Validation loss: 2.0291819187902633

Epoch: 6| Step: 10
Training loss: 1.6858500242233276
Validation loss: 2.029818614323934

Epoch: 6| Step: 11
Training loss: 2.280433177947998
Validation loss: 2.0479902631493023

Epoch: 6| Step: 12
Training loss: 2.145352840423584
Validation loss: 2.002035399918915

Epoch: 6| Step: 13
Training loss: 2.1024484634399414
Validation loss: 2.035247607897687

Epoch: 97| Step: 0
Training loss: 2.1416497230529785
Validation loss: 2.009780481297483

Epoch: 6| Step: 1
Training loss: 2.437344551086426
Validation loss: 2.020296988948699

Epoch: 6| Step: 2
Training loss: 1.304267168045044
Validation loss: 2.016198686374131

Epoch: 6| Step: 3
Training loss: 2.3962154388427734
Validation loss: 2.0345090653306697

Epoch: 6| Step: 4
Training loss: 2.0240366458892822
Validation loss: 2.0481033812287035

Epoch: 6| Step: 5
Training loss: 1.785932183265686
Validation loss: 2.048605280537759

Epoch: 6| Step: 6
Training loss: 1.8193939924240112
Validation loss: 2.03440922562794

Epoch: 6| Step: 7
Training loss: 2.1710057258605957
Validation loss: 2.032455062353483

Epoch: 6| Step: 8
Training loss: 2.867443561553955
Validation loss: 2.028238670800322

Epoch: 6| Step: 9
Training loss: 2.3491244316101074
Validation loss: 2.054884628583026

Epoch: 6| Step: 10
Training loss: 3.265872001647949
Validation loss: 2.0191388463461273

Epoch: 6| Step: 11
Training loss: 1.8962130546569824
Validation loss: 2.0655485353162213

Epoch: 6| Step: 12
Training loss: 2.3316807746887207
Validation loss: 2.0681402106438913

Epoch: 6| Step: 13
Training loss: 2.6134557723999023
Validation loss: 2.043752826670165

Epoch: 98| Step: 0
Training loss: 3.0850918292999268
Validation loss: 2.0390326951139714

Epoch: 6| Step: 1
Training loss: 2.8948140144348145
Validation loss: 2.050044487881404

Epoch: 6| Step: 2
Training loss: 2.201939344406128
Validation loss: 2.0410360072248723

Epoch: 6| Step: 3
Training loss: 1.388329267501831
Validation loss: 2.07660618007824

Epoch: 6| Step: 4
Training loss: 3.079592227935791
Validation loss: 2.072619118998128

Epoch: 6| Step: 5
Training loss: 2.7211873531341553
Validation loss: 2.0525348878675893

Epoch: 6| Step: 6
Training loss: 1.101873755455017
Validation loss: 2.0790500217868435

Epoch: 6| Step: 7
Training loss: 1.789415717124939
Validation loss: 2.048381047864114

Epoch: 6| Step: 8
Training loss: 1.9394018650054932
Validation loss: 2.0615087888574086

Epoch: 6| Step: 9
Training loss: 2.1995859146118164
Validation loss: 2.017825298411872

Epoch: 6| Step: 10
Training loss: 2.39101243019104
Validation loss: 2.0565526754625383

Epoch: 6| Step: 11
Training loss: 1.8672401905059814
Validation loss: 2.0562807616367134

Epoch: 6| Step: 12
Training loss: 2.0187840461730957
Validation loss: 2.0533282090258855

Epoch: 6| Step: 13
Training loss: 3.136612892150879
Validation loss: 2.0218579743498113

Epoch: 99| Step: 0
Training loss: 2.3808109760284424
Validation loss: 2.025634704097625

Epoch: 6| Step: 1
Training loss: 2.420870065689087
Validation loss: 2.018247514642695

Epoch: 6| Step: 2
Training loss: 3.011691093444824
Validation loss: 2.048076860366329

Epoch: 6| Step: 3
Training loss: 1.7544052600860596
Validation loss: 2.0488799118226573

Epoch: 6| Step: 4
Training loss: 1.9032822847366333
Validation loss: 2.0640515140307847

Epoch: 6| Step: 5
Training loss: 1.8681275844573975
Validation loss: 2.0308114162055393

Epoch: 6| Step: 6
Training loss: 1.9246571063995361
Validation loss: 2.0538084301897275

Epoch: 6| Step: 7
Training loss: 1.5748943090438843
Validation loss: 2.0196767840334164

Epoch: 6| Step: 8
Training loss: 2.238471269607544
Validation loss: 2.049027427550285

Epoch: 6| Step: 9
Training loss: 2.5743093490600586
Validation loss: 2.033644537771902

Epoch: 6| Step: 10
Training loss: 2.040926694869995
Validation loss: 2.0495926667285222

Epoch: 6| Step: 11
Training loss: 2.8921611309051514
Validation loss: 2.0405987898508706

Epoch: 6| Step: 12
Training loss: 2.5678534507751465
Validation loss: 2.037035967714043

Epoch: 6| Step: 13
Training loss: 2.0202512741088867
Validation loss: 2.0267897241859028

Epoch: 100| Step: 0
Training loss: 2.2833597660064697
Validation loss: 2.0753008165667133

Epoch: 6| Step: 1
Training loss: 2.8264999389648438
Validation loss: 2.076621281203403

Epoch: 6| Step: 2
Training loss: 2.6082983016967773
Validation loss: 2.0626272322029195

Epoch: 6| Step: 3
Training loss: 2.069565773010254
Validation loss: 2.028041342253326

Epoch: 6| Step: 4
Training loss: 2.251262903213501
Validation loss: 2.0659850681981733

Epoch: 6| Step: 5
Training loss: 2.108947992324829
Validation loss: 2.0519393720934467

Epoch: 6| Step: 6
Training loss: 2.397794485092163
Validation loss: 2.0384483696312032

Epoch: 6| Step: 7
Training loss: 1.8078771829605103
Validation loss: 2.036695739274384

Epoch: 6| Step: 8
Training loss: 2.270075559616089
Validation loss: 2.0703329514431696

Epoch: 6| Step: 9
Training loss: 1.8568544387817383
Validation loss: 2.05792175826206

Epoch: 6| Step: 10
Training loss: 1.8794305324554443
Validation loss: 2.0517163686854865

Epoch: 6| Step: 11
Training loss: 2.333746910095215
Validation loss: 2.0235185469350507

Epoch: 6| Step: 12
Training loss: 2.328097343444824
Validation loss: 2.027545153453786

Epoch: 6| Step: 13
Training loss: 2.339984655380249
Validation loss: 2.043468839378767

Testing loss: 2.138154787487454

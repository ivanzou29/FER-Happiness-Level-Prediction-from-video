Epoch: 1| Step: 0
Training loss: 4.74538516998291
Validation loss: 5.630513909042523

Epoch: 6| Step: 1
Training loss: 6.217649459838867
Validation loss: 5.629026751364431

Epoch: 6| Step: 2
Training loss: 7.105573654174805
Validation loss: 5.628132825256676

Epoch: 6| Step: 3
Training loss: 6.440831184387207
Validation loss: 5.623254565782444

Epoch: 6| Step: 4
Training loss: 4.836221218109131
Validation loss: 5.621658089340374

Epoch: 6| Step: 5
Training loss: 5.4916181564331055
Validation loss: 5.6175223729943715

Epoch: 6| Step: 6
Training loss: 4.576014518737793
Validation loss: 5.616017562086864

Epoch: 6| Step: 7
Training loss: 5.04600715637207
Validation loss: 5.6102650088648645

Epoch: 6| Step: 8
Training loss: 5.309073448181152
Validation loss: 5.60811827259679

Epoch: 6| Step: 9
Training loss: 5.703604698181152
Validation loss: 5.604503334209483

Epoch: 6| Step: 10
Training loss: 4.689690113067627
Validation loss: 5.60220528674382

Epoch: 6| Step: 11
Training loss: 5.730655670166016
Validation loss: 5.597033367362074

Epoch: 6| Step: 12
Training loss: 3.86234188079834
Validation loss: 5.595935534405452

Epoch: 6| Step: 13
Training loss: 6.424253463745117
Validation loss: 5.593737058742072

Epoch: 2| Step: 0
Training loss: 6.1962690353393555
Validation loss: 5.58619338209911

Epoch: 6| Step: 1
Training loss: 5.427733421325684
Validation loss: 5.583561543495424

Epoch: 6| Step: 2
Training loss: 6.2745208740234375
Validation loss: 5.581511579534059

Epoch: 6| Step: 3
Training loss: 4.557050704956055
Validation loss: 5.57806021167386

Epoch: 6| Step: 4
Training loss: 5.614346027374268
Validation loss: 5.573200810340143

Epoch: 6| Step: 5
Training loss: 5.416831970214844
Validation loss: 5.568389128613216

Epoch: 6| Step: 6
Training loss: 5.591283798217773
Validation loss: 5.5661806496240755

Epoch: 6| Step: 7
Training loss: 5.363042831420898
Validation loss: 5.559803783252675

Epoch: 6| Step: 8
Training loss: 4.607235908508301
Validation loss: 5.558202056474583

Epoch: 6| Step: 9
Training loss: 5.117319107055664
Validation loss: 5.550666568099811

Epoch: 6| Step: 10
Training loss: 5.07766056060791
Validation loss: 5.548658304317023

Epoch: 6| Step: 11
Training loss: 4.534360885620117
Validation loss: 5.5434222580284205

Epoch: 6| Step: 12
Training loss: 5.439770698547363
Validation loss: 5.54009711357855

Epoch: 6| Step: 13
Training loss: 6.118433475494385
Validation loss: 5.537440746061264

Epoch: 3| Step: 0
Training loss: 5.401147842407227
Validation loss: 5.531024717515515

Epoch: 6| Step: 1
Training loss: 5.88083553314209
Validation loss: 5.526008954612157

Epoch: 6| Step: 2
Training loss: 4.358240604400635
Validation loss: 5.5216140029250935

Epoch: 6| Step: 3
Training loss: 5.078008651733398
Validation loss: 5.516531041873399

Epoch: 6| Step: 4
Training loss: 3.946784019470215
Validation loss: 5.512094666880946

Epoch: 6| Step: 5
Training loss: 5.320837020874023
Validation loss: 5.505678587062384

Epoch: 6| Step: 6
Training loss: 6.1645894050598145
Validation loss: 5.503093929700954

Epoch: 6| Step: 7
Training loss: 5.239067554473877
Validation loss: 5.497906495166081

Epoch: 6| Step: 8
Training loss: 4.470284938812256
Validation loss: 5.4951940044280025

Epoch: 6| Step: 9
Training loss: 5.634408950805664
Validation loss: 5.487758780038485

Epoch: 6| Step: 10
Training loss: 6.453499794006348
Validation loss: 5.48412872129871

Epoch: 6| Step: 11
Training loss: 5.435946941375732
Validation loss: 5.477575799470307

Epoch: 6| Step: 12
Training loss: 5.077305793762207
Validation loss: 5.469681175806189

Epoch: 6| Step: 13
Training loss: 5.892276763916016
Validation loss: 5.466309639715379

Epoch: 4| Step: 0
Training loss: 5.191799163818359
Validation loss: 5.462541128999444

Epoch: 6| Step: 1
Training loss: 4.993012428283691
Validation loss: 5.458465278789562

Epoch: 6| Step: 2
Training loss: 4.790560722351074
Validation loss: 5.454170883342784

Epoch: 6| Step: 3
Training loss: 5.221590042114258
Validation loss: 5.446640460721908

Epoch: 6| Step: 4
Training loss: 5.916365623474121
Validation loss: 5.441206168102962

Epoch: 6| Step: 5
Training loss: 5.482161998748779
Validation loss: 5.43667398985996

Epoch: 6| Step: 6
Training loss: 4.836607933044434
Validation loss: 5.430035796216739

Epoch: 6| Step: 7
Training loss: 4.998153209686279
Validation loss: 5.423537849098124

Epoch: 6| Step: 8
Training loss: 6.313899517059326
Validation loss: 5.4186418338488505

Epoch: 6| Step: 9
Training loss: 7.323877334594727
Validation loss: 5.4144335562183015

Epoch: 6| Step: 10
Training loss: 3.979930877685547
Validation loss: 5.4059410505397345

Epoch: 6| Step: 11
Training loss: 3.9093804359436035
Validation loss: 5.4046331477421585

Epoch: 6| Step: 12
Training loss: 5.490588188171387
Validation loss: 5.392121294493316

Epoch: 6| Step: 13
Training loss: 4.126194477081299
Validation loss: 5.38798878782539

Epoch: 5| Step: 0
Training loss: 5.511292934417725
Validation loss: 5.384901856863371

Epoch: 6| Step: 1
Training loss: 5.714776039123535
Validation loss: 5.376216652572796

Epoch: 6| Step: 2
Training loss: 5.5578932762146
Validation loss: 5.368900165762953

Epoch: 6| Step: 3
Training loss: 4.628627777099609
Validation loss: 5.364694508173132

Epoch: 6| Step: 4
Training loss: 3.9148290157318115
Validation loss: 5.362143101230744

Epoch: 6| Step: 5
Training loss: 4.514839172363281
Validation loss: 5.351734674105081

Epoch: 6| Step: 6
Training loss: 5.510811805725098
Validation loss: 5.3452898251113075

Epoch: 6| Step: 7
Training loss: 5.690854072570801
Validation loss: 5.3364018829919955

Epoch: 6| Step: 8
Training loss: 5.496708393096924
Validation loss: 5.330253457510343

Epoch: 6| Step: 9
Training loss: 6.1635260581970215
Validation loss: 5.323253677737329

Epoch: 6| Step: 10
Training loss: 5.217339992523193
Validation loss: 5.316576301410634

Epoch: 6| Step: 11
Training loss: 5.683011054992676
Validation loss: 5.308723675307407

Epoch: 6| Step: 12
Training loss: 3.745899200439453
Validation loss: 5.303272231932609

Epoch: 6| Step: 13
Training loss: 3.9352025985717773
Validation loss: 5.297885582011233

Epoch: 6| Step: 0
Training loss: 4.855624675750732
Validation loss: 5.288925883590534

Epoch: 6| Step: 1
Training loss: 3.8875842094421387
Validation loss: 5.280699386391588

Epoch: 6| Step: 2
Training loss: 4.444140911102295
Validation loss: 5.273593733387608

Epoch: 6| Step: 3
Training loss: 5.403627872467041
Validation loss: 5.266653004512992

Epoch: 6| Step: 4
Training loss: 5.316417694091797
Validation loss: 5.257642381934709

Epoch: 6| Step: 5
Training loss: 5.599428176879883
Validation loss: 5.250977049591721

Epoch: 6| Step: 6
Training loss: 5.718392372131348
Validation loss: 5.245181109315606

Epoch: 6| Step: 7
Training loss: 5.320287704467773
Validation loss: 5.237651850587579

Epoch: 6| Step: 8
Training loss: 5.309011459350586
Validation loss: 5.230719381763089

Epoch: 6| Step: 9
Training loss: 4.321435451507568
Validation loss: 5.218512488949683

Epoch: 6| Step: 10
Training loss: 5.324367046356201
Validation loss: 5.214495371746761

Epoch: 6| Step: 11
Training loss: 5.222311973571777
Validation loss: 5.205294614197106

Epoch: 6| Step: 12
Training loss: 4.281833171844482
Validation loss: 5.198008619328981

Epoch: 6| Step: 13
Training loss: 5.635646343231201
Validation loss: 5.187389076396983

Epoch: 7| Step: 0
Training loss: 5.989141464233398
Validation loss: 5.180635011324319

Epoch: 6| Step: 1
Training loss: 4.097135066986084
Validation loss: 5.1728790293457685

Epoch: 6| Step: 2
Training loss: 5.760150909423828
Validation loss: 5.167288334138932

Epoch: 6| Step: 3
Training loss: 4.008898735046387
Validation loss: 5.152593602416336

Epoch: 6| Step: 4
Training loss: 5.131455421447754
Validation loss: 5.143532522263065

Epoch: 6| Step: 5
Training loss: 4.292876243591309
Validation loss: 5.136615060990857

Epoch: 6| Step: 6
Training loss: 4.257789611816406
Validation loss: 5.1281332354391775

Epoch: 6| Step: 7
Training loss: 5.6132612228393555
Validation loss: 5.1222769675716275

Epoch: 6| Step: 8
Training loss: 4.976464748382568
Validation loss: 5.108707643324329

Epoch: 6| Step: 9
Training loss: 5.818515777587891
Validation loss: 5.1000886527440885

Epoch: 6| Step: 10
Training loss: 3.566300392150879
Validation loss: 5.092576821645101

Epoch: 6| Step: 11
Training loss: 4.894435882568359
Validation loss: 5.082232716262982

Epoch: 6| Step: 12
Training loss: 5.5976080894470215
Validation loss: 5.074464336518319

Epoch: 6| Step: 13
Training loss: 4.528495788574219
Validation loss: 5.0642147269300235

Epoch: 8| Step: 0
Training loss: 3.560109853744507
Validation loss: 5.055125328802293

Epoch: 6| Step: 1
Training loss: 3.926300287246704
Validation loss: 5.042533659165906

Epoch: 6| Step: 2
Training loss: 3.8111298084259033
Validation loss: 5.030347208822927

Epoch: 6| Step: 3
Training loss: 6.064715385437012
Validation loss: 5.022449483153641

Epoch: 6| Step: 4
Training loss: 5.754477500915527
Validation loss: 5.014398579956383

Epoch: 6| Step: 5
Training loss: 4.84096622467041
Validation loss: 5.001323000077279

Epoch: 6| Step: 6
Training loss: 5.375626564025879
Validation loss: 4.986624061420399

Epoch: 6| Step: 7
Training loss: 3.7937114238739014
Validation loss: 4.979785519261514

Epoch: 6| Step: 8
Training loss: 4.650224685668945
Validation loss: 4.965129944585985

Epoch: 6| Step: 9
Training loss: 5.478552341461182
Validation loss: 4.959898502595963

Epoch: 6| Step: 10
Training loss: 4.028942108154297
Validation loss: 4.945063867876606

Epoch: 6| Step: 11
Training loss: 4.891165733337402
Validation loss: 4.934358140473725

Epoch: 6| Step: 12
Training loss: 5.611512184143066
Validation loss: 4.923800165935229

Epoch: 6| Step: 13
Training loss: 5.058586597442627
Validation loss: 4.914250266167425

Epoch: 9| Step: 0
Training loss: 5.060815334320068
Validation loss: 4.9018073081970215

Epoch: 6| Step: 1
Training loss: 3.8484513759613037
Validation loss: 4.891743429245487

Epoch: 6| Step: 2
Training loss: 4.731210708618164
Validation loss: 4.875430235298731

Epoch: 6| Step: 3
Training loss: 5.823141098022461
Validation loss: 4.865819936157555

Epoch: 6| Step: 4
Training loss: 4.522160530090332
Validation loss: 4.85257492270521

Epoch: 6| Step: 5
Training loss: 5.368368148803711
Validation loss: 4.835966925467214

Epoch: 6| Step: 6
Training loss: 3.742784261703491
Validation loss: 4.824653240942186

Epoch: 6| Step: 7
Training loss: 3.263868808746338
Validation loss: 4.816544630194223

Epoch: 6| Step: 8
Training loss: 4.568691253662109
Validation loss: 4.797112234177128

Epoch: 6| Step: 9
Training loss: 3.156032085418701
Validation loss: 4.781653352963027

Epoch: 6| Step: 10
Training loss: 5.797948837280273
Validation loss: 4.770543693214335

Epoch: 6| Step: 11
Training loss: 5.205263137817383
Validation loss: 4.766153340698571

Epoch: 6| Step: 12
Training loss: 4.482713222503662
Validation loss: 4.746148278636317

Epoch: 6| Step: 13
Training loss: 5.0264387130737305
Validation loss: 4.733178118223785

Epoch: 10| Step: 0
Training loss: 4.816667556762695
Validation loss: 4.714353443473898

Epoch: 6| Step: 1
Training loss: 4.314111709594727
Validation loss: 4.702280490629135

Epoch: 6| Step: 2
Training loss: 4.41255521774292
Validation loss: 4.685624143128754

Epoch: 6| Step: 3
Training loss: 4.857579708099365
Validation loss: 4.6734774804884385

Epoch: 6| Step: 4
Training loss: 4.167051792144775
Validation loss: 4.653008061070596

Epoch: 6| Step: 5
Training loss: 4.914787292480469
Validation loss: 4.637673736900411

Epoch: 6| Step: 6
Training loss: 3.991367816925049
Validation loss: 4.620268350006432

Epoch: 6| Step: 7
Training loss: 4.045375823974609
Validation loss: 4.604362041719498

Epoch: 6| Step: 8
Training loss: 3.969712734222412
Validation loss: 4.592975401109265

Epoch: 6| Step: 9
Training loss: 4.024230480194092
Validation loss: 4.569851552286456

Epoch: 6| Step: 10
Training loss: 4.689213752746582
Validation loss: 4.550103546470724

Epoch: 6| Step: 11
Training loss: 4.397668838500977
Validation loss: 4.540425710780646

Epoch: 6| Step: 12
Training loss: 4.458409786224365
Validation loss: 4.525037247647521

Epoch: 6| Step: 13
Training loss: 4.379382133483887
Validation loss: 4.502852844935592

Epoch: 11| Step: 0
Training loss: 4.3843994140625
Validation loss: 4.493767661433066

Epoch: 6| Step: 1
Training loss: 3.0409481525421143
Validation loss: 4.475757562985984

Epoch: 6| Step: 2
Training loss: 4.970256805419922
Validation loss: 4.453048547108968

Epoch: 6| Step: 3
Training loss: 4.982980728149414
Validation loss: 4.443896324403824

Epoch: 6| Step: 4
Training loss: 2.8790507316589355
Validation loss: 4.413255763310258

Epoch: 6| Step: 5
Training loss: 4.766776084899902
Validation loss: 4.3927494479763896

Epoch: 6| Step: 6
Training loss: 3.5044946670532227
Validation loss: 4.379137969786121

Epoch: 6| Step: 7
Training loss: 4.317821502685547
Validation loss: 4.360010157349289

Epoch: 6| Step: 8
Training loss: 4.061628341674805
Validation loss: 4.340235715271325

Epoch: 6| Step: 9
Training loss: 4.657571315765381
Validation loss: 4.3314911550091155

Epoch: 6| Step: 10
Training loss: 3.863157033920288
Validation loss: 4.305072366550404

Epoch: 6| Step: 11
Training loss: 3.2620949745178223
Validation loss: 4.280688839574014

Epoch: 6| Step: 12
Training loss: 4.775859832763672
Validation loss: 4.260406645395422

Epoch: 6| Step: 13
Training loss: 4.866344928741455
Validation loss: 4.245483888092862

Epoch: 12| Step: 0
Training loss: 3.7462892532348633
Validation loss: 4.219673269538469

Epoch: 6| Step: 1
Training loss: 4.27896785736084
Validation loss: 4.202894098015242

Epoch: 6| Step: 2
Training loss: 2.5182037353515625
Validation loss: 4.177855835166029

Epoch: 6| Step: 3
Training loss: 3.5858325958251953
Validation loss: 4.16382719368063

Epoch: 6| Step: 4
Training loss: 4.995990753173828
Validation loss: 4.137851802251673

Epoch: 6| Step: 5
Training loss: 3.946406602859497
Validation loss: 4.120803345916092

Epoch: 6| Step: 6
Training loss: 3.9117116928100586
Validation loss: 4.100224418024863

Epoch: 6| Step: 7
Training loss: 5.285194396972656
Validation loss: 4.079116462379374

Epoch: 6| Step: 8
Training loss: 3.519139289855957
Validation loss: 4.061550873582081

Epoch: 6| Step: 9
Training loss: 3.7134900093078613
Validation loss: 4.04176987114773

Epoch: 6| Step: 10
Training loss: 3.380742311477661
Validation loss: 4.022857227633076

Epoch: 6| Step: 11
Training loss: 4.492917060852051
Validation loss: 4.00303856531779

Epoch: 6| Step: 12
Training loss: 3.5351948738098145
Validation loss: 3.9889490014763287

Epoch: 6| Step: 13
Training loss: 3.7929654121398926
Validation loss: 3.965142301333848

Epoch: 13| Step: 0
Training loss: 4.4873528480529785
Validation loss: 3.942306134008592

Epoch: 6| Step: 1
Training loss: 4.003273963928223
Validation loss: 3.931647475047778

Epoch: 6| Step: 2
Training loss: 4.525507926940918
Validation loss: 3.908294241915467

Epoch: 6| Step: 3
Training loss: 2.6004855632781982
Validation loss: 3.8885335050603396

Epoch: 6| Step: 4
Training loss: 4.576657295227051
Validation loss: 3.863686059110908

Epoch: 6| Step: 5
Training loss: 2.5987017154693604
Validation loss: 3.8517164543110836

Epoch: 6| Step: 6
Training loss: 3.745305061340332
Validation loss: 3.841817835325836

Epoch: 6| Step: 7
Training loss: 3.059705972671509
Validation loss: 3.809719039547828

Epoch: 6| Step: 8
Training loss: 3.209707260131836
Validation loss: 3.7971725976595314

Epoch: 6| Step: 9
Training loss: 3.176051616668701
Validation loss: 3.7828105444549234

Epoch: 6| Step: 10
Training loss: 5.724695682525635
Validation loss: 3.766194017984534

Epoch: 6| Step: 11
Training loss: 3.367018222808838
Validation loss: 3.7382236296130764

Epoch: 6| Step: 12
Training loss: 3.2951085567474365
Validation loss: 3.7224543427908294

Epoch: 6| Step: 13
Training loss: 2.7811026573181152
Validation loss: 3.6957741219510316

Epoch: 14| Step: 0
Training loss: 2.694046974182129
Validation loss: 3.6824457312142975

Epoch: 6| Step: 1
Training loss: 2.5023114681243896
Validation loss: 3.6674349948924077

Epoch: 6| Step: 2
Training loss: 3.9633750915527344
Validation loss: 3.6420581622790267

Epoch: 6| Step: 3
Training loss: 4.069769859313965
Validation loss: 3.629859488497498

Epoch: 6| Step: 4
Training loss: 2.5403976440429688
Validation loss: 3.6223484059815765

Epoch: 6| Step: 5
Training loss: 3.236881971359253
Validation loss: 3.5924254463564966

Epoch: 6| Step: 6
Training loss: 3.508875846862793
Validation loss: 3.5852070931465394

Epoch: 6| Step: 7
Training loss: 2.5783066749572754
Validation loss: 3.564277110561248

Epoch: 6| Step: 8
Training loss: 4.162524700164795
Validation loss: 3.532365281094787

Epoch: 6| Step: 9
Training loss: 5.294007301330566
Validation loss: 3.5290068785349527

Epoch: 6| Step: 10
Training loss: 2.981356143951416
Validation loss: 3.5086662102771062

Epoch: 6| Step: 11
Training loss: 4.031754493713379
Validation loss: 3.491837065706971

Epoch: 6| Step: 12
Training loss: 3.742866039276123
Validation loss: 3.4705561822460544

Epoch: 6| Step: 13
Training loss: 2.6607613563537598
Validation loss: 3.4385224209036878

Epoch: 15| Step: 0
Training loss: 2.9993767738342285
Validation loss: 3.4238622624387025

Epoch: 6| Step: 1
Training loss: 3.3292715549468994
Validation loss: 3.413279861532232

Epoch: 6| Step: 2
Training loss: 3.2187275886535645
Validation loss: 3.37551813228156

Epoch: 6| Step: 3
Training loss: 3.8154656887054443
Validation loss: 3.35488728297654

Epoch: 6| Step: 4
Training loss: 3.343106746673584
Validation loss: 3.3380341427300566

Epoch: 6| Step: 5
Training loss: 4.232080459594727
Validation loss: 3.316098133722941

Epoch: 6| Step: 6
Training loss: 2.806849956512451
Validation loss: 3.307610865562193

Epoch: 6| Step: 7
Training loss: 2.8831963539123535
Validation loss: 3.2663967276132233

Epoch: 6| Step: 8
Training loss: 2.4753379821777344
Validation loss: 3.2493398189544678

Epoch: 6| Step: 9
Training loss: 3.590121269226074
Validation loss: 3.219707066012967

Epoch: 6| Step: 10
Training loss: 3.243293285369873
Validation loss: 3.192161677986063

Epoch: 6| Step: 11
Training loss: 3.146759033203125
Validation loss: 3.176700750986735

Epoch: 6| Step: 12
Training loss: 2.808478832244873
Validation loss: 3.1423389398923485

Epoch: 6| Step: 13
Training loss: 3.3885231018066406
Validation loss: 3.1240459052465295

Epoch: 16| Step: 0
Training loss: 3.397637367248535
Validation loss: 3.101919820231776

Epoch: 6| Step: 1
Training loss: 3.32257080078125
Validation loss: 3.0982470050934823

Epoch: 6| Step: 2
Training loss: 2.689903497695923
Validation loss: 3.069358479592108

Epoch: 6| Step: 3
Training loss: 3.2118043899536133
Validation loss: 3.0661619299201557

Epoch: 6| Step: 4
Training loss: 3.558995008468628
Validation loss: 3.0280377249563895

Epoch: 6| Step: 5
Training loss: 2.216041326522827
Validation loss: 2.9972133380110546

Epoch: 6| Step: 6
Training loss: 2.8624677658081055
Validation loss: 2.9883364990193355

Epoch: 6| Step: 7
Training loss: 3.1577565670013428
Validation loss: 2.975341889166063

Epoch: 6| Step: 8
Training loss: 2.672109603881836
Validation loss: 2.943065966329267

Epoch: 6| Step: 9
Training loss: 3.337939739227295
Validation loss: 2.913405692705544

Epoch: 6| Step: 10
Training loss: 3.554961681365967
Validation loss: 2.8992773025266585

Epoch: 6| Step: 11
Training loss: 2.8072540760040283
Validation loss: 2.8865010379463114

Epoch: 6| Step: 12
Training loss: 2.551262378692627
Validation loss: 2.8598723539742092

Epoch: 6| Step: 13
Training loss: 2.820250988006592
Validation loss: 2.8382743020211496

Epoch: 17| Step: 0
Training loss: 3.021596908569336
Validation loss: 2.82998162956648

Epoch: 6| Step: 1
Training loss: 2.5705881118774414
Validation loss: 2.8139540918411745

Epoch: 6| Step: 2
Training loss: 2.4416818618774414
Validation loss: 2.8033663995804323

Epoch: 6| Step: 3
Training loss: 2.4528238773345947
Validation loss: 2.7703527686416463

Epoch: 6| Step: 4
Training loss: 3.057819366455078
Validation loss: 2.7839533667410574

Epoch: 6| Step: 5
Training loss: 2.3883891105651855
Validation loss: 2.7558538708635556

Epoch: 6| Step: 6
Training loss: 2.806148052215576
Validation loss: 2.7228334514043664

Epoch: 6| Step: 7
Training loss: 2.745518684387207
Validation loss: 2.716344233482115

Epoch: 6| Step: 8
Training loss: 2.173452138900757
Validation loss: 2.7021603866290023

Epoch: 6| Step: 9
Training loss: 2.904703140258789
Validation loss: 2.706915011969946

Epoch: 6| Step: 10
Training loss: 3.42268967628479
Validation loss: 2.674424368848083

Epoch: 6| Step: 11
Training loss: 2.988888740539551
Validation loss: 2.6612353042889665

Epoch: 6| Step: 12
Training loss: 3.3378851413726807
Validation loss: 2.6532967782789663

Epoch: 6| Step: 13
Training loss: 3.8928310871124268
Validation loss: 2.6329214572906494

Epoch: 18| Step: 0
Training loss: 2.9874398708343506
Validation loss: 2.6151336854504

Epoch: 6| Step: 1
Training loss: 2.927509307861328
Validation loss: 2.593069391865884

Epoch: 6| Step: 2
Training loss: 2.475682258605957
Validation loss: 2.586160391889593

Epoch: 6| Step: 3
Training loss: 2.9415817260742188
Validation loss: 2.5473930861360286

Epoch: 6| Step: 4
Training loss: 2.5994482040405273
Validation loss: 2.558462381362915

Epoch: 6| Step: 5
Training loss: 3.1500139236450195
Validation loss: 2.538464633367395

Epoch: 6| Step: 6
Training loss: 2.9525675773620605
Validation loss: 2.5167517790230374

Epoch: 6| Step: 7
Training loss: 3.053433418273926
Validation loss: 2.508478703037385

Epoch: 6| Step: 8
Training loss: 2.641986608505249
Validation loss: 2.4803456439766833

Epoch: 6| Step: 9
Training loss: 1.8996329307556152
Validation loss: 2.4542993089204193

Epoch: 6| Step: 10
Training loss: 2.504952907562256
Validation loss: 2.439088054882583

Epoch: 6| Step: 11
Training loss: 2.1514012813568115
Validation loss: 2.4267341039514028

Epoch: 6| Step: 12
Training loss: 2.8157877922058105
Validation loss: 2.432723552949967

Epoch: 6| Step: 13
Training loss: 3.0897061824798584
Validation loss: 2.392380775943879

Epoch: 19| Step: 0
Training loss: 2.6514840126037598
Validation loss: 2.4095435091244277

Epoch: 6| Step: 1
Training loss: 2.1864588260650635
Validation loss: 2.376275695780272

Epoch: 6| Step: 2
Training loss: 2.5261740684509277
Validation loss: 2.4064081740635697

Epoch: 6| Step: 3
Training loss: 2.993298053741455
Validation loss: 2.3805577703701553

Epoch: 6| Step: 4
Training loss: 3.1476082801818848
Validation loss: 2.3833995942146546

Epoch: 6| Step: 5
Training loss: 2.533997058868408
Validation loss: 2.3664381606604463

Epoch: 6| Step: 6
Training loss: 1.8389625549316406
Validation loss: 2.3250746239898024

Epoch: 6| Step: 7
Training loss: 3.0777342319488525
Validation loss: 2.339234459784723

Epoch: 6| Step: 8
Training loss: 3.1979331970214844
Validation loss: 2.33010834006853

Epoch: 6| Step: 9
Training loss: 2.1444756984710693
Validation loss: 2.320280482692103

Epoch: 6| Step: 10
Training loss: 2.1939592361450195
Validation loss: 2.320191321834441

Epoch: 6| Step: 11
Training loss: 2.545929431915283
Validation loss: 2.3314819284664687

Epoch: 6| Step: 12
Training loss: 2.6673998832702637
Validation loss: 2.3161171149182063

Epoch: 6| Step: 13
Training loss: 2.426126003265381
Validation loss: 2.311513646956413

Epoch: 20| Step: 0
Training loss: 2.796268939971924
Validation loss: 2.299148885152673

Epoch: 6| Step: 1
Training loss: 2.363895893096924
Validation loss: 2.298211902700445

Epoch: 6| Step: 2
Training loss: 1.9732928276062012
Validation loss: 2.315225593505367

Epoch: 6| Step: 3
Training loss: 2.5239956378936768
Validation loss: 2.273231170510733

Epoch: 6| Step: 4
Training loss: 3.239414691925049
Validation loss: 2.2587894714006813

Epoch: 6| Step: 5
Training loss: 2.827540874481201
Validation loss: 2.2684382033604447

Epoch: 6| Step: 6
Training loss: 1.9889156818389893
Validation loss: 2.261757686573972

Epoch: 6| Step: 7
Training loss: 2.3672330379486084
Validation loss: 2.252817935841058

Epoch: 6| Step: 8
Training loss: 2.8398818969726562
Validation loss: 2.2554760953431487

Epoch: 6| Step: 9
Training loss: 2.460747241973877
Validation loss: 2.268070779820924

Epoch: 6| Step: 10
Training loss: 2.574213981628418
Validation loss: 2.2485895772134104

Epoch: 6| Step: 11
Training loss: 3.010131359100342
Validation loss: 2.2383855209555676

Epoch: 6| Step: 12
Training loss: 2.370211362838745
Validation loss: 2.2489158158661215

Epoch: 6| Step: 13
Training loss: 1.8649779558181763
Validation loss: 2.2305164772977113

Epoch: 21| Step: 0
Training loss: 2.726742744445801
Validation loss: 2.2376789649327598

Epoch: 6| Step: 1
Training loss: 2.552776336669922
Validation loss: 2.236168251242689

Epoch: 6| Step: 2
Training loss: 2.4376559257507324
Validation loss: 2.2461423386809645

Epoch: 6| Step: 3
Training loss: 2.524376392364502
Validation loss: 2.2225459647435013

Epoch: 6| Step: 4
Training loss: 2.5809452533721924
Validation loss: 2.225983767099278

Epoch: 6| Step: 5
Training loss: 2.0583035945892334
Validation loss: 2.240277715908584

Epoch: 6| Step: 6
Training loss: 2.8329553604125977
Validation loss: 2.2131521676176336

Epoch: 6| Step: 7
Training loss: 2.6683526039123535
Validation loss: 2.218890313179262

Epoch: 6| Step: 8
Training loss: 1.914063811302185
Validation loss: 2.238539888012794

Epoch: 6| Step: 9
Training loss: 2.674887180328369
Validation loss: 2.2223723678178686

Epoch: 6| Step: 10
Training loss: 2.6624832153320312
Validation loss: 2.236999911646689

Epoch: 6| Step: 11
Training loss: 1.9132627248764038
Validation loss: 2.234371200684578

Epoch: 6| Step: 12
Training loss: 2.599316358566284
Validation loss: 2.200161364770705

Epoch: 6| Step: 13
Training loss: 3.553464412689209
Validation loss: 2.221380849038401

Epoch: 22| Step: 0
Training loss: 2.6010828018188477
Validation loss: 2.2456013412885767

Epoch: 6| Step: 1
Training loss: 3.271425247192383
Validation loss: 2.2001815329315844

Epoch: 6| Step: 2
Training loss: 2.797006368637085
Validation loss: 2.2199602691076135

Epoch: 6| Step: 3
Training loss: 2.3048644065856934
Validation loss: 2.200055553067115

Epoch: 6| Step: 4
Training loss: 2.4070024490356445
Validation loss: 2.2159525758476666

Epoch: 6| Step: 5
Training loss: 1.9181574583053589
Validation loss: 2.2334711731121106

Epoch: 6| Step: 6
Training loss: 2.0197582244873047
Validation loss: 2.1931622861534037

Epoch: 6| Step: 7
Training loss: 2.934828042984009
Validation loss: 2.2034385922134563

Epoch: 6| Step: 8
Training loss: 3.217545986175537
Validation loss: 2.2150218999514015

Epoch: 6| Step: 9
Training loss: 2.4600753784179688
Validation loss: 2.2008833013555056

Epoch: 6| Step: 10
Training loss: 2.5186898708343506
Validation loss: 2.176463564236959

Epoch: 6| Step: 11
Training loss: 1.5647034645080566
Validation loss: 2.211553268535163

Epoch: 6| Step: 12
Training loss: 2.604611396789551
Validation loss: 2.2305019388916674

Epoch: 6| Step: 13
Training loss: 2.6701245307922363
Validation loss: 2.2123749948317006

Epoch: 23| Step: 0
Training loss: 1.9316153526306152
Validation loss: 2.1829969216418523

Epoch: 6| Step: 1
Training loss: 2.618102550506592
Validation loss: 2.1916710843322096

Epoch: 6| Step: 2
Training loss: 1.9815000295639038
Validation loss: 2.2151671430116058

Epoch: 6| Step: 3
Training loss: 2.6303458213806152
Validation loss: 2.178189477612895

Epoch: 6| Step: 4
Training loss: 2.623734474182129
Validation loss: 2.207576979873001

Epoch: 6| Step: 5
Training loss: 2.6322717666625977
Validation loss: 2.197573269567182

Epoch: 6| Step: 6
Training loss: 2.8719115257263184
Validation loss: 2.1944317535687516

Epoch: 6| Step: 7
Training loss: 1.8738094568252563
Validation loss: 2.1939392346207813

Epoch: 6| Step: 8
Training loss: 2.2168314456939697
Validation loss: 2.195978028799898

Epoch: 6| Step: 9
Training loss: 3.629415988922119
Validation loss: 2.1709422167911323

Epoch: 6| Step: 10
Training loss: 2.670440912246704
Validation loss: 2.177302796353576

Epoch: 6| Step: 11
Training loss: 2.524146556854248
Validation loss: 2.203687680664883

Epoch: 6| Step: 12
Training loss: 2.3706483840942383
Validation loss: 2.197078876597907

Epoch: 6| Step: 13
Training loss: 2.614816427230835
Validation loss: 2.165910590079523

Epoch: 24| Step: 0
Training loss: 2.8941164016723633
Validation loss: 2.176685634479728

Epoch: 6| Step: 1
Training loss: 2.748983860015869
Validation loss: 2.157259818046324

Epoch: 6| Step: 2
Training loss: 3.0518932342529297
Validation loss: 2.190273445139649

Epoch: 6| Step: 3
Training loss: 1.9715114831924438
Validation loss: 2.1733404141600414

Epoch: 6| Step: 4
Training loss: 2.492605686187744
Validation loss: 2.1656397786191715

Epoch: 6| Step: 5
Training loss: 2.5621814727783203
Validation loss: 2.1789767703702374

Epoch: 6| Step: 6
Training loss: 2.714864730834961
Validation loss: 2.176795913327125

Epoch: 6| Step: 7
Training loss: 2.276857852935791
Validation loss: 2.174450921755965

Epoch: 6| Step: 8
Training loss: 3.127542018890381
Validation loss: 2.184408272466352

Epoch: 6| Step: 9
Training loss: 2.131103515625
Validation loss: 2.2063310069422566

Epoch: 6| Step: 10
Training loss: 2.2245736122131348
Validation loss: 2.184132182469932

Epoch: 6| Step: 11
Training loss: 2.534304141998291
Validation loss: 2.1796362810237433

Epoch: 6| Step: 12
Training loss: 2.163320541381836
Validation loss: 2.1876369137917795

Epoch: 6| Step: 13
Training loss: 2.490978717803955
Validation loss: 2.1947486067330964

Epoch: 25| Step: 0
Training loss: 2.3271260261535645
Validation loss: 2.172116664148146

Epoch: 6| Step: 1
Training loss: 2.264714241027832
Validation loss: 2.180816296608217

Epoch: 6| Step: 2
Training loss: 2.675687789916992
Validation loss: 2.1948622939407185

Epoch: 6| Step: 3
Training loss: 2.543173313140869
Validation loss: 2.171833634376526

Epoch: 6| Step: 4
Training loss: 2.0452463626861572
Validation loss: 2.167164530805362

Epoch: 6| Step: 5
Training loss: 2.323824644088745
Validation loss: 2.1803168378850466

Epoch: 6| Step: 6
Training loss: 2.8676931858062744
Validation loss: 2.1843649007940806

Epoch: 6| Step: 7
Training loss: 3.1000094413757324
Validation loss: 2.2005767783811017

Epoch: 6| Step: 8
Training loss: 2.433318614959717
Validation loss: 2.14997023664495

Epoch: 6| Step: 9
Training loss: 2.7188992500305176
Validation loss: 2.175773832105821

Epoch: 6| Step: 10
Training loss: 2.239489793777466
Validation loss: 2.202318245364774

Epoch: 6| Step: 11
Training loss: 2.4654741287231445
Validation loss: 2.2151688170689408

Epoch: 6| Step: 12
Training loss: 2.7819128036499023
Validation loss: 2.184532084772664

Epoch: 6| Step: 13
Training loss: 2.303060293197632
Validation loss: 2.213885961040374

Epoch: 26| Step: 0
Training loss: 2.6042726039886475
Validation loss: 2.198995985010619

Epoch: 6| Step: 1
Training loss: 2.361095428466797
Validation loss: 2.1809101002190703

Epoch: 6| Step: 2
Training loss: 2.9838461875915527
Validation loss: 2.194856859022571

Epoch: 6| Step: 3
Training loss: 1.8357503414154053
Validation loss: 2.210272745419574

Epoch: 6| Step: 4
Training loss: 2.7180418968200684
Validation loss: 2.177540453531409

Epoch: 6| Step: 5
Training loss: 1.9794178009033203
Validation loss: 2.197274882306335

Epoch: 6| Step: 6
Training loss: 3.4389679431915283
Validation loss: 2.188663587775282

Epoch: 6| Step: 7
Training loss: 2.3186116218566895
Validation loss: 2.192234164925032

Epoch: 6| Step: 8
Training loss: 2.652907371520996
Validation loss: 2.1867393178324543

Epoch: 6| Step: 9
Training loss: 2.376941204071045
Validation loss: 2.211969724265478

Epoch: 6| Step: 10
Training loss: 2.1878650188446045
Validation loss: 2.1860728481764435

Epoch: 6| Step: 11
Training loss: 2.498587131500244
Validation loss: 2.1858910283734723

Epoch: 6| Step: 12
Training loss: 2.620901584625244
Validation loss: 2.2055037918911187

Epoch: 6| Step: 13
Training loss: 2.7027087211608887
Validation loss: 2.1924400688499532

Epoch: 27| Step: 0
Training loss: 2.5475854873657227
Validation loss: 2.2247925137960785

Epoch: 6| Step: 1
Training loss: 2.1960158348083496
Validation loss: 2.191490391249298

Epoch: 6| Step: 2
Training loss: 3.279505729675293
Validation loss: 2.187994764697167

Epoch: 6| Step: 3
Training loss: 3.0284597873687744
Validation loss: 2.1745109711923907

Epoch: 6| Step: 4
Training loss: 2.692173957824707
Validation loss: 2.1977853851933635

Epoch: 6| Step: 5
Training loss: 1.7971175909042358
Validation loss: 2.175643523534139

Epoch: 6| Step: 6
Training loss: 2.477748394012451
Validation loss: 2.1989932329423967

Epoch: 6| Step: 7
Training loss: 2.5622284412384033
Validation loss: 2.2032210006508777

Epoch: 6| Step: 8
Training loss: 1.957421064376831
Validation loss: 2.190685538835423

Epoch: 6| Step: 9
Training loss: 3.053190231323242
Validation loss: 2.1916395412978305

Epoch: 6| Step: 10
Training loss: 2.502537727355957
Validation loss: 2.183066528330567

Epoch: 6| Step: 11
Training loss: 2.3300681114196777
Validation loss: 2.1931291498163694

Epoch: 6| Step: 12
Training loss: 1.888932228088379
Validation loss: 2.2019342555794665

Epoch: 6| Step: 13
Training loss: 2.760739803314209
Validation loss: 2.1891812714197303

Epoch: 28| Step: 0
Training loss: 2.17171573638916
Validation loss: 2.20188561818933

Epoch: 6| Step: 1
Training loss: 2.323935031890869
Validation loss: 2.1844846766482116

Epoch: 6| Step: 2
Training loss: 2.334583282470703
Validation loss: 2.1928947253893782

Epoch: 6| Step: 3
Training loss: 2.2536187171936035
Validation loss: 2.173430117227698

Epoch: 6| Step: 4
Training loss: 2.3091378211975098
Validation loss: 2.1984802112784436

Epoch: 6| Step: 5
Training loss: 3.4847395420074463
Validation loss: 2.1791737746166926

Epoch: 6| Step: 6
Training loss: 2.010939598083496
Validation loss: 2.173530827286423

Epoch: 6| Step: 7
Training loss: 2.494316816329956
Validation loss: 2.170584673522621

Epoch: 6| Step: 8
Training loss: 2.6344499588012695
Validation loss: 2.1834256725926555

Epoch: 6| Step: 9
Training loss: 2.6591739654541016
Validation loss: 2.181964458957795

Epoch: 6| Step: 10
Training loss: 2.4310758113861084
Validation loss: 2.176059367836163

Epoch: 6| Step: 11
Training loss: 2.334235191345215
Validation loss: 2.203369125243156

Epoch: 6| Step: 12
Training loss: 2.8483572006225586
Validation loss: 2.186743966994747

Epoch: 6| Step: 13
Training loss: 2.472683906555176
Validation loss: 2.193640088522306

Epoch: 29| Step: 0
Training loss: 2.466917037963867
Validation loss: 2.1826244028665687

Epoch: 6| Step: 1
Training loss: 2.7831945419311523
Validation loss: 2.1972966296698457

Epoch: 6| Step: 2
Training loss: 3.0642967224121094
Validation loss: 2.1976549727942354

Epoch: 6| Step: 3
Training loss: 2.5251848697662354
Validation loss: 2.182423515986371

Epoch: 6| Step: 4
Training loss: 2.495354175567627
Validation loss: 2.1644720108278337

Epoch: 6| Step: 5
Training loss: 1.750744104385376
Validation loss: 2.177326456193001

Epoch: 6| Step: 6
Training loss: 1.8107450008392334
Validation loss: 2.1591221517132175

Epoch: 6| Step: 7
Training loss: 2.5914905071258545
Validation loss: 2.170551541031048

Epoch: 6| Step: 8
Training loss: 2.262385845184326
Validation loss: 2.166811994327012

Epoch: 6| Step: 9
Training loss: 2.354414939880371
Validation loss: 2.1654404568415817

Epoch: 6| Step: 10
Training loss: 2.2682900428771973
Validation loss: 2.1471681556394024

Epoch: 6| Step: 11
Training loss: 2.7955708503723145
Validation loss: 2.158618337364607

Epoch: 6| Step: 12
Training loss: 2.5233402252197266
Validation loss: 2.142707265833373

Epoch: 6| Step: 13
Training loss: 3.176064968109131
Validation loss: 2.1838909400406705

Epoch: 30| Step: 0
Training loss: 2.284187078475952
Validation loss: 2.1430817778392504

Epoch: 6| Step: 1
Training loss: 1.930252194404602
Validation loss: 2.1625733555004163

Epoch: 6| Step: 2
Training loss: 2.280512571334839
Validation loss: 2.1500694803012315

Epoch: 6| Step: 3
Training loss: 2.470364570617676
Validation loss: 2.1947668393452964

Epoch: 6| Step: 4
Training loss: 2.3982315063476562
Validation loss: 2.1370017323442685

Epoch: 6| Step: 5
Training loss: 2.7378406524658203
Validation loss: 2.1244116713923793

Epoch: 6| Step: 6
Training loss: 3.176581621170044
Validation loss: 2.152360563637108

Epoch: 6| Step: 7
Training loss: 2.57637357711792
Validation loss: 2.1508115389013804

Epoch: 6| Step: 8
Training loss: 3.1709721088409424
Validation loss: 2.1365858303603305

Epoch: 6| Step: 9
Training loss: 2.1446995735168457
Validation loss: 2.1480579107038436

Epoch: 6| Step: 10
Training loss: 3.223026752471924
Validation loss: 2.1465031972495456

Epoch: 6| Step: 11
Training loss: 1.4234482049942017
Validation loss: 2.1601518392562866

Epoch: 6| Step: 12
Training loss: 2.303417444229126
Validation loss: 2.1453872765264204

Epoch: 6| Step: 13
Training loss: 2.7581236362457275
Validation loss: 2.1522206491039646

Epoch: 31| Step: 0
Training loss: 2.5315847396850586
Validation loss: 2.1851832764123076

Epoch: 6| Step: 1
Training loss: 3.512429714202881
Validation loss: 2.132799279305243

Epoch: 6| Step: 2
Training loss: 1.9575846195220947
Validation loss: 2.1720383808177006

Epoch: 6| Step: 3
Training loss: 1.6716701984405518
Validation loss: 2.1767231251603816

Epoch: 6| Step: 4
Training loss: 2.2644364833831787
Validation loss: 2.1518767392763527

Epoch: 6| Step: 5
Training loss: 2.9139599800109863
Validation loss: 2.129801345127885

Epoch: 6| Step: 6
Training loss: 2.2664856910705566
Validation loss: 2.17436186985303

Epoch: 6| Step: 7
Training loss: 2.023662805557251
Validation loss: 2.1853159883970856

Epoch: 6| Step: 8
Training loss: 1.8812216520309448
Validation loss: 2.196172942397415

Epoch: 6| Step: 9
Training loss: 3.4569201469421387
Validation loss: 2.1788187898615354

Epoch: 6| Step: 10
Training loss: 2.8611221313476562
Validation loss: 2.1828559444796656

Epoch: 6| Step: 11
Training loss: 3.1538524627685547
Validation loss: 2.1575591717996905

Epoch: 6| Step: 12
Training loss: 2.3526864051818848
Validation loss: 2.177526125343897

Epoch: 6| Step: 13
Training loss: 1.5279089212417603
Validation loss: 2.1746718114422214

Epoch: 32| Step: 0
Training loss: 1.9796990156173706
Validation loss: 2.17112228690937

Epoch: 6| Step: 1
Training loss: 2.2857282161712646
Validation loss: 2.176179516700006

Epoch: 6| Step: 2
Training loss: 1.755375623703003
Validation loss: 2.1733907448348178

Epoch: 6| Step: 3
Training loss: 2.8182759284973145
Validation loss: 2.167658946847403

Epoch: 6| Step: 4
Training loss: 3.111510753631592
Validation loss: 2.190507881103023

Epoch: 6| Step: 5
Training loss: 2.2280850410461426
Validation loss: 2.174350469343124

Epoch: 6| Step: 6
Training loss: 2.4287266731262207
Validation loss: 2.1537836213265695

Epoch: 6| Step: 7
Training loss: 2.320178508758545
Validation loss: 2.1843385952775196

Epoch: 6| Step: 8
Training loss: 3.002838611602783
Validation loss: 2.1343243724556378

Epoch: 6| Step: 9
Training loss: 2.5217537879943848
Validation loss: 2.16958797618907

Epoch: 6| Step: 10
Training loss: 2.9481475353240967
Validation loss: 2.159122864405314

Epoch: 6| Step: 11
Training loss: 2.430203914642334
Validation loss: 2.1291406308451006

Epoch: 6| Step: 12
Training loss: 2.0554628372192383
Validation loss: 2.1589055766341505

Epoch: 6| Step: 13
Training loss: 3.0443413257598877
Validation loss: 2.1530868648200907

Epoch: 33| Step: 0
Training loss: 2.651827096939087
Validation loss: 2.181533718621859

Epoch: 6| Step: 1
Training loss: 2.4493978023529053
Validation loss: 2.1485523036731187

Epoch: 6| Step: 2
Training loss: 2.241077423095703
Validation loss: 2.1579225858052573

Epoch: 6| Step: 3
Training loss: 2.6998367309570312
Validation loss: 2.1682612819056355

Epoch: 6| Step: 4
Training loss: 2.233208417892456
Validation loss: 2.1640338564431794

Epoch: 6| Step: 5
Training loss: 2.252505302429199
Validation loss: 2.1283038950735524

Epoch: 6| Step: 6
Training loss: 2.5148065090179443
Validation loss: 2.1462606178816928

Epoch: 6| Step: 7
Training loss: 2.8093414306640625
Validation loss: 2.1405447529208277

Epoch: 6| Step: 8
Training loss: 1.9073084592819214
Validation loss: 2.163935294715307

Epoch: 6| Step: 9
Training loss: 1.923827052116394
Validation loss: 2.1585618372886413

Epoch: 6| Step: 10
Training loss: 3.4762487411499023
Validation loss: 2.15472146003477

Epoch: 6| Step: 11
Training loss: 3.060471773147583
Validation loss: 2.145868227046023

Epoch: 6| Step: 12
Training loss: 2.3798575401306152
Validation loss: 2.147178044883154

Epoch: 6| Step: 13
Training loss: 1.7121418714523315
Validation loss: 2.142579755475444

Epoch: 34| Step: 0
Training loss: 2.1114799976348877
Validation loss: 2.134378807519072

Epoch: 6| Step: 1
Training loss: 2.646885871887207
Validation loss: 2.1473801084744033

Epoch: 6| Step: 2
Training loss: 1.9409162998199463
Validation loss: 2.1383454620197253

Epoch: 6| Step: 3
Training loss: 2.4851155281066895
Validation loss: 2.1371630135402886

Epoch: 6| Step: 4
Training loss: 2.1935784816741943
Validation loss: 2.13921465668627

Epoch: 6| Step: 5
Training loss: 2.1721272468566895
Validation loss: 2.1267125375809206

Epoch: 6| Step: 6
Training loss: 2.6554582118988037
Validation loss: 2.146981541828443

Epoch: 6| Step: 7
Training loss: 2.375988483428955
Validation loss: 2.1505207361713534

Epoch: 6| Step: 8
Training loss: 2.482372283935547
Validation loss: 2.1558757238490607

Epoch: 6| Step: 9
Training loss: 2.332674980163574
Validation loss: 2.1263348440970145

Epoch: 6| Step: 10
Training loss: 2.8253183364868164
Validation loss: 2.1568085890944286

Epoch: 6| Step: 11
Training loss: 3.012782096862793
Validation loss: 2.1431585204216743

Epoch: 6| Step: 12
Training loss: 2.760741710662842
Validation loss: 2.1681277623740574

Epoch: 6| Step: 13
Training loss: 2.3664379119873047
Validation loss: 2.149268663057717

Epoch: 35| Step: 0
Training loss: 1.79957914352417
Validation loss: 2.140016976223197

Epoch: 6| Step: 1
Training loss: 2.6418046951293945
Validation loss: 2.15086333469678

Epoch: 6| Step: 2
Training loss: 2.403489828109741
Validation loss: 2.1595126685275825

Epoch: 6| Step: 3
Training loss: 2.1696481704711914
Validation loss: 2.1519860811130975

Epoch: 6| Step: 4
Training loss: 2.2317824363708496
Validation loss: 2.149533176934847

Epoch: 6| Step: 5
Training loss: 2.771984100341797
Validation loss: 2.1446608497250463

Epoch: 6| Step: 6
Training loss: 3.4731147289276123
Validation loss: 2.164240143632376

Epoch: 6| Step: 7
Training loss: 2.2880587577819824
Validation loss: 2.158530540363763

Epoch: 6| Step: 8
Training loss: 2.3347928524017334
Validation loss: 2.145842267620948

Epoch: 6| Step: 9
Training loss: 2.9673590660095215
Validation loss: 2.1469718307577152

Epoch: 6| Step: 10
Training loss: 2.0288186073303223
Validation loss: 2.124341731430382

Epoch: 6| Step: 11
Training loss: 2.0638904571533203
Validation loss: 2.1544372702157624

Epoch: 6| Step: 12
Training loss: 2.5184078216552734
Validation loss: 2.1593446859749417

Epoch: 6| Step: 13
Training loss: 3.002889394760132
Validation loss: 2.117715779171195

Epoch: 36| Step: 0
Training loss: 2.198884963989258
Validation loss: 2.1454785716149116

Epoch: 6| Step: 1
Training loss: 2.0384981632232666
Validation loss: 2.1504834877547396

Epoch: 6| Step: 2
Training loss: 2.439173936843872
Validation loss: 2.150926600220383

Epoch: 6| Step: 3
Training loss: 2.4305672645568848
Validation loss: 2.174205139119138

Epoch: 6| Step: 4
Training loss: 2.1466572284698486
Validation loss: 2.169742432973718

Epoch: 6| Step: 5
Training loss: 2.204228401184082
Validation loss: 2.1427463305893766

Epoch: 6| Step: 6
Training loss: 2.4759790897369385
Validation loss: 2.128417932859031

Epoch: 6| Step: 7
Training loss: 2.2616477012634277
Validation loss: 2.1564262092754407

Epoch: 6| Step: 8
Training loss: 2.5949747562408447
Validation loss: 2.1402228878390406

Epoch: 6| Step: 9
Training loss: 3.3011250495910645
Validation loss: 2.137942135974925

Epoch: 6| Step: 10
Training loss: 2.6279122829437256
Validation loss: 2.155118880733367

Epoch: 6| Step: 11
Training loss: 2.7380611896514893
Validation loss: 2.1396093894076604

Epoch: 6| Step: 12
Training loss: 2.6104655265808105
Validation loss: 2.1468297768664617

Epoch: 6| Step: 13
Training loss: 2.361984968185425
Validation loss: 2.1606544358755952

Epoch: 37| Step: 0
Training loss: 3.092808246612549
Validation loss: 2.113276712356075

Epoch: 6| Step: 1
Training loss: 2.5494441986083984
Validation loss: 2.139301256466937

Epoch: 6| Step: 2
Training loss: 2.9955735206604004
Validation loss: 2.139385461807251

Epoch: 6| Step: 3
Training loss: 1.9109036922454834
Validation loss: 2.1398130898834555

Epoch: 6| Step: 4
Training loss: 2.8828818798065186
Validation loss: 2.1363458928241523

Epoch: 6| Step: 5
Training loss: 2.3837509155273438
Validation loss: 2.1257428840924333

Epoch: 6| Step: 6
Training loss: 1.852964997291565
Validation loss: 2.1319850414030013

Epoch: 6| Step: 7
Training loss: 2.673379421234131
Validation loss: 2.158820764992827

Epoch: 6| Step: 8
Training loss: 2.69917893409729
Validation loss: 2.172683372292467

Epoch: 6| Step: 9
Training loss: 1.7023520469665527
Validation loss: 2.1454267553103867

Epoch: 6| Step: 10
Training loss: 2.353037118911743
Validation loss: 2.134774428541942

Epoch: 6| Step: 11
Training loss: 1.633973240852356
Validation loss: 2.138164051117436

Epoch: 6| Step: 12
Training loss: 2.5763542652130127
Validation loss: 2.138897729176347

Epoch: 6| Step: 13
Training loss: 3.248936176300049
Validation loss: 2.134579307289534

Epoch: 38| Step: 0
Training loss: 2.7376110553741455
Validation loss: 2.145056850166731

Epoch: 6| Step: 1
Training loss: 3.1379241943359375
Validation loss: 2.1037322885246685

Epoch: 6| Step: 2
Training loss: 2.445044994354248
Validation loss: 2.1259144736874487

Epoch: 6| Step: 3
Training loss: 2.0057477951049805
Validation loss: 2.13794719788336

Epoch: 6| Step: 4
Training loss: 1.71759033203125
Validation loss: 2.1312377299031904

Epoch: 6| Step: 5
Training loss: 1.9523224830627441
Validation loss: 2.106848418071706

Epoch: 6| Step: 6
Training loss: 2.7027711868286133
Validation loss: 2.145091651588358

Epoch: 6| Step: 7
Training loss: 1.8721582889556885
Validation loss: 2.1120055196105794

Epoch: 6| Step: 8
Training loss: 2.402085781097412
Validation loss: 2.1411626672232025

Epoch: 6| Step: 9
Training loss: 2.716620922088623
Validation loss: 2.115264170913286

Epoch: 6| Step: 10
Training loss: 2.3159682750701904
Validation loss: 2.1235844947958507

Epoch: 6| Step: 11
Training loss: 2.0281996726989746
Validation loss: 2.147664805894257

Epoch: 6| Step: 12
Training loss: 3.0915141105651855
Validation loss: 2.122722805187266

Epoch: 6| Step: 13
Training loss: 3.7104296684265137
Validation loss: 2.1305360614612536

Epoch: 39| Step: 0
Training loss: 2.482762336730957
Validation loss: 2.1624168811305875

Epoch: 6| Step: 1
Training loss: 2.7553181648254395
Validation loss: 2.1112620522899013

Epoch: 6| Step: 2
Training loss: 2.240858793258667
Validation loss: 2.1342053323663692

Epoch: 6| Step: 3
Training loss: 1.992348074913025
Validation loss: 2.1257551870038434

Epoch: 6| Step: 4
Training loss: 2.6324350833892822
Validation loss: 2.1579589382294686

Epoch: 6| Step: 5
Training loss: 2.0262861251831055
Validation loss: 2.156227239998438

Epoch: 6| Step: 6
Training loss: 2.6773550510406494
Validation loss: 2.141155068592359

Epoch: 6| Step: 7
Training loss: 2.3615410327911377
Validation loss: 2.1154990234682636

Epoch: 6| Step: 8
Training loss: 2.6108603477478027
Validation loss: 2.1118158422490603

Epoch: 6| Step: 9
Training loss: 2.5553536415100098
Validation loss: 2.1398781858464724

Epoch: 6| Step: 10
Training loss: 1.948303461074829
Validation loss: 2.124589486788678

Epoch: 6| Step: 11
Training loss: 2.481283664703369
Validation loss: 2.1398170391718545

Epoch: 6| Step: 12
Training loss: 2.6027703285217285
Validation loss: 2.1365058396452214

Epoch: 6| Step: 13
Training loss: 3.4211273193359375
Validation loss: 2.12415563419301

Epoch: 40| Step: 0
Training loss: 2.6921539306640625
Validation loss: 2.135441105852845

Epoch: 6| Step: 1
Training loss: 1.9879884719848633
Validation loss: 2.1184075442693566

Epoch: 6| Step: 2
Training loss: 2.8041930198669434
Validation loss: 2.1435449020836943

Epoch: 6| Step: 3
Training loss: 2.3317747116088867
Validation loss: 2.127420904815838

Epoch: 6| Step: 4
Training loss: 2.1651883125305176
Validation loss: 2.1262653566175893

Epoch: 6| Step: 5
Training loss: 3.011251211166382
Validation loss: 2.134515134237146

Epoch: 6| Step: 6
Training loss: 2.538975954055786
Validation loss: 2.1469931525568806

Epoch: 6| Step: 7
Training loss: 3.1392970085144043
Validation loss: 2.1038646877452893

Epoch: 6| Step: 8
Training loss: 2.192392349243164
Validation loss: 2.1358697542580227

Epoch: 6| Step: 9
Training loss: 2.274796962738037
Validation loss: 2.1339917644377677

Epoch: 6| Step: 10
Training loss: 2.651935577392578
Validation loss: 2.1453171084004063

Epoch: 6| Step: 11
Training loss: 1.6538647413253784
Validation loss: 2.1307950455655336

Epoch: 6| Step: 12
Training loss: 2.209336280822754
Validation loss: 2.1260229079954085

Epoch: 6| Step: 13
Training loss: 2.6906330585479736
Validation loss: 2.1478097515721477

Epoch: 41| Step: 0
Training loss: 2.797238826751709
Validation loss: 2.134167677612715

Epoch: 6| Step: 1
Training loss: 2.239396333694458
Validation loss: 2.1279380359957294

Epoch: 6| Step: 2
Training loss: 2.3736960887908936
Validation loss: 2.143455825826173

Epoch: 6| Step: 3
Training loss: 2.777423858642578
Validation loss: 2.136587917163808

Epoch: 6| Step: 4
Training loss: 1.626225471496582
Validation loss: 2.155467774278374

Epoch: 6| Step: 5
Training loss: 1.9683282375335693
Validation loss: 2.1504490503700833

Epoch: 6| Step: 6
Training loss: 3.2361717224121094
Validation loss: 2.1655798778739026

Epoch: 6| Step: 7
Training loss: 2.3356847763061523
Validation loss: 2.1704566671002294

Epoch: 6| Step: 8
Training loss: 3.158552646636963
Validation loss: 2.1561749122476064

Epoch: 6| Step: 9
Training loss: 2.2143986225128174
Validation loss: 2.1350625740584506

Epoch: 6| Step: 10
Training loss: 2.2309224605560303
Validation loss: 2.12715051251073

Epoch: 6| Step: 11
Training loss: 2.577500820159912
Validation loss: 2.148770034954112

Epoch: 6| Step: 12
Training loss: 1.9104700088500977
Validation loss: 2.117871815158475

Epoch: 6| Step: 13
Training loss: 2.971043825149536
Validation loss: 2.105801845109591

Epoch: 42| Step: 0
Training loss: 2.7919726371765137
Validation loss: 2.135255636707429

Epoch: 6| Step: 1
Training loss: 3.132913112640381
Validation loss: 2.1410083258023827

Epoch: 6| Step: 2
Training loss: 2.6956162452697754
Validation loss: 2.1118900570818173

Epoch: 6| Step: 3
Training loss: 2.1904125213623047
Validation loss: 2.1367419765841578

Epoch: 6| Step: 4
Training loss: 1.547947883605957
Validation loss: 2.1222716787809968

Epoch: 6| Step: 5
Training loss: 2.591005325317383
Validation loss: 2.138943104333775

Epoch: 6| Step: 6
Training loss: 3.2490484714508057
Validation loss: 2.1521197134448635

Epoch: 6| Step: 7
Training loss: 1.6275769472122192
Validation loss: 2.118321370053035

Epoch: 6| Step: 8
Training loss: 1.897965669631958
Validation loss: 2.123399721678867

Epoch: 6| Step: 9
Training loss: 2.4482202529907227
Validation loss: 2.1487015498581754

Epoch: 6| Step: 10
Training loss: 1.942865252494812
Validation loss: 2.140984847981443

Epoch: 6| Step: 11
Training loss: 2.9192371368408203
Validation loss: 2.1505670316757692

Epoch: 6| Step: 12
Training loss: 2.3732428550720215
Validation loss: 2.150131730623143

Epoch: 6| Step: 13
Training loss: 2.7891013622283936
Validation loss: 2.142452116935484

Epoch: 43| Step: 0
Training loss: 1.7665576934814453
Validation loss: 2.143060650876773

Epoch: 6| Step: 1
Training loss: 2.1543455123901367
Validation loss: 2.1323269874818864

Epoch: 6| Step: 2
Training loss: 2.5896995067596436
Validation loss: 2.12933063250716

Epoch: 6| Step: 3
Training loss: 2.1983556747436523
Validation loss: 2.1072825539496636

Epoch: 6| Step: 4
Training loss: 2.7469983100891113
Validation loss: 2.14880637199648

Epoch: 6| Step: 5
Training loss: 2.434971332550049
Validation loss: 2.1181284202042447

Epoch: 6| Step: 6
Training loss: 2.625985622406006
Validation loss: 2.1258165900425245

Epoch: 6| Step: 7
Training loss: 2.7170329093933105
Validation loss: 2.1382110272684405

Epoch: 6| Step: 8
Training loss: 2.572378635406494
Validation loss: 2.1341415579601

Epoch: 6| Step: 9
Training loss: 1.986338496208191
Validation loss: 2.0980016018754695

Epoch: 6| Step: 10
Training loss: 2.3666117191314697
Validation loss: 2.1255943416267313

Epoch: 6| Step: 11
Training loss: 2.1441783905029297
Validation loss: 2.127582247539233

Epoch: 6| Step: 12
Training loss: 3.414950370788574
Validation loss: 2.149518701337999

Epoch: 6| Step: 13
Training loss: 2.109877586364746
Validation loss: 2.1099111110933366

Epoch: 44| Step: 0
Training loss: 2.789381980895996
Validation loss: 2.122037767082132

Epoch: 6| Step: 1
Training loss: 1.7598680257797241
Validation loss: 2.1661296685536704

Epoch: 6| Step: 2
Training loss: 2.258791446685791
Validation loss: 2.1258737246195474

Epoch: 6| Step: 3
Training loss: 2.3107128143310547
Validation loss: 2.123804521817033

Epoch: 6| Step: 4
Training loss: 2.618764638900757
Validation loss: 2.1255506982085524

Epoch: 6| Step: 5
Training loss: 1.9658119678497314
Validation loss: 2.121510439021613

Epoch: 6| Step: 6
Training loss: 1.9632551670074463
Validation loss: 2.131887288503749

Epoch: 6| Step: 7
Training loss: 2.009033203125
Validation loss: 2.1221175809060373

Epoch: 6| Step: 8
Training loss: 3.414205551147461
Validation loss: 2.1393974211908158

Epoch: 6| Step: 9
Training loss: 2.9014720916748047
Validation loss: 2.1345065768047045

Epoch: 6| Step: 10
Training loss: 2.250957489013672
Validation loss: 2.136946437179401

Epoch: 6| Step: 11
Training loss: 1.6510543823242188
Validation loss: 2.1330367134463404

Epoch: 6| Step: 12
Training loss: 3.177224636077881
Validation loss: 2.134699659962808

Epoch: 6| Step: 13
Training loss: 3.068443536758423
Validation loss: 2.1230203695194696

Epoch: 45| Step: 0
Training loss: 2.444685459136963
Validation loss: 2.102545484419792

Epoch: 6| Step: 1
Training loss: 2.994370698928833
Validation loss: 2.1451331069392543

Epoch: 6| Step: 2
Training loss: 2.8465189933776855
Validation loss: 2.129180872312156

Epoch: 6| Step: 3
Training loss: 1.8320343494415283
Validation loss: 2.1314411317148516

Epoch: 6| Step: 4
Training loss: 2.6203460693359375
Validation loss: 2.139159289739465

Epoch: 6| Step: 5
Training loss: 1.9087005853652954
Validation loss: 2.1575325201916438

Epoch: 6| Step: 6
Training loss: 3.3381316661834717
Validation loss: 2.1416529865675074

Epoch: 6| Step: 7
Training loss: 2.1274924278259277
Validation loss: 2.1528238763091383

Epoch: 6| Step: 8
Training loss: 2.8530757427215576
Validation loss: 2.1361393543981735

Epoch: 6| Step: 9
Training loss: 2.5249783992767334
Validation loss: 2.126088129576816

Epoch: 6| Step: 10
Training loss: 2.2645559310913086
Validation loss: 2.1089430227074573

Epoch: 6| Step: 11
Training loss: 1.9857550859451294
Validation loss: 2.136814971123972

Epoch: 6| Step: 12
Training loss: 1.874640703201294
Validation loss: 2.1505376600450083

Epoch: 6| Step: 13
Training loss: 2.389130115509033
Validation loss: 2.15313155933093

Epoch: 46| Step: 0
Training loss: 2.2430295944213867
Validation loss: 2.1652036815561275

Epoch: 6| Step: 1
Training loss: 2.4138479232788086
Validation loss: 2.122251133764944

Epoch: 6| Step: 2
Training loss: 2.853355884552002
Validation loss: 2.128635609021751

Epoch: 6| Step: 3
Training loss: 2.882855176925659
Validation loss: 2.110881831056328

Epoch: 6| Step: 4
Training loss: 1.9946861267089844
Validation loss: 2.153572667029596

Epoch: 6| Step: 5
Training loss: 3.207472324371338
Validation loss: 2.113276068882276

Epoch: 6| Step: 6
Training loss: 2.3863301277160645
Validation loss: 2.1365224366546958

Epoch: 6| Step: 7
Training loss: 1.9349045753479004
Validation loss: 2.1365151482243694

Epoch: 6| Step: 8
Training loss: 2.6248722076416016
Validation loss: 2.140954681622085

Epoch: 6| Step: 9
Training loss: 1.902944564819336
Validation loss: 2.1309985345409763

Epoch: 6| Step: 10
Training loss: 2.8900341987609863
Validation loss: 2.129150261161148

Epoch: 6| Step: 11
Training loss: 2.1934471130371094
Validation loss: 2.1338318060803156

Epoch: 6| Step: 12
Training loss: 2.186598777770996
Validation loss: 2.107337477386639

Epoch: 6| Step: 13
Training loss: 2.2405261993408203
Validation loss: 2.131828649069673

Epoch: 47| Step: 0
Training loss: 2.497793674468994
Validation loss: 2.1027806356389034

Epoch: 6| Step: 1
Training loss: 3.1484458446502686
Validation loss: 2.1266096612458587

Epoch: 6| Step: 2
Training loss: 2.3912298679351807
Validation loss: 2.134962584382744

Epoch: 6| Step: 3
Training loss: 2.6257457733154297
Validation loss: 2.145238084177817

Epoch: 6| Step: 4
Training loss: 2.5842714309692383
Validation loss: 2.143098000557192

Epoch: 6| Step: 5
Training loss: 2.315260410308838
Validation loss: 2.1298020603836223

Epoch: 6| Step: 6
Training loss: 1.9086711406707764
Validation loss: 2.1255936648256037

Epoch: 6| Step: 7
Training loss: 2.2999558448791504
Validation loss: 2.124676727479504

Epoch: 6| Step: 8
Training loss: 2.9803061485290527
Validation loss: 2.152685703769807

Epoch: 6| Step: 9
Training loss: 2.0647807121276855
Validation loss: 2.131041560121762

Epoch: 6| Step: 10
Training loss: 2.0945053100585938
Validation loss: 2.1221921469575618

Epoch: 6| Step: 11
Training loss: 1.8233895301818848
Validation loss: 2.1317195815424763

Epoch: 6| Step: 12
Training loss: 3.0289041996002197
Validation loss: 2.1318616815792617

Epoch: 6| Step: 13
Training loss: 1.9971773624420166
Validation loss: 2.135544897407614

Epoch: 48| Step: 0
Training loss: 2.4516000747680664
Validation loss: 2.128474945663124

Epoch: 6| Step: 1
Training loss: 2.8362228870391846
Validation loss: 2.1274358457134617

Epoch: 6| Step: 2
Training loss: 1.831318974494934
Validation loss: 2.1370900407914193

Epoch: 6| Step: 3
Training loss: 2.762934446334839
Validation loss: 2.116170685778382

Epoch: 6| Step: 4
Training loss: 1.6364284753799438
Validation loss: 2.1268094380696616

Epoch: 6| Step: 5
Training loss: 2.013113498687744
Validation loss: 2.0995977053078274

Epoch: 6| Step: 6
Training loss: 2.554884910583496
Validation loss: 2.1177534428975915

Epoch: 6| Step: 7
Training loss: 2.9785780906677246
Validation loss: 2.103229991851314

Epoch: 6| Step: 8
Training loss: 2.137725353240967
Validation loss: 2.0930984468870264

Epoch: 6| Step: 9
Training loss: 1.9452205896377563
Validation loss: 2.1130863069206156

Epoch: 6| Step: 10
Training loss: 2.5056111812591553
Validation loss: 2.119062131451022

Epoch: 6| Step: 11
Training loss: 3.29964280128479
Validation loss: 2.1135386087561168

Epoch: 6| Step: 12
Training loss: 2.5166163444519043
Validation loss: 2.097955717835375

Epoch: 6| Step: 13
Training loss: 2.614318370819092
Validation loss: 2.0865007344112603

Epoch: 49| Step: 0
Training loss: 1.528487205505371
Validation loss: 2.092895280930304

Epoch: 6| Step: 1
Training loss: 2.4292068481445312
Validation loss: 2.1073293609003865

Epoch: 6| Step: 2
Training loss: 2.421046733856201
Validation loss: 2.1352884359257196

Epoch: 6| Step: 3
Training loss: 2.7429049015045166
Validation loss: 2.1255489549329205

Epoch: 6| Step: 4
Training loss: 1.8879400491714478
Validation loss: 2.106161654636424

Epoch: 6| Step: 5
Training loss: 2.605134963989258
Validation loss: 2.1038576915699947

Epoch: 6| Step: 6
Training loss: 1.5904302597045898
Validation loss: 2.1298543125070553

Epoch: 6| Step: 7
Training loss: 3.239621877670288
Validation loss: 2.1041252318248955

Epoch: 6| Step: 8
Training loss: 2.693502426147461
Validation loss: 2.1195898030393865

Epoch: 6| Step: 9
Training loss: 2.5753846168518066
Validation loss: 2.100484671131257

Epoch: 6| Step: 10
Training loss: 2.2030673027038574
Validation loss: 2.1277189229124334

Epoch: 6| Step: 11
Training loss: 2.369673252105713
Validation loss: 2.115116282175946

Epoch: 6| Step: 12
Training loss: 2.959178924560547
Validation loss: 2.1184874260297386

Epoch: 6| Step: 13
Training loss: 2.5026049613952637
Validation loss: 2.1089643329702397

Epoch: 50| Step: 0
Training loss: 2.542816400527954
Validation loss: 2.1303071898798787

Epoch: 6| Step: 1
Training loss: 1.5789299011230469
Validation loss: 2.1220248835061186

Epoch: 6| Step: 2
Training loss: 1.8710347414016724
Validation loss: 2.109817881737986

Epoch: 6| Step: 3
Training loss: 2.418687105178833
Validation loss: 2.111087567062788

Epoch: 6| Step: 4
Training loss: 2.5237865447998047
Validation loss: 2.108158056453992

Epoch: 6| Step: 5
Training loss: 2.094240665435791
Validation loss: 2.08746535547318

Epoch: 6| Step: 6
Training loss: 1.5361024141311646
Validation loss: 2.110431791633688

Epoch: 6| Step: 7
Training loss: 3.2674145698547363
Validation loss: 2.1405461065230833

Epoch: 6| Step: 8
Training loss: 2.953536033630371
Validation loss: 2.1128383708256546

Epoch: 6| Step: 9
Training loss: 1.9879847764968872
Validation loss: 2.0972944382698304

Epoch: 6| Step: 10
Training loss: 2.766864538192749
Validation loss: 2.1329185167948403

Epoch: 6| Step: 11
Training loss: 3.09375
Validation loss: 2.113783081372579

Epoch: 6| Step: 12
Training loss: 2.791128158569336
Validation loss: 2.1066662111589984

Epoch: 6| Step: 13
Training loss: 2.3752310276031494
Validation loss: 2.0878338685599704

Epoch: 51| Step: 0
Training loss: 2.4224348068237305
Validation loss: 2.103184974321755

Epoch: 6| Step: 1
Training loss: 2.0215377807617188
Validation loss: 2.0770559541640745

Epoch: 6| Step: 2
Training loss: 3.017570972442627
Validation loss: 2.093829436968732

Epoch: 6| Step: 3
Training loss: 2.2096304893493652
Validation loss: 2.1216057256985734

Epoch: 6| Step: 4
Training loss: 1.6508042812347412
Validation loss: 2.104253310029225

Epoch: 6| Step: 5
Training loss: 2.684783458709717
Validation loss: 2.1304204951050463

Epoch: 6| Step: 6
Training loss: 2.682527542114258
Validation loss: 2.1162952428222983

Epoch: 6| Step: 7
Training loss: 1.927000880241394
Validation loss: 2.1032781754770586

Epoch: 6| Step: 8
Training loss: 2.893129348754883
Validation loss: 2.1163389246950866

Epoch: 6| Step: 9
Training loss: 2.5288209915161133
Validation loss: 2.1072606091858237

Epoch: 6| Step: 10
Training loss: 2.96713924407959
Validation loss: 2.098556351918046

Epoch: 6| Step: 11
Training loss: 1.796372652053833
Validation loss: 2.1010426782792613

Epoch: 6| Step: 12
Training loss: 2.5186688899993896
Validation loss: 2.1099345376414638

Epoch: 6| Step: 13
Training loss: 2.1623778343200684
Validation loss: 2.119970175527757

Epoch: 52| Step: 0
Training loss: 2.763522148132324
Validation loss: 2.106457999957505

Epoch: 6| Step: 1
Training loss: 2.5156197547912598
Validation loss: 2.093714328222377

Epoch: 6| Step: 2
Training loss: 2.2691075801849365
Validation loss: 2.101856207334867

Epoch: 6| Step: 3
Training loss: 2.266756534576416
Validation loss: 2.123961510196809

Epoch: 6| Step: 4
Training loss: 2.3812379837036133
Validation loss: 2.0988897995282243

Epoch: 6| Step: 5
Training loss: 2.8357081413269043
Validation loss: 2.105861597163703

Epoch: 6| Step: 6
Training loss: 1.7813096046447754
Validation loss: 2.121208098626906

Epoch: 6| Step: 7
Training loss: 2.5070934295654297
Validation loss: 2.103381336376231

Epoch: 6| Step: 8
Training loss: 2.7256367206573486
Validation loss: 2.1041479956719185

Epoch: 6| Step: 9
Training loss: 2.126603603363037
Validation loss: 2.112611004101333

Epoch: 6| Step: 10
Training loss: 2.3822035789489746
Validation loss: 2.1044054569736605

Epoch: 6| Step: 11
Training loss: 3.134512424468994
Validation loss: 2.0889053037089687

Epoch: 6| Step: 12
Training loss: 1.9246662855148315
Validation loss: 2.092899045636577

Epoch: 6| Step: 13
Training loss: 1.6448249816894531
Validation loss: 2.108862897401215

Epoch: 53| Step: 0
Training loss: 2.3369131088256836
Validation loss: 2.0987091551544848

Epoch: 6| Step: 1
Training loss: 3.046255111694336
Validation loss: 2.1240270137786865

Epoch: 6| Step: 2
Training loss: 1.998180866241455
Validation loss: 2.1262511809666953

Epoch: 6| Step: 3
Training loss: 2.077514171600342
Validation loss: 2.1121589855481218

Epoch: 6| Step: 4
Training loss: 2.0035619735717773
Validation loss: 2.1029119363395115

Epoch: 6| Step: 5
Training loss: 3.303335666656494
Validation loss: 2.107571032739455

Epoch: 6| Step: 6
Training loss: 3.0348544120788574
Validation loss: 2.095366193402198

Epoch: 6| Step: 7
Training loss: 2.478412389755249
Validation loss: 2.088429717607396

Epoch: 6| Step: 8
Training loss: 2.264331340789795
Validation loss: 2.107746731850409

Epoch: 6| Step: 9
Training loss: 2.061858654022217
Validation loss: 2.083528839131837

Epoch: 6| Step: 10
Training loss: 2.8752541542053223
Validation loss: 2.110029505145165

Epoch: 6| Step: 11
Training loss: 2.3343453407287598
Validation loss: 2.1134717041446316

Epoch: 6| Step: 12
Training loss: 2.2198472023010254
Validation loss: 2.121878876481005

Epoch: 6| Step: 13
Training loss: 1.1025595664978027
Validation loss: 2.1130206956658313

Epoch: 54| Step: 0
Training loss: 2.420989513397217
Validation loss: 2.100838295875057

Epoch: 6| Step: 1
Training loss: 1.565880298614502
Validation loss: 2.108231608585645

Epoch: 6| Step: 2
Training loss: 2.399247169494629
Validation loss: 2.0932246805519186

Epoch: 6| Step: 3
Training loss: 3.022639513015747
Validation loss: 2.109835991295435

Epoch: 6| Step: 4
Training loss: 2.2490830421447754
Validation loss: 2.1009897467910603

Epoch: 6| Step: 5
Training loss: 2.419848918914795
Validation loss: 2.1072286008506693

Epoch: 6| Step: 6
Training loss: 2.839743137359619
Validation loss: 2.114023562400572

Epoch: 6| Step: 7
Training loss: 1.9120807647705078
Validation loss: 2.1141087726880143

Epoch: 6| Step: 8
Training loss: 2.3481674194335938
Validation loss: 2.1276666938617663

Epoch: 6| Step: 9
Training loss: 3.074277400970459
Validation loss: 2.1153369154981387

Epoch: 6| Step: 10
Training loss: 3.0679826736450195
Validation loss: 2.1220317091993106

Epoch: 6| Step: 11
Training loss: 2.2582082748413086
Validation loss: 2.1244511091580955

Epoch: 6| Step: 12
Training loss: 1.2626852989196777
Validation loss: 2.1040144171766055

Epoch: 6| Step: 13
Training loss: 3.0750529766082764
Validation loss: 2.1236290547155563

Epoch: 55| Step: 0
Training loss: 1.5859631299972534
Validation loss: 2.121535620381755

Epoch: 6| Step: 1
Training loss: 1.9373328685760498
Validation loss: 2.1316712492255756

Epoch: 6| Step: 2
Training loss: 1.7916347980499268
Validation loss: 2.124592863103395

Epoch: 6| Step: 3
Training loss: 2.5247538089752197
Validation loss: 2.119592729435172

Epoch: 6| Step: 4
Training loss: 2.151289701461792
Validation loss: 2.1081698530463764

Epoch: 6| Step: 5
Training loss: 2.642364501953125
Validation loss: 2.091458587236302

Epoch: 6| Step: 6
Training loss: 1.9951400756835938
Validation loss: 2.0942161698495187

Epoch: 6| Step: 7
Training loss: 3.034046173095703
Validation loss: 2.088836009784411

Epoch: 6| Step: 8
Training loss: 2.675459861755371
Validation loss: 2.085466164414601

Epoch: 6| Step: 9
Training loss: 2.1415860652923584
Validation loss: 2.091465021974297

Epoch: 6| Step: 10
Training loss: 2.799983501434326
Validation loss: 2.084530129227587

Epoch: 6| Step: 11
Training loss: 2.9968788623809814
Validation loss: 2.099203760905932

Epoch: 6| Step: 12
Training loss: 3.2095489501953125
Validation loss: 2.073845148086548

Epoch: 6| Step: 13
Training loss: 1.5777581930160522
Validation loss: 2.0765915429720314

Epoch: 56| Step: 0
Training loss: 2.4503488540649414
Validation loss: 2.086300475623018

Epoch: 6| Step: 1
Training loss: 2.6814165115356445
Validation loss: 2.0974639282431653

Epoch: 6| Step: 2
Training loss: 2.12782621383667
Validation loss: 2.085624069295904

Epoch: 6| Step: 3
Training loss: 2.921006679534912
Validation loss: 2.070940686810401

Epoch: 6| Step: 4
Training loss: 2.6901659965515137
Validation loss: 2.079269716816564

Epoch: 6| Step: 5
Training loss: 2.681983232498169
Validation loss: 2.0637812127349195

Epoch: 6| Step: 6
Training loss: 2.0305016040802
Validation loss: 2.1022117086636123

Epoch: 6| Step: 7
Training loss: 1.9854458570480347
Validation loss: 2.0914225860308577

Epoch: 6| Step: 8
Training loss: 2.1403298377990723
Validation loss: 2.093880876418083

Epoch: 6| Step: 9
Training loss: 1.931039571762085
Validation loss: 2.1014168570118565

Epoch: 6| Step: 10
Training loss: 3.229504108428955
Validation loss: 2.0972766876220703

Epoch: 6| Step: 11
Training loss: 1.6214778423309326
Validation loss: 2.080808235752967

Epoch: 6| Step: 12
Training loss: 2.7919487953186035
Validation loss: 2.100457873395694

Epoch: 6| Step: 13
Training loss: 1.8285386562347412
Validation loss: 2.099822859610281

Epoch: 57| Step: 0
Training loss: 2.616532802581787
Validation loss: 2.1002698380460023

Epoch: 6| Step: 1
Training loss: 2.0256154537200928
Validation loss: 2.121138923911638

Epoch: 6| Step: 2
Training loss: 1.8357023000717163
Validation loss: 2.0942683014818417

Epoch: 6| Step: 3
Training loss: 2.499331474304199
Validation loss: 2.107468108977041

Epoch: 6| Step: 4
Training loss: 2.454475164413452
Validation loss: 2.0923686950437483

Epoch: 6| Step: 5
Training loss: 2.538421392440796
Validation loss: 2.1353578644414104

Epoch: 6| Step: 6
Training loss: 2.341216564178467
Validation loss: 2.102629710269231

Epoch: 6| Step: 7
Training loss: 2.147040843963623
Validation loss: 2.074305303635136

Epoch: 6| Step: 8
Training loss: 2.789217710494995
Validation loss: 2.097041645357686

Epoch: 6| Step: 9
Training loss: 2.4488656520843506
Validation loss: 2.1259601295635266

Epoch: 6| Step: 10
Training loss: 3.2422547340393066
Validation loss: 2.1137202939679547

Epoch: 6| Step: 11
Training loss: 1.560386300086975
Validation loss: 2.102265091352565

Epoch: 6| Step: 12
Training loss: 2.5192599296569824
Validation loss: 2.1046042673049437

Epoch: 6| Step: 13
Training loss: 2.445418357849121
Validation loss: 2.104053907496955

Epoch: 58| Step: 0
Training loss: 2.6202871799468994
Validation loss: 2.1241838726946103

Epoch: 6| Step: 1
Training loss: 1.6211707592010498
Validation loss: 2.1190270198288785

Epoch: 6| Step: 2
Training loss: 1.8127593994140625
Validation loss: 2.0860208644661853

Epoch: 6| Step: 3
Training loss: 2.899672269821167
Validation loss: 2.06343569806827

Epoch: 6| Step: 4
Training loss: 3.197028636932373
Validation loss: 2.092328276685489

Epoch: 6| Step: 5
Training loss: 2.6663191318511963
Validation loss: 2.12284158378519

Epoch: 6| Step: 6
Training loss: 2.934506893157959
Validation loss: 2.072674244962713

Epoch: 6| Step: 7
Training loss: 2.0387473106384277
Validation loss: 2.099585005032119

Epoch: 6| Step: 8
Training loss: 1.7696385383605957
Validation loss: 2.1200457055081605

Epoch: 6| Step: 9
Training loss: 2.9015564918518066
Validation loss: 2.0973776771176245

Epoch: 6| Step: 10
Training loss: 2.0972487926483154
Validation loss: 2.1163989574678483

Epoch: 6| Step: 11
Training loss: 2.1699652671813965
Validation loss: 2.099168958202485

Epoch: 6| Step: 12
Training loss: 2.3931548595428467
Validation loss: 2.1112177115614696

Epoch: 6| Step: 13
Training loss: 1.8543404340744019
Validation loss: 2.101757590488721

Epoch: 59| Step: 0
Training loss: 1.770444631576538
Validation loss: 2.113458523186304

Epoch: 6| Step: 1
Training loss: 2.140366554260254
Validation loss: 2.088694305830104

Epoch: 6| Step: 2
Training loss: 2.54984974861145
Validation loss: 2.106915327810472

Epoch: 6| Step: 3
Training loss: 2.5063228607177734
Validation loss: 2.0999136945252777

Epoch: 6| Step: 4
Training loss: 2.2023048400878906
Validation loss: 2.10149319453906

Epoch: 6| Step: 5
Training loss: 2.8217861652374268
Validation loss: 2.106919914163569

Epoch: 6| Step: 6
Training loss: 2.095412015914917
Validation loss: 2.1026125005496445

Epoch: 6| Step: 7
Training loss: 1.866286277770996
Validation loss: 2.08859363807145

Epoch: 6| Step: 8
Training loss: 2.6383907794952393
Validation loss: 2.0924277843967563

Epoch: 6| Step: 9
Training loss: 1.9068719148635864
Validation loss: 2.0765270238281577

Epoch: 6| Step: 10
Training loss: 2.4761078357696533
Validation loss: 2.1022636557138092

Epoch: 6| Step: 11
Training loss: 2.20576548576355
Validation loss: 2.094281129939582

Epoch: 6| Step: 12
Training loss: 3.1893129348754883
Validation loss: 2.0939008471786336

Epoch: 6| Step: 13
Training loss: 2.925504446029663
Validation loss: 2.1011186953513854

Epoch: 60| Step: 0
Training loss: 2.613572359085083
Validation loss: 2.076857546324371

Epoch: 6| Step: 1
Training loss: 1.3655325174331665
Validation loss: 2.12183722372978

Epoch: 6| Step: 2
Training loss: 2.5625593662261963
Validation loss: 2.0890363288182083

Epoch: 6| Step: 3
Training loss: 2.879162311553955
Validation loss: 2.094787766856532

Epoch: 6| Step: 4
Training loss: 2.167654037475586
Validation loss: 2.097297359538335

Epoch: 6| Step: 5
Training loss: 2.783965587615967
Validation loss: 2.090785111150434

Epoch: 6| Step: 6
Training loss: 1.71701979637146
Validation loss: 2.0787178777879283

Epoch: 6| Step: 7
Training loss: 1.7459325790405273
Validation loss: 2.0953627581237466

Epoch: 6| Step: 8
Training loss: 2.4797396659851074
Validation loss: 2.096552384796963

Epoch: 6| Step: 9
Training loss: 2.710336685180664
Validation loss: 2.0977648150536323

Epoch: 6| Step: 10
Training loss: 2.475144386291504
Validation loss: 2.0791460839650964

Epoch: 6| Step: 11
Training loss: 2.773033618927002
Validation loss: 2.1060830931509695

Epoch: 6| Step: 12
Training loss: 2.295318841934204
Validation loss: 2.092249894654879

Epoch: 6| Step: 13
Training loss: 2.5992109775543213
Validation loss: 2.0899295729975544

Epoch: 61| Step: 0
Training loss: 3.264901876449585
Validation loss: 2.042297286372031

Epoch: 6| Step: 1
Training loss: 1.5469621419906616
Validation loss: 2.0704129690765054

Epoch: 6| Step: 2
Training loss: 3.1280946731567383
Validation loss: 2.098041816424298

Epoch: 6| Step: 3
Training loss: 1.8114327192306519
Validation loss: 2.0944839523684595

Epoch: 6| Step: 4
Training loss: 2.21511173248291
Validation loss: 2.0892118036106067

Epoch: 6| Step: 5
Training loss: 2.640622615814209
Validation loss: 2.087398330370585

Epoch: 6| Step: 6
Training loss: 2.576134204864502
Validation loss: 2.1011348283419045

Epoch: 6| Step: 7
Training loss: 2.2953083515167236
Validation loss: 2.091373519230914

Epoch: 6| Step: 8
Training loss: 2.7457032203674316
Validation loss: 2.089162524028491

Epoch: 6| Step: 9
Training loss: 2.1763763427734375
Validation loss: 2.088265939425397

Epoch: 6| Step: 10
Training loss: 1.9038329124450684
Validation loss: 2.0886148406613256

Epoch: 6| Step: 11
Training loss: 1.9471251964569092
Validation loss: 2.081016094453873

Epoch: 6| Step: 12
Training loss: 2.2720842361450195
Validation loss: 2.0977288446118756

Epoch: 6| Step: 13
Training loss: 3.234659433364868
Validation loss: 2.0863346835618377

Epoch: 62| Step: 0
Training loss: 2.2353949546813965
Validation loss: 2.067530639709965

Epoch: 6| Step: 1
Training loss: 2.7122397422790527
Validation loss: 2.090893332676221

Epoch: 6| Step: 2
Training loss: 2.092113494873047
Validation loss: 2.069469500613469

Epoch: 6| Step: 3
Training loss: 2.404829263687134
Validation loss: 2.0810596635264735

Epoch: 6| Step: 4
Training loss: 2.212655544281006
Validation loss: 2.067876213340349

Epoch: 6| Step: 5
Training loss: 2.522671937942505
Validation loss: 2.1158803727037165

Epoch: 6| Step: 6
Training loss: 1.6499625444412231
Validation loss: 2.0985604511794222

Epoch: 6| Step: 7
Training loss: 2.8874707221984863
Validation loss: 2.0969568490982056

Epoch: 6| Step: 8
Training loss: 2.557229995727539
Validation loss: 2.092261875829389

Epoch: 6| Step: 9
Training loss: 2.256626844406128
Validation loss: 2.101891780412325

Epoch: 6| Step: 10
Training loss: 2.5965824127197266
Validation loss: 2.0979107656786518

Epoch: 6| Step: 11
Training loss: 2.8621721267700195
Validation loss: 2.1161803122489684

Epoch: 6| Step: 12
Training loss: 2.0270986557006836
Validation loss: 2.0664879134906236

Epoch: 6| Step: 13
Training loss: 2.1505892276763916
Validation loss: 2.0995511572848082

Epoch: 63| Step: 0
Training loss: 2.770692825317383
Validation loss: 2.112870611170287

Epoch: 6| Step: 1
Training loss: 2.2506537437438965
Validation loss: 2.0937012651915192

Epoch: 6| Step: 2
Training loss: 1.7672083377838135
Validation loss: 2.0500285548548542

Epoch: 6| Step: 3
Training loss: 1.9067142009735107
Validation loss: 2.0699870137758154

Epoch: 6| Step: 4
Training loss: 2.3288679122924805
Validation loss: 2.075578704957039

Epoch: 6| Step: 5
Training loss: 2.687164783477783
Validation loss: 2.0890353674529702

Epoch: 6| Step: 6
Training loss: 2.428370952606201
Validation loss: 2.0673869732887513

Epoch: 6| Step: 7
Training loss: 2.250124454498291
Validation loss: 2.0735779141867035

Epoch: 6| Step: 8
Training loss: 1.5430846214294434
Validation loss: 2.0737455250114523

Epoch: 6| Step: 9
Training loss: 3.054013252258301
Validation loss: 2.093019449582664

Epoch: 6| Step: 10
Training loss: 2.134629726409912
Validation loss: 2.0642189671916347

Epoch: 6| Step: 11
Training loss: 3.182297706604004
Validation loss: 2.073558154926505

Epoch: 6| Step: 12
Training loss: 2.962550163269043
Validation loss: 2.0724777085806734

Epoch: 6| Step: 13
Training loss: 1.7188481092453003
Validation loss: 2.0611124077150897

Epoch: 64| Step: 0
Training loss: 2.3521668910980225
Validation loss: 2.078012333121351

Epoch: 6| Step: 1
Training loss: 2.0680413246154785
Validation loss: 2.084647801614577

Epoch: 6| Step: 2
Training loss: 2.5655181407928467
Validation loss: 2.0554955082554973

Epoch: 6| Step: 3
Training loss: 2.840391159057617
Validation loss: 2.0791309174670967

Epoch: 6| Step: 4
Training loss: 2.7918403148651123
Validation loss: 2.084879111218196

Epoch: 6| Step: 5
Training loss: 2.2121527194976807
Validation loss: 2.060346229102022

Epoch: 6| Step: 6
Training loss: 2.5054030418395996
Validation loss: 2.0852700689787507

Epoch: 6| Step: 7
Training loss: 2.214813232421875
Validation loss: 2.1079819715151222

Epoch: 6| Step: 8
Training loss: 2.056335926055908
Validation loss: 2.0766007464419127

Epoch: 6| Step: 9
Training loss: 2.1190285682678223
Validation loss: 2.081770517492807

Epoch: 6| Step: 10
Training loss: 1.5207027196884155
Validation loss: 2.0729372809010167

Epoch: 6| Step: 11
Training loss: 2.2446115016937256
Validation loss: 2.101795214478688

Epoch: 6| Step: 12
Training loss: 2.368466854095459
Validation loss: 2.0864241828200636

Epoch: 6| Step: 13
Training loss: 3.49287486076355
Validation loss: 2.0946814885703464

Epoch: 65| Step: 0
Training loss: 2.329066753387451
Validation loss: 2.083687413123346

Epoch: 6| Step: 1
Training loss: 2.753847360610962
Validation loss: 2.0939153061118176

Epoch: 6| Step: 2
Training loss: 2.3573503494262695
Validation loss: 2.0776440379440144

Epoch: 6| Step: 3
Training loss: 2.102388858795166
Validation loss: 2.073971413796948

Epoch: 6| Step: 4
Training loss: 2.5111539363861084
Validation loss: 2.076474943468648

Epoch: 6| Step: 5
Training loss: 2.052326202392578
Validation loss: 2.0798347098853

Epoch: 6| Step: 6
Training loss: 2.513922691345215
Validation loss: 2.080724513658913

Epoch: 6| Step: 7
Training loss: 1.6498744487762451
Validation loss: 2.0929813308100544

Epoch: 6| Step: 8
Training loss: 1.9211630821228027
Validation loss: 2.085792895286314

Epoch: 6| Step: 9
Training loss: 2.4885172843933105
Validation loss: 2.0947678166051067

Epoch: 6| Step: 10
Training loss: 2.511763095855713
Validation loss: 2.093160461353999

Epoch: 6| Step: 11
Training loss: 2.4267690181732178
Validation loss: 2.090621946960367

Epoch: 6| Step: 12
Training loss: 2.9002552032470703
Validation loss: 2.1094549189331713

Epoch: 6| Step: 13
Training loss: 2.165421485900879
Validation loss: 2.0841860399451306

Epoch: 66| Step: 0
Training loss: 3.113199234008789
Validation loss: 2.1013117682549263

Epoch: 6| Step: 1
Training loss: 1.8212246894836426
Validation loss: 2.0831230507102063

Epoch: 6| Step: 2
Training loss: 2.1338157653808594
Validation loss: 2.098665452772571

Epoch: 6| Step: 3
Training loss: 2.126701831817627
Validation loss: 2.067952117612285

Epoch: 6| Step: 4
Training loss: 1.5004680156707764
Validation loss: 2.0857144235282816

Epoch: 6| Step: 5
Training loss: 2.8748083114624023
Validation loss: 2.080941964221257

Epoch: 6| Step: 6
Training loss: 2.7861766815185547
Validation loss: 2.0682447700090307

Epoch: 6| Step: 7
Training loss: 2.4004158973693848
Validation loss: 2.0818233541263047

Epoch: 6| Step: 8
Training loss: 3.277104139328003
Validation loss: 2.0598387846382717

Epoch: 6| Step: 9
Training loss: 2.390561103820801
Validation loss: 2.0754175416884886

Epoch: 6| Step: 10
Training loss: 1.5678489208221436
Validation loss: 2.0573649816615607

Epoch: 6| Step: 11
Training loss: 2.485694408416748
Validation loss: 2.074634777602329

Epoch: 6| Step: 12
Training loss: 2.6336328983306885
Validation loss: 2.059741131721004

Epoch: 6| Step: 13
Training loss: 1.1023011207580566
Validation loss: 2.094809701365809

Epoch: 67| Step: 0
Training loss: 2.218536853790283
Validation loss: 2.0877997721395185

Epoch: 6| Step: 1
Training loss: 1.7002129554748535
Validation loss: 2.0643615261200936

Epoch: 6| Step: 2
Training loss: 2.297792911529541
Validation loss: 2.065424229509087

Epoch: 6| Step: 3
Training loss: 3.320727825164795
Validation loss: 2.0820573324798257

Epoch: 6| Step: 4
Training loss: 1.985152244567871
Validation loss: 2.0597530564954205

Epoch: 6| Step: 5
Training loss: 2.1647567749023438
Validation loss: 2.051579370293566

Epoch: 6| Step: 6
Training loss: 2.6867451667785645
Validation loss: 2.097966545371599

Epoch: 6| Step: 7
Training loss: 2.3248958587646484
Validation loss: 2.072686735019889

Epoch: 6| Step: 8
Training loss: 2.5589346885681152
Validation loss: 2.0642322442864858

Epoch: 6| Step: 9
Training loss: 3.2467777729034424
Validation loss: 2.068075922227675

Epoch: 6| Step: 10
Training loss: 2.068042516708374
Validation loss: 2.0617617099515853

Epoch: 6| Step: 11
Training loss: 2.0661118030548096
Validation loss: 2.0615659375344553

Epoch: 6| Step: 12
Training loss: 2.564606189727783
Validation loss: 2.0691083759389897

Epoch: 6| Step: 13
Training loss: 1.5547900199890137
Validation loss: 2.0826430795013264

Epoch: 68| Step: 0
Training loss: 2.3988795280456543
Validation loss: 2.0584756866578133

Epoch: 6| Step: 1
Training loss: 2.1060783863067627
Validation loss: 2.080500441212808

Epoch: 6| Step: 2
Training loss: 1.6591917276382446
Validation loss: 2.1053449030845397

Epoch: 6| Step: 3
Training loss: 2.3046317100524902
Validation loss: 2.0879103304237447

Epoch: 6| Step: 4
Training loss: 2.382749080657959
Validation loss: 2.0876503349632345

Epoch: 6| Step: 5
Training loss: 2.36639142036438
Validation loss: 2.0743577211133895

Epoch: 6| Step: 6
Training loss: 2.978550910949707
Validation loss: 2.0700146152127172

Epoch: 6| Step: 7
Training loss: 2.0575361251831055
Validation loss: 2.070948657169137

Epoch: 6| Step: 8
Training loss: 2.161173105239868
Validation loss: 2.081494390323598

Epoch: 6| Step: 9
Training loss: 2.724622964859009
Validation loss: 2.073515046027399

Epoch: 6| Step: 10
Training loss: 2.0936872959136963
Validation loss: 2.083072821299235

Epoch: 6| Step: 11
Training loss: 2.4582200050354004
Validation loss: 2.074653105069232

Epoch: 6| Step: 12
Training loss: 2.9531476497650146
Validation loss: 2.065212085682859

Epoch: 6| Step: 13
Training loss: 2.360352039337158
Validation loss: 2.072339279677278

Epoch: 69| Step: 0
Training loss: 2.4438529014587402
Validation loss: 2.079044057476905

Epoch: 6| Step: 1
Training loss: 2.4976885318756104
Validation loss: 2.07709171951458

Epoch: 6| Step: 2
Training loss: 2.5666568279266357
Validation loss: 2.072482601288826

Epoch: 6| Step: 3
Training loss: 2.831191301345825
Validation loss: 2.0739578918744157

Epoch: 6| Step: 4
Training loss: 2.157665729522705
Validation loss: 2.0900741366929907

Epoch: 6| Step: 5
Training loss: 2.6020493507385254
Validation loss: 2.0838051995923443

Epoch: 6| Step: 6
Training loss: 1.6116799116134644
Validation loss: 2.075275044287405

Epoch: 6| Step: 7
Training loss: 2.991636276245117
Validation loss: 2.0795668632753435

Epoch: 6| Step: 8
Training loss: 1.5878181457519531
Validation loss: 2.065373848843318

Epoch: 6| Step: 9
Training loss: 1.428147315979004
Validation loss: 2.0856402048500637

Epoch: 6| Step: 10
Training loss: 2.9196741580963135
Validation loss: 2.059448197323789

Epoch: 6| Step: 11
Training loss: 2.4519524574279785
Validation loss: 2.06642742823529

Epoch: 6| Step: 12
Training loss: 2.126615047454834
Validation loss: 2.08261864800607

Epoch: 6| Step: 13
Training loss: 2.433356285095215
Validation loss: 2.0811881737042497

Epoch: 70| Step: 0
Training loss: 2.262885332107544
Validation loss: 2.065923580559351

Epoch: 6| Step: 1
Training loss: 1.88248610496521
Validation loss: 2.0690184523982387

Epoch: 6| Step: 2
Training loss: 1.983252763748169
Validation loss: 2.0880096958529566

Epoch: 6| Step: 3
Training loss: 1.6538395881652832
Validation loss: 2.0612810298960698

Epoch: 6| Step: 4
Training loss: 2.806760311126709
Validation loss: 2.0587392109696583

Epoch: 6| Step: 5
Training loss: 2.562016725540161
Validation loss: 2.0543033269143876

Epoch: 6| Step: 6
Training loss: 2.7394468784332275
Validation loss: 2.053362541301276

Epoch: 6| Step: 7
Training loss: 2.2621407508850098
Validation loss: 2.074977074899981

Epoch: 6| Step: 8
Training loss: 2.220299243927002
Validation loss: 2.043493732329338

Epoch: 6| Step: 9
Training loss: 3.1845195293426514
Validation loss: 2.0881130938888877

Epoch: 6| Step: 10
Training loss: 1.642263412475586
Validation loss: 2.0678424014840076

Epoch: 6| Step: 11
Training loss: 1.9271087646484375
Validation loss: 2.048987559092942

Epoch: 6| Step: 12
Training loss: 3.199862480163574
Validation loss: 2.0839123546436267

Epoch: 6| Step: 13
Training loss: 2.204908609390259
Validation loss: 2.073766246918709

Epoch: 71| Step: 0
Training loss: 2.3697781562805176
Validation loss: 2.0395765484020276

Epoch: 6| Step: 1
Training loss: 2.021305561065674
Validation loss: 2.0695816945004206

Epoch: 6| Step: 2
Training loss: 1.6431310176849365
Validation loss: 2.051127323540308

Epoch: 6| Step: 3
Training loss: 2.581085443496704
Validation loss: 2.037165382856964

Epoch: 6| Step: 4
Training loss: 2.801126718521118
Validation loss: 2.070586790320694

Epoch: 6| Step: 5
Training loss: 2.4891786575317383
Validation loss: 2.067949861608526

Epoch: 6| Step: 6
Training loss: 2.4470717906951904
Validation loss: 2.0559756678919636

Epoch: 6| Step: 7
Training loss: 2.1894688606262207
Validation loss: 2.0622564387577835

Epoch: 6| Step: 8
Training loss: 2.777486562728882
Validation loss: 2.050564435220534

Epoch: 6| Step: 9
Training loss: 2.223308563232422
Validation loss: 2.060774762143371

Epoch: 6| Step: 10
Training loss: 2.96999454498291
Validation loss: 2.06221325935856

Epoch: 6| Step: 11
Training loss: 1.7082977294921875
Validation loss: 2.0784648592754076

Epoch: 6| Step: 12
Training loss: 2.20280122756958
Validation loss: 2.0709469369662705

Epoch: 6| Step: 13
Training loss: 1.8468080759048462
Validation loss: 2.065965856275251

Epoch: 72| Step: 0
Training loss: 2.493955135345459
Validation loss: 2.0719484001077633

Epoch: 6| Step: 1
Training loss: 2.5558009147644043
Validation loss: 2.0865709525282665

Epoch: 6| Step: 2
Training loss: 2.58040714263916
Validation loss: 2.0574811453460367

Epoch: 6| Step: 3
Training loss: 2.1451528072357178
Validation loss: 2.0960334295867593

Epoch: 6| Step: 4
Training loss: 1.7479629516601562
Validation loss: 2.0707212801902526

Epoch: 6| Step: 5
Training loss: 2.1395750045776367
Validation loss: 2.071443262920585

Epoch: 6| Step: 6
Training loss: 3.2716777324676514
Validation loss: 2.0730808845130344

Epoch: 6| Step: 7
Training loss: 1.552154779434204
Validation loss: 2.0565453460139613

Epoch: 6| Step: 8
Training loss: 1.963729977607727
Validation loss: 2.061963763288272

Epoch: 6| Step: 9
Training loss: 2.414198875427246
Validation loss: 2.0800329408337994

Epoch: 6| Step: 10
Training loss: 2.152209520339966
Validation loss: 2.0643707808627876

Epoch: 6| Step: 11
Training loss: 2.370879650115967
Validation loss: 2.080222293894778

Epoch: 6| Step: 12
Training loss: 2.880894899368286
Validation loss: 2.0507706749823784

Epoch: 6| Step: 13
Training loss: 2.0419113636016846
Validation loss: 2.061729372188609

Epoch: 73| Step: 0
Training loss: 1.9897589683532715
Validation loss: 2.059572667203924

Epoch: 6| Step: 1
Training loss: 2.245760202407837
Validation loss: 2.0648687988199215

Epoch: 6| Step: 2
Training loss: 2.7155795097351074
Validation loss: 2.0754459493903705

Epoch: 6| Step: 3
Training loss: 2.27502179145813
Validation loss: 2.0664222394266436

Epoch: 6| Step: 4
Training loss: 2.121333599090576
Validation loss: 2.0766027781271164

Epoch: 6| Step: 5
Training loss: 1.8593673706054688
Validation loss: 2.0891346444365797

Epoch: 6| Step: 6
Training loss: 2.3040573596954346
Validation loss: 2.0762741334976687

Epoch: 6| Step: 7
Training loss: 2.400087356567383
Validation loss: 2.065067796296971

Epoch: 6| Step: 8
Training loss: 2.695547580718994
Validation loss: 2.069577432447864

Epoch: 6| Step: 9
Training loss: 2.205547332763672
Validation loss: 2.0701169031922535

Epoch: 6| Step: 10
Training loss: 2.061396598815918
Validation loss: 2.083366068460608

Epoch: 6| Step: 11
Training loss: 3.025344133377075
Validation loss: 2.0643027162039154

Epoch: 6| Step: 12
Training loss: 2.300283432006836
Validation loss: 2.078860095752183

Epoch: 6| Step: 13
Training loss: 2.514536142349243
Validation loss: 2.088129287124962

Epoch: 74| Step: 0
Training loss: 2.146209239959717
Validation loss: 2.0698364832068004

Epoch: 6| Step: 1
Training loss: 1.9505558013916016
Validation loss: 2.06010546991902

Epoch: 6| Step: 2
Training loss: 2.1491000652313232
Validation loss: 2.0893690509180867

Epoch: 6| Step: 3
Training loss: 2.8618879318237305
Validation loss: 2.0789155255081835

Epoch: 6| Step: 4
Training loss: 2.2256808280944824
Validation loss: 2.0785213593513734

Epoch: 6| Step: 5
Training loss: 2.1245808601379395
Validation loss: 2.1114422223901235

Epoch: 6| Step: 6
Training loss: 2.1677441596984863
Validation loss: 2.0517944443610405

Epoch: 6| Step: 7
Training loss: 2.7802329063415527
Validation loss: 2.0568123607225317

Epoch: 6| Step: 8
Training loss: 3.0272746086120605
Validation loss: 2.0760741515826155

Epoch: 6| Step: 9
Training loss: 1.944433569908142
Validation loss: 2.061737505338525

Epoch: 6| Step: 10
Training loss: 2.744255781173706
Validation loss: 2.0768709131466445

Epoch: 6| Step: 11
Training loss: 2.036811351776123
Validation loss: 2.057680105650297

Epoch: 6| Step: 12
Training loss: 2.176701545715332
Validation loss: 2.1023405803147184

Epoch: 6| Step: 13
Training loss: 2.141179323196411
Validation loss: 2.106407509055189

Epoch: 75| Step: 0
Training loss: 2.391084909439087
Validation loss: 2.0751365256565872

Epoch: 6| Step: 1
Training loss: 1.7812955379486084
Validation loss: 2.088580935232101

Epoch: 6| Step: 2
Training loss: 2.2103261947631836
Validation loss: 2.095421270657611

Epoch: 6| Step: 3
Training loss: 2.7118301391601562
Validation loss: 2.07969682703736

Epoch: 6| Step: 4
Training loss: 2.5939207077026367
Validation loss: 2.089139692244991

Epoch: 6| Step: 5
Training loss: 2.2463772296905518
Validation loss: 2.1007670253835697

Epoch: 6| Step: 6
Training loss: 2.9742867946624756
Validation loss: 2.0938737956426476

Epoch: 6| Step: 7
Training loss: 1.5740153789520264
Validation loss: 2.0784256471100675

Epoch: 6| Step: 8
Training loss: 2.2893447875976562
Validation loss: 2.0782859325408936

Epoch: 6| Step: 9
Training loss: 2.0242342948913574
Validation loss: 2.0964219736796554

Epoch: 6| Step: 10
Training loss: 2.669053077697754
Validation loss: 2.0752173046911917

Epoch: 6| Step: 11
Training loss: 2.6695966720581055
Validation loss: 2.0680178711491246

Epoch: 6| Step: 12
Training loss: 1.8351140022277832
Validation loss: 2.074495179678804

Epoch: 6| Step: 13
Training loss: 2.4093947410583496
Validation loss: 2.0975816224211004

Epoch: 76| Step: 0
Training loss: 2.9889798164367676
Validation loss: 2.051057938606508

Epoch: 6| Step: 1
Training loss: 2.6324803829193115
Validation loss: 2.089152951394358

Epoch: 6| Step: 2
Training loss: 1.926563024520874
Validation loss: 2.059073673781528

Epoch: 6| Step: 3
Training loss: 2.601217746734619
Validation loss: 2.0871376068361345

Epoch: 6| Step: 4
Training loss: 2.8103811740875244
Validation loss: 2.080633682589377

Epoch: 6| Step: 5
Training loss: 1.8398507833480835
Validation loss: 2.0676259635597147

Epoch: 6| Step: 6
Training loss: 2.7860584259033203
Validation loss: 2.0674701839364986

Epoch: 6| Step: 7
Training loss: 2.058612823486328
Validation loss: 2.0725697894250192

Epoch: 6| Step: 8
Training loss: 2.1367135047912598
Validation loss: 2.0646804712151967

Epoch: 6| Step: 9
Training loss: 2.8475492000579834
Validation loss: 2.0770974979605725

Epoch: 6| Step: 10
Training loss: 1.8267486095428467
Validation loss: 2.056537257727756

Epoch: 6| Step: 11
Training loss: 2.167426109313965
Validation loss: 2.0795163134092927

Epoch: 6| Step: 12
Training loss: 1.8954031467437744
Validation loss: 2.0804813382446126

Epoch: 6| Step: 13
Training loss: 1.2682064771652222
Validation loss: 2.0603007142261793

Epoch: 77| Step: 0
Training loss: 2.529801845550537
Validation loss: 2.0778473026009014

Epoch: 6| Step: 1
Training loss: 2.2143611907958984
Validation loss: 2.055308207388847

Epoch: 6| Step: 2
Training loss: 2.4018654823303223
Validation loss: 2.0736658098877117

Epoch: 6| Step: 3
Training loss: 2.843726634979248
Validation loss: 2.063225648736441

Epoch: 6| Step: 4
Training loss: 2.6402878761291504
Validation loss: 2.0337082980781473

Epoch: 6| Step: 5
Training loss: 2.1849536895751953
Validation loss: 2.067540527671896

Epoch: 6| Step: 6
Training loss: 1.9565058946609497
Validation loss: 2.0600916262595885

Epoch: 6| Step: 7
Training loss: 2.0920417308807373
Validation loss: 2.067797859509786

Epoch: 6| Step: 8
Training loss: 2.2236111164093018
Validation loss: 2.0689531039166194

Epoch: 6| Step: 9
Training loss: 2.785186767578125
Validation loss: 2.059039059505668

Epoch: 6| Step: 10
Training loss: 2.276319980621338
Validation loss: 2.080539554677984

Epoch: 6| Step: 11
Training loss: 1.9064351320266724
Validation loss: 2.0724966320940243

Epoch: 6| Step: 12
Training loss: 1.9499696493148804
Validation loss: 2.081830847647882

Epoch: 6| Step: 13
Training loss: 2.588388681411743
Validation loss: 2.0579609896547053

Epoch: 78| Step: 0
Training loss: 2.1991398334503174
Validation loss: 2.0829222676574544

Epoch: 6| Step: 1
Training loss: 2.230762004852295
Validation loss: 2.034105162466726

Epoch: 6| Step: 2
Training loss: 1.8879704475402832
Validation loss: 2.0722507379388295

Epoch: 6| Step: 3
Training loss: 2.229053497314453
Validation loss: 2.0780847252056165

Epoch: 6| Step: 4
Training loss: 1.7573264837265015
Validation loss: 2.064110690547574

Epoch: 6| Step: 5
Training loss: 2.6462433338165283
Validation loss: 2.0482798827591764

Epoch: 6| Step: 6
Training loss: 2.054805278778076
Validation loss: 2.059488199090445

Epoch: 6| Step: 7
Training loss: 2.575382709503174
Validation loss: 2.049499665537188

Epoch: 6| Step: 8
Training loss: 2.218376636505127
Validation loss: 2.0398383268746

Epoch: 6| Step: 9
Training loss: 2.545330762863159
Validation loss: 2.0576821578446256

Epoch: 6| Step: 10
Training loss: 2.102890729904175
Validation loss: 2.056942709030644

Epoch: 6| Step: 11
Training loss: 1.9929627180099487
Validation loss: 2.06973700113194

Epoch: 6| Step: 12
Training loss: 3.0207414627075195
Validation loss: 2.0772657573864026

Epoch: 6| Step: 13
Training loss: 3.078843116760254
Validation loss: 2.0981019940427554

Epoch: 79| Step: 0
Training loss: 2.6390976905822754
Validation loss: 2.0770913785503757

Epoch: 6| Step: 1
Training loss: 1.9204533100128174
Validation loss: 2.0843019895656134

Epoch: 6| Step: 2
Training loss: 2.2067341804504395
Validation loss: 2.0607702629540556

Epoch: 6| Step: 3
Training loss: 1.6138705015182495
Validation loss: 2.0683104325366277

Epoch: 6| Step: 4
Training loss: 2.109071731567383
Validation loss: 2.0592212318092264

Epoch: 6| Step: 5
Training loss: 2.835080623626709
Validation loss: 2.0919005922091904

Epoch: 6| Step: 6
Training loss: 2.056589126586914
Validation loss: 2.0778962040460236

Epoch: 6| Step: 7
Training loss: 1.9828805923461914
Validation loss: 2.083894257904381

Epoch: 6| Step: 8
Training loss: 2.7658114433288574
Validation loss: 2.07563159542699

Epoch: 6| Step: 9
Training loss: 2.8787856101989746
Validation loss: 2.088275453095795

Epoch: 6| Step: 10
Training loss: 2.24062442779541
Validation loss: 2.062719537365821

Epoch: 6| Step: 11
Training loss: 2.443376064300537
Validation loss: 2.075493174214517

Epoch: 6| Step: 12
Training loss: 1.9330341815948486
Validation loss: 2.091791081172164

Epoch: 6| Step: 13
Training loss: 2.964259386062622
Validation loss: 2.0898966379063104

Epoch: 80| Step: 0
Training loss: 2.6995131969451904
Validation loss: 2.0856962255252305

Epoch: 6| Step: 1
Training loss: 1.272099494934082
Validation loss: 2.088610869581981

Epoch: 6| Step: 2
Training loss: 3.004655361175537
Validation loss: 2.0805730204428396

Epoch: 6| Step: 3
Training loss: 1.8140076398849487
Validation loss: 2.070086891933154

Epoch: 6| Step: 4
Training loss: 2.4991512298583984
Validation loss: 2.1020959807980444

Epoch: 6| Step: 5
Training loss: 1.857783555984497
Validation loss: 2.078051438895605

Epoch: 6| Step: 6
Training loss: 2.718001127243042
Validation loss: 2.0644542965837704

Epoch: 6| Step: 7
Training loss: 2.304856777191162
Validation loss: 2.060106808139432

Epoch: 6| Step: 8
Training loss: 2.182434320449829
Validation loss: 2.0759533669358943

Epoch: 6| Step: 9
Training loss: 2.241903066635132
Validation loss: 2.050519376672724

Epoch: 6| Step: 10
Training loss: 2.0734238624572754
Validation loss: 2.0639025344643542

Epoch: 6| Step: 11
Training loss: 2.4984374046325684
Validation loss: 2.047104953437723

Epoch: 6| Step: 12
Training loss: 2.672051191329956
Validation loss: 2.0578171847968973

Epoch: 6| Step: 13
Training loss: 2.6389105319976807
Validation loss: 2.066725387368151

Epoch: 81| Step: 0
Training loss: 1.991377353668213
Validation loss: 2.0340141519423454

Epoch: 6| Step: 1
Training loss: 2.7313640117645264
Validation loss: 2.052221408454321

Epoch: 6| Step: 2
Training loss: 2.1348981857299805
Validation loss: 2.050011199007752

Epoch: 6| Step: 3
Training loss: 2.3734374046325684
Validation loss: 2.0623288052056425

Epoch: 6| Step: 4
Training loss: 2.2714896202087402
Validation loss: 2.0666798981287147

Epoch: 6| Step: 5
Training loss: 1.8994578123092651
Validation loss: 2.0515224856715046

Epoch: 6| Step: 6
Training loss: 2.6435298919677734
Validation loss: 2.039770569852603

Epoch: 6| Step: 7
Training loss: 1.787801742553711
Validation loss: 2.0707952489135084

Epoch: 6| Step: 8
Training loss: 2.513798713684082
Validation loss: 2.0560645006036244

Epoch: 6| Step: 9
Training loss: 2.6930723190307617
Validation loss: 2.0483674464687223

Epoch: 6| Step: 10
Training loss: 2.279249906539917
Validation loss: 2.0420294051529257

Epoch: 6| Step: 11
Training loss: 1.619760274887085
Validation loss: 2.0421764043069657

Epoch: 6| Step: 12
Training loss: 2.675917863845825
Validation loss: 2.0244578469184136

Epoch: 6| Step: 13
Training loss: 3.0521011352539062
Validation loss: 2.0181649961779193

Epoch: 82| Step: 0
Training loss: 2.5268337726593018
Validation loss: 2.0575743682922853

Epoch: 6| Step: 1
Training loss: 1.2264351844787598
Validation loss: 2.0556043450550368

Epoch: 6| Step: 2
Training loss: 2.916132926940918
Validation loss: 2.0476325916987594

Epoch: 6| Step: 3
Training loss: 2.1400156021118164
Validation loss: 2.0471152259457495

Epoch: 6| Step: 4
Training loss: 2.226651430130005
Validation loss: 2.0676496708264915

Epoch: 6| Step: 5
Training loss: 2.9461214542388916
Validation loss: 2.0410303966973418

Epoch: 6| Step: 6
Training loss: 2.4105005264282227
Validation loss: 2.056706118327315

Epoch: 6| Step: 7
Training loss: 1.783821940422058
Validation loss: 2.05627465504472

Epoch: 6| Step: 8
Training loss: 2.4700570106506348
Validation loss: 2.0767974494605936

Epoch: 6| Step: 9
Training loss: 2.45420241355896
Validation loss: 2.0716834581026466

Epoch: 6| Step: 10
Training loss: 2.2328543663024902
Validation loss: 2.053355109307074

Epoch: 6| Step: 11
Training loss: 2.5066184997558594
Validation loss: 2.075139650734522

Epoch: 6| Step: 12
Training loss: 2.1788907051086426
Validation loss: 2.0586227601574314

Epoch: 6| Step: 13
Training loss: 2.265922784805298
Validation loss: 2.0575240863266813

Epoch: 83| Step: 0
Training loss: 2.264293670654297
Validation loss: 2.078201731046041

Epoch: 6| Step: 1
Training loss: 2.335327386856079
Validation loss: 2.05311781103893

Epoch: 6| Step: 2
Training loss: 1.8975622653961182
Validation loss: 2.049305405668033

Epoch: 6| Step: 3
Training loss: 2.598599672317505
Validation loss: 2.058971194810765

Epoch: 6| Step: 4
Training loss: 2.4251015186309814
Validation loss: 2.062688483986803

Epoch: 6| Step: 5
Training loss: 2.19980788230896
Validation loss: 2.054240631800826

Epoch: 6| Step: 6
Training loss: 2.212808609008789
Validation loss: 2.04425952511449

Epoch: 6| Step: 7
Training loss: 3.220698595046997
Validation loss: 2.037564471203794

Epoch: 6| Step: 8
Training loss: 1.8145482540130615
Validation loss: 2.030975531506282

Epoch: 6| Step: 9
Training loss: 2.3894729614257812
Validation loss: 2.0344867091025076

Epoch: 6| Step: 10
Training loss: 2.464376926422119
Validation loss: 2.0360267969869796

Epoch: 6| Step: 11
Training loss: 1.5023256540298462
Validation loss: 2.0713152218890447

Epoch: 6| Step: 12
Training loss: 3.0761570930480957
Validation loss: 2.0578877772054365

Epoch: 6| Step: 13
Training loss: 1.836129069328308
Validation loss: 2.025683877288654

Epoch: 84| Step: 0
Training loss: 3.2016682624816895
Validation loss: 2.0334952082685245

Epoch: 6| Step: 1
Training loss: 2.1530725955963135
Validation loss: 2.0708876681584183

Epoch: 6| Step: 2
Training loss: 2.3000731468200684
Validation loss: 2.0600364644040345

Epoch: 6| Step: 3
Training loss: 2.2056994438171387
Validation loss: 2.0612160467332408

Epoch: 6| Step: 4
Training loss: 2.4235076904296875
Validation loss: 2.067316319352837

Epoch: 6| Step: 5
Training loss: 2.0358238220214844
Validation loss: 2.0493124146615305

Epoch: 6| Step: 6
Training loss: 1.4267691373825073
Validation loss: 2.069733709417364

Epoch: 6| Step: 7
Training loss: 2.290463924407959
Validation loss: 2.041150904470874

Epoch: 6| Step: 8
Training loss: 2.5946044921875
Validation loss: 2.0498223022748063

Epoch: 6| Step: 9
Training loss: 1.6729322671890259
Validation loss: 2.0782306501942296

Epoch: 6| Step: 10
Training loss: 2.0477919578552246
Validation loss: 2.035288497965823

Epoch: 6| Step: 11
Training loss: 2.511909008026123
Validation loss: 2.083064486903529

Epoch: 6| Step: 12
Training loss: 2.8500595092773438
Validation loss: 2.04923358655745

Epoch: 6| Step: 13
Training loss: 2.4929521083831787
Validation loss: 2.0736796202198153

Epoch: 85| Step: 0
Training loss: 2.5195250511169434
Validation loss: 2.0793932458405853

Epoch: 6| Step: 1
Training loss: 1.8792943954467773
Validation loss: 2.0519125282123523

Epoch: 6| Step: 2
Training loss: 2.380540370941162
Validation loss: 2.0922771679457797

Epoch: 6| Step: 3
Training loss: 1.9235634803771973
Validation loss: 2.0874999889763455

Epoch: 6| Step: 4
Training loss: 2.5633323192596436
Validation loss: 2.0875266290480092

Epoch: 6| Step: 5
Training loss: 3.424442768096924
Validation loss: 2.0963128125795754

Epoch: 6| Step: 6
Training loss: 2.311784505844116
Validation loss: 2.0890719864958074

Epoch: 6| Step: 7
Training loss: 2.2755181789398193
Validation loss: 2.1097986057240474

Epoch: 6| Step: 8
Training loss: 2.110063076019287
Validation loss: 2.111656924729706

Epoch: 6| Step: 9
Training loss: 2.864027261734009
Validation loss: 2.0692140991969774

Epoch: 6| Step: 10
Training loss: 1.6880311965942383
Validation loss: 2.113912279887866

Epoch: 6| Step: 11
Training loss: 1.9022085666656494
Validation loss: 2.0964266100237445

Epoch: 6| Step: 12
Training loss: 2.016042709350586
Validation loss: 2.0983166771550334

Epoch: 6| Step: 13
Training loss: 2.5516891479492188
Validation loss: 2.097824608125994

Epoch: 86| Step: 0
Training loss: 2.116687536239624
Validation loss: 2.0999457451605026

Epoch: 6| Step: 1
Training loss: 2.2471776008605957
Validation loss: 2.075827439626058

Epoch: 6| Step: 2
Training loss: 2.146200180053711
Validation loss: 2.0799824114768737

Epoch: 6| Step: 3
Training loss: 1.580380916595459
Validation loss: 2.058164991358275

Epoch: 6| Step: 4
Training loss: 2.1408891677856445
Validation loss: 2.0577774586216098

Epoch: 6| Step: 5
Training loss: 1.8268922567367554
Validation loss: 2.0496437549591064

Epoch: 6| Step: 6
Training loss: 2.475630760192871
Validation loss: 2.0758520582670807

Epoch: 6| Step: 7
Training loss: 2.3129982948303223
Validation loss: 2.052291936771844

Epoch: 6| Step: 8
Training loss: 2.907835006713867
Validation loss: 2.060104070171233

Epoch: 6| Step: 9
Training loss: 2.7039875984191895
Validation loss: 2.046294273868684

Epoch: 6| Step: 10
Training loss: 3.1206817626953125
Validation loss: 2.0445565613367225

Epoch: 6| Step: 11
Training loss: 1.5237007141113281
Validation loss: 2.0396761278952322

Epoch: 6| Step: 12
Training loss: 2.6144344806671143
Validation loss: 2.054449353166806

Epoch: 6| Step: 13
Training loss: 2.492232322692871
Validation loss: 2.044915509480302

Epoch: 87| Step: 0
Training loss: 2.9459614753723145
Validation loss: 2.0692961908155874

Epoch: 6| Step: 1
Training loss: 3.0196685791015625
Validation loss: 2.0366896506278747

Epoch: 6| Step: 2
Training loss: 2.594573497772217
Validation loss: 2.0626243442617436

Epoch: 6| Step: 3
Training loss: 2.7810776233673096
Validation loss: 2.028631792273573

Epoch: 6| Step: 4
Training loss: 2.3483729362487793
Validation loss: 2.0668313349446943

Epoch: 6| Step: 5
Training loss: 2.0219132900238037
Validation loss: 2.052659942257789

Epoch: 6| Step: 6
Training loss: 1.8640244007110596
Validation loss: 2.025192165887484

Epoch: 6| Step: 7
Training loss: 1.3781955242156982
Validation loss: 2.068582223307702

Epoch: 6| Step: 8
Training loss: 2.613245964050293
Validation loss: 2.046152817305698

Epoch: 6| Step: 9
Training loss: 2.3718395233154297
Validation loss: 2.0630100555317377

Epoch: 6| Step: 10
Training loss: 2.2317757606506348
Validation loss: 2.0535787638797554

Epoch: 6| Step: 11
Training loss: 1.7025675773620605
Validation loss: 2.04984103479693

Epoch: 6| Step: 12
Training loss: 2.047278881072998
Validation loss: 2.0462808147553475

Epoch: 6| Step: 13
Training loss: 2.3419947624206543
Validation loss: 2.0542927890695553

Epoch: 88| Step: 0
Training loss: 3.0015621185302734
Validation loss: 2.040916746662509

Epoch: 6| Step: 1
Training loss: 1.9150581359863281
Validation loss: 2.054795149833925

Epoch: 6| Step: 2
Training loss: 3.2394886016845703
Validation loss: 2.055466305825018

Epoch: 6| Step: 3
Training loss: 2.2498321533203125
Validation loss: 2.0719721266018447

Epoch: 6| Step: 4
Training loss: 2.3490161895751953
Validation loss: 2.0251090142034713

Epoch: 6| Step: 5
Training loss: 1.7143750190734863
Validation loss: 2.0502448645971154

Epoch: 6| Step: 6
Training loss: 1.3195850849151611
Validation loss: 2.041798678777551

Epoch: 6| Step: 7
Training loss: 2.2158279418945312
Validation loss: 2.05452795438869

Epoch: 6| Step: 8
Training loss: 2.270695924758911
Validation loss: 2.054034686857654

Epoch: 6| Step: 9
Training loss: 1.9971520900726318
Validation loss: 2.0652571929398404

Epoch: 6| Step: 10
Training loss: 2.0654067993164062
Validation loss: 2.0660553901426253

Epoch: 6| Step: 11
Training loss: 3.0455455780029297
Validation loss: 2.059550267393871

Epoch: 6| Step: 12
Training loss: 2.0778872966766357
Validation loss: 2.062822808501541

Epoch: 6| Step: 13
Training loss: 2.9902503490448
Validation loss: 2.0305926081954793

Epoch: 89| Step: 0
Training loss: 1.6326547861099243
Validation loss: 2.038456029789422

Epoch: 6| Step: 1
Training loss: 1.993185043334961
Validation loss: 2.0291902736950944

Epoch: 6| Step: 2
Training loss: 2.8344171047210693
Validation loss: 2.05734226780553

Epoch: 6| Step: 3
Training loss: 3.518745183944702
Validation loss: 2.05460722343896

Epoch: 6| Step: 4
Training loss: 2.258634090423584
Validation loss: 2.0378790619552776

Epoch: 6| Step: 5
Training loss: 2.2128305435180664
Validation loss: 2.0504434288188977

Epoch: 6| Step: 6
Training loss: 2.7974462509155273
Validation loss: 2.04078842747596

Epoch: 6| Step: 7
Training loss: 1.9399616718292236
Validation loss: 2.0453068389687488

Epoch: 6| Step: 8
Training loss: 1.7163734436035156
Validation loss: 2.046430277568038

Epoch: 6| Step: 9
Training loss: 2.1741600036621094
Validation loss: 2.05246594900726

Epoch: 6| Step: 10
Training loss: 2.485938787460327
Validation loss: 2.063355991917272

Epoch: 6| Step: 11
Training loss: 2.4267067909240723
Validation loss: 2.053299121959235

Epoch: 6| Step: 12
Training loss: 2.16549015045166
Validation loss: 2.0741220033296974

Epoch: 6| Step: 13
Training loss: 2.039829730987549
Validation loss: 2.051523397045751

Epoch: 90| Step: 0
Training loss: 2.1057190895080566
Validation loss: 2.037782553703554

Epoch: 6| Step: 1
Training loss: 3.144202470779419
Validation loss: 2.022630868419524

Epoch: 6| Step: 2
Training loss: 1.8160741329193115
Validation loss: 2.056566212766914

Epoch: 6| Step: 3
Training loss: 2.6124157905578613
Validation loss: 2.0954695337562153

Epoch: 6| Step: 4
Training loss: 2.4756970405578613
Validation loss: 2.038128927189817

Epoch: 6| Step: 5
Training loss: 3.0146737098693848
Validation loss: 2.0700641703862015

Epoch: 6| Step: 6
Training loss: 1.35469388961792
Validation loss: 2.0425579676064114

Epoch: 6| Step: 7
Training loss: 2.0441598892211914
Validation loss: 2.009749215136292

Epoch: 6| Step: 8
Training loss: 1.9667370319366455
Validation loss: 2.056551118050852

Epoch: 6| Step: 9
Training loss: 2.743466377258301
Validation loss: 2.0399274620958554

Epoch: 6| Step: 10
Training loss: 1.7833592891693115
Validation loss: 2.0145854757678126

Epoch: 6| Step: 11
Training loss: 2.2539315223693848
Validation loss: 2.0485007121998775

Epoch: 6| Step: 12
Training loss: 2.289487838745117
Validation loss: 2.0242683515753797

Epoch: 6| Step: 13
Training loss: 2.6612069606781006
Validation loss: 2.0406677646021687

Epoch: 91| Step: 0
Training loss: 1.9025118350982666
Validation loss: 2.055134783508957

Epoch: 6| Step: 1
Training loss: 3.388216257095337
Validation loss: 2.026355463971374

Epoch: 6| Step: 2
Training loss: 1.3739721775054932
Validation loss: 2.0439065028262395

Epoch: 6| Step: 3
Training loss: 2.168018341064453
Validation loss: 2.046091630894651

Epoch: 6| Step: 4
Training loss: 2.6257987022399902
Validation loss: 2.054999295101371

Epoch: 6| Step: 5
Training loss: 1.430837631225586
Validation loss: 2.045535610568139

Epoch: 6| Step: 6
Training loss: 2.016585111618042
Validation loss: 2.0633404383095364

Epoch: 6| Step: 7
Training loss: 3.056643009185791
Validation loss: 2.0681582702103483

Epoch: 6| Step: 8
Training loss: 1.623081922531128
Validation loss: 2.0437219681278354

Epoch: 6| Step: 9
Training loss: 2.860145092010498
Validation loss: 2.04751742142503

Epoch: 6| Step: 10
Training loss: 2.344289779663086
Validation loss: 2.0584056890139015

Epoch: 6| Step: 11
Training loss: 2.6853246688842773
Validation loss: 2.0685291059555544

Epoch: 6| Step: 12
Training loss: 2.3772239685058594
Validation loss: 2.0711993299504763

Epoch: 6| Step: 13
Training loss: 1.9757685661315918
Validation loss: 2.059136623977333

Epoch: 92| Step: 0
Training loss: 2.398332118988037
Validation loss: 2.0930924236133532

Epoch: 6| Step: 1
Training loss: 1.6221587657928467
Validation loss: 2.0783578606062036

Epoch: 6| Step: 2
Training loss: 2.0572640895843506
Validation loss: 2.083380264620627

Epoch: 6| Step: 3
Training loss: 1.9707725048065186
Validation loss: 2.0834267190707627

Epoch: 6| Step: 4
Training loss: 2.433600425720215
Validation loss: 2.0596260973202285

Epoch: 6| Step: 5
Training loss: 1.7862284183502197
Validation loss: 2.0586176738944104

Epoch: 6| Step: 6
Training loss: 2.40771746635437
Validation loss: 2.0722011058561263

Epoch: 6| Step: 7
Training loss: 2.4439468383789062
Validation loss: 2.0907278445459183

Epoch: 6| Step: 8
Training loss: 3.0283970832824707
Validation loss: 2.1045213386576664

Epoch: 6| Step: 9
Training loss: 2.3739724159240723
Validation loss: 2.0813075842395907

Epoch: 6| Step: 10
Training loss: 2.407404899597168
Validation loss: 2.092814935150967

Epoch: 6| Step: 11
Training loss: 2.5525739192962646
Validation loss: 2.113386059320101

Epoch: 6| Step: 12
Training loss: 2.398507595062256
Validation loss: 2.113540655823164

Epoch: 6| Step: 13
Training loss: 1.9446260929107666
Validation loss: 2.0977474053700766

Epoch: 93| Step: 0
Training loss: 1.8930613994598389
Validation loss: 2.0784289029336747

Epoch: 6| Step: 1
Training loss: 2.6089086532592773
Validation loss: 2.103854033254808

Epoch: 6| Step: 2
Training loss: 2.5551977157592773
Validation loss: 2.124931125230687

Epoch: 6| Step: 3
Training loss: 1.7169864177703857
Validation loss: 2.1018441107965287

Epoch: 6| Step: 4
Training loss: 2.331977367401123
Validation loss: 2.0945392770151936

Epoch: 6| Step: 5
Training loss: 2.138906478881836
Validation loss: 2.090470334535004

Epoch: 6| Step: 6
Training loss: 2.0046818256378174
Validation loss: 2.1050357882694533

Epoch: 6| Step: 7
Training loss: 1.856292486190796
Validation loss: 2.0810641165702575

Epoch: 6| Step: 8
Training loss: 2.2004952430725098
Validation loss: 2.071068658623644

Epoch: 6| Step: 9
Training loss: 2.6707632541656494
Validation loss: 2.0607320916268135

Epoch: 6| Step: 10
Training loss: 2.0701704025268555
Validation loss: 2.0774883724028066

Epoch: 6| Step: 11
Training loss: 2.9202818870544434
Validation loss: 2.0773207692689795

Epoch: 6| Step: 12
Training loss: 2.3091928958892822
Validation loss: 2.0661064219731156

Epoch: 6| Step: 13
Training loss: 3.4003231525421143
Validation loss: 2.054574904903289

Epoch: 94| Step: 0
Training loss: 1.7042548656463623
Validation loss: 2.0775836975343767

Epoch: 6| Step: 1
Training loss: 2.639655113220215
Validation loss: 2.0744005915939168

Epoch: 6| Step: 2
Training loss: 2.485471725463867
Validation loss: 2.063573618088999

Epoch: 6| Step: 3
Training loss: 2.482929229736328
Validation loss: 2.037373504331035

Epoch: 6| Step: 4
Training loss: 2.3637425899505615
Validation loss: 2.0736871611687446

Epoch: 6| Step: 5
Training loss: 2.8261606693267822
Validation loss: 2.0873907804489136

Epoch: 6| Step: 6
Training loss: 2.413296699523926
Validation loss: 2.0854673218983475

Epoch: 6| Step: 7
Training loss: 2.6456077098846436
Validation loss: 2.049194469246813

Epoch: 6| Step: 8
Training loss: 2.3140876293182373
Validation loss: 2.0680014292399087

Epoch: 6| Step: 9
Training loss: 2.077253818511963
Validation loss: 2.0699161406486266

Epoch: 6| Step: 10
Training loss: 2.5236432552337646
Validation loss: 2.051682791402263

Epoch: 6| Step: 11
Training loss: 1.5916231870651245
Validation loss: 2.0447153506740445

Epoch: 6| Step: 12
Training loss: 2.044682025909424
Validation loss: 2.051581798061248

Epoch: 6| Step: 13
Training loss: 1.5078074932098389
Validation loss: 2.041581306406247

Epoch: 95| Step: 0
Training loss: 2.244769334793091
Validation loss: 2.056272675914149

Epoch: 6| Step: 1
Training loss: 1.6956453323364258
Validation loss: 2.062485723085301

Epoch: 6| Step: 2
Training loss: 2.206059455871582
Validation loss: 2.0476265286886566

Epoch: 6| Step: 3
Training loss: 1.9987421035766602
Validation loss: 2.0438851861543554

Epoch: 6| Step: 4
Training loss: 2.0592546463012695
Validation loss: 2.0656232756953083

Epoch: 6| Step: 5
Training loss: 2.0084540843963623
Validation loss: 2.060491549071445

Epoch: 6| Step: 6
Training loss: 3.189040184020996
Validation loss: 2.038673016332811

Epoch: 6| Step: 7
Training loss: 3.2065329551696777
Validation loss: 2.0410486780187136

Epoch: 6| Step: 8
Training loss: 1.2265324592590332
Validation loss: 2.072961171468099

Epoch: 6| Step: 9
Training loss: 2.5827887058258057
Validation loss: 2.0404438382835797

Epoch: 6| Step: 10
Training loss: 2.3126189708709717
Validation loss: 2.0451218607605144

Epoch: 6| Step: 11
Training loss: 2.699500560760498
Validation loss: 2.059875097326053

Epoch: 6| Step: 12
Training loss: 2.540548801422119
Validation loss: 2.0545467945837204

Epoch: 6| Step: 13
Training loss: 1.6560298204421997
Validation loss: 2.0611604311132945

Epoch: 96| Step: 0
Training loss: 2.7074320316314697
Validation loss: 2.067045452774212

Epoch: 6| Step: 1
Training loss: 2.2117042541503906
Validation loss: 2.073557807553199

Epoch: 6| Step: 2
Training loss: 2.9500436782836914
Validation loss: 2.0765241781870523

Epoch: 6| Step: 3
Training loss: 2.826904535293579
Validation loss: 2.0601584706255185

Epoch: 6| Step: 4
Training loss: 2.2777295112609863
Validation loss: 2.0715102405958277

Epoch: 6| Step: 5
Training loss: 1.232623815536499
Validation loss: 2.061107721380008

Epoch: 6| Step: 6
Training loss: 2.3271431922912598
Validation loss: 2.0462041747185493

Epoch: 6| Step: 7
Training loss: 2.0332984924316406
Validation loss: 2.0746395434102705

Epoch: 6| Step: 8
Training loss: 2.6152138710021973
Validation loss: 2.101467909351472

Epoch: 6| Step: 9
Training loss: 2.419666290283203
Validation loss: 2.057701959404894

Epoch: 6| Step: 10
Training loss: 2.0082030296325684
Validation loss: 2.0779208598598355

Epoch: 6| Step: 11
Training loss: 2.187070369720459
Validation loss: 2.06446046726678

Epoch: 6| Step: 12
Training loss: 2.108725070953369
Validation loss: 2.0651040743756037

Epoch: 6| Step: 13
Training loss: 2.032731294631958
Validation loss: 2.0524932133254183

Epoch: 97| Step: 0
Training loss: 1.8374311923980713
Validation loss: 2.073976247541366

Epoch: 6| Step: 1
Training loss: 2.8135411739349365
Validation loss: 2.070716868164719

Epoch: 6| Step: 2
Training loss: 2.4533474445343018
Validation loss: 2.0668200036530853

Epoch: 6| Step: 3
Training loss: 2.0971457958221436
Validation loss: 2.0650301364160355

Epoch: 6| Step: 4
Training loss: 2.312868595123291
Validation loss: 2.0853110821016374

Epoch: 6| Step: 5
Training loss: 2.5080204010009766
Validation loss: 2.082748357967664

Epoch: 6| Step: 6
Training loss: 1.9777421951293945
Validation loss: 2.076820506844469

Epoch: 6| Step: 7
Training loss: 2.1447532176971436
Validation loss: 2.088438057130383

Epoch: 6| Step: 8
Training loss: 2.5920703411102295
Validation loss: 2.083186618743404

Epoch: 6| Step: 9
Training loss: 2.019151449203491
Validation loss: 2.0798931403826644

Epoch: 6| Step: 10
Training loss: 3.106076240539551
Validation loss: 2.0912028666465514

Epoch: 6| Step: 11
Training loss: 2.3076701164245605
Validation loss: 2.112355389902669

Epoch: 6| Step: 12
Training loss: 1.873615026473999
Validation loss: 2.098940599349237

Epoch: 6| Step: 13
Training loss: 1.8819113969802856
Validation loss: 2.0853942004583215

Epoch: 98| Step: 0
Training loss: 2.278134822845459
Validation loss: 2.070225590018816

Epoch: 6| Step: 1
Training loss: 2.3864173889160156
Validation loss: 2.0576292135382213

Epoch: 6| Step: 2
Training loss: 2.279942750930786
Validation loss: 2.084004848234115

Epoch: 6| Step: 3
Training loss: 2.8530707359313965
Validation loss: 2.046963564811214

Epoch: 6| Step: 4
Training loss: 2.297240972518921
Validation loss: 2.0800312552400815

Epoch: 6| Step: 5
Training loss: 2.12461519241333
Validation loss: 2.0534149498067875

Epoch: 6| Step: 6
Training loss: 2.2225871086120605
Validation loss: 2.0585051121250277

Epoch: 6| Step: 7
Training loss: 1.8377034664154053
Validation loss: 2.0656233769591137

Epoch: 6| Step: 8
Training loss: 1.6794090270996094
Validation loss: 2.070585097036054

Epoch: 6| Step: 9
Training loss: 2.2649776935577393
Validation loss: 2.0691332996532483

Epoch: 6| Step: 10
Training loss: 2.6835598945617676
Validation loss: 2.035112352781398

Epoch: 6| Step: 11
Training loss: 2.163757562637329
Validation loss: 2.047321493907641

Epoch: 6| Step: 12
Training loss: 2.3903393745422363
Validation loss: 2.0419463496054373

Epoch: 6| Step: 13
Training loss: 2.6125502586364746
Validation loss: 2.0586791089785996

Epoch: 99| Step: 0
Training loss: 1.5068998336791992
Validation loss: 2.0746418981141943

Epoch: 6| Step: 1
Training loss: 2.3844640254974365
Validation loss: 2.046323271207912

Epoch: 6| Step: 2
Training loss: 2.2566041946411133
Validation loss: 2.056098243241669

Epoch: 6| Step: 3
Training loss: 2.2224626541137695
Validation loss: 2.0749618776382937

Epoch: 6| Step: 4
Training loss: 1.5992581844329834
Validation loss: 2.0518818773249143

Epoch: 6| Step: 5
Training loss: 2.4702887535095215
Validation loss: 2.0705420112097137

Epoch: 6| Step: 6
Training loss: 3.0103790760040283
Validation loss: 2.0498444893026866

Epoch: 6| Step: 7
Training loss: 3.3971595764160156
Validation loss: 2.0583413980340444

Epoch: 6| Step: 8
Training loss: 2.3998451232910156
Validation loss: 2.044786286610429

Epoch: 6| Step: 9
Training loss: 1.281052589416504
Validation loss: 2.060534205487979

Epoch: 6| Step: 10
Training loss: 2.569786310195923
Validation loss: 2.0689841919047858

Epoch: 6| Step: 11
Training loss: 1.952070951461792
Validation loss: 2.0578571058088735

Epoch: 6| Step: 12
Training loss: 2.030585527420044
Validation loss: 2.0696972326565812

Epoch: 6| Step: 13
Training loss: 3.0929596424102783
Validation loss: 2.0587672084890385

Epoch: 100| Step: 0
Training loss: 2.456660747528076
Validation loss: 2.0451248384291127

Epoch: 6| Step: 1
Training loss: 2.3765921592712402
Validation loss: 2.0494426065875637

Epoch: 6| Step: 2
Training loss: 1.8323229551315308
Validation loss: 2.0691149106589695

Epoch: 6| Step: 3
Training loss: 2.234466552734375
Validation loss: 2.077550757315851

Epoch: 6| Step: 4
Training loss: 2.1254453659057617
Validation loss: 2.0583209453090543

Epoch: 6| Step: 5
Training loss: 2.911860466003418
Validation loss: 2.101294327807683

Epoch: 6| Step: 6
Training loss: 1.860608696937561
Validation loss: 2.042411611926171

Epoch: 6| Step: 7
Training loss: 2.177111864089966
Validation loss: 2.070779503032725

Epoch: 6| Step: 8
Training loss: 2.319902181625366
Validation loss: 2.0740735812853743

Epoch: 6| Step: 9
Training loss: 2.073603630065918
Validation loss: 2.0600135505840345

Epoch: 6| Step: 10
Training loss: 2.87589168548584
Validation loss: 2.071529879364916

Epoch: 6| Step: 11
Training loss: 2.3058786392211914
Validation loss: 2.075148359421761

Epoch: 6| Step: 12
Training loss: 1.7846229076385498
Validation loss: 2.0795742619422173

Epoch: 6| Step: 13
Training loss: 3.1695287227630615
Validation loss: 2.0633195395110757

Epoch: 101| Step: 0
Training loss: 2.1610891819000244
Validation loss: 2.083698708523986

Epoch: 6| Step: 1
Training loss: 2.194913864135742
Validation loss: 2.0755072947471374

Epoch: 6| Step: 2
Training loss: 2.6442365646362305
Validation loss: 2.0726792940529446

Epoch: 6| Step: 3
Training loss: 2.715977907180786
Validation loss: 2.0494946408015426

Epoch: 6| Step: 4
Training loss: 2.1608939170837402
Validation loss: 2.0752337414731263

Epoch: 6| Step: 5
Training loss: 1.953290343284607
Validation loss: 2.0865543696188156

Epoch: 6| Step: 6
Training loss: 2.3031344413757324
Validation loss: 2.059987565522553

Epoch: 6| Step: 7
Training loss: 2.206928253173828
Validation loss: 2.065324209069693

Epoch: 6| Step: 8
Training loss: 2.07415509223938
Validation loss: 2.108426529874084

Epoch: 6| Step: 9
Training loss: 2.0069637298583984
Validation loss: 2.0796549474039385

Epoch: 6| Step: 10
Training loss: 1.6483275890350342
Validation loss: 2.073621074358622

Epoch: 6| Step: 11
Training loss: 2.5154948234558105
Validation loss: 2.0564598319351033

Epoch: 6| Step: 12
Training loss: 2.558962821960449
Validation loss: 2.078591744105021

Epoch: 6| Step: 13
Training loss: 2.8404481410980225
Validation loss: 2.0486129355686966

Epoch: 102| Step: 0
Training loss: 2.0291764736175537
Validation loss: 2.0857028422817105

Epoch: 6| Step: 1
Training loss: 2.2396621704101562
Validation loss: 2.0478076627177577

Epoch: 6| Step: 2
Training loss: 3.0200724601745605
Validation loss: 2.045440689209969

Epoch: 6| Step: 3
Training loss: 2.448038101196289
Validation loss: 2.0541765612940632

Epoch: 6| Step: 4
Training loss: 1.0039863586425781
Validation loss: 2.066061250625118

Epoch: 6| Step: 5
Training loss: 2.0509161949157715
Validation loss: 2.055121450014012

Epoch: 6| Step: 6
Training loss: 2.1661763191223145
Validation loss: 2.0592060332657187

Epoch: 6| Step: 7
Training loss: 2.6742753982543945
Validation loss: 2.0702483397658153

Epoch: 6| Step: 8
Training loss: 2.6311283111572266
Validation loss: 2.0840404366934173

Epoch: 6| Step: 9
Training loss: 2.321084499359131
Validation loss: 2.0429431520482546

Epoch: 6| Step: 10
Training loss: 2.37583589553833
Validation loss: 2.077530691700597

Epoch: 6| Step: 11
Training loss: 2.842951774597168
Validation loss: 2.065956243904688

Epoch: 6| Step: 12
Training loss: 2.1160216331481934
Validation loss: 2.08724884833059

Epoch: 6| Step: 13
Training loss: 1.662273645401001
Validation loss: 2.064933817873719

Epoch: 103| Step: 0
Training loss: 2.562113046646118
Validation loss: 2.0848922588491954

Epoch: 6| Step: 1
Training loss: 1.5970414876937866
Validation loss: 2.0639873755875455

Epoch: 6| Step: 2
Training loss: 2.4903855323791504
Validation loss: 2.057732005273142

Epoch: 6| Step: 3
Training loss: 2.4402287006378174
Validation loss: 2.054651068102929

Epoch: 6| Step: 4
Training loss: 2.4257681369781494
Validation loss: 2.02561253886069

Epoch: 6| Step: 5
Training loss: 2.6427597999572754
Validation loss: 2.0798941337934105

Epoch: 6| Step: 6
Training loss: 2.5912575721740723
Validation loss: 2.066961725552877

Epoch: 6| Step: 7
Training loss: 1.846110463142395
Validation loss: 2.0658218963171846

Epoch: 6| Step: 8
Training loss: 2.0006253719329834
Validation loss: 2.0480058911026164

Epoch: 6| Step: 9
Training loss: 2.542607545852661
Validation loss: 2.0402769529691307

Epoch: 6| Step: 10
Training loss: 2.7024002075195312
Validation loss: 2.0385699772065684

Epoch: 6| Step: 11
Training loss: 1.8256103992462158
Validation loss: 2.0404556028304563

Epoch: 6| Step: 12
Training loss: 2.5972976684570312
Validation loss: 2.0501221815745034

Epoch: 6| Step: 13
Training loss: 1.2030519247055054
Validation loss: 2.0701545797368532

Epoch: 104| Step: 0
Training loss: 1.237427830696106
Validation loss: 2.04139700499914

Epoch: 6| Step: 1
Training loss: 2.3243017196655273
Validation loss: 2.0203074998753046

Epoch: 6| Step: 2
Training loss: 1.9760444164276123
Validation loss: 2.0458436576269006

Epoch: 6| Step: 3
Training loss: 2.6676058769226074
Validation loss: 2.033356817819739

Epoch: 6| Step: 4
Training loss: 2.490859031677246
Validation loss: 2.037323554356893

Epoch: 6| Step: 5
Training loss: 2.5086841583251953
Validation loss: 2.057169314353697

Epoch: 6| Step: 6
Training loss: 2.709947109222412
Validation loss: 2.036058199021124

Epoch: 6| Step: 7
Training loss: 1.27332603931427
Validation loss: 2.036081252559539

Epoch: 6| Step: 8
Training loss: 2.8614211082458496
Validation loss: 2.058687519001704

Epoch: 6| Step: 9
Training loss: 2.054776191711426
Validation loss: 2.0392069406406854

Epoch: 6| Step: 10
Training loss: 2.314680337905884
Validation loss: 2.06781433474633

Epoch: 6| Step: 11
Training loss: 2.3526768684387207
Validation loss: 2.0222286255128923

Epoch: 6| Step: 12
Training loss: 2.6135501861572266
Validation loss: 2.036214877200383

Epoch: 6| Step: 13
Training loss: 2.7119438648223877
Validation loss: 2.0418262558598674

Epoch: 105| Step: 0
Training loss: 2.9239501953125
Validation loss: 2.0345716937895744

Epoch: 6| Step: 1
Training loss: 2.410503387451172
Validation loss: 2.0444887940601637

Epoch: 6| Step: 2
Training loss: 2.375354766845703
Validation loss: 2.0256716923047136

Epoch: 6| Step: 3
Training loss: 3.4999032020568848
Validation loss: 2.015156953565536

Epoch: 6| Step: 4
Training loss: 2.3079962730407715
Validation loss: 2.0575008315424763

Epoch: 6| Step: 5
Training loss: 1.6200103759765625
Validation loss: 2.045651125651534

Epoch: 6| Step: 6
Training loss: 1.7804960012435913
Validation loss: 2.029068116218813

Epoch: 6| Step: 7
Training loss: 2.9348273277282715
Validation loss: 2.0402064169606855

Epoch: 6| Step: 8
Training loss: 1.853074550628662
Validation loss: 2.0267618612576555

Epoch: 6| Step: 9
Training loss: 1.5746281147003174
Validation loss: 2.030047042395479

Epoch: 6| Step: 10
Training loss: 1.5068044662475586
Validation loss: 2.043915117940595

Epoch: 6| Step: 11
Training loss: 2.3324856758117676
Validation loss: 2.0304054944745955

Epoch: 6| Step: 12
Training loss: 2.529719352722168
Validation loss: 2.058641924653002

Epoch: 6| Step: 13
Training loss: 1.9888030290603638
Validation loss: 2.0139048304609073

Epoch: 106| Step: 0
Training loss: 2.6591687202453613
Validation loss: 2.031755998570432

Epoch: 6| Step: 1
Training loss: 1.8811386823654175
Validation loss: 2.0371588430097027

Epoch: 6| Step: 2
Training loss: 2.3206002712249756
Validation loss: 2.0499174697424776

Epoch: 6| Step: 3
Training loss: 1.937661051750183
Validation loss: 2.0630367955853863

Epoch: 6| Step: 4
Training loss: 2.342285633087158
Validation loss: 2.0291649564619987

Epoch: 6| Step: 5
Training loss: 2.117792844772339
Validation loss: 2.068726744703067

Epoch: 6| Step: 6
Training loss: 2.0202901363372803
Validation loss: 2.0423806162290674

Epoch: 6| Step: 7
Training loss: 1.6370562314987183
Validation loss: 2.068209489186605

Epoch: 6| Step: 8
Training loss: 2.4279487133026123
Validation loss: 2.06742327444015

Epoch: 6| Step: 9
Training loss: 2.6356496810913086
Validation loss: 2.06251859664917

Epoch: 6| Step: 10
Training loss: 2.860086441040039
Validation loss: 2.0569610390611874

Epoch: 6| Step: 11
Training loss: 2.2177391052246094
Validation loss: 2.0539039847671345

Epoch: 6| Step: 12
Training loss: 2.5681591033935547
Validation loss: 2.060494133221206

Epoch: 6| Step: 13
Training loss: 2.0311546325683594
Validation loss: 2.0545904136473134

Epoch: 107| Step: 0
Training loss: 1.614946961402893
Validation loss: 2.074424389869936

Epoch: 6| Step: 1
Training loss: 2.1200473308563232
Validation loss: 2.121311927354464

Epoch: 6| Step: 2
Training loss: 2.1020116806030273
Validation loss: 2.063101586475167

Epoch: 6| Step: 3
Training loss: 2.1321544647216797
Validation loss: 2.083581196364536

Epoch: 6| Step: 4
Training loss: 2.0027222633361816
Validation loss: 2.0753331504842287

Epoch: 6| Step: 5
Training loss: 2.26554012298584
Validation loss: 2.0708727426426385

Epoch: 6| Step: 6
Training loss: 2.643489360809326
Validation loss: 2.076725782886628

Epoch: 6| Step: 7
Training loss: 2.2566030025482178
Validation loss: 2.1016970885697233

Epoch: 6| Step: 8
Training loss: 2.674617052078247
Validation loss: 2.101510350422193

Epoch: 6| Step: 9
Training loss: 1.886460542678833
Validation loss: 2.0975769437769407

Epoch: 6| Step: 10
Training loss: 2.483273983001709
Validation loss: 2.1009124350804154

Epoch: 6| Step: 11
Training loss: 2.7585182189941406
Validation loss: 2.1142739711269254

Epoch: 6| Step: 12
Training loss: 1.8980680704116821
Validation loss: 2.083874756290067

Epoch: 6| Step: 13
Training loss: 3.242338180541992
Validation loss: 2.081173909607754

Epoch: 108| Step: 0
Training loss: 2.1918060779571533
Validation loss: 2.08048681546283

Epoch: 6| Step: 1
Training loss: 2.152754306793213
Validation loss: 2.054059454189834

Epoch: 6| Step: 2
Training loss: 2.4514479637145996
Validation loss: 2.0667106541254188

Epoch: 6| Step: 3
Training loss: 1.6684818267822266
Validation loss: 2.098230608047978

Epoch: 6| Step: 4
Training loss: 1.3043887615203857
Validation loss: 2.082418469972508

Epoch: 6| Step: 5
Training loss: 2.8726768493652344
Validation loss: 2.0993100340648363

Epoch: 6| Step: 6
Training loss: 2.691345691680908
Validation loss: 2.0722982383543447

Epoch: 6| Step: 7
Training loss: 2.8060684204101562
Validation loss: 2.058314525952903

Epoch: 6| Step: 8
Training loss: 2.8676788806915283
Validation loss: 2.063541043189264

Epoch: 6| Step: 9
Training loss: 1.55877685546875
Validation loss: 2.0761911330684537

Epoch: 6| Step: 10
Training loss: 1.6590213775634766
Validation loss: 2.075846920731247

Epoch: 6| Step: 11
Training loss: 2.943112373352051
Validation loss: 2.045968449243935

Epoch: 6| Step: 12
Training loss: 2.112687826156616
Validation loss: 2.0552037915875836

Epoch: 6| Step: 13
Training loss: 2.7413041591644287
Validation loss: 2.0667538053245953

Epoch: 109| Step: 0
Training loss: 2.412748336791992
Validation loss: 2.064221825650943

Epoch: 6| Step: 1
Training loss: 2.111708164215088
Validation loss: 2.093546146987587

Epoch: 6| Step: 2
Training loss: 2.6691339015960693
Validation loss: 2.0447087390448457

Epoch: 6| Step: 3
Training loss: 1.5533442497253418
Validation loss: 2.0694862117049513

Epoch: 6| Step: 4
Training loss: 2.319796562194824
Validation loss: 2.0715302780110347

Epoch: 6| Step: 5
Training loss: 1.5456547737121582
Validation loss: 2.070088319880988

Epoch: 6| Step: 6
Training loss: 2.6267058849334717
Validation loss: 2.0740501252553796

Epoch: 6| Step: 7
Training loss: 1.673447847366333
Validation loss: 2.050430590106595

Epoch: 6| Step: 8
Training loss: 2.7817275524139404
Validation loss: 2.065891806797315

Epoch: 6| Step: 9
Training loss: 2.4654369354248047
Validation loss: 2.056188009118521

Epoch: 6| Step: 10
Training loss: 2.356104612350464
Validation loss: 2.0695325238730318

Epoch: 6| Step: 11
Training loss: 2.7973828315734863
Validation loss: 2.0834504506921254

Epoch: 6| Step: 12
Training loss: 2.4363245964050293
Validation loss: 2.0360381628877375

Epoch: 6| Step: 13
Training loss: 1.757675051689148
Validation loss: 2.0721851805204987

Epoch: 110| Step: 0
Training loss: 2.307246446609497
Validation loss: 2.0790993500781316

Epoch: 6| Step: 1
Training loss: 1.935020923614502
Validation loss: 2.0592351062323457

Epoch: 6| Step: 2
Training loss: 1.241776943206787
Validation loss: 2.0513734638050036

Epoch: 6| Step: 3
Training loss: 1.781906008720398
Validation loss: 2.0616085375508955

Epoch: 6| Step: 4
Training loss: 2.890188694000244
Validation loss: 2.052073019807057

Epoch: 6| Step: 5
Training loss: 2.0960426330566406
Validation loss: 2.070291062837006

Epoch: 6| Step: 6
Training loss: 2.3535962104797363
Validation loss: 2.070706502083809

Epoch: 6| Step: 7
Training loss: 3.325641632080078
Validation loss: 2.062451961219952

Epoch: 6| Step: 8
Training loss: 2.60103702545166
Validation loss: 2.019055897189725

Epoch: 6| Step: 9
Training loss: 1.981398105621338
Validation loss: 2.088198851513606

Epoch: 6| Step: 10
Training loss: 1.8801673650741577
Validation loss: 2.0859659179564445

Epoch: 6| Step: 11
Training loss: 2.235123872756958
Validation loss: 2.084500910133444

Epoch: 6| Step: 12
Training loss: 2.8120193481445312
Validation loss: 2.1027551517691663

Epoch: 6| Step: 13
Training loss: 2.1656100749969482
Validation loss: 2.081169269418204

Epoch: 111| Step: 0
Training loss: 2.77921199798584
Validation loss: 2.0854167951050626

Epoch: 6| Step: 1
Training loss: 2.063612222671509
Validation loss: 2.0617306924635366

Epoch: 6| Step: 2
Training loss: 1.3833614587783813
Validation loss: 2.0633854071299234

Epoch: 6| Step: 3
Training loss: 2.78989839553833
Validation loss: 2.0530795538297264

Epoch: 6| Step: 4
Training loss: 1.6267642974853516
Validation loss: 2.082053833110358

Epoch: 6| Step: 5
Training loss: 2.3677754402160645
Validation loss: 2.044573007091399

Epoch: 6| Step: 6
Training loss: 2.002696990966797
Validation loss: 2.061018750231753

Epoch: 6| Step: 7
Training loss: 3.1209044456481934
Validation loss: 2.0607219678099438

Epoch: 6| Step: 8
Training loss: 2.72049617767334
Validation loss: 2.071860105760636

Epoch: 6| Step: 9
Training loss: 2.3800222873687744
Validation loss: 2.027795669853046

Epoch: 6| Step: 10
Training loss: 1.8670783042907715
Validation loss: 2.0421717756537983

Epoch: 6| Step: 11
Training loss: 2.1785616874694824
Validation loss: 2.068701832525192

Epoch: 6| Step: 12
Training loss: 2.316938638687134
Validation loss: 2.0598786313046693

Epoch: 6| Step: 13
Training loss: 2.399770736694336
Validation loss: 2.0614892282793598

Epoch: 112| Step: 0
Training loss: 1.6921796798706055
Validation loss: 2.0717342335690736

Epoch: 6| Step: 1
Training loss: 2.700373411178589
Validation loss: 2.036083044544343

Epoch: 6| Step: 2
Training loss: 2.432323932647705
Validation loss: 2.070258370009802

Epoch: 6| Step: 3
Training loss: 2.872440814971924
Validation loss: 2.0452255433605564

Epoch: 6| Step: 4
Training loss: 2.4778754711151123
Validation loss: 2.0422884136117916

Epoch: 6| Step: 5
Training loss: 2.833566665649414
Validation loss: 2.0579889756377026

Epoch: 6| Step: 6
Training loss: 1.1396994590759277
Validation loss: 2.0468557470588276

Epoch: 6| Step: 7
Training loss: 2.06308650970459
Validation loss: 2.0734527649418

Epoch: 6| Step: 8
Training loss: 1.7803175449371338
Validation loss: 2.0700067807269353

Epoch: 6| Step: 9
Training loss: 2.1232705116271973
Validation loss: 2.0801072197575725

Epoch: 6| Step: 10
Training loss: 2.0457966327667236
Validation loss: 2.0521487933333202

Epoch: 6| Step: 11
Training loss: 2.5769810676574707
Validation loss: 2.0704212419448362

Epoch: 6| Step: 12
Training loss: 2.4570319652557373
Validation loss: 2.04833968224064

Epoch: 6| Step: 13
Training loss: 2.891237258911133
Validation loss: 2.0748226616972234

Epoch: 113| Step: 0
Training loss: 1.7077248096466064
Validation loss: 2.0526716914228214

Epoch: 6| Step: 1
Training loss: 1.9835896492004395
Validation loss: 2.0582369271145073

Epoch: 6| Step: 2
Training loss: 2.623260498046875
Validation loss: 2.0612521812479985

Epoch: 6| Step: 3
Training loss: 2.3878440856933594
Validation loss: 2.07732738730728

Epoch: 6| Step: 4
Training loss: 2.1120033264160156
Validation loss: 2.0758315850329656

Epoch: 6| Step: 5
Training loss: 3.2389745712280273
Validation loss: 2.043017097698745

Epoch: 6| Step: 6
Training loss: 2.073288917541504
Validation loss: 2.0326126108887377

Epoch: 6| Step: 7
Training loss: 2.2685725688934326
Validation loss: 2.055676280811269

Epoch: 6| Step: 8
Training loss: 2.287637710571289
Validation loss: 2.0714341748145317

Epoch: 6| Step: 9
Training loss: 1.9074100255966187
Validation loss: 2.0557840831818117

Epoch: 6| Step: 10
Training loss: 2.0433316230773926
Validation loss: 2.0559832306318384

Epoch: 6| Step: 11
Training loss: 2.752498149871826
Validation loss: 2.0651727107263382

Epoch: 6| Step: 12
Training loss: 2.161911725997925
Validation loss: 2.0745669231619885

Epoch: 6| Step: 13
Training loss: 2.228743076324463
Validation loss: 2.056285729972265

Epoch: 114| Step: 0
Training loss: 2.794558525085449
Validation loss: 2.058195807600534

Epoch: 6| Step: 1
Training loss: 2.686088800430298
Validation loss: 2.069401971755489

Epoch: 6| Step: 2
Training loss: 2.9810166358947754
Validation loss: 2.0615059560345066

Epoch: 6| Step: 3
Training loss: 2.409743309020996
Validation loss: 2.0449185371398926

Epoch: 6| Step: 4
Training loss: 1.8437840938568115
Validation loss: 2.089024500180316

Epoch: 6| Step: 5
Training loss: 1.375241756439209
Validation loss: 2.09273091952006

Epoch: 6| Step: 6
Training loss: 2.450234889984131
Validation loss: 2.098108873572401

Epoch: 6| Step: 7
Training loss: 1.8230562210083008
Validation loss: 2.1051620027070403

Epoch: 6| Step: 8
Training loss: 1.8924765586853027
Validation loss: 2.0784463382536367

Epoch: 6| Step: 9
Training loss: 2.340487241744995
Validation loss: 2.0761607026541107

Epoch: 6| Step: 10
Training loss: 1.917495608329773
Validation loss: 2.0766327317043016

Epoch: 6| Step: 11
Training loss: 2.779207229614258
Validation loss: 2.0781452450700986

Epoch: 6| Step: 12
Training loss: 2.3254551887512207
Validation loss: 2.077813697117631

Epoch: 6| Step: 13
Training loss: 1.9091955423355103
Validation loss: 2.068397146399303

Epoch: 115| Step: 0
Training loss: 2.4540507793426514
Validation loss: 2.0760780970255532

Epoch: 6| Step: 1
Training loss: 2.0026941299438477
Validation loss: 2.0426918101567093

Epoch: 6| Step: 2
Training loss: 2.5863499641418457
Validation loss: 2.0539484780321837

Epoch: 6| Step: 3
Training loss: 2.3059544563293457
Validation loss: 2.039777466045913

Epoch: 6| Step: 4
Training loss: 3.2566308975219727
Validation loss: 2.0703689026576217

Epoch: 6| Step: 5
Training loss: 1.7847175598144531
Validation loss: 2.0470649888438563

Epoch: 6| Step: 6
Training loss: 2.1092028617858887
Validation loss: 2.0648460798366095

Epoch: 6| Step: 7
Training loss: 2.607322931289673
Validation loss: 2.057175153045244

Epoch: 6| Step: 8
Training loss: 1.609374761581421
Validation loss: 2.0534225574103733

Epoch: 6| Step: 9
Training loss: 2.3731603622436523
Validation loss: 2.085802535856924

Epoch: 6| Step: 10
Training loss: 2.081969738006592
Validation loss: 2.076770645315929

Epoch: 6| Step: 11
Training loss: 1.7894859313964844
Validation loss: 2.0782177832818802

Epoch: 6| Step: 12
Training loss: 2.30891752243042
Validation loss: 2.0366452381175053

Epoch: 6| Step: 13
Training loss: 2.6522109508514404
Validation loss: 2.058199920961934

Epoch: 116| Step: 0
Training loss: 2.146007537841797
Validation loss: 2.059992854313184

Epoch: 6| Step: 1
Training loss: 2.6638286113739014
Validation loss: 2.0617090297001663

Epoch: 6| Step: 2
Training loss: 2.3755476474761963
Validation loss: 2.0493316035116873

Epoch: 6| Step: 3
Training loss: 2.4560494422912598
Validation loss: 2.07535736022457

Epoch: 6| Step: 4
Training loss: 1.7831356525421143
Validation loss: 2.076016408140941

Epoch: 6| Step: 5
Training loss: 2.8112473487854004
Validation loss: 2.0424300445023404

Epoch: 6| Step: 6
Training loss: 2.717925786972046
Validation loss: 2.0606698925777147

Epoch: 6| Step: 7
Training loss: 2.550372362136841
Validation loss: 2.0567958047313075

Epoch: 6| Step: 8
Training loss: 2.0149166584014893
Validation loss: 2.047027346908405

Epoch: 6| Step: 9
Training loss: 2.507932662963867
Validation loss: 2.0619195302327475

Epoch: 6| Step: 10
Training loss: 1.9034240245819092
Validation loss: 2.06276201176387

Epoch: 6| Step: 11
Training loss: 2.0120463371276855
Validation loss: 2.066012477362028

Epoch: 6| Step: 12
Training loss: 1.707819938659668
Validation loss: 2.0335420639284196

Epoch: 6| Step: 13
Training loss: 1.9692702293395996
Validation loss: 2.069612051851006

Epoch: 117| Step: 0
Training loss: 1.5471651554107666
Validation loss: 2.0542943836540304

Epoch: 6| Step: 1
Training loss: 2.059971570968628
Validation loss: 2.0656776376949844

Epoch: 6| Step: 2
Training loss: 2.4756126403808594
Validation loss: 2.087334172700041

Epoch: 6| Step: 3
Training loss: 2.186485528945923
Validation loss: 2.04894192500781

Epoch: 6| Step: 4
Training loss: 2.6906955242156982
Validation loss: 2.055313628206971

Epoch: 6| Step: 5
Training loss: 2.3246333599090576
Validation loss: 2.0389430176827217

Epoch: 6| Step: 6
Training loss: 2.886998176574707
Validation loss: 2.037530558083647

Epoch: 6| Step: 7
Training loss: 2.8554368019104004
Validation loss: 2.0632856045999834

Epoch: 6| Step: 8
Training loss: 1.9236433506011963
Validation loss: 2.044514404830112

Epoch: 6| Step: 9
Training loss: 1.72597074508667
Validation loss: 2.066555587194299

Epoch: 6| Step: 10
Training loss: 2.255643129348755
Validation loss: 2.0748214593497654

Epoch: 6| Step: 11
Training loss: 1.8299007415771484
Validation loss: 2.0760122858067995

Epoch: 6| Step: 12
Training loss: 2.0284342765808105
Validation loss: 2.068232810625466

Epoch: 6| Step: 13
Training loss: 3.2301831245422363
Validation loss: 2.0558514133576424

Epoch: 118| Step: 0
Training loss: 2.210455894470215
Validation loss: 2.0341491212127027

Epoch: 6| Step: 1
Training loss: 2.8738436698913574
Validation loss: 2.0728241128306233

Epoch: 6| Step: 2
Training loss: 3.0783133506774902
Validation loss: 2.063065485287738

Epoch: 6| Step: 3
Training loss: 1.7948354482650757
Validation loss: 2.0351050797329155

Epoch: 6| Step: 4
Training loss: 2.392371416091919
Validation loss: 2.0640356156133834

Epoch: 6| Step: 5
Training loss: 1.9211108684539795
Validation loss: 2.0572508496622883

Epoch: 6| Step: 6
Training loss: 1.8564789295196533
Validation loss: 2.042252198342354

Epoch: 6| Step: 7
Training loss: 2.120296001434326
Validation loss: 2.042712724337014

Epoch: 6| Step: 8
Training loss: 2.1456432342529297
Validation loss: 2.05327691314041

Epoch: 6| Step: 9
Training loss: 2.636776924133301
Validation loss: 2.035075395337997

Epoch: 6| Step: 10
Training loss: 2.145451068878174
Validation loss: 2.03999319640539

Epoch: 6| Step: 11
Training loss: 1.5783337354660034
Validation loss: 2.0377007838218444

Epoch: 6| Step: 12
Training loss: 2.4978060722351074
Validation loss: 2.053034838809762

Epoch: 6| Step: 13
Training loss: 2.596261501312256
Validation loss: 2.0563692072386384

Epoch: 119| Step: 0
Training loss: 2.358922004699707
Validation loss: 2.0537996317750666

Epoch: 6| Step: 1
Training loss: 1.8670328855514526
Validation loss: 2.047159255191844

Epoch: 6| Step: 2
Training loss: 2.3186657428741455
Validation loss: 2.060978968938192

Epoch: 6| Step: 3
Training loss: 2.4662234783172607
Validation loss: 2.058652267661146

Epoch: 6| Step: 4
Training loss: 2.5279605388641357
Validation loss: 2.0642234894537155

Epoch: 6| Step: 5
Training loss: 1.750077724456787
Validation loss: 2.0658545506897794

Epoch: 6| Step: 6
Training loss: 1.7560815811157227
Validation loss: 2.0596242335534867

Epoch: 6| Step: 7
Training loss: 2.3696861267089844
Validation loss: 2.0561593655617005

Epoch: 6| Step: 8
Training loss: 2.6408185958862305
Validation loss: 2.0951068273154636

Epoch: 6| Step: 9
Training loss: 2.8950488567352295
Validation loss: 2.082426844104644

Epoch: 6| Step: 10
Training loss: 1.3619723320007324
Validation loss: 2.0939153291845836

Epoch: 6| Step: 11
Training loss: 2.3265578746795654
Validation loss: 2.0898655511999644

Epoch: 6| Step: 12
Training loss: 2.567592144012451
Validation loss: 2.0572495460510254

Epoch: 6| Step: 13
Training loss: 2.47188663482666
Validation loss: 2.0792560846574846

Epoch: 120| Step: 0
Training loss: 2.463078737258911
Validation loss: 2.081290634729529

Epoch: 6| Step: 1
Training loss: 2.3816897869110107
Validation loss: 2.077397113205284

Epoch: 6| Step: 2
Training loss: 2.0262680053710938
Validation loss: 2.0667251912496423

Epoch: 6| Step: 3
Training loss: 2.1489486694335938
Validation loss: 2.1064004321252146

Epoch: 6| Step: 4
Training loss: 2.690487861633301
Validation loss: 2.1038208520540627

Epoch: 6| Step: 5
Training loss: 2.501619577407837
Validation loss: 2.0953073911769415

Epoch: 6| Step: 6
Training loss: 1.843245506286621
Validation loss: 2.0970143246394333

Epoch: 6| Step: 7
Training loss: 2.394304037094116
Validation loss: 2.0688203765499975

Epoch: 6| Step: 8
Training loss: 2.8906760215759277
Validation loss: 2.068270298742479

Epoch: 6| Step: 9
Training loss: 1.7333159446716309
Validation loss: 2.0725219659907843

Epoch: 6| Step: 10
Training loss: 2.4400954246520996
Validation loss: 2.089849341300226

Epoch: 6| Step: 11
Training loss: 1.503248929977417
Validation loss: 2.065009597809084

Epoch: 6| Step: 12
Training loss: 1.887366771697998
Validation loss: 2.0980641790615615

Epoch: 6| Step: 13
Training loss: 3.3551511764526367
Validation loss: 2.067627878599269

Epoch: 121| Step: 0
Training loss: 3.42185378074646
Validation loss: 2.0729573721526773

Epoch: 6| Step: 1
Training loss: 2.0592236518859863
Validation loss: 2.0797022619555072

Epoch: 6| Step: 2
Training loss: 2.2890710830688477
Validation loss: 2.0743494713178245

Epoch: 6| Step: 3
Training loss: 2.3611412048339844
Validation loss: 2.0724786814822944

Epoch: 6| Step: 4
Training loss: 2.2781565189361572
Validation loss: 2.0681210743483676

Epoch: 6| Step: 5
Training loss: 1.9890902042388916
Validation loss: 2.069914069226993

Epoch: 6| Step: 6
Training loss: 2.7138514518737793
Validation loss: 2.0709034524938112

Epoch: 6| Step: 7
Training loss: 1.6380053758621216
Validation loss: 2.081836523548249

Epoch: 6| Step: 8
Training loss: 1.5721572637557983
Validation loss: 2.075913792015404

Epoch: 6| Step: 9
Training loss: 2.2806718349456787
Validation loss: 2.0872334267503474

Epoch: 6| Step: 10
Training loss: 1.9642387628555298
Validation loss: 2.057898780351044

Epoch: 6| Step: 11
Training loss: 2.0015580654144287
Validation loss: 2.0395000275745185

Epoch: 6| Step: 12
Training loss: 2.13795804977417
Validation loss: 2.063390206265193

Epoch: 6| Step: 13
Training loss: 3.264531373977661
Validation loss: 2.0722421881973103

Epoch: 122| Step: 0
Training loss: 2.371243476867676
Validation loss: 2.057939890892275

Epoch: 6| Step: 1
Training loss: 1.8762906789779663
Validation loss: 2.0559223390394643

Epoch: 6| Step: 2
Training loss: 2.035923957824707
Validation loss: 2.0751825532605572

Epoch: 6| Step: 3
Training loss: 3.059278726577759
Validation loss: 2.0970143092575895

Epoch: 6| Step: 4
Training loss: 2.148927688598633
Validation loss: 2.0723522901535034

Epoch: 6| Step: 5
Training loss: 2.312265396118164
Validation loss: 2.0776054448978876

Epoch: 6| Step: 6
Training loss: 3.1182351112365723
Validation loss: 2.078252212975615

Epoch: 6| Step: 7
Training loss: 2.3241302967071533
Validation loss: 2.0698536211444485

Epoch: 6| Step: 8
Training loss: 1.47721266746521
Validation loss: 2.088445117396693

Epoch: 6| Step: 9
Training loss: 2.169595718383789
Validation loss: 2.0673155271878807

Epoch: 6| Step: 10
Training loss: 1.990511178970337
Validation loss: 2.10256334658592

Epoch: 6| Step: 11
Training loss: 2.136950731277466
Validation loss: 2.06330374235748

Epoch: 6| Step: 12
Training loss: 2.410095691680908
Validation loss: 2.0868208818538214

Epoch: 6| Step: 13
Training loss: 1.804747462272644
Validation loss: 2.0608066102509857

Epoch: 123| Step: 0
Training loss: 2.143843173980713
Validation loss: 2.0819087054139827

Epoch: 6| Step: 1
Training loss: 2.265676975250244
Validation loss: 2.0622227371379895

Epoch: 6| Step: 2
Training loss: 2.675753593444824
Validation loss: 2.0717839323064333

Epoch: 6| Step: 3
Training loss: 2.346916675567627
Validation loss: 2.060933692480928

Epoch: 6| Step: 4
Training loss: 1.8849411010742188
Validation loss: 2.085146211808728

Epoch: 6| Step: 5
Training loss: 1.8767614364624023
Validation loss: 2.0906751566035773

Epoch: 6| Step: 6
Training loss: 2.2557759284973145
Validation loss: 2.067510199803178

Epoch: 6| Step: 7
Training loss: 1.9040181636810303
Validation loss: 2.0659124940954228

Epoch: 6| Step: 8
Training loss: 2.1656265258789062
Validation loss: 2.0701534030258015

Epoch: 6| Step: 9
Training loss: 2.6894583702087402
Validation loss: 2.0715066566262195

Epoch: 6| Step: 10
Training loss: 2.4014639854431152
Validation loss: 2.068883930483172

Epoch: 6| Step: 11
Training loss: 2.1875033378601074
Validation loss: 2.0650206817093717

Epoch: 6| Step: 12
Training loss: 2.4694321155548096
Validation loss: 2.0725553740737257

Epoch: 6| Step: 13
Training loss: 2.3409156799316406
Validation loss: 2.0542135943648634

Epoch: 124| Step: 0
Training loss: 1.8158934116363525
Validation loss: 2.0979155776321248

Epoch: 6| Step: 1
Training loss: 2.7241392135620117
Validation loss: 2.08939088800902

Epoch: 6| Step: 2
Training loss: 2.0791194438934326
Validation loss: 2.065622611712384

Epoch: 6| Step: 3
Training loss: 2.737940549850464
Validation loss: 2.0514252237094346

Epoch: 6| Step: 4
Training loss: 2.6873345375061035
Validation loss: 2.0713673727486723

Epoch: 6| Step: 5
Training loss: 2.2898693084716797
Validation loss: 2.056269927691388

Epoch: 6| Step: 6
Training loss: 1.2805311679840088
Validation loss: 2.062872225238431

Epoch: 6| Step: 7
Training loss: 1.9207696914672852
Validation loss: 2.0695988542290142

Epoch: 6| Step: 8
Training loss: 1.9193248748779297
Validation loss: 2.0612425214500836

Epoch: 6| Step: 9
Training loss: 3.4086928367614746
Validation loss: 2.028874981787897

Epoch: 6| Step: 10
Training loss: 2.2471132278442383
Validation loss: 2.0514900402356218

Epoch: 6| Step: 11
Training loss: 2.206796407699585
Validation loss: 2.092361847559611

Epoch: 6| Step: 12
Training loss: 2.570301055908203
Validation loss: 2.078666922866657

Epoch: 6| Step: 13
Training loss: 1.3237786293029785
Validation loss: 2.0493825943239274

Epoch: 125| Step: 0
Training loss: 1.210540771484375
Validation loss: 2.0684984576317573

Epoch: 6| Step: 1
Training loss: 3.1418423652648926
Validation loss: 2.0352823401010163

Epoch: 6| Step: 2
Training loss: 3.0121307373046875
Validation loss: 2.028720671130765

Epoch: 6| Step: 3
Training loss: 2.9981541633605957
Validation loss: 2.048873902649008

Epoch: 6| Step: 4
Training loss: 1.8814644813537598
Validation loss: 2.0573534401514197

Epoch: 6| Step: 5
Training loss: 2.7328977584838867
Validation loss: 2.0587200810832362

Epoch: 6| Step: 6
Training loss: 1.9785354137420654
Validation loss: 2.055386895774513

Epoch: 6| Step: 7
Training loss: 2.1102066040039062
Validation loss: 2.067244191323557

Epoch: 6| Step: 8
Training loss: 2.227231502532959
Validation loss: 2.055825400096114

Epoch: 6| Step: 9
Training loss: 1.8872950077056885
Validation loss: 2.0616884321294804

Epoch: 6| Step: 10
Training loss: 1.7526988983154297
Validation loss: 2.0515093777769353

Epoch: 6| Step: 11
Training loss: 2.972414493560791
Validation loss: 2.0662772476032214

Epoch: 6| Step: 12
Training loss: 1.451966404914856
Validation loss: 2.0706830806629632

Epoch: 6| Step: 13
Training loss: 1.9887161254882812
Validation loss: 2.06481397023765

Epoch: 126| Step: 0
Training loss: 2.111829996109009
Validation loss: 2.059759774515706

Epoch: 6| Step: 1
Training loss: 2.18746018409729
Validation loss: 2.055300863840247

Epoch: 6| Step: 2
Training loss: 2.4993743896484375
Validation loss: 2.04409219244475

Epoch: 6| Step: 3
Training loss: 2.238353729248047
Validation loss: 2.06447277146001

Epoch: 6| Step: 4
Training loss: 2.1316208839416504
Validation loss: 2.081776208775018

Epoch: 6| Step: 5
Training loss: 2.8711318969726562
Validation loss: 2.0453165115848666

Epoch: 6| Step: 6
Training loss: 2.4360427856445312
Validation loss: 2.0784900111536824

Epoch: 6| Step: 7
Training loss: 1.7095824480056763
Validation loss: 2.046832189765028

Epoch: 6| Step: 8
Training loss: 2.081054925918579
Validation loss: 2.06113677383751

Epoch: 6| Step: 9
Training loss: 2.1906380653381348
Validation loss: 2.0756554065212125

Epoch: 6| Step: 10
Training loss: 2.0791430473327637
Validation loss: 2.0608015739789574

Epoch: 6| Step: 11
Training loss: 2.2019646167755127
Validation loss: 2.044692152289934

Epoch: 6| Step: 12
Training loss: 2.53414249420166
Validation loss: 2.0751405890269945

Epoch: 6| Step: 13
Training loss: 2.3019919395446777
Validation loss: 2.0668241285508677

Epoch: 127| Step: 0
Training loss: 2.381463050842285
Validation loss: 2.050128988040391

Epoch: 6| Step: 1
Training loss: 2.3405797481536865
Validation loss: 2.1028211860246557

Epoch: 6| Step: 2
Training loss: 2.586078405380249
Validation loss: 2.0573507406378306

Epoch: 6| Step: 3
Training loss: 2.572600841522217
Validation loss: 2.0600462395657777

Epoch: 6| Step: 4
Training loss: 1.8390693664550781
Validation loss: 2.07306142263515

Epoch: 6| Step: 5
Training loss: 2.2466790676116943
Validation loss: 2.073702235375681

Epoch: 6| Step: 6
Training loss: 2.109623908996582
Validation loss: 2.0851646559212798

Epoch: 6| Step: 7
Training loss: 1.7895355224609375
Validation loss: 2.085677295602778

Epoch: 6| Step: 8
Training loss: 1.9884215593338013
Validation loss: 2.0798968627888668

Epoch: 6| Step: 9
Training loss: 2.104469060897827
Validation loss: 2.0592115220203193

Epoch: 6| Step: 10
Training loss: 2.7223362922668457
Validation loss: 2.1118884522427797

Epoch: 6| Step: 11
Training loss: 2.0813465118408203
Validation loss: 2.103831709072154

Epoch: 6| Step: 12
Training loss: 2.6402437686920166
Validation loss: 2.115726129983061

Epoch: 6| Step: 13
Training loss: 1.9271526336669922
Validation loss: 2.088634644785235

Epoch: 128| Step: 0
Training loss: 3.0392231941223145
Validation loss: 2.0620951242344354

Epoch: 6| Step: 1
Training loss: 2.2445411682128906
Validation loss: 2.070390485948132

Epoch: 6| Step: 2
Training loss: 1.7927336692810059
Validation loss: 2.0870275138526835

Epoch: 6| Step: 3
Training loss: 2.0517568588256836
Validation loss: 2.0792973272262083

Epoch: 6| Step: 4
Training loss: 2.1827425956726074
Validation loss: 2.0741426867823445

Epoch: 6| Step: 5
Training loss: 2.6369400024414062
Validation loss: 2.069428061926237

Epoch: 6| Step: 6
Training loss: 2.0106306076049805
Validation loss: 2.065784526127641

Epoch: 6| Step: 7
Training loss: 1.6266276836395264
Validation loss: 2.048308121260776

Epoch: 6| Step: 8
Training loss: 2.637057304382324
Validation loss: 2.0567496386907433

Epoch: 6| Step: 9
Training loss: 1.2912843227386475
Validation loss: 2.0549097650794574

Epoch: 6| Step: 10
Training loss: 2.470909357070923
Validation loss: 2.0420583255829348

Epoch: 6| Step: 11
Training loss: 2.453700304031372
Validation loss: 2.0523165989947576

Epoch: 6| Step: 12
Training loss: 2.82753586769104
Validation loss: 2.046199922920555

Epoch: 6| Step: 13
Training loss: 2.510566473007202
Validation loss: 2.0795084148324947

Epoch: 129| Step: 0
Training loss: 2.3628158569335938
Validation loss: 2.0494022407839374

Epoch: 6| Step: 1
Training loss: 2.2807867527008057
Validation loss: 2.0574215740285893

Epoch: 6| Step: 2
Training loss: 1.8982808589935303
Validation loss: 2.051854936025476

Epoch: 6| Step: 3
Training loss: 2.1212024688720703
Validation loss: 2.089753262458309

Epoch: 6| Step: 4
Training loss: 1.862535834312439
Validation loss: 2.0646305917411722

Epoch: 6| Step: 5
Training loss: 2.8593273162841797
Validation loss: 2.0519688231970674

Epoch: 6| Step: 6
Training loss: 2.431328058242798
Validation loss: 2.0632876298760854

Epoch: 6| Step: 7
Training loss: 2.3942949771881104
Validation loss: 2.063663908230361

Epoch: 6| Step: 8
Training loss: 2.442775011062622
Validation loss: 2.104482299538069

Epoch: 6| Step: 9
Training loss: 2.0389041900634766
Validation loss: 2.073296549499676

Epoch: 6| Step: 10
Training loss: 2.373302459716797
Validation loss: 2.0669512735900057

Epoch: 6| Step: 11
Training loss: 2.1022329330444336
Validation loss: 2.075434059225103

Epoch: 6| Step: 12
Training loss: 2.2545626163482666
Validation loss: 2.0651437467144382

Epoch: 6| Step: 13
Training loss: 1.9168543815612793
Validation loss: 2.076720758150983

Epoch: 130| Step: 0
Training loss: 2.260500907897949
Validation loss: 2.0746852287682156

Epoch: 6| Step: 1
Training loss: 2.356135845184326
Validation loss: 2.0790512946344193

Epoch: 6| Step: 2
Training loss: 1.9123156070709229
Validation loss: 2.065322652939827

Epoch: 6| Step: 3
Training loss: 2.1882805824279785
Validation loss: 2.0869124832973687

Epoch: 6| Step: 4
Training loss: 2.9894657135009766
Validation loss: 2.0822465804315384

Epoch: 6| Step: 5
Training loss: 2.316047430038452
Validation loss: 2.0846798496861614

Epoch: 6| Step: 6
Training loss: 2.3844311237335205
Validation loss: 2.0932321035733787

Epoch: 6| Step: 7
Training loss: 2.2518038749694824
Validation loss: 2.0679474107680784

Epoch: 6| Step: 8
Training loss: 1.830806851387024
Validation loss: 2.092456756099578

Epoch: 6| Step: 9
Training loss: 1.7943744659423828
Validation loss: 2.0629326092299594

Epoch: 6| Step: 10
Training loss: 2.1615116596221924
Validation loss: 2.080699492526311

Epoch: 6| Step: 11
Training loss: 2.8357577323913574
Validation loss: 2.0800582593487156

Epoch: 6| Step: 12
Training loss: 1.9324796199798584
Validation loss: 2.074175286036666

Epoch: 6| Step: 13
Training loss: 2.1484272480010986
Validation loss: 2.0956441869017897

Epoch: 131| Step: 0
Training loss: 1.6357847452163696
Validation loss: 2.079401141853743

Epoch: 6| Step: 1
Training loss: 2.8077025413513184
Validation loss: 2.0523536743656283

Epoch: 6| Step: 2
Training loss: 2.2080202102661133
Validation loss: 2.0678251930462417

Epoch: 6| Step: 3
Training loss: 2.441042423248291
Validation loss: 2.0517050963576122

Epoch: 6| Step: 4
Training loss: 2.165349006652832
Validation loss: 2.0644470171261857

Epoch: 6| Step: 5
Training loss: 2.4988346099853516
Validation loss: 2.0402673341894664

Epoch: 6| Step: 6
Training loss: 1.823329210281372
Validation loss: 2.06142605504682

Epoch: 6| Step: 7
Training loss: 1.9725993871688843
Validation loss: 2.072673536116077

Epoch: 6| Step: 8
Training loss: 1.9158707857131958
Validation loss: 2.0532197490815194

Epoch: 6| Step: 9
Training loss: 2.027026653289795
Validation loss: 2.052585538997445

Epoch: 6| Step: 10
Training loss: 2.754002571105957
Validation loss: 2.0525088579423967

Epoch: 6| Step: 11
Training loss: 2.2502973079681396
Validation loss: 2.04545319977627

Epoch: 6| Step: 12
Training loss: 2.4873127937316895
Validation loss: 2.0389242787514963

Epoch: 6| Step: 13
Training loss: 2.2739415168762207
Validation loss: 2.0662380380015217

Epoch: 132| Step: 0
Training loss: 2.4232072830200195
Validation loss: 2.048275778370519

Epoch: 6| Step: 1
Training loss: 1.6077542304992676
Validation loss: 2.0370359100321287

Epoch: 6| Step: 2
Training loss: 1.58662748336792
Validation loss: 2.0524948040644326

Epoch: 6| Step: 3
Training loss: 1.9194071292877197
Validation loss: 2.036653886559189

Epoch: 6| Step: 4
Training loss: 2.4039230346679688
Validation loss: 2.0698888148030927

Epoch: 6| Step: 5
Training loss: 2.3336217403411865
Validation loss: 2.056897078790972

Epoch: 6| Step: 6
Training loss: 2.461764335632324
Validation loss: 2.036132877872836

Epoch: 6| Step: 7
Training loss: 2.462644100189209
Validation loss: 2.0411040731655654

Epoch: 6| Step: 8
Training loss: 1.7097598314285278
Validation loss: 2.055011862067766

Epoch: 6| Step: 9
Training loss: 2.7129392623901367
Validation loss: 2.028926691701335

Epoch: 6| Step: 10
Training loss: 2.7197158336639404
Validation loss: 2.0362691494726364

Epoch: 6| Step: 11
Training loss: 2.8059158325195312
Validation loss: 2.056145878248317

Epoch: 6| Step: 12
Training loss: 1.7886087894439697
Validation loss: 2.055157958820302

Epoch: 6| Step: 13
Training loss: 3.005359649658203
Validation loss: 2.064111767276641

Epoch: 133| Step: 0
Training loss: 2.620741844177246
Validation loss: 2.060439602021248

Epoch: 6| Step: 1
Training loss: 2.530933380126953
Validation loss: 2.048955689194382

Epoch: 6| Step: 2
Training loss: 2.258911371231079
Validation loss: 2.0556315991186325

Epoch: 6| Step: 3
Training loss: 2.110592842102051
Validation loss: 2.054795518998177

Epoch: 6| Step: 4
Training loss: 1.4146382808685303
Validation loss: 2.0711770032041814

Epoch: 6| Step: 5
Training loss: 2.912008762359619
Validation loss: 2.0696243291260092

Epoch: 6| Step: 6
Training loss: 2.0692858695983887
Validation loss: 2.0689683806511665

Epoch: 6| Step: 7
Training loss: 1.6651222705841064
Validation loss: 2.061038000609285

Epoch: 6| Step: 8
Training loss: 1.9799221754074097
Validation loss: 2.067879843455489

Epoch: 6| Step: 9
Training loss: 2.385067939758301
Validation loss: 2.065492432604554

Epoch: 6| Step: 10
Training loss: 2.8140578269958496
Validation loss: 2.040614098630926

Epoch: 6| Step: 11
Training loss: 2.3038439750671387
Validation loss: 2.0693922427392777

Epoch: 6| Step: 12
Training loss: 1.6336638927459717
Validation loss: 2.067159938555892

Epoch: 6| Step: 13
Training loss: 2.904423713684082
Validation loss: 2.039350977507971

Epoch: 134| Step: 0
Training loss: 1.7515268325805664
Validation loss: 2.0648466925467215

Epoch: 6| Step: 1
Training loss: 1.9491811990737915
Validation loss: 2.091658607605965

Epoch: 6| Step: 2
Training loss: 1.9866466522216797
Validation loss: 2.078229227373677

Epoch: 6| Step: 3
Training loss: 2.1524767875671387
Validation loss: 2.069704594150666

Epoch: 6| Step: 4
Training loss: 1.7944188117980957
Validation loss: 2.0616770611014417

Epoch: 6| Step: 5
Training loss: 2.2706446647644043
Validation loss: 2.0597104257152927

Epoch: 6| Step: 6
Training loss: 2.831153392791748
Validation loss: 2.0737696898880826

Epoch: 6| Step: 7
Training loss: 1.9645260572433472
Validation loss: 2.085411105104672

Epoch: 6| Step: 8
Training loss: 2.3372673988342285
Validation loss: 2.0722604400368145

Epoch: 6| Step: 9
Training loss: 2.4896812438964844
Validation loss: 2.0636166885334957

Epoch: 6| Step: 10
Training loss: 3.128258466720581
Validation loss: 2.087021299587783

Epoch: 6| Step: 11
Training loss: 1.913509488105774
Validation loss: 2.045856591193907

Epoch: 6| Step: 12
Training loss: 2.269850492477417
Validation loss: 2.089263591715085

Epoch: 6| Step: 13
Training loss: 3.0137088298797607
Validation loss: 2.0425898375049716

Epoch: 135| Step: 0
Training loss: 2.669100046157837
Validation loss: 2.0730918735586186

Epoch: 6| Step: 1
Training loss: 1.845017910003662
Validation loss: 2.076499044254262

Epoch: 6| Step: 2
Training loss: 1.9134690761566162
Validation loss: 2.0782731117740756

Epoch: 6| Step: 3
Training loss: 2.3997654914855957
Validation loss: 2.0915736665007887

Epoch: 6| Step: 4
Training loss: 2.5393285751342773
Validation loss: 2.0566324956955446

Epoch: 6| Step: 5
Training loss: 2.1983702182769775
Validation loss: 2.068366824939687

Epoch: 6| Step: 6
Training loss: 2.966118335723877
Validation loss: 2.0925437045353714

Epoch: 6| Step: 7
Training loss: 2.216851234436035
Validation loss: 2.055850254592075

Epoch: 6| Step: 8
Training loss: 2.22271728515625
Validation loss: 2.0887210240928074

Epoch: 6| Step: 9
Training loss: 2.0710864067077637
Validation loss: 2.067804754421275

Epoch: 6| Step: 10
Training loss: 2.0328078269958496
Validation loss: 2.05494579961223

Epoch: 6| Step: 11
Training loss: 1.789132833480835
Validation loss: 2.05696443844867

Epoch: 6| Step: 12
Training loss: 2.574094772338867
Validation loss: 2.065980024235223

Epoch: 6| Step: 13
Training loss: 1.8941519260406494
Validation loss: 2.0690924147123932

Epoch: 136| Step: 0
Training loss: 2.755016803741455
Validation loss: 2.059293803348336

Epoch: 6| Step: 1
Training loss: 2.090430736541748
Validation loss: 2.0687246181631602

Epoch: 6| Step: 2
Training loss: 2.8037421703338623
Validation loss: 2.0457258506487777

Epoch: 6| Step: 3
Training loss: 1.9286271333694458
Validation loss: 2.0423459596531366

Epoch: 6| Step: 4
Training loss: 2.4716081619262695
Validation loss: 2.0489792593063845

Epoch: 6| Step: 5
Training loss: 2.33609938621521
Validation loss: 2.0467993469648462

Epoch: 6| Step: 6
Training loss: 1.3018863201141357
Validation loss: 2.0524356467749483

Epoch: 6| Step: 7
Training loss: 2.1926934719085693
Validation loss: 2.0579385244718162

Epoch: 6| Step: 8
Training loss: 2.3090038299560547
Validation loss: 2.0467680500399683

Epoch: 6| Step: 9
Training loss: 1.7072604894638062
Validation loss: 2.0372928829603296

Epoch: 6| Step: 10
Training loss: 2.5790395736694336
Validation loss: 2.052413009828137

Epoch: 6| Step: 11
Training loss: 2.736758232116699
Validation loss: 2.039794134837325

Epoch: 6| Step: 12
Training loss: 2.168600559234619
Validation loss: 2.0347803869555072

Epoch: 6| Step: 13
Training loss: 2.5550637245178223
Validation loss: 2.0697470416304884

Epoch: 137| Step: 0
Training loss: 2.6076390743255615
Validation loss: 2.0304029885158745

Epoch: 6| Step: 1
Training loss: 2.1166038513183594
Validation loss: 2.0636659194064397

Epoch: 6| Step: 2
Training loss: 2.5935330390930176
Validation loss: 2.0634621317668627

Epoch: 6| Step: 3
Training loss: 2.5165510177612305
Validation loss: 2.057255850043348

Epoch: 6| Step: 4
Training loss: 2.0269765853881836
Validation loss: 2.068642376571573

Epoch: 6| Step: 5
Training loss: 1.940051555633545
Validation loss: 2.0649871621080624

Epoch: 6| Step: 6
Training loss: 1.790867805480957
Validation loss: 2.059005352758592

Epoch: 6| Step: 7
Training loss: 2.17207670211792
Validation loss: 2.051211210989183

Epoch: 6| Step: 8
Training loss: 2.333930492401123
Validation loss: 2.0849752400511052

Epoch: 6| Step: 9
Training loss: 2.082214832305908
Validation loss: 2.082718318508517

Epoch: 6| Step: 10
Training loss: 2.0768284797668457
Validation loss: 2.0633531590943694

Epoch: 6| Step: 11
Training loss: 2.5973691940307617
Validation loss: 2.0911307027263026

Epoch: 6| Step: 12
Training loss: 2.279928684234619
Validation loss: 2.09393149165697

Epoch: 6| Step: 13
Training loss: 2.5430617332458496
Validation loss: 2.072246575868258

Epoch: 138| Step: 0
Training loss: 1.5545122623443604
Validation loss: 2.0637734859220442

Epoch: 6| Step: 1
Training loss: 1.7493773698806763
Validation loss: 2.082989825997301

Epoch: 6| Step: 2
Training loss: 1.9969980716705322
Validation loss: 2.0784879371684086

Epoch: 6| Step: 3
Training loss: 2.473839282989502
Validation loss: 2.0755278013085805

Epoch: 6| Step: 4
Training loss: 2.4965505599975586
Validation loss: 2.086144519108598

Epoch: 6| Step: 5
Training loss: 2.071640729904175
Validation loss: 2.068853726951025

Epoch: 6| Step: 6
Training loss: 3.1898677349090576
Validation loss: 2.0763027129634732

Epoch: 6| Step: 7
Training loss: 1.8740230798721313
Validation loss: 2.0610446891477032

Epoch: 6| Step: 8
Training loss: 2.0743308067321777
Validation loss: 2.059236329088929

Epoch: 6| Step: 9
Training loss: 2.774442672729492
Validation loss: 2.039886522036727

Epoch: 6| Step: 10
Training loss: 2.1610984802246094
Validation loss: 2.0577431571099067

Epoch: 6| Step: 11
Training loss: 2.0302772521972656
Validation loss: 2.0454649925231934

Epoch: 6| Step: 12
Training loss: 2.634192943572998
Validation loss: 2.072774241047521

Epoch: 6| Step: 13
Training loss: 2.5109739303588867
Validation loss: 2.065770568386201

Epoch: 139| Step: 0
Training loss: 2.115741014480591
Validation loss: 2.0787288155606998

Epoch: 6| Step: 1
Training loss: 2.599576950073242
Validation loss: 2.0370594314349595

Epoch: 6| Step: 2
Training loss: 2.2123184204101562
Validation loss: 2.0713088550875263

Epoch: 6| Step: 3
Training loss: 1.680413842201233
Validation loss: 2.0613436698913574

Epoch: 6| Step: 4
Training loss: 1.8436601161956787
Validation loss: 2.0678661792509017

Epoch: 6| Step: 5
Training loss: 2.179919958114624
Validation loss: 2.0789379842819704

Epoch: 6| Step: 6
Training loss: 2.2631688117980957
Validation loss: 2.056924189290693

Epoch: 6| Step: 7
Training loss: 2.285562515258789
Validation loss: 2.0802551495131625

Epoch: 6| Step: 8
Training loss: 2.2942802906036377
Validation loss: 2.0710190778137534

Epoch: 6| Step: 9
Training loss: 2.2699337005615234
Validation loss: 2.0519312825254215

Epoch: 6| Step: 10
Training loss: 1.8660714626312256
Validation loss: 2.102766929134246

Epoch: 6| Step: 11
Training loss: 2.66923451423645
Validation loss: 2.0718105749417375

Epoch: 6| Step: 12
Training loss: 2.608116626739502
Validation loss: 2.070490596114948

Epoch: 6| Step: 13
Training loss: 2.680026054382324
Validation loss: 2.0664892927292855

Epoch: 140| Step: 0
Training loss: 2.372527599334717
Validation loss: 2.0391726827108734

Epoch: 6| Step: 1
Training loss: 1.8056436777114868
Validation loss: 2.078805349206412

Epoch: 6| Step: 2
Training loss: 2.9008631706237793
Validation loss: 2.0944414702794885

Epoch: 6| Step: 3
Training loss: 2.418577194213867
Validation loss: 2.0957898555263395

Epoch: 6| Step: 4
Training loss: 2.5249075889587402
Validation loss: 2.0834831153192828

Epoch: 6| Step: 5
Training loss: 2.4249379634857178
Validation loss: 2.0816876567820066

Epoch: 6| Step: 6
Training loss: 2.200979709625244
Validation loss: 2.098173508080103

Epoch: 6| Step: 7
Training loss: 2.499485731124878
Validation loss: 2.1162372571165844

Epoch: 6| Step: 8
Training loss: 1.8636574745178223
Validation loss: 2.0690901099994616

Epoch: 6| Step: 9
Training loss: 1.90095055103302
Validation loss: 2.0799606589860815

Epoch: 6| Step: 10
Training loss: 1.7226059436798096
Validation loss: 2.096032548976201

Epoch: 6| Step: 11
Training loss: 2.0668163299560547
Validation loss: 2.0680761157825427

Epoch: 6| Step: 12
Training loss: 2.220975875854492
Validation loss: 2.0944161979101037

Epoch: 6| Step: 13
Training loss: 2.2822022438049316
Validation loss: 2.079404159258771

Epoch: 141| Step: 0
Training loss: 2.3997669219970703
Validation loss: 2.079832423117853

Epoch: 6| Step: 1
Training loss: 2.785371780395508
Validation loss: 2.0979233146995626

Epoch: 6| Step: 2
Training loss: 1.5436599254608154
Validation loss: 2.077064717969587

Epoch: 6| Step: 3
Training loss: 2.4444119930267334
Validation loss: 2.078584296728975

Epoch: 6| Step: 4
Training loss: 2.2612075805664062
Validation loss: 2.077583234797242

Epoch: 6| Step: 5
Training loss: 2.527679920196533
Validation loss: 2.0744033475076

Epoch: 6| Step: 6
Training loss: 2.4851601123809814
Validation loss: 2.055621962393484

Epoch: 6| Step: 7
Training loss: 2.2373979091644287
Validation loss: 2.0765164257377706

Epoch: 6| Step: 8
Training loss: 2.060060977935791
Validation loss: 2.088886299440938

Epoch: 6| Step: 9
Training loss: 2.154433488845825
Validation loss: 2.061281224732758

Epoch: 6| Step: 10
Training loss: 2.476682424545288
Validation loss: 2.065734254416599

Epoch: 6| Step: 11
Training loss: 2.123368740081787
Validation loss: 2.0710812499446254

Epoch: 6| Step: 12
Training loss: 1.3670194149017334
Validation loss: 2.083769829042496

Epoch: 6| Step: 13
Training loss: 2.7561259269714355
Validation loss: 2.074803254937613

Epoch: 142| Step: 0
Training loss: 2.711268424987793
Validation loss: 2.0872568340711695

Epoch: 6| Step: 1
Training loss: 2.5865211486816406
Validation loss: 2.065298100953461

Epoch: 6| Step: 2
Training loss: 2.502715587615967
Validation loss: 2.0594708470888037

Epoch: 6| Step: 3
Training loss: 2.5252528190612793
Validation loss: 2.074078272747737

Epoch: 6| Step: 4
Training loss: 1.792819619178772
Validation loss: 2.090061092889437

Epoch: 6| Step: 5
Training loss: 1.7424029111862183
Validation loss: 2.066769460196136

Epoch: 6| Step: 6
Training loss: 2.5443549156188965
Validation loss: 2.062901980133467

Epoch: 6| Step: 7
Training loss: 2.2952473163604736
Validation loss: 2.045732675060149

Epoch: 6| Step: 8
Training loss: 1.897658348083496
Validation loss: 2.0678330185592815

Epoch: 6| Step: 9
Training loss: 2.75057315826416
Validation loss: 2.0736604762333695

Epoch: 6| Step: 10
Training loss: 1.9304335117340088
Validation loss: 2.0592393618758007

Epoch: 6| Step: 11
Training loss: 1.7829794883728027
Validation loss: 2.0655922171890095

Epoch: 6| Step: 12
Training loss: 1.797670602798462
Validation loss: 2.0569573397277505

Epoch: 6| Step: 13
Training loss: 2.7104337215423584
Validation loss: 2.0561171244549494

Epoch: 143| Step: 0
Training loss: 2.49623966217041
Validation loss: 2.0758755642880677

Epoch: 6| Step: 1
Training loss: 1.3701857328414917
Validation loss: 2.0740804005694646

Epoch: 6| Step: 2
Training loss: 2.4510395526885986
Validation loss: 2.065926453118683

Epoch: 6| Step: 3
Training loss: 2.543680191040039
Validation loss: 2.0496349783353907

Epoch: 6| Step: 4
Training loss: 2.233534336090088
Validation loss: 2.063369545885312

Epoch: 6| Step: 5
Training loss: 2.4619503021240234
Validation loss: 2.0853529155895276

Epoch: 6| Step: 6
Training loss: 2.278867721557617
Validation loss: 2.046770764935401

Epoch: 6| Step: 7
Training loss: 1.9587199687957764
Validation loss: 2.082155681425525

Epoch: 6| Step: 8
Training loss: 2.126678943634033
Validation loss: 2.083854465074437

Epoch: 6| Step: 9
Training loss: 2.290879249572754
Validation loss: 2.084546791609897

Epoch: 6| Step: 10
Training loss: 2.0625686645507812
Validation loss: 2.073198051862819

Epoch: 6| Step: 11
Training loss: 2.562469482421875
Validation loss: 2.09531654337401

Epoch: 6| Step: 12
Training loss: 2.1675851345062256
Validation loss: 2.089415934778029

Epoch: 6| Step: 13
Training loss: 2.2460765838623047
Validation loss: 2.101331041705224

Epoch: 144| Step: 0
Training loss: 2.050549030303955
Validation loss: 2.0818643159763788

Epoch: 6| Step: 1
Training loss: 1.7221245765686035
Validation loss: 2.0624335017255557

Epoch: 6| Step: 2
Training loss: 1.9415581226348877
Validation loss: 2.065532845835532

Epoch: 6| Step: 3
Training loss: 1.941992998123169
Validation loss: 2.069933440095635

Epoch: 6| Step: 4
Training loss: 2.700944423675537
Validation loss: 2.0755467414855957

Epoch: 6| Step: 5
Training loss: 2.0399839878082275
Validation loss: 2.092837243951777

Epoch: 6| Step: 6
Training loss: 2.292790412902832
Validation loss: 2.0961398565641014

Epoch: 6| Step: 7
Training loss: 2.0437519550323486
Validation loss: 2.0572864791398406

Epoch: 6| Step: 8
Training loss: 1.8866204023361206
Validation loss: 2.063558540036601

Epoch: 6| Step: 9
Training loss: 2.440791606903076
Validation loss: 2.055545233911084

Epoch: 6| Step: 10
Training loss: 2.7310562133789062
Validation loss: 2.0679554003541187

Epoch: 6| Step: 11
Training loss: 2.8549656867980957
Validation loss: 2.081588557971421

Epoch: 6| Step: 12
Training loss: 2.7200088500976562
Validation loss: 2.0596049985577984

Epoch: 6| Step: 13
Training loss: 1.7483010292053223
Validation loss: 2.0610114579559653

Epoch: 145| Step: 0
Training loss: 1.7635416984558105
Validation loss: 2.0490258163021458

Epoch: 6| Step: 1
Training loss: 1.5241906642913818
Validation loss: 2.0443967593613492

Epoch: 6| Step: 2
Training loss: 2.00212025642395
Validation loss: 2.0655933631363737

Epoch: 6| Step: 3
Training loss: 2.293696403503418
Validation loss: 2.0327913671411495

Epoch: 6| Step: 4
Training loss: 2.412794589996338
Validation loss: 2.0291988695821455

Epoch: 6| Step: 5
Training loss: 2.788247585296631
Validation loss: 2.056829637096774

Epoch: 6| Step: 6
Training loss: 2.4888458251953125
Validation loss: 2.0443791240774174

Epoch: 6| Step: 7
Training loss: 2.387218952178955
Validation loss: 2.0574274114383164

Epoch: 6| Step: 8
Training loss: 2.0747742652893066
Validation loss: 2.0598010414390155

Epoch: 6| Step: 9
Training loss: 2.017073154449463
Validation loss: 2.0379530934877295

Epoch: 6| Step: 10
Training loss: 1.828969955444336
Validation loss: 2.042948894603278

Epoch: 6| Step: 11
Training loss: 2.3537850379943848
Validation loss: 2.034746831463229

Epoch: 6| Step: 12
Training loss: 2.5654592514038086
Validation loss: 2.0464846831496044

Epoch: 6| Step: 13
Training loss: 2.8459274768829346
Validation loss: 2.0611717470230593

Epoch: 146| Step: 0
Training loss: 2.324737071990967
Validation loss: 2.0499209998756327

Epoch: 6| Step: 1
Training loss: 1.4262806177139282
Validation loss: 2.0651190755187825

Epoch: 6| Step: 2
Training loss: 1.5499522686004639
Validation loss: 2.064464510128062

Epoch: 6| Step: 3
Training loss: 3.2406954765319824
Validation loss: 2.062784970447581

Epoch: 6| Step: 4
Training loss: 3.542717933654785
Validation loss: 2.0424804815682034

Epoch: 6| Step: 5
Training loss: 1.5186505317687988
Validation loss: 2.0313626796968522

Epoch: 6| Step: 6
Training loss: 1.9984840154647827
Validation loss: 2.069739169971917

Epoch: 6| Step: 7
Training loss: 2.2439494132995605
Validation loss: 2.0734186069939726

Epoch: 6| Step: 8
Training loss: 2.4395413398742676
Validation loss: 2.048847944505753

Epoch: 6| Step: 9
Training loss: 2.295696258544922
Validation loss: 2.0659622453874156

Epoch: 6| Step: 10
Training loss: 2.1798853874206543
Validation loss: 2.0787149821558306

Epoch: 6| Step: 11
Training loss: 2.4242138862609863
Validation loss: 2.080010842251521

Epoch: 6| Step: 12
Training loss: 2.2330563068389893
Validation loss: 2.079634967670646

Epoch: 6| Step: 13
Training loss: 1.53048837184906
Validation loss: 2.0598725657309256

Epoch: 147| Step: 0
Training loss: 2.2573306560516357
Validation loss: 2.0769874459953717

Epoch: 6| Step: 1
Training loss: 2.4964849948883057
Validation loss: 2.0536746286576792

Epoch: 6| Step: 2
Training loss: 2.078911304473877
Validation loss: 2.0782979662700365

Epoch: 6| Step: 3
Training loss: 1.76835298538208
Validation loss: 2.057900313408144

Epoch: 6| Step: 4
Training loss: 2.6581544876098633
Validation loss: 2.0654668141436834

Epoch: 6| Step: 5
Training loss: 2.1919422149658203
Validation loss: 2.0454207440858245

Epoch: 6| Step: 6
Training loss: 1.743551254272461
Validation loss: 2.071758702237119

Epoch: 6| Step: 7
Training loss: 2.7229502201080322
Validation loss: 2.0505938863241546

Epoch: 6| Step: 8
Training loss: 2.065821647644043
Validation loss: 2.081912834157226

Epoch: 6| Step: 9
Training loss: 2.52724552154541
Validation loss: 2.076670580012824

Epoch: 6| Step: 10
Training loss: 2.1570019721984863
Validation loss: 2.090941221483292

Epoch: 6| Step: 11
Training loss: 2.522188186645508
Validation loss: 2.063241962463625

Epoch: 6| Step: 12
Training loss: 1.975034236907959
Validation loss: 2.087909754886422

Epoch: 6| Step: 13
Training loss: 2.0315160751342773
Validation loss: 2.1109508263167513

Epoch: 148| Step: 0
Training loss: 2.5835442543029785
Validation loss: 2.0986117855195077

Epoch: 6| Step: 1
Training loss: 2.516026496887207
Validation loss: 2.0956211269542737

Epoch: 6| Step: 2
Training loss: 2.2444489002227783
Validation loss: 2.075170559267844

Epoch: 6| Step: 3
Training loss: 2.9710187911987305
Validation loss: 2.0799278341313845

Epoch: 6| Step: 4
Training loss: 2.5790517330169678
Validation loss: 2.079266881430021

Epoch: 6| Step: 5
Training loss: 1.9137763977050781
Validation loss: 2.0607904208603727

Epoch: 6| Step: 6
Training loss: 2.489013910293579
Validation loss: 2.086380133064844

Epoch: 6| Step: 7
Training loss: 2.2694430351257324
Validation loss: 2.0714274234669183

Epoch: 6| Step: 8
Training loss: 2.335592746734619
Validation loss: 2.0614604437223045

Epoch: 6| Step: 9
Training loss: 2.389425754547119
Validation loss: 2.077561095196714

Epoch: 6| Step: 10
Training loss: 1.585587501525879
Validation loss: 2.046462103884707

Epoch: 6| Step: 11
Training loss: 1.8145346641540527
Validation loss: 2.0655502119371967

Epoch: 6| Step: 12
Training loss: 1.2226431369781494
Validation loss: 2.06445784466241

Epoch: 6| Step: 13
Training loss: 2.5262019634246826
Validation loss: 2.0546052584084133

Epoch: 149| Step: 0
Training loss: 2.5793938636779785
Validation loss: 2.0669198574558383

Epoch: 6| Step: 1
Training loss: 1.9927330017089844
Validation loss: 2.0765413391974663

Epoch: 6| Step: 2
Training loss: 2.404595375061035
Validation loss: 2.079299338402287

Epoch: 6| Step: 3
Training loss: 2.3989310264587402
Validation loss: 2.054039737229706

Epoch: 6| Step: 4
Training loss: 1.853665828704834
Validation loss: 2.0751528688656387

Epoch: 6| Step: 5
Training loss: 2.0296554565429688
Validation loss: 2.0567592638795094

Epoch: 6| Step: 6
Training loss: 1.9505586624145508
Validation loss: 2.061286635296319

Epoch: 6| Step: 7
Training loss: 2.333665132522583
Validation loss: 2.055099200176936

Epoch: 6| Step: 8
Training loss: 1.7752615213394165
Validation loss: 2.0598468562608123

Epoch: 6| Step: 9
Training loss: 2.9808714389801025
Validation loss: 2.045175702341141

Epoch: 6| Step: 10
Training loss: 2.177619457244873
Validation loss: 2.07427716255188

Epoch: 6| Step: 11
Training loss: 2.3615427017211914
Validation loss: 2.0427749951680503

Epoch: 6| Step: 12
Training loss: 2.2397701740264893
Validation loss: 2.0640309395328647

Epoch: 6| Step: 13
Training loss: 1.9466073513031006
Validation loss: 2.046493238018405

Epoch: 150| Step: 0
Training loss: 1.9984018802642822
Validation loss: 2.0656238602053736

Epoch: 6| Step: 1
Training loss: 2.2392983436584473
Validation loss: 2.0774706050913823

Epoch: 6| Step: 2
Training loss: 1.7823905944824219
Validation loss: 2.059792108433221

Epoch: 6| Step: 3
Training loss: 2.189225196838379
Validation loss: 2.0586961084796536

Epoch: 6| Step: 4
Training loss: 2.2976467609405518
Validation loss: 2.0635481470374653

Epoch: 6| Step: 5
Training loss: 2.09881854057312
Validation loss: 2.0838979431377944

Epoch: 6| Step: 6
Training loss: 2.5541203022003174
Validation loss: 2.0490137377092914

Epoch: 6| Step: 7
Training loss: 1.403557300567627
Validation loss: 2.083823661650381

Epoch: 6| Step: 8
Training loss: 1.6886913776397705
Validation loss: 2.0474101804917857

Epoch: 6| Step: 9
Training loss: 3.149529457092285
Validation loss: 2.0733860513215423

Epoch: 6| Step: 10
Training loss: 2.637099266052246
Validation loss: 2.0709685023112963

Epoch: 6| Step: 11
Training loss: 2.8239784240722656
Validation loss: 2.032651612835546

Epoch: 6| Step: 12
Training loss: 2.5546860694885254
Validation loss: 2.0785968252407607

Epoch: 6| Step: 13
Training loss: 2.1247129440307617
Validation loss: 2.0411854456829768

Epoch: 151| Step: 0
Training loss: 1.7834066152572632
Validation loss: 2.0669858481294368

Epoch: 6| Step: 1
Training loss: 2.856478691101074
Validation loss: 2.069915634329601

Epoch: 6| Step: 2
Training loss: 1.9764995574951172
Validation loss: 2.059119703949139

Epoch: 6| Step: 3
Training loss: 2.1578903198242188
Validation loss: 2.0589165687561035

Epoch: 6| Step: 4
Training loss: 1.739162564277649
Validation loss: 2.087716107727379

Epoch: 6| Step: 5
Training loss: 2.1705846786499023
Validation loss: 2.059129991839009

Epoch: 6| Step: 6
Training loss: 3.321113109588623
Validation loss: 2.0411474038195867

Epoch: 6| Step: 7
Training loss: 2.49147891998291
Validation loss: 2.041426052329361

Epoch: 6| Step: 8
Training loss: 2.3361239433288574
Validation loss: 2.080284659580518

Epoch: 6| Step: 9
Training loss: 2.1950912475585938
Validation loss: 2.054902022884738

Epoch: 6| Step: 10
Training loss: 2.5234978199005127
Validation loss: 2.0374745873994726

Epoch: 6| Step: 11
Training loss: 2.0001232624053955
Validation loss: 2.088850644326979

Epoch: 6| Step: 12
Training loss: 1.13507080078125
Validation loss: 2.065909972754858

Epoch: 6| Step: 13
Training loss: 3.076193332672119
Validation loss: 2.0843367217689432

Epoch: 152| Step: 0
Training loss: 1.5415325164794922
Validation loss: 2.060851150943387

Epoch: 6| Step: 1
Training loss: 2.8716697692871094
Validation loss: 2.037698482954374

Epoch: 6| Step: 2
Training loss: 2.2653050422668457
Validation loss: 2.070171099837108

Epoch: 6| Step: 3
Training loss: 2.1252787113189697
Validation loss: 2.065114120001434

Epoch: 6| Step: 4
Training loss: 2.33479642868042
Validation loss: 2.0887202447460544

Epoch: 6| Step: 5
Training loss: 2.0975356101989746
Validation loss: 2.088855666498984

Epoch: 6| Step: 6
Training loss: 2.160311222076416
Validation loss: 2.0931198545681533

Epoch: 6| Step: 7
Training loss: 1.9704868793487549
Validation loss: 2.0929613049312303

Epoch: 6| Step: 8
Training loss: 2.7197866439819336
Validation loss: 2.0672405996630268

Epoch: 6| Step: 9
Training loss: 2.476160764694214
Validation loss: 2.090429518812446

Epoch: 6| Step: 10
Training loss: 1.7130327224731445
Validation loss: 2.075285020694938

Epoch: 6| Step: 11
Training loss: 2.2579166889190674
Validation loss: 2.060818842662278

Epoch: 6| Step: 12
Training loss: 2.5176100730895996
Validation loss: 2.088459330220376

Epoch: 6| Step: 13
Training loss: 2.016181707382202
Validation loss: 2.080573753644061

Epoch: 153| Step: 0
Training loss: 2.9340081214904785
Validation loss: 2.0728729847938783

Epoch: 6| Step: 1
Training loss: 2.5349597930908203
Validation loss: 2.0884579919999644

Epoch: 6| Step: 2
Training loss: 2.5223007202148438
Validation loss: 2.0762492661835044

Epoch: 6| Step: 3
Training loss: 1.8289692401885986
Validation loss: 2.0809307354752735

Epoch: 6| Step: 4
Training loss: 1.6667921543121338
Validation loss: 2.070145494194441

Epoch: 6| Step: 5
Training loss: 2.422013282775879
Validation loss: 2.0829123091954056

Epoch: 6| Step: 6
Training loss: 2.4773175716400146
Validation loss: 2.0982525169208484

Epoch: 6| Step: 7
Training loss: 1.9876010417938232
Validation loss: 2.0826644461642028

Epoch: 6| Step: 8
Training loss: 2.174100875854492
Validation loss: 2.083571821130732

Epoch: 6| Step: 9
Training loss: 2.011539936065674
Validation loss: 2.089469937868016

Epoch: 6| Step: 10
Training loss: 2.840823173522949
Validation loss: 2.077776748646972

Epoch: 6| Step: 11
Training loss: 2.4156947135925293
Validation loss: 2.0884248389992663

Epoch: 6| Step: 12
Training loss: 1.1391254663467407
Validation loss: 2.057409183953398

Epoch: 6| Step: 13
Training loss: 2.242128610610962
Validation loss: 2.07219022832891

Epoch: 154| Step: 0
Training loss: 1.8420696258544922
Validation loss: 2.0708856710823635

Epoch: 6| Step: 1
Training loss: 2.2713215351104736
Validation loss: 2.0581270110222603

Epoch: 6| Step: 2
Training loss: 2.222161293029785
Validation loss: 2.0833898987821353

Epoch: 6| Step: 3
Training loss: 2.3416483402252197
Validation loss: 2.0756761771376415

Epoch: 6| Step: 4
Training loss: 2.420473337173462
Validation loss: 2.0729695174001876

Epoch: 6| Step: 5
Training loss: 2.633558511734009
Validation loss: 2.0953024369414135

Epoch: 6| Step: 6
Training loss: 2.5673012733459473
Validation loss: 2.069251655250467

Epoch: 6| Step: 7
Training loss: 1.851421594619751
Validation loss: 2.059476311488818

Epoch: 6| Step: 8
Training loss: 2.725440502166748
Validation loss: 2.0849759283886162

Epoch: 6| Step: 9
Training loss: 1.6056448221206665
Validation loss: 2.0818700431495585

Epoch: 6| Step: 10
Training loss: 1.5129456520080566
Validation loss: 2.093879750979844

Epoch: 6| Step: 11
Training loss: 2.1571977138519287
Validation loss: 2.0793614566967054

Epoch: 6| Step: 12
Training loss: 2.708632707595825
Validation loss: 2.092280782679076

Epoch: 6| Step: 13
Training loss: 2.6879210472106934
Validation loss: 2.0542087337022186

Epoch: 155| Step: 0
Training loss: 2.1251637935638428
Validation loss: 2.0961621730558333

Epoch: 6| Step: 1
Training loss: 1.8740602731704712
Validation loss: 2.072503084777504

Epoch: 6| Step: 2
Training loss: 1.9922490119934082
Validation loss: 2.0689626329688617

Epoch: 6| Step: 3
Training loss: 1.9796267747879028
Validation loss: 2.0623849873901694

Epoch: 6| Step: 4
Training loss: 2.6044528484344482
Validation loss: 2.0509134274657055

Epoch: 6| Step: 5
Training loss: 1.9165652990341187
Validation loss: 2.056901461334639

Epoch: 6| Step: 6
Training loss: 1.9685430526733398
Validation loss: 2.0554288830808414

Epoch: 6| Step: 7
Training loss: 3.0722503662109375
Validation loss: 2.0444384903036137

Epoch: 6| Step: 8
Training loss: 3.12362003326416
Validation loss: 2.0405487975766583

Epoch: 6| Step: 9
Training loss: 2.1040992736816406
Validation loss: 2.067535282463156

Epoch: 6| Step: 10
Training loss: 2.2742061614990234
Validation loss: 2.0698931268466416

Epoch: 6| Step: 11
Training loss: 2.100861072540283
Validation loss: 2.050268029653898

Epoch: 6| Step: 12
Training loss: 1.7404000759124756
Validation loss: 2.0770663702359764

Epoch: 6| Step: 13
Training loss: 2.3070545196533203
Validation loss: 2.0420092344284058

Epoch: 156| Step: 0
Training loss: 1.9843007326126099
Validation loss: 2.061019646224155

Epoch: 6| Step: 1
Training loss: 2.0827198028564453
Validation loss: 2.0668523657706475

Epoch: 6| Step: 2
Training loss: 2.6248950958251953
Validation loss: 2.0365125261327273

Epoch: 6| Step: 3
Training loss: 2.55763578414917
Validation loss: 2.0639879844521962

Epoch: 6| Step: 4
Training loss: 2.105699062347412
Validation loss: 2.0708437427397697

Epoch: 6| Step: 5
Training loss: 2.2878692150115967
Validation loss: 2.0591645727875414

Epoch: 6| Step: 6
Training loss: 2.4532647132873535
Validation loss: 2.072226239788917

Epoch: 6| Step: 7
Training loss: 2.6212313175201416
Validation loss: 2.047094795011705

Epoch: 6| Step: 8
Training loss: 2.3836123943328857
Validation loss: 2.0815204779307046

Epoch: 6| Step: 9
Training loss: 2.6045937538146973
Validation loss: 2.0483863020455964

Epoch: 6| Step: 10
Training loss: 1.81327223777771
Validation loss: 2.0696844618807555

Epoch: 6| Step: 11
Training loss: 1.9653462171554565
Validation loss: 2.0845630617551905

Epoch: 6| Step: 12
Training loss: 1.887258529663086
Validation loss: 2.076094264625221

Epoch: 6| Step: 13
Training loss: 1.7455700635910034
Validation loss: 2.0802686470811085

Epoch: 157| Step: 0
Training loss: 2.6403820514678955
Validation loss: 2.0666628217184417

Epoch: 6| Step: 1
Training loss: 2.8389673233032227
Validation loss: 2.0896560568963327

Epoch: 6| Step: 2
Training loss: 2.1237478256225586
Validation loss: 2.0772892159800374

Epoch: 6| Step: 3
Training loss: 1.9960789680480957
Validation loss: 2.069841049050772

Epoch: 6| Step: 4
Training loss: 1.937332272529602
Validation loss: 2.078170209802607

Epoch: 6| Step: 5
Training loss: 2.2939443588256836
Validation loss: 2.0843681494394937

Epoch: 6| Step: 6
Training loss: 1.8310515880584717
Validation loss: 2.0925687871953493

Epoch: 6| Step: 7
Training loss: 1.7981799840927124
Validation loss: 2.101896292419844

Epoch: 6| Step: 8
Training loss: 2.1235404014587402
Validation loss: 2.0820565326239473

Epoch: 6| Step: 9
Training loss: 3.472151517868042
Validation loss: 2.0980708432453934

Epoch: 6| Step: 10
Training loss: 1.8265790939331055
Validation loss: 2.1165779136842295

Epoch: 6| Step: 11
Training loss: 2.6305158138275146
Validation loss: 2.1337905135205997

Epoch: 6| Step: 12
Training loss: 1.8508095741271973
Validation loss: 2.1096379539018035

Epoch: 6| Step: 13
Training loss: 1.7037575244903564
Validation loss: 2.1008635464534966

Epoch: 158| Step: 0
Training loss: 2.0150954723358154
Validation loss: 2.095939508048437

Epoch: 6| Step: 1
Training loss: 1.806981086730957
Validation loss: 2.0864210705603323

Epoch: 6| Step: 2
Training loss: 2.131437063217163
Validation loss: 2.0827741904925277

Epoch: 6| Step: 3
Training loss: 2.546966552734375
Validation loss: 2.0790205053103867

Epoch: 6| Step: 4
Training loss: 1.5038458108901978
Validation loss: 2.0908027220797796

Epoch: 6| Step: 5
Training loss: 2.363290786743164
Validation loss: 2.06264070797992

Epoch: 6| Step: 6
Training loss: 2.1493005752563477
Validation loss: 2.1163509750878937

Epoch: 6| Step: 7
Training loss: 2.307101249694824
Validation loss: 2.064254096759263

Epoch: 6| Step: 8
Training loss: 2.116224765777588
Validation loss: 2.087223469570119

Epoch: 6| Step: 9
Training loss: 2.415518283843994
Validation loss: 2.0991302818380375

Epoch: 6| Step: 10
Training loss: 2.6688690185546875
Validation loss: 2.060058770641204

Epoch: 6| Step: 11
Training loss: 2.8802480697631836
Validation loss: 2.0532494360400784

Epoch: 6| Step: 12
Training loss: 1.9846314191818237
Validation loss: 2.0724014171990017

Epoch: 6| Step: 13
Training loss: 2.252364158630371
Validation loss: 2.076684567236131

Epoch: 159| Step: 0
Training loss: 2.5615081787109375
Validation loss: 2.071909995489223

Epoch: 6| Step: 1
Training loss: 2.4915971755981445
Validation loss: 2.0524666129901843

Epoch: 6| Step: 2
Training loss: 2.3031625747680664
Validation loss: 2.068586234123476

Epoch: 6| Step: 3
Training loss: 2.441579818725586
Validation loss: 2.0823560043047835

Epoch: 6| Step: 4
Training loss: 2.4902701377868652
Validation loss: 2.0587972159026773

Epoch: 6| Step: 5
Training loss: 2.611241340637207
Validation loss: 2.049253399654101

Epoch: 6| Step: 6
Training loss: 2.3823211193084717
Validation loss: 2.0892041114068802

Epoch: 6| Step: 7
Training loss: 1.4206035137176514
Validation loss: 2.0749306883863223

Epoch: 6| Step: 8
Training loss: 2.3961663246154785
Validation loss: 2.0755452853377148

Epoch: 6| Step: 9
Training loss: 1.821296215057373
Validation loss: 2.059181091605976

Epoch: 6| Step: 10
Training loss: 2.308168888092041
Validation loss: 2.0689608602113623

Epoch: 6| Step: 11
Training loss: 2.251455545425415
Validation loss: 2.0695114404924455

Epoch: 6| Step: 12
Training loss: 2.0194554328918457
Validation loss: 2.08062623008605

Epoch: 6| Step: 13
Training loss: 1.3892642259597778
Validation loss: 2.0464309774419314

Epoch: 160| Step: 0
Training loss: 1.8629471063613892
Validation loss: 2.059075629839333

Epoch: 6| Step: 1
Training loss: 2.6769423484802246
Validation loss: 2.068462437199008

Epoch: 6| Step: 2
Training loss: 1.9989192485809326
Validation loss: 2.0755811378520024

Epoch: 6| Step: 3
Training loss: 2.200643539428711
Validation loss: 2.0540850303506337

Epoch: 6| Step: 4
Training loss: 1.9749975204467773
Validation loss: 2.0620160641208773

Epoch: 6| Step: 5
Training loss: 2.6042208671569824
Validation loss: 2.0623408620075514

Epoch: 6| Step: 6
Training loss: 2.4927053451538086
Validation loss: 2.0745245654095887

Epoch: 6| Step: 7
Training loss: 2.432011127471924
Validation loss: 2.05507545445555

Epoch: 6| Step: 8
Training loss: 1.9613080024719238
Validation loss: 2.057232874695973

Epoch: 6| Step: 9
Training loss: 1.922734260559082
Validation loss: 2.056337715477072

Epoch: 6| Step: 10
Training loss: 2.690885066986084
Validation loss: 2.0504165900650846

Epoch: 6| Step: 11
Training loss: 2.0612990856170654
Validation loss: 2.0771918706996466

Epoch: 6| Step: 12
Training loss: 2.3991026878356934
Validation loss: 2.0913455114569715

Epoch: 6| Step: 13
Training loss: 1.450079321861267
Validation loss: 2.0735780398050943

Epoch: 161| Step: 0
Training loss: 2.193183422088623
Validation loss: 2.0748358336828088

Epoch: 6| Step: 1
Training loss: 2.8322415351867676
Validation loss: 2.0646596621441584

Epoch: 6| Step: 2
Training loss: 1.5443775653839111
Validation loss: 2.0676408531845256

Epoch: 6| Step: 3
Training loss: 2.1827847957611084
Validation loss: 2.0383367205178864

Epoch: 6| Step: 4
Training loss: 2.1828529834747314
Validation loss: 2.0520265794569448

Epoch: 6| Step: 5
Training loss: 2.535578489303589
Validation loss: 2.081585871276035

Epoch: 6| Step: 6
Training loss: 1.790470004081726
Validation loss: 2.0715269529691307

Epoch: 6| Step: 7
Training loss: 1.756903052330017
Validation loss: 2.0582403495747554

Epoch: 6| Step: 8
Training loss: 2.4755077362060547
Validation loss: 2.0781006377230407

Epoch: 6| Step: 9
Training loss: 1.9845126867294312
Validation loss: 2.048086617582588

Epoch: 6| Step: 10
Training loss: 1.8724679946899414
Validation loss: 2.0556521364437637

Epoch: 6| Step: 11
Training loss: 3.0401787757873535
Validation loss: 2.024211325953084

Epoch: 6| Step: 12
Training loss: 2.4936771392822266
Validation loss: 2.04319627054276

Epoch: 6| Step: 13
Training loss: 2.2012109756469727
Validation loss: 2.0474999617504817

Epoch: 162| Step: 0
Training loss: 2.4897384643554688
Validation loss: 2.0507666487847604

Epoch: 6| Step: 1
Training loss: 2.038541793823242
Validation loss: 2.0712167088703444

Epoch: 6| Step: 2
Training loss: 1.5100017786026
Validation loss: 2.0313286550583376

Epoch: 6| Step: 3
Training loss: 2.0452046394348145
Validation loss: 2.072949549203278

Epoch: 6| Step: 4
Training loss: 2.2449612617492676
Validation loss: 2.060788449420724

Epoch: 6| Step: 5
Training loss: 2.700746536254883
Validation loss: 2.115264415740967

Epoch: 6| Step: 6
Training loss: 2.440810441970825
Validation loss: 2.0626024853798652

Epoch: 6| Step: 7
Training loss: 2.5938234329223633
Validation loss: 2.0827381713415987

Epoch: 6| Step: 8
Training loss: 1.7807056903839111
Validation loss: 2.070567536097701

Epoch: 6| Step: 9
Training loss: 2.2140772342681885
Validation loss: 2.06345901437985

Epoch: 6| Step: 10
Training loss: 2.2595229148864746
Validation loss: 2.0635540908382786

Epoch: 6| Step: 11
Training loss: 1.9332143068313599
Validation loss: 2.0840505989648963

Epoch: 6| Step: 12
Training loss: 2.5155985355377197
Validation loss: 2.077798212728193

Epoch: 6| Step: 13
Training loss: 2.3959665298461914
Validation loss: 2.092583658874676

Epoch: 163| Step: 0
Training loss: 2.29583477973938
Validation loss: 2.0714614673327376

Epoch: 6| Step: 1
Training loss: 1.9489942789077759
Validation loss: 2.088741094835343

Epoch: 6| Step: 2
Training loss: 2.5860257148742676
Validation loss: 2.0546276800094114

Epoch: 6| Step: 3
Training loss: 2.54060959815979
Validation loss: 2.0781380450853737

Epoch: 6| Step: 4
Training loss: 2.3470571041107178
Validation loss: 2.0823587576548257

Epoch: 6| Step: 5
Training loss: 2.228080987930298
Validation loss: 2.0993607787675757

Epoch: 6| Step: 6
Training loss: 1.695878028869629
Validation loss: 2.0751974121216805

Epoch: 6| Step: 7
Training loss: 1.932246208190918
Validation loss: 2.0925786879754837

Epoch: 6| Step: 8
Training loss: 2.8539140224456787
Validation loss: 2.1058573081929195

Epoch: 6| Step: 9
Training loss: 1.9159300327301025
Validation loss: 2.099719093691918

Epoch: 6| Step: 10
Training loss: 1.5010626316070557
Validation loss: 2.094284780563847

Epoch: 6| Step: 11
Training loss: 2.677799701690674
Validation loss: 2.086197458287721

Epoch: 6| Step: 12
Training loss: 2.2093234062194824
Validation loss: 2.1235006022196945

Epoch: 6| Step: 13
Training loss: 2.4992868900299072
Validation loss: 2.1166369761190107

Epoch: 164| Step: 0
Training loss: 2.0111608505249023
Validation loss: 2.100397435567712

Epoch: 6| Step: 1
Training loss: 2.324700355529785
Validation loss: 2.0740940288830827

Epoch: 6| Step: 2
Training loss: 2.325058937072754
Validation loss: 2.0823030984529884

Epoch: 6| Step: 3
Training loss: 1.8753173351287842
Validation loss: 2.082942521700295

Epoch: 6| Step: 4
Training loss: 2.1721620559692383
Validation loss: 2.0599023206259615

Epoch: 6| Step: 5
Training loss: 2.334272861480713
Validation loss: 2.0784512232708674

Epoch: 6| Step: 6
Training loss: 1.8923479318618774
Validation loss: 2.0632580595631755

Epoch: 6| Step: 7
Training loss: 2.459713935852051
Validation loss: 2.0613113308465607

Epoch: 6| Step: 8
Training loss: 2.066586494445801
Validation loss: 2.0700958492935344

Epoch: 6| Step: 9
Training loss: 2.5531883239746094
Validation loss: 2.050066401881556

Epoch: 6| Step: 10
Training loss: 2.508234977722168
Validation loss: 2.0720910128726753

Epoch: 6| Step: 11
Training loss: 2.3067355155944824
Validation loss: 2.0547301461619716

Epoch: 6| Step: 12
Training loss: 2.2125556468963623
Validation loss: 2.075606175648269

Epoch: 6| Step: 13
Training loss: 1.973214864730835
Validation loss: 2.068488356887653

Epoch: 165| Step: 0
Training loss: 1.7186322212219238
Validation loss: 2.0657913864299817

Epoch: 6| Step: 1
Training loss: 2.3834152221679688
Validation loss: 2.0797933763073337

Epoch: 6| Step: 2
Training loss: 2.119194984436035
Validation loss: 2.055580603179111

Epoch: 6| Step: 3
Training loss: 2.751753807067871
Validation loss: 2.0603480621050765

Epoch: 6| Step: 4
Training loss: 3.0156326293945312
Validation loss: 2.0497416091221634

Epoch: 6| Step: 5
Training loss: 1.9408247470855713
Validation loss: 2.050454995965445

Epoch: 6| Step: 6
Training loss: 1.415442705154419
Validation loss: 2.0475222423512447

Epoch: 6| Step: 7
Training loss: 2.0498363971710205
Validation loss: 2.0319975101819603

Epoch: 6| Step: 8
Training loss: 1.5771710872650146
Validation loss: 2.060666084289551

Epoch: 6| Step: 9
Training loss: 2.0268778800964355
Validation loss: 2.0658980274713166

Epoch: 6| Step: 10
Training loss: 2.912156105041504
Validation loss: 2.0606777847454114

Epoch: 6| Step: 11
Training loss: 2.091036319732666
Validation loss: 2.053223207432737

Epoch: 6| Step: 12
Training loss: 2.522156238555908
Validation loss: 2.0401633298525246

Epoch: 6| Step: 13
Training loss: 2.7226028442382812
Validation loss: 2.0422769490108696

Epoch: 166| Step: 0
Training loss: 1.9351109266281128
Validation loss: 2.0528092409974787

Epoch: 6| Step: 1
Training loss: 3.0670454502105713
Validation loss: 2.052717880536151

Epoch: 6| Step: 2
Training loss: 2.5290756225585938
Validation loss: 2.059576419091994

Epoch: 6| Step: 3
Training loss: 2.5799784660339355
Validation loss: 2.0676632004399456

Epoch: 6| Step: 4
Training loss: 1.9166948795318604
Validation loss: 2.0533322826508553

Epoch: 6| Step: 5
Training loss: 2.456228256225586
Validation loss: 2.07873301224042

Epoch: 6| Step: 6
Training loss: 2.5482382774353027
Validation loss: 2.082995940280217

Epoch: 6| Step: 7
Training loss: 1.8154418468475342
Validation loss: 2.0834278342544392

Epoch: 6| Step: 8
Training loss: 2.037116527557373
Validation loss: 2.0713400584395214

Epoch: 6| Step: 9
Training loss: 1.3328725099563599
Validation loss: 2.0766908199556413

Epoch: 6| Step: 10
Training loss: 2.0045437812805176
Validation loss: 2.0822395227288686

Epoch: 6| Step: 11
Training loss: 2.1804871559143066
Validation loss: 2.086105231315859

Epoch: 6| Step: 12
Training loss: 2.9394850730895996
Validation loss: 2.098883041771509

Epoch: 6| Step: 13
Training loss: 2.137486457824707
Validation loss: 2.0988892021999566

Epoch: 167| Step: 0
Training loss: 1.8969058990478516
Validation loss: 2.1009739624556674

Epoch: 6| Step: 1
Training loss: 2.8003005981445312
Validation loss: 2.0851782573166715

Epoch: 6| Step: 2
Training loss: 1.840093731880188
Validation loss: 2.0872994340876097

Epoch: 6| Step: 3
Training loss: 2.3415474891662598
Validation loss: 2.097177026092365

Epoch: 6| Step: 4
Training loss: 1.325296401977539
Validation loss: 2.076125592313787

Epoch: 6| Step: 5
Training loss: 1.8488528728485107
Validation loss: 2.07088166411205

Epoch: 6| Step: 6
Training loss: 2.362821578979492
Validation loss: 2.0665874686292423

Epoch: 6| Step: 7
Training loss: 2.4794039726257324
Validation loss: 2.0766608522784327

Epoch: 6| Step: 8
Training loss: 2.186244010925293
Validation loss: 2.076202510505594

Epoch: 6| Step: 9
Training loss: 2.752375364303589
Validation loss: 2.057627406171573

Epoch: 6| Step: 10
Training loss: 2.1289114952087402
Validation loss: 2.077875875657605

Epoch: 6| Step: 11
Training loss: 1.8256986141204834
Validation loss: 2.0528160243906

Epoch: 6| Step: 12
Training loss: 2.640712261199951
Validation loss: 2.095104948166878

Epoch: 6| Step: 13
Training loss: 2.9289894104003906
Validation loss: 2.058739019978431

Epoch: 168| Step: 0
Training loss: 2.097245693206787
Validation loss: 2.0807150999704995

Epoch: 6| Step: 1
Training loss: 2.2822651863098145
Validation loss: 2.0603293193283903

Epoch: 6| Step: 2
Training loss: 2.8162105083465576
Validation loss: 2.0645141652835313

Epoch: 6| Step: 3
Training loss: 1.6665070056915283
Validation loss: 2.0871254679977254

Epoch: 6| Step: 4
Training loss: 2.189424514770508
Validation loss: 2.070669456194806

Epoch: 6| Step: 5
Training loss: 1.7263764142990112
Validation loss: 2.0802999234968618

Epoch: 6| Step: 6
Training loss: 2.5026397705078125
Validation loss: 2.0439746341397687

Epoch: 6| Step: 7
Training loss: 2.1198694705963135
Validation loss: 2.0457978735687914

Epoch: 6| Step: 8
Training loss: 2.470188856124878
Validation loss: 2.063226884411227

Epoch: 6| Step: 9
Training loss: 2.5311827659606934
Validation loss: 2.065027689421049

Epoch: 6| Step: 10
Training loss: 1.9647955894470215
Validation loss: 2.0686959989609255

Epoch: 6| Step: 11
Training loss: 2.9506752490997314
Validation loss: 2.064974995069606

Epoch: 6| Step: 12
Training loss: 1.7745647430419922
Validation loss: 2.0732180380052134

Epoch: 6| Step: 13
Training loss: 2.13163685798645
Validation loss: 2.07928071611671

Epoch: 169| Step: 0
Training loss: 1.8742127418518066
Validation loss: 2.076536370861915

Epoch: 6| Step: 1
Training loss: 2.9919209480285645
Validation loss: 2.0872014696880052

Epoch: 6| Step: 2
Training loss: 2.3301029205322266
Validation loss: 2.0584909146831882

Epoch: 6| Step: 3
Training loss: 2.532958507537842
Validation loss: 2.0866018418342835

Epoch: 6| Step: 4
Training loss: 1.566004991531372
Validation loss: 2.0883165790188696

Epoch: 6| Step: 5
Training loss: 2.15684175491333
Validation loss: 2.069854913219329

Epoch: 6| Step: 6
Training loss: 2.747629165649414
Validation loss: 2.08151162311595

Epoch: 6| Step: 7
Training loss: 1.4760401248931885
Validation loss: 2.0516949763862034

Epoch: 6| Step: 8
Training loss: 2.6015501022338867
Validation loss: 2.068213326956636

Epoch: 6| Step: 9
Training loss: 2.3592681884765625
Validation loss: 2.0701135666139665

Epoch: 6| Step: 10
Training loss: 2.833855152130127
Validation loss: 2.0946638661046184

Epoch: 6| Step: 11
Training loss: 1.1760718822479248
Validation loss: 2.063688079516093

Epoch: 6| Step: 12
Training loss: 2.0996885299682617
Validation loss: 2.0561648876436296

Epoch: 6| Step: 13
Training loss: 2.6485235691070557
Validation loss: 2.060294779398108

Epoch: 170| Step: 0
Training loss: 3.054307460784912
Validation loss: 2.059284105095812

Epoch: 6| Step: 1
Training loss: 2.6174099445343018
Validation loss: 2.050904581623693

Epoch: 6| Step: 2
Training loss: 1.9250099658966064
Validation loss: 2.067969872105506

Epoch: 6| Step: 3
Training loss: 2.0413765907287598
Validation loss: 2.066105898990426

Epoch: 6| Step: 4
Training loss: 1.7847135066986084
Validation loss: 2.0531867755356656

Epoch: 6| Step: 5
Training loss: 2.007345676422119
Validation loss: 2.0649916894974245

Epoch: 6| Step: 6
Training loss: 2.5336127281188965
Validation loss: 2.064217600771176

Epoch: 6| Step: 7
Training loss: 2.154125213623047
Validation loss: 2.0528486749177337

Epoch: 6| Step: 8
Training loss: 2.0443217754364014
Validation loss: 2.0643600571540093

Epoch: 6| Step: 9
Training loss: 2.0733487606048584
Validation loss: 2.083745769275132

Epoch: 6| Step: 10
Training loss: 1.7701491117477417
Validation loss: 2.0478814032769974

Epoch: 6| Step: 11
Training loss: 2.095534324645996
Validation loss: 2.069516055045589

Epoch: 6| Step: 12
Training loss: 2.724087953567505
Validation loss: 2.0610106350273214

Epoch: 6| Step: 13
Training loss: 2.198197603225708
Validation loss: 2.055149483424361

Epoch: 171| Step: 0
Training loss: 1.274638295173645
Validation loss: 2.059064584393655

Epoch: 6| Step: 1
Training loss: 1.9941225051879883
Validation loss: 2.05969399534246

Epoch: 6| Step: 2
Training loss: 2.7631659507751465
Validation loss: 2.0720137344893588

Epoch: 6| Step: 3
Training loss: 1.9745419025421143
Validation loss: 2.049773822548569

Epoch: 6| Step: 4
Training loss: 1.960373878479004
Validation loss: 2.0657216502774145

Epoch: 6| Step: 5
Training loss: 2.2638282775878906
Validation loss: 2.066586080417838

Epoch: 6| Step: 6
Training loss: 1.830560564994812
Validation loss: 2.0741193653434835

Epoch: 6| Step: 7
Training loss: 1.8811311721801758
Validation loss: 2.060868369635715

Epoch: 6| Step: 8
Training loss: 2.2591140270233154
Validation loss: 2.0621789488741147

Epoch: 6| Step: 9
Training loss: 2.9037489891052246
Validation loss: 2.0566634593471402

Epoch: 6| Step: 10
Training loss: 2.289968252182007
Validation loss: 2.050083571864713

Epoch: 6| Step: 11
Training loss: 3.1855530738830566
Validation loss: 2.0784538868934876

Epoch: 6| Step: 12
Training loss: 2.4808878898620605
Validation loss: 2.068611075801234

Epoch: 6| Step: 13
Training loss: 2.047760486602783
Validation loss: 2.045787292142068

Epoch: 172| Step: 0
Training loss: 2.5336384773254395
Validation loss: 2.0790422270374913

Epoch: 6| Step: 1
Training loss: 1.771594762802124
Validation loss: 2.056141689259519

Epoch: 6| Step: 2
Training loss: 2.209418773651123
Validation loss: 2.061259574787591

Epoch: 6| Step: 3
Training loss: 2.1628878116607666
Validation loss: 2.04312159938197

Epoch: 6| Step: 4
Training loss: 2.2546682357788086
Validation loss: 2.0707707071817048

Epoch: 6| Step: 5
Training loss: 2.1361827850341797
Validation loss: 2.0794117258441065

Epoch: 6| Step: 6
Training loss: 1.812164545059204
Validation loss: 2.068647215443273

Epoch: 6| Step: 7
Training loss: 2.199789524078369
Validation loss: 2.088059545845114

Epoch: 6| Step: 8
Training loss: 2.3945841789245605
Validation loss: 2.0963387399591427

Epoch: 6| Step: 9
Training loss: 2.346437931060791
Validation loss: 2.0782070621367423

Epoch: 6| Step: 10
Training loss: 3.5213050842285156
Validation loss: 2.078254794561735

Epoch: 6| Step: 11
Training loss: 1.142008900642395
Validation loss: 2.0532282936957573

Epoch: 6| Step: 12
Training loss: 2.7397146224975586
Validation loss: 2.059930911628149

Epoch: 6| Step: 13
Training loss: 1.8151313066482544
Validation loss: 2.07813327030469

Epoch: 173| Step: 0
Training loss: 2.7934017181396484
Validation loss: 2.0985207365405176

Epoch: 6| Step: 1
Training loss: 2.6609013080596924
Validation loss: 2.0975310674277683

Epoch: 6| Step: 2
Training loss: 2.450134038925171
Validation loss: 2.105626365189911

Epoch: 6| Step: 3
Training loss: 2.614981174468994
Validation loss: 2.1029610223667596

Epoch: 6| Step: 4
Training loss: 2.874209403991699
Validation loss: 2.087826559620519

Epoch: 6| Step: 5
Training loss: 2.1453604698181152
Validation loss: 2.10878735972989

Epoch: 6| Step: 6
Training loss: 1.8994379043579102
Validation loss: 2.0925516466940604

Epoch: 6| Step: 7
Training loss: 1.5559889078140259
Validation loss: 2.10770913862413

Epoch: 6| Step: 8
Training loss: 1.96634840965271
Validation loss: 2.091559825404998

Epoch: 6| Step: 9
Training loss: 1.4623595476150513
Validation loss: 2.1049360870033182

Epoch: 6| Step: 10
Training loss: 1.6449962854385376
Validation loss: 2.109244291500379

Epoch: 6| Step: 11
Training loss: 2.4314613342285156
Validation loss: 2.108680307224233

Epoch: 6| Step: 12
Training loss: 1.9821226596832275
Validation loss: 2.083944130969304

Epoch: 6| Step: 13
Training loss: 2.76639461517334
Validation loss: 2.102781900795557

Epoch: 174| Step: 0
Training loss: 2.118530035018921
Validation loss: 2.098665480972618

Epoch: 6| Step: 1
Training loss: 2.540114402770996
Validation loss: 2.0811550130126295

Epoch: 6| Step: 2
Training loss: 2.5359601974487305
Validation loss: 2.0983663630741898

Epoch: 6| Step: 3
Training loss: 2.6278576850891113
Validation loss: 2.085451429890048

Epoch: 6| Step: 4
Training loss: 2.738593101501465
Validation loss: 2.079855160046649

Epoch: 6| Step: 5
Training loss: 2.079969882965088
Validation loss: 2.1170971714040285

Epoch: 6| Step: 6
Training loss: 1.2722694873809814
Validation loss: 2.086301657461351

Epoch: 6| Step: 7
Training loss: 1.6142299175262451
Validation loss: 2.084946422166722

Epoch: 6| Step: 8
Training loss: 1.8525817394256592
Validation loss: 2.0751109430866856

Epoch: 6| Step: 9
Training loss: 2.4607551097869873
Validation loss: 2.075029152695851

Epoch: 6| Step: 10
Training loss: 2.8689703941345215
Validation loss: 2.070804918965986

Epoch: 6| Step: 11
Training loss: 1.8797433376312256
Validation loss: 2.103749268798418

Epoch: 6| Step: 12
Training loss: 1.9815385341644287
Validation loss: 2.088260417343468

Epoch: 6| Step: 13
Training loss: 2.4589314460754395
Validation loss: 2.0691830163360923

Epoch: 175| Step: 0
Training loss: 1.8467968702316284
Validation loss: 2.081442367646002

Epoch: 6| Step: 1
Training loss: 2.590193271636963
Validation loss: 2.0724928737968527

Epoch: 6| Step: 2
Training loss: 2.283151626586914
Validation loss: 2.0964356724933912

Epoch: 6| Step: 3
Training loss: 2.0531935691833496
Validation loss: 2.066366704561377

Epoch: 6| Step: 4
Training loss: 2.254119396209717
Validation loss: 2.1009284988526375

Epoch: 6| Step: 5
Training loss: 1.6127657890319824
Validation loss: 2.0588997384553314

Epoch: 6| Step: 6
Training loss: 1.8065155744552612
Validation loss: 2.077760998920728

Epoch: 6| Step: 7
Training loss: 1.9150614738464355
Validation loss: 2.076525716371434

Epoch: 6| Step: 8
Training loss: 2.2365968227386475
Validation loss: 2.064748764038086

Epoch: 6| Step: 9
Training loss: 2.3349368572235107
Validation loss: 2.0779079621837986

Epoch: 6| Step: 10
Training loss: 2.996950626373291
Validation loss: 2.071536901176617

Epoch: 6| Step: 11
Training loss: 2.4338932037353516
Validation loss: 2.0893458627885386

Epoch: 6| Step: 12
Training loss: 1.7179458141326904
Validation loss: 2.05650632996713

Epoch: 6| Step: 13
Training loss: 3.3455638885498047
Validation loss: 2.0520332192861908

Epoch: 176| Step: 0
Training loss: 1.9519548416137695
Validation loss: 2.0470973189159105

Epoch: 6| Step: 1
Training loss: 1.84153413772583
Validation loss: 2.066971350741643

Epoch: 6| Step: 2
Training loss: 2.8971986770629883
Validation loss: 2.080803335353892

Epoch: 6| Step: 3
Training loss: 1.4934802055358887
Validation loss: 2.059566596502899

Epoch: 6| Step: 4
Training loss: 3.128101348876953
Validation loss: 2.0722250528233026

Epoch: 6| Step: 5
Training loss: 2.203248977661133
Validation loss: 2.0574046719458794

Epoch: 6| Step: 6
Training loss: 2.2584688663482666
Validation loss: 2.081727616248592

Epoch: 6| Step: 7
Training loss: 2.1680517196655273
Validation loss: 2.07699006090882

Epoch: 6| Step: 8
Training loss: 2.1897449493408203
Validation loss: 2.075322599821193

Epoch: 6| Step: 9
Training loss: 1.9335136413574219
Validation loss: 2.0546331879913167

Epoch: 6| Step: 10
Training loss: 1.9503114223480225
Validation loss: 2.0621144438302643

Epoch: 6| Step: 11
Training loss: 2.277890920639038
Validation loss: 2.090350394607872

Epoch: 6| Step: 12
Training loss: 2.243847131729126
Validation loss: 2.0545609048617783

Epoch: 6| Step: 13
Training loss: 2.564225196838379
Validation loss: 2.0811040914186867

Epoch: 177| Step: 0
Training loss: 2.477156400680542
Validation loss: 2.084250796225763

Epoch: 6| Step: 1
Training loss: 2.9451842308044434
Validation loss: 2.0690586079833326

Epoch: 6| Step: 2
Training loss: 2.632962942123413
Validation loss: 2.0925091928051365

Epoch: 6| Step: 3
Training loss: 1.987019419670105
Validation loss: 2.0849467759491294

Epoch: 6| Step: 4
Training loss: 2.048766613006592
Validation loss: 2.0887933495224162

Epoch: 6| Step: 5
Training loss: 1.7407344579696655
Validation loss: 2.0864564564920243

Epoch: 6| Step: 6
Training loss: 2.0224289894104004
Validation loss: 2.0896722244960007

Epoch: 6| Step: 7
Training loss: 2.3968687057495117
Validation loss: 2.08747051992724

Epoch: 6| Step: 8
Training loss: 2.2599716186523438
Validation loss: 2.088489427361437

Epoch: 6| Step: 9
Training loss: 2.227658748626709
Validation loss: 2.0561919366159747

Epoch: 6| Step: 10
Training loss: 2.4978995323181152
Validation loss: 2.0805413658900926

Epoch: 6| Step: 11
Training loss: 1.764448881149292
Validation loss: 2.063498834128021

Epoch: 6| Step: 12
Training loss: 2.09061336517334
Validation loss: 2.0659804113449587

Epoch: 6| Step: 13
Training loss: 1.4388147592544556
Validation loss: 2.0776695487319783

Epoch: 178| Step: 0
Training loss: 2.415213108062744
Validation loss: 2.076570495482414

Epoch: 6| Step: 1
Training loss: 1.9310752153396606
Validation loss: 2.099295336713073

Epoch: 6| Step: 2
Training loss: 2.122812271118164
Validation loss: 2.084155813340218

Epoch: 6| Step: 3
Training loss: 2.1449623107910156
Validation loss: 2.0866840154893938

Epoch: 6| Step: 4
Training loss: 2.4598958492279053
Validation loss: 2.0453083720258487

Epoch: 6| Step: 5
Training loss: 1.618335485458374
Validation loss: 2.075236633259763

Epoch: 6| Step: 6
Training loss: 1.9065414667129517
Validation loss: 2.059041484709709

Epoch: 6| Step: 7
Training loss: 1.9488683938980103
Validation loss: 2.06252335732983

Epoch: 6| Step: 8
Training loss: 2.1278560161590576
Validation loss: 2.0835151121180546

Epoch: 6| Step: 9
Training loss: 2.8185954093933105
Validation loss: 2.069037070838354

Epoch: 6| Step: 10
Training loss: 2.006649971008301
Validation loss: 2.069119514957551

Epoch: 6| Step: 11
Training loss: 2.598904848098755
Validation loss: 2.0750366462174283

Epoch: 6| Step: 12
Training loss: 2.7481627464294434
Validation loss: 2.059851259313604

Epoch: 6| Step: 13
Training loss: 2.2445719242095947
Validation loss: 2.0740001714357765

Epoch: 179| Step: 0
Training loss: 2.7938144207000732
Validation loss: 2.0613640610889723

Epoch: 6| Step: 1
Training loss: 2.488762378692627
Validation loss: 2.058895541775611

Epoch: 6| Step: 2
Training loss: 2.277149200439453
Validation loss: 2.0596782007525043

Epoch: 6| Step: 3
Training loss: 1.7113862037658691
Validation loss: 2.069700256470711

Epoch: 6| Step: 4
Training loss: 2.2096188068389893
Validation loss: 2.0712014693085865

Epoch: 6| Step: 5
Training loss: 2.413933277130127
Validation loss: 2.0600765558981124

Epoch: 6| Step: 6
Training loss: 2.2635228633880615
Validation loss: 2.057866801497757

Epoch: 6| Step: 7
Training loss: 1.9795880317687988
Validation loss: 2.0506124355459727

Epoch: 6| Step: 8
Training loss: 2.1906752586364746
Validation loss: 2.060978592083018

Epoch: 6| Step: 9
Training loss: 2.352752923965454
Validation loss: 2.0644580189899733

Epoch: 6| Step: 10
Training loss: 2.2140820026397705
Validation loss: 2.0661007870909986

Epoch: 6| Step: 11
Training loss: 2.1854915618896484
Validation loss: 2.040201469134259

Epoch: 6| Step: 12
Training loss: 1.9861502647399902
Validation loss: 2.0861474621680474

Epoch: 6| Step: 13
Training loss: 2.029222249984741
Validation loss: 2.053712890994164

Epoch: 180| Step: 0
Training loss: 1.8536841869354248
Validation loss: 2.0598572684872534

Epoch: 6| Step: 1
Training loss: 1.691161870956421
Validation loss: 2.0677579731069584

Epoch: 6| Step: 2
Training loss: 2.387552499771118
Validation loss: 2.061951393722206

Epoch: 6| Step: 3
Training loss: 1.5831812620162964
Validation loss: 2.08522020616839

Epoch: 6| Step: 4
Training loss: 2.7521960735321045
Validation loss: 2.0606204745590047

Epoch: 6| Step: 5
Training loss: 2.3382625579833984
Validation loss: 2.052526794454103

Epoch: 6| Step: 6
Training loss: 2.226349353790283
Validation loss: 2.0653168014300767

Epoch: 6| Step: 7
Training loss: 2.9669229984283447
Validation loss: 2.0487288736527964

Epoch: 6| Step: 8
Training loss: 2.1839160919189453
Validation loss: 2.0520206651379986

Epoch: 6| Step: 9
Training loss: 2.745413064956665
Validation loss: 2.0631458169670513

Epoch: 6| Step: 10
Training loss: 2.151615619659424
Validation loss: 2.0544074299514934

Epoch: 6| Step: 11
Training loss: 1.9383788108825684
Validation loss: 2.046965326032331

Epoch: 6| Step: 12
Training loss: 2.0060436725616455
Validation loss: 2.0901658663185696

Epoch: 6| Step: 13
Training loss: 2.0963406562805176
Validation loss: 2.0761030694489837

Epoch: 181| Step: 0
Training loss: 2.1822824478149414
Validation loss: 2.0573716778909006

Epoch: 6| Step: 1
Training loss: 2.097496747970581
Validation loss: 2.062714015283892

Epoch: 6| Step: 2
Training loss: 2.323643207550049
Validation loss: 2.067572352706745

Epoch: 6| Step: 3
Training loss: 1.8903050422668457
Validation loss: 2.088786317456153

Epoch: 6| Step: 4
Training loss: 2.780669689178467
Validation loss: 2.0728596282261673

Epoch: 6| Step: 5
Training loss: 1.8832374811172485
Validation loss: 2.087415473435515

Epoch: 6| Step: 6
Training loss: 2.9160358905792236
Validation loss: 2.0949238936106362

Epoch: 6| Step: 7
Training loss: 1.9049220085144043
Validation loss: 2.0883242827589794

Epoch: 6| Step: 8
Training loss: 2.303868055343628
Validation loss: 2.098971382264168

Epoch: 6| Step: 9
Training loss: 2.722512722015381
Validation loss: 2.0812259386944514

Epoch: 6| Step: 10
Training loss: 2.0007195472717285
Validation loss: 2.0961070881094983

Epoch: 6| Step: 11
Training loss: 1.776796579360962
Validation loss: 2.0835212725465015

Epoch: 6| Step: 12
Training loss: 1.9386484622955322
Validation loss: 2.079733071788665

Epoch: 6| Step: 13
Training loss: 2.448218584060669
Validation loss: 2.080966329061857

Epoch: 182| Step: 0
Training loss: 1.8040874004364014
Validation loss: 2.0695793859420286

Epoch: 6| Step: 1
Training loss: 2.2868423461914062
Validation loss: 2.100471629891344

Epoch: 6| Step: 2
Training loss: 2.0780701637268066
Validation loss: 2.0705411203445925

Epoch: 6| Step: 3
Training loss: 1.9390838146209717
Validation loss: 2.115696983952676

Epoch: 6| Step: 4
Training loss: 3.2971622943878174
Validation loss: 2.09614324056974

Epoch: 6| Step: 5
Training loss: 2.1733880043029785
Validation loss: 2.099941204952937

Epoch: 6| Step: 6
Training loss: 2.199425220489502
Validation loss: 2.0922507880836405

Epoch: 6| Step: 7
Training loss: 2.4692583084106445
Validation loss: 2.0880534328440183

Epoch: 6| Step: 8
Training loss: 2.467991352081299
Validation loss: 2.07850020675249

Epoch: 6| Step: 9
Training loss: 2.2120473384857178
Validation loss: 2.089224141131165

Epoch: 6| Step: 10
Training loss: 1.5734279155731201
Validation loss: 2.069325241991269

Epoch: 6| Step: 11
Training loss: 2.2230300903320312
Validation loss: 2.0832414268165507

Epoch: 6| Step: 12
Training loss: 2.0166735649108887
Validation loss: 2.081596853912518

Epoch: 6| Step: 13
Training loss: 2.5022289752960205
Validation loss: 2.085290306357927

Epoch: 183| Step: 0
Training loss: 2.1201322078704834
Validation loss: 2.100575849574099

Epoch: 6| Step: 1
Training loss: 2.969748020172119
Validation loss: 2.0812007047796763

Epoch: 6| Step: 2
Training loss: 3.0035200119018555
Validation loss: 2.073769343796597

Epoch: 6| Step: 3
Training loss: 1.7209961414337158
Validation loss: 2.0991904479201122

Epoch: 6| Step: 4
Training loss: 2.1376819610595703
Validation loss: 2.0841954984972553

Epoch: 6| Step: 5
Training loss: 1.4953300952911377
Validation loss: 2.0596006698505853

Epoch: 6| Step: 6
Training loss: 2.231175422668457
Validation loss: 2.066860601466189

Epoch: 6| Step: 7
Training loss: 2.3534188270568848
Validation loss: 2.0996061614764634

Epoch: 6| Step: 8
Training loss: 1.7144876718521118
Validation loss: 2.0999156608376452

Epoch: 6| Step: 9
Training loss: 2.0776100158691406
Validation loss: 2.0974466108506724

Epoch: 6| Step: 10
Training loss: 2.5024607181549072
Validation loss: 2.110416735372236

Epoch: 6| Step: 11
Training loss: 1.897425651550293
Validation loss: 2.080450193856352

Epoch: 6| Step: 12
Training loss: 2.3153178691864014
Validation loss: 2.0708457500703874

Epoch: 6| Step: 13
Training loss: 2.8987786769866943
Validation loss: 2.0895514103674118

Epoch: 184| Step: 0
Training loss: 2.1756656169891357
Validation loss: 2.109551360530238

Epoch: 6| Step: 1
Training loss: 2.041109561920166
Validation loss: 2.0950067158668273

Epoch: 6| Step: 2
Training loss: 2.5568721294403076
Validation loss: 2.114347392512906

Epoch: 6| Step: 3
Training loss: 2.4934656620025635
Validation loss: 2.1103554335973596

Epoch: 6| Step: 4
Training loss: 2.294229030609131
Validation loss: 2.099226874689902

Epoch: 6| Step: 5
Training loss: 2.370880126953125
Validation loss: 2.1084815481657624

Epoch: 6| Step: 6
Training loss: 2.4305496215820312
Validation loss: 2.1330534437651276

Epoch: 6| Step: 7
Training loss: 1.8136703968048096
Validation loss: 2.0857461960084978

Epoch: 6| Step: 8
Training loss: 2.162471294403076
Validation loss: 2.1042837942800214

Epoch: 6| Step: 9
Training loss: 2.2225162982940674
Validation loss: 2.0866203667015157

Epoch: 6| Step: 10
Training loss: 2.0871260166168213
Validation loss: 2.0769418003738567

Epoch: 6| Step: 11
Training loss: 2.1140623092651367
Validation loss: 2.1042873782496296

Epoch: 6| Step: 12
Training loss: 2.3548049926757812
Validation loss: 2.1150030038690053

Epoch: 6| Step: 13
Training loss: 1.7981913089752197
Validation loss: 2.0813458388851536

Epoch: 185| Step: 0
Training loss: 2.019446849822998
Validation loss: 2.0952404981018393

Epoch: 6| Step: 1
Training loss: 2.1339144706726074
Validation loss: 2.0966631212542133

Epoch: 6| Step: 2
Training loss: 2.600325584411621
Validation loss: 2.101851841454865

Epoch: 6| Step: 3
Training loss: 2.31689715385437
Validation loss: 2.085203875777542

Epoch: 6| Step: 4
Training loss: 2.0753307342529297
Validation loss: 2.0799083914808048

Epoch: 6| Step: 5
Training loss: 2.959651470184326
Validation loss: 2.0847199296438568

Epoch: 6| Step: 6
Training loss: 2.148273468017578
Validation loss: 2.0894634903118177

Epoch: 6| Step: 7
Training loss: 2.6184511184692383
Validation loss: 2.0792561705394457

Epoch: 6| Step: 8
Training loss: 1.8935966491699219
Validation loss: 2.0762039179443033

Epoch: 6| Step: 9
Training loss: 1.7238599061965942
Validation loss: 2.0911381372841458

Epoch: 6| Step: 10
Training loss: 2.319368362426758
Validation loss: 2.0890625394800657

Epoch: 6| Step: 11
Training loss: 1.973633885383606
Validation loss: 2.078477244223318

Epoch: 6| Step: 12
Training loss: 1.9210134744644165
Validation loss: 2.0773127822465796

Epoch: 6| Step: 13
Training loss: 2.0996086597442627
Validation loss: 2.1065177814934843

Epoch: 186| Step: 0
Training loss: 2.5080065727233887
Validation loss: 2.073205401820521

Epoch: 6| Step: 1
Training loss: 1.5513763427734375
Validation loss: 2.07039644897625

Epoch: 6| Step: 2
Training loss: 1.4910004138946533
Validation loss: 2.0983319205622517

Epoch: 6| Step: 3
Training loss: 1.4199168682098389
Validation loss: 2.082672121704266

Epoch: 6| Step: 4
Training loss: 2.5548837184906006
Validation loss: 2.071562618337652

Epoch: 6| Step: 5
Training loss: 3.0158636569976807
Validation loss: 2.065698869766728

Epoch: 6| Step: 6
Training loss: 2.213451385498047
Validation loss: 2.0846928473441833

Epoch: 6| Step: 7
Training loss: 3.0951931476593018
Validation loss: 2.1148177013602307

Epoch: 6| Step: 8
Training loss: 1.833461046218872
Validation loss: 2.092530982468718

Epoch: 6| Step: 9
Training loss: 1.63936448097229
Validation loss: 2.088401989270282

Epoch: 6| Step: 10
Training loss: 1.7871856689453125
Validation loss: 2.076151106947212

Epoch: 6| Step: 11
Training loss: 2.881223201751709
Validation loss: 2.0959578739699496

Epoch: 6| Step: 12
Training loss: 2.549586772918701
Validation loss: 2.0892004479644117

Epoch: 6| Step: 13
Training loss: 2.6763315200805664
Validation loss: 2.078463597964215

Epoch: 187| Step: 0
Training loss: 1.7555408477783203
Validation loss: 2.1047704604364212

Epoch: 6| Step: 1
Training loss: 2.2534682750701904
Validation loss: 2.0780638815254293

Epoch: 6| Step: 2
Training loss: 2.596911907196045
Validation loss: 2.085517588482108

Epoch: 6| Step: 3
Training loss: 2.911017417907715
Validation loss: 2.0842993387611966

Epoch: 6| Step: 4
Training loss: 2.6253609657287598
Validation loss: 2.0720488448296823

Epoch: 6| Step: 5
Training loss: 1.9465631246566772
Validation loss: 2.096246629632929

Epoch: 6| Step: 6
Training loss: 3.3221895694732666
Validation loss: 2.0784352517897084

Epoch: 6| Step: 7
Training loss: 1.4517157077789307
Validation loss: 2.0520396360787014

Epoch: 6| Step: 8
Training loss: 2.0186078548431396
Validation loss: 2.106025606073359

Epoch: 6| Step: 9
Training loss: 2.0662999153137207
Validation loss: 2.06013152932608

Epoch: 6| Step: 10
Training loss: 1.8822593688964844
Validation loss: 2.065897810843683

Epoch: 6| Step: 11
Training loss: 1.8579683303833008
Validation loss: 2.0769472455465667

Epoch: 6| Step: 12
Training loss: 2.12028169631958
Validation loss: 2.0803971162406345

Epoch: 6| Step: 13
Training loss: 2.1796019077301025
Validation loss: 2.0809788050190097

Epoch: 188| Step: 0
Training loss: 1.9216376543045044
Validation loss: 2.0635996839051605

Epoch: 6| Step: 1
Training loss: 2.009892702102661
Validation loss: 2.0875889203881703

Epoch: 6| Step: 2
Training loss: 2.272653102874756
Validation loss: 2.0699360806454896

Epoch: 6| Step: 3
Training loss: 2.7322793006896973
Validation loss: 2.0874818525006695

Epoch: 6| Step: 4
Training loss: 2.196488380432129
Validation loss: 2.084978763775159

Epoch: 6| Step: 5
Training loss: 2.811659336090088
Validation loss: 2.1000832690987536

Epoch: 6| Step: 6
Training loss: 2.9053211212158203
Validation loss: 2.0446383004547446

Epoch: 6| Step: 7
Training loss: 2.43937349319458
Validation loss: 2.065265292762428

Epoch: 6| Step: 8
Training loss: 2.16499662399292
Validation loss: 2.0897179700995006

Epoch: 6| Step: 9
Training loss: 1.347632646560669
Validation loss: 2.0833960015286683

Epoch: 6| Step: 10
Training loss: 1.6672389507293701
Validation loss: 2.0919323762257895

Epoch: 6| Step: 11
Training loss: 1.8202788829803467
Validation loss: 2.0750311497719056

Epoch: 6| Step: 12
Training loss: 2.487952470779419
Validation loss: 2.0888257936764787

Epoch: 6| Step: 13
Training loss: 1.9677823781967163
Validation loss: 2.0850624974055956

Epoch: 189| Step: 0
Training loss: 1.9367479085922241
Validation loss: 2.0755454135197464

Epoch: 6| Step: 1
Training loss: 1.9971853494644165
Validation loss: 2.061174287590929

Epoch: 6| Step: 2
Training loss: 2.084486722946167
Validation loss: 2.06243763303244

Epoch: 6| Step: 3
Training loss: 2.312540054321289
Validation loss: 2.0910808450432232

Epoch: 6| Step: 4
Training loss: 2.3966946601867676
Validation loss: 2.087178308476684

Epoch: 6| Step: 5
Training loss: 2.503385066986084
Validation loss: 2.0915963290840067

Epoch: 6| Step: 6
Training loss: 2.264017105102539
Validation loss: 2.0880947010491484

Epoch: 6| Step: 7
Training loss: 2.385971784591675
Validation loss: 2.099770651068739

Epoch: 6| Step: 8
Training loss: 2.459824562072754
Validation loss: 2.127572205758864

Epoch: 6| Step: 9
Training loss: 2.1682233810424805
Validation loss: 2.1024933425329064

Epoch: 6| Step: 10
Training loss: 2.139378070831299
Validation loss: 2.0937798856407084

Epoch: 6| Step: 11
Training loss: 1.935245156288147
Validation loss: 2.1245768313766806

Epoch: 6| Step: 12
Training loss: 2.0014801025390625
Validation loss: 2.1229176854574554

Epoch: 6| Step: 13
Training loss: 2.5171940326690674
Validation loss: 2.0825222615272767

Epoch: 190| Step: 0
Training loss: 2.588286876678467
Validation loss: 2.0749446781732703

Epoch: 6| Step: 1
Training loss: 2.366056442260742
Validation loss: 2.0959915781533844

Epoch: 6| Step: 2
Training loss: 2.7478456497192383
Validation loss: 2.100812283895349

Epoch: 6| Step: 3
Training loss: 1.5211873054504395
Validation loss: 2.069059410402852

Epoch: 6| Step: 4
Training loss: 2.466693878173828
Validation loss: 2.072606084167316

Epoch: 6| Step: 5
Training loss: 1.786942958831787
Validation loss: 2.089659406292823

Epoch: 6| Step: 6
Training loss: 1.7278492450714111
Validation loss: 2.108434315650694

Epoch: 6| Step: 7
Training loss: 2.998440742492676
Validation loss: 2.0950379807461976

Epoch: 6| Step: 8
Training loss: 1.908944845199585
Validation loss: 2.0801508170302196

Epoch: 6| Step: 9
Training loss: 2.011000394821167
Validation loss: 2.0601571888052006

Epoch: 6| Step: 10
Training loss: 1.7879440784454346
Validation loss: 2.0609941918362855

Epoch: 6| Step: 11
Training loss: 2.4849748611450195
Validation loss: 2.075662678287875

Epoch: 6| Step: 12
Training loss: 2.2983102798461914
Validation loss: 2.110603699120142

Epoch: 6| Step: 13
Training loss: 1.8391757011413574
Validation loss: 2.0955539839242094

Epoch: 191| Step: 0
Training loss: 1.9195783138275146
Validation loss: 2.0678190992724512

Epoch: 6| Step: 1
Training loss: 2.2058868408203125
Validation loss: 2.072140800055637

Epoch: 6| Step: 2
Training loss: 2.078714370727539
Validation loss: 2.0187191950377597

Epoch: 6| Step: 3
Training loss: 2.208402633666992
Validation loss: 2.0632209418922343

Epoch: 6| Step: 4
Training loss: 1.702354907989502
Validation loss: 2.0544129994607743

Epoch: 6| Step: 5
Training loss: 2.122390031814575
Validation loss: 2.0643636847055085

Epoch: 6| Step: 6
Training loss: 2.0163817405700684
Validation loss: 2.044345394257576

Epoch: 6| Step: 7
Training loss: 2.094097137451172
Validation loss: 2.060165000218217

Epoch: 6| Step: 8
Training loss: 1.7885665893554688
Validation loss: 2.058362862115265

Epoch: 6| Step: 9
Training loss: 2.720123767852783
Validation loss: 2.058312434022145

Epoch: 6| Step: 10
Training loss: 2.930109739303589
Validation loss: 2.081077916647798

Epoch: 6| Step: 11
Training loss: 2.3578479290008545
Validation loss: 2.0658204965693976

Epoch: 6| Step: 12
Training loss: 2.490023136138916
Validation loss: 2.051014261861001

Epoch: 6| Step: 13
Training loss: 2.1849565505981445
Validation loss: 2.0427545680794665

Epoch: 192| Step: 0
Training loss: 2.668942928314209
Validation loss: 2.045833917074306

Epoch: 6| Step: 1
Training loss: 2.1974716186523438
Validation loss: 2.0673602152896184

Epoch: 6| Step: 2
Training loss: 1.6505123376846313
Validation loss: 2.1002183037419475

Epoch: 6| Step: 3
Training loss: 2.692892551422119
Validation loss: 2.085123372334306

Epoch: 6| Step: 4
Training loss: 2.021282196044922
Validation loss: 2.080159205262379

Epoch: 6| Step: 5
Training loss: 2.073866367340088
Validation loss: 2.0898859167611725

Epoch: 6| Step: 6
Training loss: 2.766360282897949
Validation loss: 2.087434475139905

Epoch: 6| Step: 7
Training loss: 2.6890854835510254
Validation loss: 2.086741396175918

Epoch: 6| Step: 8
Training loss: 2.380075693130493
Validation loss: 2.10117849996013

Epoch: 6| Step: 9
Training loss: 1.4541916847229004
Validation loss: 2.0867520096481487

Epoch: 6| Step: 10
Training loss: 2.137547254562378
Validation loss: 2.100479643831971

Epoch: 6| Step: 11
Training loss: 1.9997713565826416
Validation loss: 2.0904412500319944

Epoch: 6| Step: 12
Training loss: 1.7193536758422852
Validation loss: 2.097186615390162

Epoch: 6| Step: 13
Training loss: 2.7787234783172607
Validation loss: 2.097517769823792

Epoch: 193| Step: 0
Training loss: 2.480809211730957
Validation loss: 2.1136296026168333

Epoch: 6| Step: 1
Training loss: 3.1552066802978516
Validation loss: 2.0787522049360376

Epoch: 6| Step: 2
Training loss: 1.964619517326355
Validation loss: 2.076840854460193

Epoch: 6| Step: 3
Training loss: 1.6843585968017578
Validation loss: 2.0647827258674045

Epoch: 6| Step: 4
Training loss: 2.596662759780884
Validation loss: 2.0745377322678924

Epoch: 6| Step: 5
Training loss: 2.519648551940918
Validation loss: 2.081255051397508

Epoch: 6| Step: 6
Training loss: 1.8782219886779785
Validation loss: 2.0639504001986597

Epoch: 6| Step: 7
Training loss: 1.893284559249878
Validation loss: 2.0975083381898942

Epoch: 6| Step: 8
Training loss: 2.3057031631469727
Validation loss: 2.0758258245324575

Epoch: 6| Step: 9
Training loss: 2.5149359703063965
Validation loss: 2.0762018990773026

Epoch: 6| Step: 10
Training loss: 1.8123706579208374
Validation loss: 2.0555494703272337

Epoch: 6| Step: 11
Training loss: 2.0830891132354736
Validation loss: 2.059532469318759

Epoch: 6| Step: 12
Training loss: 1.744959831237793
Validation loss: 2.0739010918524956

Epoch: 6| Step: 13
Training loss: 2.080113410949707
Validation loss: 2.0877434412638345

Epoch: 194| Step: 0
Training loss: 2.0875296592712402
Validation loss: 2.069943053748018

Epoch: 6| Step: 1
Training loss: 1.455258846282959
Validation loss: 2.0631183655031267

Epoch: 6| Step: 2
Training loss: 1.9999771118164062
Validation loss: 2.0637492390089136

Epoch: 6| Step: 3
Training loss: 1.8929425477981567
Validation loss: 2.04637844844531

Epoch: 6| Step: 4
Training loss: 3.2300543785095215
Validation loss: 2.054761521277889

Epoch: 6| Step: 5
Training loss: 3.001689910888672
Validation loss: 2.0774872533736692

Epoch: 6| Step: 6
Training loss: 2.0251991748809814
Validation loss: 2.0367917027524722

Epoch: 6| Step: 7
Training loss: 2.117011547088623
Validation loss: 2.0592188245506695

Epoch: 6| Step: 8
Training loss: 2.2963013648986816
Validation loss: 2.0541455835424443

Epoch: 6| Step: 9
Training loss: 1.9011319875717163
Validation loss: 2.0556943557595693

Epoch: 6| Step: 10
Training loss: 2.1352574825286865
Validation loss: 2.077109453498676

Epoch: 6| Step: 11
Training loss: 2.3144524097442627
Validation loss: 2.0666835795166674

Epoch: 6| Step: 12
Training loss: 1.6660171747207642
Validation loss: 2.064473275215395

Epoch: 6| Step: 13
Training loss: 2.616732597351074
Validation loss: 2.0961543308791293

Epoch: 195| Step: 0
Training loss: 2.435598134994507
Validation loss: 2.067086342842348

Epoch: 6| Step: 1
Training loss: 2.198042631149292
Validation loss: 2.076836254007073

Epoch: 6| Step: 2
Training loss: 2.1740493774414062
Validation loss: 2.0541815988479124

Epoch: 6| Step: 3
Training loss: 1.7626872062683105
Validation loss: 2.066298366874777

Epoch: 6| Step: 4
Training loss: 2.3665647506713867
Validation loss: 2.0540018876393638

Epoch: 6| Step: 5
Training loss: 1.722703456878662
Validation loss: 2.079960487222159

Epoch: 6| Step: 6
Training loss: 1.8325049877166748
Validation loss: 2.0519379044091828

Epoch: 6| Step: 7
Training loss: 2.0151731967926025
Validation loss: 2.076298498338269

Epoch: 6| Step: 8
Training loss: 2.2379562854766846
Validation loss: 2.0604837479129916

Epoch: 6| Step: 9
Training loss: 2.045525550842285
Validation loss: 2.056130240040441

Epoch: 6| Step: 10
Training loss: 2.4013290405273438
Validation loss: 2.081036408742269

Epoch: 6| Step: 11
Training loss: 2.3322558403015137
Validation loss: 2.111011078280787

Epoch: 6| Step: 12
Training loss: 3.461362838745117
Validation loss: 2.079754291042205

Epoch: 6| Step: 13
Training loss: 1.5471004247665405
Validation loss: 2.08609874145959

Epoch: 196| Step: 0
Training loss: 2.3589727878570557
Validation loss: 2.0782038473313853

Epoch: 6| Step: 1
Training loss: 1.684291124343872
Validation loss: 2.062361776187856

Epoch: 6| Step: 2
Training loss: 2.4894533157348633
Validation loss: 2.0932864168638825

Epoch: 6| Step: 3
Training loss: 1.651989221572876
Validation loss: 2.104615908797069

Epoch: 6| Step: 4
Training loss: 2.1716508865356445
Validation loss: 2.07356963106381

Epoch: 6| Step: 5
Training loss: 2.486222267150879
Validation loss: 2.07083632100013

Epoch: 6| Step: 6
Training loss: 2.0773346424102783
Validation loss: 2.0835802798630088

Epoch: 6| Step: 7
Training loss: 2.720561981201172
Validation loss: 2.1017425367909093

Epoch: 6| Step: 8
Training loss: 1.3866804838180542
Validation loss: 2.0785398406367146

Epoch: 6| Step: 9
Training loss: 2.0377278327941895
Validation loss: 2.101213042454053

Epoch: 6| Step: 10
Training loss: 3.005730152130127
Validation loss: 2.075164202720888

Epoch: 6| Step: 11
Training loss: 1.9258841276168823
Validation loss: 2.0969699275109077

Epoch: 6| Step: 12
Training loss: 2.4228382110595703
Validation loss: 2.0571651894559144

Epoch: 6| Step: 13
Training loss: 2.5880696773529053
Validation loss: 2.0737235674294094

Epoch: 197| Step: 0
Training loss: 1.9174054861068726
Validation loss: 2.0913257393785702

Epoch: 6| Step: 1
Training loss: 1.931205153465271
Validation loss: 2.093492615607477

Epoch: 6| Step: 2
Training loss: 2.1087875366210938
Validation loss: 2.0858534574508667

Epoch: 6| Step: 3
Training loss: 2.584104061126709
Validation loss: 2.0880342991121355

Epoch: 6| Step: 4
Training loss: 2.4593563079833984
Validation loss: 2.0855427800968127

Epoch: 6| Step: 5
Training loss: 3.1542305946350098
Validation loss: 2.0781409689175185

Epoch: 6| Step: 6
Training loss: 1.7335225343704224
Validation loss: 2.063161321865615

Epoch: 6| Step: 7
Training loss: 2.0307021141052246
Validation loss: 2.080151434867613

Epoch: 6| Step: 8
Training loss: 1.8963298797607422
Validation loss: 2.0689965358344455

Epoch: 6| Step: 9
Training loss: 2.3095765113830566
Validation loss: 2.0806079474828576

Epoch: 6| Step: 10
Training loss: 2.514523506164551
Validation loss: 2.0991720332894275

Epoch: 6| Step: 11
Training loss: 1.713385820388794
Validation loss: 2.0862484221817343

Epoch: 6| Step: 12
Training loss: 2.005009889602661
Validation loss: 2.0798796197419525

Epoch: 6| Step: 13
Training loss: 2.957681179046631
Validation loss: 2.0515722536271617

Epoch: 198| Step: 0
Training loss: 1.5499448776245117
Validation loss: 2.066682146441552

Epoch: 6| Step: 1
Training loss: 2.4369444847106934
Validation loss: 2.074375403824673

Epoch: 6| Step: 2
Training loss: 2.052931308746338
Validation loss: 2.0753418604532876

Epoch: 6| Step: 3
Training loss: 1.910928726196289
Validation loss: 2.0451069390901955

Epoch: 6| Step: 4
Training loss: 1.48160982131958
Validation loss: 2.063907011862724

Epoch: 6| Step: 5
Training loss: 2.28751540184021
Validation loss: 2.0677235882769347

Epoch: 6| Step: 6
Training loss: 2.830519199371338
Validation loss: 2.0726192638438237

Epoch: 6| Step: 7
Training loss: 2.193291664123535
Validation loss: 2.059466228690199

Epoch: 6| Step: 8
Training loss: 1.794862985610962
Validation loss: 2.0992679442128828

Epoch: 6| Step: 9
Training loss: 2.171186685562134
Validation loss: 2.0720474681546612

Epoch: 6| Step: 10
Training loss: 2.6087663173675537
Validation loss: 2.0755450469191357

Epoch: 6| Step: 11
Training loss: 2.7327332496643066
Validation loss: 2.0755859357054516

Epoch: 6| Step: 12
Training loss: 2.641204595565796
Validation loss: 2.0772866331120974

Epoch: 6| Step: 13
Training loss: 1.9581778049468994
Validation loss: 2.0786329136099866

Epoch: 199| Step: 0
Training loss: 2.3215041160583496
Validation loss: 2.09438415240216

Epoch: 6| Step: 1
Training loss: 2.574836254119873
Validation loss: 2.0730934707067346

Epoch: 6| Step: 2
Training loss: 1.5650097131729126
Validation loss: 2.08833041755102

Epoch: 6| Step: 3
Training loss: 2.619955062866211
Validation loss: 2.106709772540677

Epoch: 6| Step: 4
Training loss: 1.7653071880340576
Validation loss: 2.094817275642067

Epoch: 6| Step: 5
Training loss: 2.4579734802246094
Validation loss: 2.0763365440471198

Epoch: 6| Step: 6
Training loss: 1.0453059673309326
Validation loss: 2.075928811104067

Epoch: 6| Step: 7
Training loss: 2.4478330612182617
Validation loss: 2.0859387202929427

Epoch: 6| Step: 8
Training loss: 2.7865028381347656
Validation loss: 2.063739733029437

Epoch: 6| Step: 9
Training loss: 2.1004745960235596
Validation loss: 2.1172463355525846

Epoch: 6| Step: 10
Training loss: 2.0494985580444336
Validation loss: 2.0571671109045706

Epoch: 6| Step: 11
Training loss: 2.512835741043091
Validation loss: 2.0872979574306036

Epoch: 6| Step: 12
Training loss: 2.7439889907836914
Validation loss: 2.101758549290319

Epoch: 6| Step: 13
Training loss: 1.26149320602417
Validation loss: 2.059134852501654

Epoch: 200| Step: 0
Training loss: 1.4673064947128296
Validation loss: 2.086896627180038

Epoch: 6| Step: 1
Training loss: 1.8073545694351196
Validation loss: 2.0820393972499396

Epoch: 6| Step: 2
Training loss: 2.766679525375366
Validation loss: 2.094755159911289

Epoch: 6| Step: 3
Training loss: 2.4574499130249023
Validation loss: 2.077160314847064

Epoch: 6| Step: 4
Training loss: 2.0675339698791504
Validation loss: 2.0691348993650047

Epoch: 6| Step: 5
Training loss: 2.1729514598846436
Validation loss: 2.0559732067969536

Epoch: 6| Step: 6
Training loss: 2.9186739921569824
Validation loss: 2.0636665667257

Epoch: 6| Step: 7
Training loss: 1.6707042455673218
Validation loss: 2.0544064493589502

Epoch: 6| Step: 8
Training loss: 1.8588318824768066
Validation loss: 2.0537530555520007

Epoch: 6| Step: 9
Training loss: 2.7348694801330566
Validation loss: 2.0693809178567704

Epoch: 6| Step: 10
Training loss: 2.8688015937805176
Validation loss: 2.057359098106302

Epoch: 6| Step: 11
Training loss: 1.847362995147705
Validation loss: 2.055996561563143

Epoch: 6| Step: 12
Training loss: 1.984157919883728
Validation loss: 2.06465543085529

Epoch: 6| Step: 13
Training loss: 2.1647419929504395
Validation loss: 2.074769030335129

Epoch: 201| Step: 0
Training loss: 2.196422576904297
Validation loss: 2.0594715533717984

Epoch: 6| Step: 1
Training loss: 1.5760852098464966
Validation loss: 2.06388363786923

Epoch: 6| Step: 2
Training loss: 3.0216572284698486
Validation loss: 2.083561087167391

Epoch: 6| Step: 3
Training loss: 2.1647698879241943
Validation loss: 2.075459828940771

Epoch: 6| Step: 4
Training loss: 2.5289058685302734
Validation loss: 2.05996851382717

Epoch: 6| Step: 5
Training loss: 2.782867431640625
Validation loss: 2.077252577709895

Epoch: 6| Step: 6
Training loss: 1.819199562072754
Validation loss: 2.0662618734503306

Epoch: 6| Step: 7
Training loss: 1.9750702381134033
Validation loss: 2.111714348998121

Epoch: 6| Step: 8
Training loss: 2.6821696758270264
Validation loss: 2.089559278180522

Epoch: 6| Step: 9
Training loss: 2.1613192558288574
Validation loss: 2.070163288424092

Epoch: 6| Step: 10
Training loss: 2.3214731216430664
Validation loss: 2.0759914536629953

Epoch: 6| Step: 11
Training loss: 2.3323261737823486
Validation loss: 2.1013254452777166

Epoch: 6| Step: 12
Training loss: 1.4044642448425293
Validation loss: 2.0838837726141817

Epoch: 6| Step: 13
Training loss: 1.5428881645202637
Validation loss: 2.0653323101741012

Epoch: 202| Step: 0
Training loss: 2.69541335105896
Validation loss: 2.09257871361189

Epoch: 6| Step: 1
Training loss: 1.958198070526123
Validation loss: 2.087352906503985

Epoch: 6| Step: 2
Training loss: 1.8663667440414429
Validation loss: 2.083485395677628

Epoch: 6| Step: 3
Training loss: 1.8877323865890503
Validation loss: 2.1075054458392564

Epoch: 6| Step: 4
Training loss: 2.118194341659546
Validation loss: 2.130268716043042

Epoch: 6| Step: 5
Training loss: 2.4726264476776123
Validation loss: 2.1150183664855136

Epoch: 6| Step: 6
Training loss: 1.8438916206359863
Validation loss: 2.1162751208069506

Epoch: 6| Step: 7
Training loss: 2.182681083679199
Validation loss: 2.12149017600603

Epoch: 6| Step: 8
Training loss: 1.778958797454834
Validation loss: 2.0944811656910884

Epoch: 6| Step: 9
Training loss: 2.386333703994751
Validation loss: 2.1092740258862896

Epoch: 6| Step: 10
Training loss: 2.338118076324463
Validation loss: 2.0937206975875364

Epoch: 6| Step: 11
Training loss: 2.0911316871643066
Validation loss: 2.090024704574257

Epoch: 6| Step: 12
Training loss: 2.4621756076812744
Validation loss: 2.094034000109601

Epoch: 6| Step: 13
Training loss: 3.060194969177246
Validation loss: 2.0845100187486216

Epoch: 203| Step: 0
Training loss: 2.510458469390869
Validation loss: 2.1316359402031027

Epoch: 6| Step: 1
Training loss: 2.51699161529541
Validation loss: 2.0968475393069688

Epoch: 6| Step: 2
Training loss: 2.09102725982666
Validation loss: 2.117726831025975

Epoch: 6| Step: 3
Training loss: 2.4780707359313965
Validation loss: 2.1071683283775084

Epoch: 6| Step: 4
Training loss: 2.1419010162353516
Validation loss: 2.088421347320721

Epoch: 6| Step: 5
Training loss: 2.4154865741729736
Validation loss: 2.076339243560709

Epoch: 6| Step: 6
Training loss: 2.196840286254883
Validation loss: 2.1250575178412983

Epoch: 6| Step: 7
Training loss: 2.36812686920166
Validation loss: 2.078512296881727

Epoch: 6| Step: 8
Training loss: 2.3470511436462402
Validation loss: 2.094713157223117

Epoch: 6| Step: 9
Training loss: 1.6892871856689453
Validation loss: 2.095703463400564

Epoch: 6| Step: 10
Training loss: 2.4997477531433105
Validation loss: 2.073164213088251

Epoch: 6| Step: 11
Training loss: 1.8979642391204834
Validation loss: 2.094829543944328

Epoch: 6| Step: 12
Training loss: 2.119727611541748
Validation loss: 2.091619327504148

Epoch: 6| Step: 13
Training loss: 1.4356151819229126
Validation loss: 2.077503619655486

Epoch: 204| Step: 0
Training loss: 2.266420602798462
Validation loss: 2.0706969499588013

Epoch: 6| Step: 1
Training loss: 1.4963620901107788
Validation loss: 2.088231109803723

Epoch: 6| Step: 2
Training loss: 2.115394115447998
Validation loss: 2.075320792454545

Epoch: 6| Step: 3
Training loss: 2.582120656967163
Validation loss: 2.1038959128882295

Epoch: 6| Step: 4
Training loss: 2.2503662109375
Validation loss: 2.094486524981837

Epoch: 6| Step: 5
Training loss: 2.295149087905884
Validation loss: 2.08664692730032

Epoch: 6| Step: 6
Training loss: 2.6939291954040527
Validation loss: 2.0786273735825733

Epoch: 6| Step: 7
Training loss: 2.5159430503845215
Validation loss: 2.06588404152983

Epoch: 6| Step: 8
Training loss: 1.6783533096313477
Validation loss: 2.0794068549268987

Epoch: 6| Step: 9
Training loss: 2.161513328552246
Validation loss: 2.086381212357552

Epoch: 6| Step: 10
Training loss: 2.9656171798706055
Validation loss: 2.077606757481893

Epoch: 6| Step: 11
Training loss: 1.255515217781067
Validation loss: 2.065179427464803

Epoch: 6| Step: 12
Training loss: 2.3778486251831055
Validation loss: 2.0919205501515377

Epoch: 6| Step: 13
Training loss: 1.862844705581665
Validation loss: 2.0884821248310868

Epoch: 205| Step: 0
Training loss: 1.9039329290390015
Validation loss: 2.0745833740439465

Epoch: 6| Step: 1
Training loss: 2.2560157775878906
Validation loss: 2.0724998033174904

Epoch: 6| Step: 2
Training loss: 2.209280014038086
Validation loss: 2.068340075913296

Epoch: 6| Step: 3
Training loss: 1.7607882022857666
Validation loss: 2.0914997131593767

Epoch: 6| Step: 4
Training loss: 2.641684055328369
Validation loss: 2.073757463885892

Epoch: 6| Step: 5
Training loss: 1.8604919910430908
Validation loss: 2.0569020214901177

Epoch: 6| Step: 6
Training loss: 2.3341171741485596
Validation loss: 2.082922375330361

Epoch: 6| Step: 7
Training loss: 1.8611903190612793
Validation loss: 2.1010546671446932

Epoch: 6| Step: 8
Training loss: 2.9750046730041504
Validation loss: 2.079271589556048

Epoch: 6| Step: 9
Training loss: 2.2678613662719727
Validation loss: 2.0908436518843456

Epoch: 6| Step: 10
Training loss: 1.8049263954162598
Validation loss: 2.133022416022516

Epoch: 6| Step: 11
Training loss: 2.3199853897094727
Validation loss: 2.0676649924247497

Epoch: 6| Step: 12
Training loss: 1.8882368803024292
Validation loss: 2.08382591637232

Epoch: 6| Step: 13
Training loss: 2.7387187480926514
Validation loss: 2.0769150462201846

Epoch: 206| Step: 0
Training loss: 2.131993532180786
Validation loss: 2.0800219479427544

Epoch: 6| Step: 1
Training loss: 1.5370573997497559
Validation loss: 2.088153987802485

Epoch: 6| Step: 2
Training loss: 2.853557586669922
Validation loss: 2.1186469370318997

Epoch: 6| Step: 3
Training loss: 2.082693099975586
Validation loss: 2.0986375629260974

Epoch: 6| Step: 4
Training loss: 1.8457196950912476
Validation loss: 2.0982469461297475

Epoch: 6| Step: 5
Training loss: 2.3920176029205322
Validation loss: 2.1091025901097122

Epoch: 6| Step: 6
Training loss: 2.4050145149230957
Validation loss: 2.076191794487738

Epoch: 6| Step: 7
Training loss: 2.063875198364258
Validation loss: 2.0944200587529007

Epoch: 6| Step: 8
Training loss: 2.084376335144043
Validation loss: 2.0888117538985385

Epoch: 6| Step: 9
Training loss: 2.2280943393707275
Validation loss: 2.096535554496191

Epoch: 6| Step: 10
Training loss: 1.723607063293457
Validation loss: 2.082619479907456

Epoch: 6| Step: 11
Training loss: 2.533493995666504
Validation loss: 2.108860282487767

Epoch: 6| Step: 12
Training loss: 2.064809799194336
Validation loss: 2.0792747107885217

Epoch: 6| Step: 13
Training loss: 2.988922595977783
Validation loss: 2.1057528206097182

Epoch: 207| Step: 0
Training loss: 1.4293792247772217
Validation loss: 2.095460138013286

Epoch: 6| Step: 1
Training loss: 1.7324750423431396
Validation loss: 2.093287701247841

Epoch: 6| Step: 2
Training loss: 1.8939247131347656
Validation loss: 2.0697709232248287

Epoch: 6| Step: 3
Training loss: 2.7620656490325928
Validation loss: 2.090302751910302

Epoch: 6| Step: 4
Training loss: 2.58701753616333
Validation loss: 2.093598255547144

Epoch: 6| Step: 5
Training loss: 2.058166980743408
Validation loss: 2.0924709535414174

Epoch: 6| Step: 6
Training loss: 2.123781681060791
Validation loss: 2.1011132014695035

Epoch: 6| Step: 7
Training loss: 2.2811079025268555
Validation loss: 2.103021906268212

Epoch: 6| Step: 8
Training loss: 1.7383570671081543
Validation loss: 2.058674568771034

Epoch: 6| Step: 9
Training loss: 2.5660364627838135
Validation loss: 2.0644233918959096

Epoch: 6| Step: 10
Training loss: 2.2457656860351562
Validation loss: 2.0616191176957983

Epoch: 6| Step: 11
Training loss: 2.250009536743164
Validation loss: 2.0659171765850437

Epoch: 6| Step: 12
Training loss: 2.0181121826171875
Validation loss: 2.05242500253903

Epoch: 6| Step: 13
Training loss: 3.2425787448883057
Validation loss: 2.06649306256284

Epoch: 208| Step: 0
Training loss: 2.776808261871338
Validation loss: 2.072723296380812

Epoch: 6| Step: 1
Training loss: 2.036703109741211
Validation loss: 2.0655121623828845

Epoch: 6| Step: 2
Training loss: 1.888266682624817
Validation loss: 2.0603927361067904

Epoch: 6| Step: 3
Training loss: 1.939677357673645
Validation loss: 2.069509316516179

Epoch: 6| Step: 4
Training loss: 1.3763291835784912
Validation loss: 2.068211224771315

Epoch: 6| Step: 5
Training loss: 2.0613136291503906
Validation loss: 2.0887007995318343

Epoch: 6| Step: 6
Training loss: 2.1068320274353027
Validation loss: 2.1002772444037983

Epoch: 6| Step: 7
Training loss: 3.0044116973876953
Validation loss: 2.0743347444841937

Epoch: 6| Step: 8
Training loss: 2.5403685569763184
Validation loss: 2.095067254958614

Epoch: 6| Step: 9
Training loss: 2.2663207054138184
Validation loss: 2.111147875426918

Epoch: 6| Step: 10
Training loss: 2.4986393451690674
Validation loss: 2.098050122619957

Epoch: 6| Step: 11
Training loss: 2.235555648803711
Validation loss: 2.0930172935608895

Epoch: 6| Step: 12
Training loss: 1.8004934787750244
Validation loss: 2.081468835953743

Epoch: 6| Step: 13
Training loss: 2.0884625911712646
Validation loss: 2.1125478488142773

Epoch: 209| Step: 0
Training loss: 2.539240598678589
Validation loss: 2.0933083526549803

Epoch: 6| Step: 1
Training loss: 2.734527826309204
Validation loss: 2.0710866810173116

Epoch: 6| Step: 2
Training loss: 1.7168304920196533
Validation loss: 2.07585653438363

Epoch: 6| Step: 3
Training loss: 1.893753170967102
Validation loss: 2.093050515779885

Epoch: 6| Step: 4
Training loss: 2.6265783309936523
Validation loss: 2.0696857642101985

Epoch: 6| Step: 5
Training loss: 2.702711343765259
Validation loss: 2.078908484469178

Epoch: 6| Step: 6
Training loss: 1.7614573240280151
Validation loss: 2.085162385817497

Epoch: 6| Step: 7
Training loss: 2.231090545654297
Validation loss: 2.09122605733974

Epoch: 6| Step: 8
Training loss: 1.661789059638977
Validation loss: 2.1056698009531987

Epoch: 6| Step: 9
Training loss: 2.2702269554138184
Validation loss: 2.089600909140802

Epoch: 6| Step: 10
Training loss: 2.0426740646362305
Validation loss: 2.0930882551336802

Epoch: 6| Step: 11
Training loss: 2.148991584777832
Validation loss: 2.0504551241474767

Epoch: 6| Step: 12
Training loss: 2.1930437088012695
Validation loss: 2.05923225930942

Epoch: 6| Step: 13
Training loss: 2.246957778930664
Validation loss: 2.0743862941700923

Epoch: 210| Step: 0
Training loss: 2.0764012336730957
Validation loss: 2.0738091878993536

Epoch: 6| Step: 1
Training loss: 1.6620415449142456
Validation loss: 2.0910587708155313

Epoch: 6| Step: 2
Training loss: 1.6163485050201416
Validation loss: 2.0788033470030753

Epoch: 6| Step: 3
Training loss: 2.7285194396972656
Validation loss: 2.080375850841563

Epoch: 6| Step: 4
Training loss: 2.4451651573181152
Validation loss: 2.1005799744718816

Epoch: 6| Step: 5
Training loss: 2.1502819061279297
Validation loss: 2.097851517379925

Epoch: 6| Step: 6
Training loss: 2.3873696327209473
Validation loss: 2.1085195131199335

Epoch: 6| Step: 7
Training loss: 1.86057448387146
Validation loss: 2.1271272756720103

Epoch: 6| Step: 8
Training loss: 1.9438939094543457
Validation loss: 2.0772067321244108

Epoch: 6| Step: 9
Training loss: 2.558870553970337
Validation loss: 2.1053281253384006

Epoch: 6| Step: 10
Training loss: 1.9925893545150757
Validation loss: 2.0899656280394523

Epoch: 6| Step: 11
Training loss: 2.62443208694458
Validation loss: 2.0958837347645916

Epoch: 6| Step: 12
Training loss: 2.516075849533081
Validation loss: 2.0927036590473627

Epoch: 6| Step: 13
Training loss: 1.9955363273620605
Validation loss: 2.096506113647133

Epoch: 211| Step: 0
Training loss: 2.1022114753723145
Validation loss: 2.0671807642905944

Epoch: 6| Step: 1
Training loss: 2.5070691108703613
Validation loss: 2.075259725252787

Epoch: 6| Step: 2
Training loss: 2.1497676372528076
Validation loss: 2.069702140746578

Epoch: 6| Step: 3
Training loss: 2.244330883026123
Validation loss: 2.099254395372124

Epoch: 6| Step: 4
Training loss: 2.2504782676696777
Validation loss: 2.07758693797614

Epoch: 6| Step: 5
Training loss: 2.0429916381835938
Validation loss: 2.0913503554559525

Epoch: 6| Step: 6
Training loss: 2.6369080543518066
Validation loss: 2.08738874876371

Epoch: 6| Step: 7
Training loss: 1.9998259544372559
Validation loss: 2.09678114357815

Epoch: 6| Step: 8
Training loss: 2.229334831237793
Validation loss: 2.1088018776268087

Epoch: 6| Step: 9
Training loss: 2.3541958332061768
Validation loss: 2.09671389800246

Epoch: 6| Step: 10
Training loss: 2.1916232109069824
Validation loss: 2.1111106795649373

Epoch: 6| Step: 11
Training loss: 2.032510995864868
Validation loss: 2.0955698631143056

Epoch: 6| Step: 12
Training loss: 1.9994581937789917
Validation loss: 2.104555904224355

Epoch: 6| Step: 13
Training loss: 2.019599676132202
Validation loss: 2.100364295385217

Epoch: 212| Step: 0
Training loss: 1.747671127319336
Validation loss: 2.1178104800562703

Epoch: 6| Step: 1
Training loss: 1.9886268377304077
Validation loss: 2.110639135042826

Epoch: 6| Step: 2
Training loss: 2.5789709091186523
Validation loss: 2.074820400566183

Epoch: 6| Step: 3
Training loss: 1.9483948945999146
Validation loss: 2.0940649381247898

Epoch: 6| Step: 4
Training loss: 1.8073065280914307
Validation loss: 2.087675645787229

Epoch: 6| Step: 5
Training loss: 2.7950637340545654
Validation loss: 2.0973189928198375

Epoch: 6| Step: 6
Training loss: 1.6271427869796753
Validation loss: 2.0829594494194112

Epoch: 6| Step: 7
Training loss: 2.815458297729492
Validation loss: 2.133780143594229

Epoch: 6| Step: 8
Training loss: 2.1807026863098145
Validation loss: 2.0976387249526156

Epoch: 6| Step: 9
Training loss: 1.8952511548995972
Validation loss: 2.105299613809073

Epoch: 6| Step: 10
Training loss: 2.6384048461914062
Validation loss: 2.1051601363766577

Epoch: 6| Step: 11
Training loss: 2.3439245223999023
Validation loss: 2.0884010637960126

Epoch: 6| Step: 12
Training loss: 1.8279303312301636
Validation loss: 2.09416106952134

Epoch: 6| Step: 13
Training loss: 2.279280662536621
Validation loss: 2.0687081019083657

Epoch: 213| Step: 0
Training loss: 1.7457587718963623
Validation loss: 2.1032565562955794

Epoch: 6| Step: 1
Training loss: 2.2344555854797363
Validation loss: 2.062198623534172

Epoch: 6| Step: 2
Training loss: 2.6926186084747314
Validation loss: 2.0682658277532107

Epoch: 6| Step: 3
Training loss: 2.5036914348602295
Validation loss: 2.0720957248441634

Epoch: 6| Step: 4
Training loss: 1.0565955638885498
Validation loss: 2.0326645284570675

Epoch: 6| Step: 5
Training loss: 2.920626163482666
Validation loss: 2.090964396794637

Epoch: 6| Step: 6
Training loss: 2.070512294769287
Validation loss: 2.094418780778044

Epoch: 6| Step: 7
Training loss: 2.184535503387451
Validation loss: 2.057774956508349

Epoch: 6| Step: 8
Training loss: 1.8822218179702759
Validation loss: 2.084386871707055

Epoch: 6| Step: 9
Training loss: 2.4574036598205566
Validation loss: 2.077849531686434

Epoch: 6| Step: 10
Training loss: 2.6316163539886475
Validation loss: 2.071233839117071

Epoch: 6| Step: 11
Training loss: 2.168152332305908
Validation loss: 2.0559148506451677

Epoch: 6| Step: 12
Training loss: 1.9950447082519531
Validation loss: 2.0704212675812426

Epoch: 6| Step: 13
Training loss: 2.2983202934265137
Validation loss: 2.060454476264215

Epoch: 214| Step: 0
Training loss: 2.4204485416412354
Validation loss: 2.039366361915424

Epoch: 6| Step: 1
Training loss: 2.9024465084075928
Validation loss: 2.0580834457951207

Epoch: 6| Step: 2
Training loss: 1.445800542831421
Validation loss: 2.0689379733095885

Epoch: 6| Step: 3
Training loss: 2.0833022594451904
Validation loss: 2.0826159138833322

Epoch: 6| Step: 4
Training loss: 2.0158145427703857
Validation loss: 2.0793300495352796

Epoch: 6| Step: 5
Training loss: 2.0322203636169434
Validation loss: 2.0610121809026247

Epoch: 6| Step: 6
Training loss: 2.121084690093994
Validation loss: 2.0646474310146865

Epoch: 6| Step: 7
Training loss: 2.1903738975524902
Validation loss: 2.0881916194833736

Epoch: 6| Step: 8
Training loss: 2.535261631011963
Validation loss: 2.0797338716445433

Epoch: 6| Step: 9
Training loss: 2.539123296737671
Validation loss: 2.063069117966519

Epoch: 6| Step: 10
Training loss: 2.1520681381225586
Validation loss: 2.0670980176618023

Epoch: 6| Step: 11
Training loss: 2.013246536254883
Validation loss: 2.0638621558425245

Epoch: 6| Step: 12
Training loss: 1.9827864170074463
Validation loss: 2.090835080351881

Epoch: 6| Step: 13
Training loss: 2.1013271808624268
Validation loss: 2.0878929553493375

Epoch: 215| Step: 0
Training loss: 2.069603681564331
Validation loss: 2.0696588126561974

Epoch: 6| Step: 1
Training loss: 1.9547936916351318
Validation loss: 2.1023461741785847

Epoch: 6| Step: 2
Training loss: 2.970557928085327
Validation loss: 2.1036629356363767

Epoch: 6| Step: 3
Training loss: 2.059375762939453
Validation loss: 2.1376906389831216

Epoch: 6| Step: 4
Training loss: 1.9382048845291138
Validation loss: 2.0856498108115247

Epoch: 6| Step: 5
Training loss: 2.488394021987915
Validation loss: 2.09068569573023

Epoch: 6| Step: 6
Training loss: 2.005289077758789
Validation loss: 2.1115590603120866

Epoch: 6| Step: 7
Training loss: 2.76541805267334
Validation loss: 2.0938661008752804

Epoch: 6| Step: 8
Training loss: 2.5338103771209717
Validation loss: 2.111566505124492

Epoch: 6| Step: 9
Training loss: 2.5570788383483887
Validation loss: 2.0879656050794866

Epoch: 6| Step: 10
Training loss: 1.491321086883545
Validation loss: 2.094582441032574

Epoch: 6| Step: 11
Training loss: 1.899399995803833
Validation loss: 2.1288309276744886

Epoch: 6| Step: 12
Training loss: 1.3687083721160889
Validation loss: 2.093344755070184

Epoch: 6| Step: 13
Training loss: 2.3525872230529785
Validation loss: 2.100582047175336

Epoch: 216| Step: 0
Training loss: 2.3869848251342773
Validation loss: 2.105965527155066

Epoch: 6| Step: 1
Training loss: 2.5052945613861084
Validation loss: 2.1046649115059965

Epoch: 6| Step: 2
Training loss: 1.8058282136917114
Validation loss: 2.0938919385274253

Epoch: 6| Step: 3
Training loss: 2.3694629669189453
Validation loss: 2.0790640090101506

Epoch: 6| Step: 4
Training loss: 2.0639655590057373
Validation loss: 2.0988221078790645

Epoch: 6| Step: 5
Training loss: 1.89730966091156
Validation loss: 2.099111162206178

Epoch: 6| Step: 6
Training loss: 1.8499960899353027
Validation loss: 2.074556491708243

Epoch: 6| Step: 7
Training loss: 1.8691798448562622
Validation loss: 2.08421056501327

Epoch: 6| Step: 8
Training loss: 2.285818338394165
Validation loss: 2.1023195200068976

Epoch: 6| Step: 9
Training loss: 2.017622709274292
Validation loss: 2.0684750054472234

Epoch: 6| Step: 10
Training loss: 2.6398730278015137
Validation loss: 2.078838607316376

Epoch: 6| Step: 11
Training loss: 1.9088143110275269
Validation loss: 2.0899561118054133

Epoch: 6| Step: 12
Training loss: 2.170525074005127
Validation loss: 2.075447315810829

Epoch: 6| Step: 13
Training loss: 2.9908461570739746
Validation loss: 2.102339647149527

Epoch: 217| Step: 0
Training loss: 2.0983529090881348
Validation loss: 2.075387203565208

Epoch: 6| Step: 1
Training loss: 1.8772584199905396
Validation loss: 2.1275697703002603

Epoch: 6| Step: 2
Training loss: 2.418945074081421
Validation loss: 2.096617898633403

Epoch: 6| Step: 3
Training loss: 1.8623268604278564
Validation loss: 2.0860980441493373

Epoch: 6| Step: 4
Training loss: 1.7332080602645874
Validation loss: 2.1033955094634846

Epoch: 6| Step: 5
Training loss: 2.143277168273926
Validation loss: 2.1115813434764905

Epoch: 6| Step: 6
Training loss: 2.468635082244873
Validation loss: 2.0886441738374772

Epoch: 6| Step: 7
Training loss: 2.1124114990234375
Validation loss: 2.091592547714069

Epoch: 6| Step: 8
Training loss: 1.8319898843765259
Validation loss: 2.1187949295966857

Epoch: 6| Step: 9
Training loss: 2.781705379486084
Validation loss: 2.12317923063873

Epoch: 6| Step: 10
Training loss: 2.076137065887451
Validation loss: 2.102325047216108

Epoch: 6| Step: 11
Training loss: 1.8919302225112915
Validation loss: 2.1141826286110827

Epoch: 6| Step: 12
Training loss: 2.5663132667541504
Validation loss: 2.116343298266011

Epoch: 6| Step: 13
Training loss: 2.8732147216796875
Validation loss: 2.1123858472352386

Epoch: 218| Step: 0
Training loss: 2.6595382690429688
Validation loss: 2.1011911976721978

Epoch: 6| Step: 1
Training loss: 2.021186351776123
Validation loss: 2.1075123035779564

Epoch: 6| Step: 2
Training loss: 2.4068479537963867
Validation loss: 2.107340133318337

Epoch: 6| Step: 3
Training loss: 2.3238587379455566
Validation loss: 2.1098980493442987

Epoch: 6| Step: 4
Training loss: 1.9822993278503418
Validation loss: 2.0990108751481578

Epoch: 6| Step: 5
Training loss: 1.8753796815872192
Validation loss: 2.098204393540659

Epoch: 6| Step: 6
Training loss: 1.7174875736236572
Validation loss: 2.105933177855707

Epoch: 6| Step: 7
Training loss: 2.1388721466064453
Validation loss: 2.098623198847617

Epoch: 6| Step: 8
Training loss: 2.9432003498077393
Validation loss: 2.0733995065894177

Epoch: 6| Step: 9
Training loss: 1.7083297967910767
Validation loss: 2.090391728185838

Epoch: 6| Step: 10
Training loss: 2.1581380367279053
Validation loss: 2.086047475055982

Epoch: 6| Step: 11
Training loss: 2.1838667392730713
Validation loss: 2.1018671451076383

Epoch: 6| Step: 12
Training loss: 2.4832353591918945
Validation loss: 2.0823873371206303

Epoch: 6| Step: 13
Training loss: 1.4912123680114746
Validation loss: 2.0705798184999855

Epoch: 219| Step: 0
Training loss: 2.6557915210723877
Validation loss: 2.0861056171437746

Epoch: 6| Step: 1
Training loss: 1.9987345933914185
Validation loss: 2.1253410731592486

Epoch: 6| Step: 2
Training loss: 1.7486687898635864
Validation loss: 2.084986895643255

Epoch: 6| Step: 3
Training loss: 2.1008450984954834
Validation loss: 2.0790207744926534

Epoch: 6| Step: 4
Training loss: 2.183709144592285
Validation loss: 2.112459013538976

Epoch: 6| Step: 5
Training loss: 1.6643855571746826
Validation loss: 2.077616524952714

Epoch: 6| Step: 6
Training loss: 1.659328818321228
Validation loss: 2.067930629176478

Epoch: 6| Step: 7
Training loss: 2.232508659362793
Validation loss: 2.089455355880081

Epoch: 6| Step: 8
Training loss: 2.5344433784484863
Validation loss: 2.104697504351216

Epoch: 6| Step: 9
Training loss: 2.750509738922119
Validation loss: 2.0988741074838946

Epoch: 6| Step: 10
Training loss: 2.155606985092163
Validation loss: 2.0673872373437368

Epoch: 6| Step: 11
Training loss: 2.048415422439575
Validation loss: 2.1093765317752795

Epoch: 6| Step: 12
Training loss: 2.5400948524475098
Validation loss: 2.08324461214004

Epoch: 6| Step: 13
Training loss: 2.239656686782837
Validation loss: 2.088254319724216

Epoch: 220| Step: 0
Training loss: 1.7560980319976807
Validation loss: 2.1057187254710863

Epoch: 6| Step: 1
Training loss: 1.7126646041870117
Validation loss: 2.098074746388261

Epoch: 6| Step: 2
Training loss: 2.4902596473693848
Validation loss: 2.0563944667898197

Epoch: 6| Step: 3
Training loss: 2.558868885040283
Validation loss: 2.0863297318899505

Epoch: 6| Step: 4
Training loss: 2.0451550483703613
Validation loss: 2.0759769908843504

Epoch: 6| Step: 5
Training loss: 2.4280781745910645
Validation loss: 2.064099964275155

Epoch: 6| Step: 6
Training loss: 1.855193018913269
Validation loss: 2.0807796678235455

Epoch: 6| Step: 7
Training loss: 2.6431069374084473
Validation loss: 2.0722467412230787

Epoch: 6| Step: 8
Training loss: 2.1717381477355957
Validation loss: 2.077144792003016

Epoch: 6| Step: 9
Training loss: 2.1224207878112793
Validation loss: 2.06225138197663

Epoch: 6| Step: 10
Training loss: 2.036952257156372
Validation loss: 2.0838785914964575

Epoch: 6| Step: 11
Training loss: 2.0513558387756348
Validation loss: 2.1019412638038717

Epoch: 6| Step: 12
Training loss: 2.2278590202331543
Validation loss: 2.1127725903705885

Epoch: 6| Step: 13
Training loss: 2.216233015060425
Validation loss: 2.075221061706543

Epoch: 221| Step: 0
Training loss: 2.8811986446380615
Validation loss: 2.0692909366341046

Epoch: 6| Step: 1
Training loss: 2.281010150909424
Validation loss: 2.0681072332525767

Epoch: 6| Step: 2
Training loss: 1.2607074975967407
Validation loss: 2.0521799056760726

Epoch: 6| Step: 3
Training loss: 2.201683759689331
Validation loss: 2.072638906458373

Epoch: 6| Step: 4
Training loss: 3.0597686767578125
Validation loss: 2.0664985679811045

Epoch: 6| Step: 5
Training loss: 2.7605555057525635
Validation loss: 2.095594351009656

Epoch: 6| Step: 6
Training loss: 2.207332134246826
Validation loss: 2.0844973107819915

Epoch: 6| Step: 7
Training loss: 1.4056761264801025
Validation loss: 2.0610637152066795

Epoch: 6| Step: 8
Training loss: 2.5295963287353516
Validation loss: 2.079490674439297

Epoch: 6| Step: 9
Training loss: 1.7612006664276123
Validation loss: 2.0517544490034862

Epoch: 6| Step: 10
Training loss: 1.3904920816421509
Validation loss: 2.0502128267800934

Epoch: 6| Step: 11
Training loss: 2.437018394470215
Validation loss: 2.078525170203178

Epoch: 6| Step: 12
Training loss: 2.209801197052002
Validation loss: 2.0685350818018757

Epoch: 6| Step: 13
Training loss: 1.9392046928405762
Validation loss: 2.0454295527550483

Epoch: 222| Step: 0
Training loss: 1.4515787363052368
Validation loss: 2.070460122118714

Epoch: 6| Step: 1
Training loss: 2.0323433876037598
Validation loss: 2.0990968699096353

Epoch: 6| Step: 2
Training loss: 1.688552737236023
Validation loss: 2.0770466968577397

Epoch: 6| Step: 3
Training loss: 1.982723593711853
Validation loss: 2.0694727743825605

Epoch: 6| Step: 4
Training loss: 2.516618490219116
Validation loss: 2.105156062751688

Epoch: 6| Step: 5
Training loss: 2.815070629119873
Validation loss: 2.1044716091566187

Epoch: 6| Step: 6
Training loss: 2.1489906311035156
Validation loss: 2.0755398183740597

Epoch: 6| Step: 7
Training loss: 2.2147834300994873
Validation loss: 2.115694947140191

Epoch: 6| Step: 8
Training loss: 3.1303436756134033
Validation loss: 2.1100951638273013

Epoch: 6| Step: 9
Training loss: 1.9528605937957764
Validation loss: 2.1086850384230256

Epoch: 6| Step: 10
Training loss: 2.1773924827575684
Validation loss: 2.118981856171803

Epoch: 6| Step: 11
Training loss: 1.9734416007995605
Validation loss: 2.102861459537219

Epoch: 6| Step: 12
Training loss: 2.388176441192627
Validation loss: 2.117098846743184

Epoch: 6| Step: 13
Training loss: 1.8346644639968872
Validation loss: 2.0944237683409

Epoch: 223| Step: 0
Training loss: 3.1806704998016357
Validation loss: 2.1247397045935354

Epoch: 6| Step: 1
Training loss: 1.7870562076568604
Validation loss: 2.1170317024312992

Epoch: 6| Step: 2
Training loss: 2.10219669342041
Validation loss: 2.1211801703258226

Epoch: 6| Step: 3
Training loss: 1.8687381744384766
Validation loss: 2.097444893211447

Epoch: 6| Step: 4
Training loss: 2.222926616668701
Validation loss: 2.0954294127802693

Epoch: 6| Step: 5
Training loss: 2.451453685760498
Validation loss: 2.092918736960298

Epoch: 6| Step: 6
Training loss: 2.0127243995666504
Validation loss: 2.1186952937033867

Epoch: 6| Step: 7
Training loss: 2.6530649662017822
Validation loss: 2.1225558532181608

Epoch: 6| Step: 8
Training loss: 2.765538215637207
Validation loss: 2.098353365416168

Epoch: 6| Step: 9
Training loss: 1.9480375051498413
Validation loss: 2.1005109497295913

Epoch: 6| Step: 10
Training loss: 1.6264212131500244
Validation loss: 2.1091233991807505

Epoch: 6| Step: 11
Training loss: 1.5738468170166016
Validation loss: 2.117183396893163

Epoch: 6| Step: 12
Training loss: 2.3656749725341797
Validation loss: 2.0462004138577368

Epoch: 6| Step: 13
Training loss: 1.6319079399108887
Validation loss: 2.1256512954670894

Epoch: 224| Step: 0
Training loss: 1.7036750316619873
Validation loss: 2.124836992192012

Epoch: 6| Step: 1
Training loss: 2.6356041431427
Validation loss: 2.096855484029298

Epoch: 6| Step: 2
Training loss: 2.161134958267212
Validation loss: 2.120999854098084

Epoch: 6| Step: 3
Training loss: 1.2348203659057617
Validation loss: 2.1012136167095554

Epoch: 6| Step: 4
Training loss: 2.177868127822876
Validation loss: 2.1290985794477564

Epoch: 6| Step: 5
Training loss: 2.415696620941162
Validation loss: 2.147706393272646

Epoch: 6| Step: 6
Training loss: 2.3873324394226074
Validation loss: 2.1159694528066986

Epoch: 6| Step: 7
Training loss: 1.931909441947937
Validation loss: 2.120975625130438

Epoch: 6| Step: 8
Training loss: 2.438011884689331
Validation loss: 2.1112965512019333

Epoch: 6| Step: 9
Training loss: 2.217365264892578
Validation loss: 2.1015666530978296

Epoch: 6| Step: 10
Training loss: 2.5522682666778564
Validation loss: 2.1077008555012364

Epoch: 6| Step: 11
Training loss: 2.2842774391174316
Validation loss: 2.0910990263826106

Epoch: 6| Step: 12
Training loss: 2.2416577339172363
Validation loss: 2.0967396600272066

Epoch: 6| Step: 13
Training loss: 2.5610554218292236
Validation loss: 2.1181957362800516

Epoch: 225| Step: 0
Training loss: 2.8185033798217773
Validation loss: 2.0958118489993516

Epoch: 6| Step: 1
Training loss: 2.551928997039795
Validation loss: 2.107033142479517

Epoch: 6| Step: 2
Training loss: 1.6378142833709717
Validation loss: 2.0872442645411335

Epoch: 6| Step: 3
Training loss: 2.0132126808166504
Validation loss: 2.104935330729331

Epoch: 6| Step: 4
Training loss: 2.446589469909668
Validation loss: 2.085627035428119

Epoch: 6| Step: 5
Training loss: 2.116513252258301
Validation loss: 2.094236114973663

Epoch: 6| Step: 6
Training loss: 2.072700023651123
Validation loss: 2.084353567451559

Epoch: 6| Step: 7
Training loss: 2.380129814147949
Validation loss: 2.1282225975426297

Epoch: 6| Step: 8
Training loss: 1.8009541034698486
Validation loss: 2.1108058344933296

Epoch: 6| Step: 9
Training loss: 2.2060842514038086
Validation loss: 2.122737817866828

Epoch: 6| Step: 10
Training loss: 2.023451328277588
Validation loss: 2.1000642161215506

Epoch: 6| Step: 11
Training loss: 1.3163251876831055
Validation loss: 2.079010994203629

Epoch: 6| Step: 12
Training loss: 2.7562615871429443
Validation loss: 2.086503541597756

Epoch: 6| Step: 13
Training loss: 1.8895983695983887
Validation loss: 2.0823629197253974

Epoch: 226| Step: 0
Training loss: 2.0368685722351074
Validation loss: 2.1052280497807327

Epoch: 6| Step: 1
Training loss: 2.253547191619873
Validation loss: 2.114966166916714

Epoch: 6| Step: 2
Training loss: 2.255486488342285
Validation loss: 2.1084727676965858

Epoch: 6| Step: 3
Training loss: 2.340606927871704
Validation loss: 2.08980889730556

Epoch: 6| Step: 4
Training loss: 2.337700366973877
Validation loss: 2.079928933933217

Epoch: 6| Step: 5
Training loss: 2.7243032455444336
Validation loss: 2.0970386971709547

Epoch: 6| Step: 6
Training loss: 2.5829105377197266
Validation loss: 2.1168312282972437

Epoch: 6| Step: 7
Training loss: 1.9669262170791626
Validation loss: 2.0820787234972884

Epoch: 6| Step: 8
Training loss: 1.8739728927612305
Validation loss: 2.107821572211481

Epoch: 6| Step: 9
Training loss: 2.2248127460479736
Validation loss: 2.0915444948339976

Epoch: 6| Step: 10
Training loss: 2.11430025100708
Validation loss: 2.10539851650115

Epoch: 6| Step: 11
Training loss: 1.90110445022583
Validation loss: 2.106595230358903

Epoch: 6| Step: 12
Training loss: 1.5154305696487427
Validation loss: 2.0785245126293552

Epoch: 6| Step: 13
Training loss: 1.835945963859558
Validation loss: 2.096502455331946

Epoch: 227| Step: 0
Training loss: 2.3688926696777344
Validation loss: 2.085658255443778

Epoch: 6| Step: 1
Training loss: 1.9212353229522705
Validation loss: 2.084335393803094

Epoch: 6| Step: 2
Training loss: 2.2025582790374756
Validation loss: 2.072484565037553

Epoch: 6| Step: 3
Training loss: 2.255854606628418
Validation loss: 2.092737360667157

Epoch: 6| Step: 4
Training loss: 1.961925983428955
Validation loss: 2.0963521375451037

Epoch: 6| Step: 5
Training loss: 1.8106861114501953
Validation loss: 2.091727131156511

Epoch: 6| Step: 6
Training loss: 2.3831567764282227
Validation loss: 2.1520135864134757

Epoch: 6| Step: 7
Training loss: 1.705378532409668
Validation loss: 2.1203856788655764

Epoch: 6| Step: 8
Training loss: 2.486755132675171
Validation loss: 2.119333231320945

Epoch: 6| Step: 9
Training loss: 1.9685869216918945
Validation loss: 2.1044768800017652

Epoch: 6| Step: 10
Training loss: 3.140080690383911
Validation loss: 2.098467080823837

Epoch: 6| Step: 11
Training loss: 2.193418264389038
Validation loss: 2.119637232954784

Epoch: 6| Step: 12
Training loss: 2.002070903778076
Validation loss: 2.134337063758604

Epoch: 6| Step: 13
Training loss: 2.2567591667175293
Validation loss: 2.0993588739825833

Epoch: 228| Step: 0
Training loss: 2.305124044418335
Validation loss: 2.097745651839882

Epoch: 6| Step: 1
Training loss: 1.7894365787506104
Validation loss: 2.114817765451247

Epoch: 6| Step: 2
Training loss: 2.0609138011932373
Validation loss: 2.1039533909930976

Epoch: 6| Step: 3
Training loss: 1.894477128982544
Validation loss: 2.128272687235186

Epoch: 6| Step: 4
Training loss: 2.1560630798339844
Validation loss: 2.0884304943905083

Epoch: 6| Step: 5
Training loss: 2.181802988052368
Validation loss: 2.0666107387952906

Epoch: 6| Step: 6
Training loss: 2.212831497192383
Validation loss: 2.078237256696147

Epoch: 6| Step: 7
Training loss: 3.5135390758514404
Validation loss: 2.0959436611462663

Epoch: 6| Step: 8
Training loss: 1.173408031463623
Validation loss: 2.050558759320167

Epoch: 6| Step: 9
Training loss: 1.7038524150848389
Validation loss: 2.0844927398107385

Epoch: 6| Step: 10
Training loss: 2.312807321548462
Validation loss: 2.068385249824934

Epoch: 6| Step: 11
Training loss: 2.393657922744751
Validation loss: 2.0720016264146373

Epoch: 6| Step: 12
Training loss: 2.4645681381225586
Validation loss: 2.075129532044934

Epoch: 6| Step: 13
Training loss: 2.0079545974731445
Validation loss: 2.0669283482336227

Epoch: 229| Step: 0
Training loss: 2.180176019668579
Validation loss: 2.065448398231178

Epoch: 6| Step: 1
Training loss: 2.1674559116363525
Validation loss: 2.0450192061803674

Epoch: 6| Step: 2
Training loss: 2.12619686126709
Validation loss: 2.1073065701351372

Epoch: 6| Step: 3
Training loss: 1.632793664932251
Validation loss: 2.0775403591894333

Epoch: 6| Step: 4
Training loss: 2.649038314819336
Validation loss: 2.0951247651089906

Epoch: 6| Step: 5
Training loss: 2.889967679977417
Validation loss: 2.0735206527094685

Epoch: 6| Step: 6
Training loss: 1.3563934564590454
Validation loss: 2.105614512197433

Epoch: 6| Step: 7
Training loss: 2.3338723182678223
Validation loss: 2.076951975463539

Epoch: 6| Step: 8
Training loss: 2.566433906555176
Validation loss: 2.0594195947852185

Epoch: 6| Step: 9
Training loss: 1.9486055374145508
Validation loss: 2.0898675534033004

Epoch: 6| Step: 10
Training loss: 2.7153751850128174
Validation loss: 2.0868340282030005

Epoch: 6| Step: 11
Training loss: 2.2470932006835938
Validation loss: 2.0814533900189143

Epoch: 6| Step: 12
Training loss: 1.497543215751648
Validation loss: 2.0598564891405005

Epoch: 6| Step: 13
Training loss: 2.041039228439331
Validation loss: 2.0877516538866105

Epoch: 230| Step: 0
Training loss: 2.431755781173706
Validation loss: 2.09961716077661

Epoch: 6| Step: 1
Training loss: 2.670567274093628
Validation loss: 2.0564450935650895

Epoch: 6| Step: 2
Training loss: 1.6877062320709229
Validation loss: 2.111385537732032

Epoch: 6| Step: 3
Training loss: 1.479148507118225
Validation loss: 2.0986906251599713

Epoch: 6| Step: 4
Training loss: 2.4327681064605713
Validation loss: 2.0778041949836155

Epoch: 6| Step: 5
Training loss: 1.8994249105453491
Validation loss: 2.082394511468949

Epoch: 6| Step: 6
Training loss: 2.0210819244384766
Validation loss: 2.087812385251445

Epoch: 6| Step: 7
Training loss: 2.1205756664276123
Validation loss: 2.08028967021614

Epoch: 6| Step: 8
Training loss: 2.5400118827819824
Validation loss: 2.080515129591829

Epoch: 6| Step: 9
Training loss: 2.255183458328247
Validation loss: 2.0964272470884424

Epoch: 6| Step: 10
Training loss: 2.068610668182373
Validation loss: 2.0837305873952885

Epoch: 6| Step: 11
Training loss: 2.2317733764648438
Validation loss: 2.080479161713713

Epoch: 6| Step: 12
Training loss: 2.159560203552246
Validation loss: 2.1195608851730183

Epoch: 6| Step: 13
Training loss: 2.6680097579956055
Validation loss: 2.0928908471138246

Epoch: 231| Step: 0
Training loss: 2.426643133163452
Validation loss: 2.088285924285971

Epoch: 6| Step: 1
Training loss: 2.099134922027588
Validation loss: 2.1053944095488517

Epoch: 6| Step: 2
Training loss: 2.287562370300293
Validation loss: 2.0798249988145727

Epoch: 6| Step: 3
Training loss: 2.373634099960327
Validation loss: 2.0892549701916274

Epoch: 6| Step: 4
Training loss: 2.4520974159240723
Validation loss: 2.0936440806235037

Epoch: 6| Step: 5
Training loss: 1.7469985485076904
Validation loss: 2.117320545258061

Epoch: 6| Step: 6
Training loss: 1.855786681175232
Validation loss: 2.0863205873838035

Epoch: 6| Step: 7
Training loss: 2.340514659881592
Validation loss: 2.1039879014415126

Epoch: 6| Step: 8
Training loss: 1.717561960220337
Validation loss: 2.100263992945353

Epoch: 6| Step: 9
Training loss: 2.0865976810455322
Validation loss: 2.0791998063364336

Epoch: 6| Step: 10
Training loss: 2.044130802154541
Validation loss: 2.0732177098592124

Epoch: 6| Step: 11
Training loss: 2.3945014476776123
Validation loss: 2.080526613420056

Epoch: 6| Step: 12
Training loss: 2.336909294128418
Validation loss: 2.0805291770606913

Epoch: 6| Step: 13
Training loss: 1.7902005910873413
Validation loss: 2.082480469057637

Epoch: 232| Step: 0
Training loss: 2.4642717838287354
Validation loss: 2.1024399521530315

Epoch: 6| Step: 1
Training loss: 1.9300154447555542
Validation loss: 2.105010432581748

Epoch: 6| Step: 2
Training loss: 1.9946354627609253
Validation loss: 2.1028109596621607

Epoch: 6| Step: 3
Training loss: 1.9293636083602905
Validation loss: 2.0945504096246537

Epoch: 6| Step: 4
Training loss: 2.529843807220459
Validation loss: 2.0860280336872226

Epoch: 6| Step: 5
Training loss: 2.0722579956054688
Validation loss: 2.1122065667183167

Epoch: 6| Step: 6
Training loss: 2.6573729515075684
Validation loss: 2.0764677332293604

Epoch: 6| Step: 7
Training loss: 2.032444953918457
Validation loss: 2.0768233729946997

Epoch: 6| Step: 8
Training loss: 1.9627492427825928
Validation loss: 2.0908741284442205

Epoch: 6| Step: 9
Training loss: 2.3178205490112305
Validation loss: 2.088295180310485

Epoch: 6| Step: 10
Training loss: 2.132314920425415
Validation loss: 2.09336325686465

Epoch: 6| Step: 11
Training loss: 2.264960289001465
Validation loss: 2.081536759612381

Epoch: 6| Step: 12
Training loss: 1.9491397142410278
Validation loss: 2.104661049381379

Epoch: 6| Step: 13
Training loss: 1.8325282335281372
Validation loss: 2.089162595810429

Epoch: 233| Step: 0
Training loss: 2.0955610275268555
Validation loss: 2.093438030571066

Epoch: 6| Step: 1
Training loss: 1.443594217300415
Validation loss: 2.0926635393532376

Epoch: 6| Step: 2
Training loss: 2.2879161834716797
Validation loss: 2.1075245488074517

Epoch: 6| Step: 3
Training loss: 2.4634857177734375
Validation loss: 2.0880377331087665

Epoch: 6| Step: 4
Training loss: 2.26698637008667
Validation loss: 2.10290870230685

Epoch: 6| Step: 5
Training loss: 1.9487698078155518
Validation loss: 2.09460291811215

Epoch: 6| Step: 6
Training loss: 2.0663647651672363
Validation loss: 2.0941897207690823

Epoch: 6| Step: 7
Training loss: 2.1657469272613525
Validation loss: 2.0800339303990847

Epoch: 6| Step: 8
Training loss: 2.480135917663574
Validation loss: 2.0759298827058528

Epoch: 6| Step: 9
Training loss: 2.8680009841918945
Validation loss: 2.0811716984677058

Epoch: 6| Step: 10
Training loss: 1.6422468423843384
Validation loss: 2.097677457717157

Epoch: 6| Step: 11
Training loss: 2.0262398719787598
Validation loss: 2.1048714883865847

Epoch: 6| Step: 12
Training loss: 2.0613837242126465
Validation loss: 2.09526498599719

Epoch: 6| Step: 13
Training loss: 2.8607075214385986
Validation loss: 2.095604891418129

Epoch: 234| Step: 0
Training loss: 2.109959125518799
Validation loss: 2.121457076841785

Epoch: 6| Step: 1
Training loss: 1.7276134490966797
Validation loss: 2.126244371937167

Epoch: 6| Step: 2
Training loss: 2.171830654144287
Validation loss: 2.1166970037644908

Epoch: 6| Step: 3
Training loss: 2.2608630657196045
Validation loss: 2.101751553115024

Epoch: 6| Step: 4
Training loss: 1.9751975536346436
Validation loss: 2.0999543846294446

Epoch: 6| Step: 5
Training loss: 2.0301454067230225
Validation loss: 2.126684373424899

Epoch: 6| Step: 6
Training loss: 2.5791242122650146
Validation loss: 2.1302428706999748

Epoch: 6| Step: 7
Training loss: 2.5515408515930176
Validation loss: 2.12188551246479

Epoch: 6| Step: 8
Training loss: 3.0651330947875977
Validation loss: 2.1188096871940036

Epoch: 6| Step: 9
Training loss: 1.9446191787719727
Validation loss: 2.1028693235048683

Epoch: 6| Step: 10
Training loss: 1.9749436378479004
Validation loss: 2.109589374193581

Epoch: 6| Step: 11
Training loss: 2.154256820678711
Validation loss: 2.0916590254793883

Epoch: 6| Step: 12
Training loss: 1.982399344444275
Validation loss: 2.1270925921778523

Epoch: 6| Step: 13
Training loss: 1.7115483283996582
Validation loss: 2.1339176906052457

Epoch: 235| Step: 0
Training loss: 2.535386562347412
Validation loss: 2.1059993774660173

Epoch: 6| Step: 1
Training loss: 1.7898807525634766
Validation loss: 2.1118160601585143

Epoch: 6| Step: 2
Training loss: 2.369678020477295
Validation loss: 2.1189231923831406

Epoch: 6| Step: 3
Training loss: 1.6766331195831299
Validation loss: 2.124968462092902

Epoch: 6| Step: 4
Training loss: 2.157100200653076
Validation loss: 2.104228065859887

Epoch: 6| Step: 5
Training loss: 2.5672388076782227
Validation loss: 2.121364926779142

Epoch: 6| Step: 6
Training loss: 2.791306972503662
Validation loss: 2.1376631388100247

Epoch: 6| Step: 7
Training loss: 1.8301856517791748
Validation loss: 2.1407810308599986

Epoch: 6| Step: 8
Training loss: 2.0248665809631348
Validation loss: 2.105552855358329

Epoch: 6| Step: 9
Training loss: 2.726067304611206
Validation loss: 2.1321935422958864

Epoch: 6| Step: 10
Training loss: 2.016948938369751
Validation loss: 2.0976510483731508

Epoch: 6| Step: 11
Training loss: 1.1303319931030273
Validation loss: 2.1226118777387883

Epoch: 6| Step: 12
Training loss: 2.3467795848846436
Validation loss: 2.096326579329788

Epoch: 6| Step: 13
Training loss: 2.446070671081543
Validation loss: 2.135189284560501

Epoch: 236| Step: 0
Training loss: 2.02822208404541
Validation loss: 2.087511393331712

Epoch: 6| Step: 1
Training loss: 2.514613628387451
Validation loss: 2.0822499490553334

Epoch: 6| Step: 2
Training loss: 2.4123387336730957
Validation loss: 2.0893003453490553

Epoch: 6| Step: 3
Training loss: 1.4786396026611328
Validation loss: 2.099259361144035

Epoch: 6| Step: 4
Training loss: 1.735863447189331
Validation loss: 2.1103481554215953

Epoch: 6| Step: 5
Training loss: 2.1299405097961426
Validation loss: 2.0875134032259703

Epoch: 6| Step: 6
Training loss: 2.3483800888061523
Validation loss: 2.1158017676363707

Epoch: 6| Step: 7
Training loss: 1.2751619815826416
Validation loss: 2.086550840767481

Epoch: 6| Step: 8
Training loss: 1.7999346256256104
Validation loss: 2.109166242743051

Epoch: 6| Step: 9
Training loss: 2.461794376373291
Validation loss: 2.087479547787738

Epoch: 6| Step: 10
Training loss: 2.812354803085327
Validation loss: 2.0771028764786257

Epoch: 6| Step: 11
Training loss: 1.9612177610397339
Validation loss: 2.0743798299502303

Epoch: 6| Step: 12
Training loss: 2.767155408859253
Validation loss: 2.0734984951634563

Epoch: 6| Step: 13
Training loss: 2.86611270904541
Validation loss: 2.0959734557777323

Epoch: 237| Step: 0
Training loss: 1.5101873874664307
Validation loss: 2.0723292994242843

Epoch: 6| Step: 1
Training loss: 3.026794910430908
Validation loss: 2.087447034415378

Epoch: 6| Step: 2
Training loss: 2.252559185028076
Validation loss: 2.109020633082236

Epoch: 6| Step: 3
Training loss: 2.1973869800567627
Validation loss: 2.1078678074703423

Epoch: 6| Step: 4
Training loss: 2.0635807514190674
Validation loss: 2.106396472582253

Epoch: 6| Step: 5
Training loss: 1.5098474025726318
Validation loss: 2.0752421181689025

Epoch: 6| Step: 6
Training loss: 2.2417545318603516
Validation loss: 2.0692062172838437

Epoch: 6| Step: 7
Training loss: 2.413616895675659
Validation loss: 2.0732479941460396

Epoch: 6| Step: 8
Training loss: 2.01914644241333
Validation loss: 2.1038643621629283

Epoch: 6| Step: 9
Training loss: 2.7389111518859863
Validation loss: 2.1008413991620465

Epoch: 6| Step: 10
Training loss: 2.565579414367676
Validation loss: 2.087955318471437

Epoch: 6| Step: 11
Training loss: 1.6395041942596436
Validation loss: 2.0700664456172655

Epoch: 6| Step: 12
Training loss: 2.0581982135772705
Validation loss: 2.0929368542086695

Epoch: 6| Step: 13
Training loss: 2.543978452682495
Validation loss: 2.097696594012681

Epoch: 238| Step: 0
Training loss: 2.0934672355651855
Validation loss: 2.114794695249168

Epoch: 6| Step: 1
Training loss: 1.9248391389846802
Validation loss: 2.1019351431118545

Epoch: 6| Step: 2
Training loss: 2.1943156719207764
Validation loss: 2.106740185009536

Epoch: 6| Step: 3
Training loss: 2.408134698867798
Validation loss: 2.0876974239144275

Epoch: 6| Step: 4
Training loss: 2.1727819442749023
Validation loss: 2.13134434915358

Epoch: 6| Step: 5
Training loss: 1.3849034309387207
Validation loss: 2.0975203373098887

Epoch: 6| Step: 6
Training loss: 2.815828800201416
Validation loss: 2.096380781101924

Epoch: 6| Step: 7
Training loss: 2.379178524017334
Validation loss: 2.1052327309885333

Epoch: 6| Step: 8
Training loss: 3.0032601356506348
Validation loss: 2.1254310441273514

Epoch: 6| Step: 9
Training loss: 1.5977363586425781
Validation loss: 2.110304596603558

Epoch: 6| Step: 10
Training loss: 1.6918134689331055
Validation loss: 2.106709036775815

Epoch: 6| Step: 11
Training loss: 1.922368049621582
Validation loss: 2.0734460225669284

Epoch: 6| Step: 12
Training loss: 2.1055359840393066
Validation loss: 2.1091120781437045

Epoch: 6| Step: 13
Training loss: 2.3033599853515625
Validation loss: 2.1259522643140567

Epoch: 239| Step: 0
Training loss: 2.165438175201416
Validation loss: 2.113868841560938

Epoch: 6| Step: 1
Training loss: 2.1745669841766357
Validation loss: 2.124359587187408

Epoch: 6| Step: 2
Training loss: 2.1412081718444824
Validation loss: 2.0960196782183904

Epoch: 6| Step: 3
Training loss: 2.573953866958618
Validation loss: 2.119035990007462

Epoch: 6| Step: 4
Training loss: 2.3818295001983643
Validation loss: 2.0915411185192805

Epoch: 6| Step: 5
Training loss: 1.6493003368377686
Validation loss: 2.0796024286618797

Epoch: 6| Step: 6
Training loss: 2.1083271503448486
Validation loss: 2.1129638328347156

Epoch: 6| Step: 7
Training loss: 2.0243964195251465
Validation loss: 2.103159858334449

Epoch: 6| Step: 8
Training loss: 1.9389727115631104
Validation loss: 2.092793991488795

Epoch: 6| Step: 9
Training loss: 1.9086742401123047
Validation loss: 2.0980423188978627

Epoch: 6| Step: 10
Training loss: 2.851895570755005
Validation loss: 2.0586577564157467

Epoch: 6| Step: 11
Training loss: 1.7621541023254395
Validation loss: 2.105120805002028

Epoch: 6| Step: 12
Training loss: 1.9264696836471558
Validation loss: 2.0978229789323706

Epoch: 6| Step: 13
Training loss: 2.7800962924957275
Validation loss: 2.0921905809833157

Epoch: 240| Step: 0
Training loss: 2.2619504928588867
Validation loss: 2.100839208531123

Epoch: 6| Step: 1
Training loss: 2.11776065826416
Validation loss: 2.06684902662872

Epoch: 6| Step: 2
Training loss: 1.7666774988174438
Validation loss: 2.0904707242083806

Epoch: 6| Step: 3
Training loss: 2.1141180992126465
Validation loss: 2.0842665549247497

Epoch: 6| Step: 4
Training loss: 3.0735421180725098
Validation loss: 2.1131194304394465

Epoch: 6| Step: 5
Training loss: 2.0243940353393555
Validation loss: 2.0972609032866774

Epoch: 6| Step: 6
Training loss: 2.6544246673583984
Validation loss: 2.103327020522087

Epoch: 6| Step: 7
Training loss: 1.665004849433899
Validation loss: 2.0990762531116443

Epoch: 6| Step: 8
Training loss: 2.2426958084106445
Validation loss: 2.090676963970225

Epoch: 6| Step: 9
Training loss: 2.5416877269744873
Validation loss: 2.1012448008342455

Epoch: 6| Step: 10
Training loss: 1.901524543762207
Validation loss: 2.1040758317516697

Epoch: 6| Step: 11
Training loss: 1.0413004159927368
Validation loss: 2.08889328792531

Epoch: 6| Step: 12
Training loss: 2.3844377994537354
Validation loss: 2.072380899101175

Epoch: 6| Step: 13
Training loss: 2.4889984130859375
Validation loss: 2.121498313001407

Epoch: 241| Step: 0
Training loss: 2.050323963165283
Validation loss: 2.09445761224275

Epoch: 6| Step: 1
Training loss: 1.5884513854980469
Validation loss: 2.0952141054214968

Epoch: 6| Step: 2
Training loss: 1.9410489797592163
Validation loss: 2.0863338080785607

Epoch: 6| Step: 3
Training loss: 2.2462475299835205
Validation loss: 2.104929631756198

Epoch: 6| Step: 4
Training loss: 2.290616989135742
Validation loss: 2.0877349415133075

Epoch: 6| Step: 5
Training loss: 1.9863481521606445
Validation loss: 2.114871178903887

Epoch: 6| Step: 6
Training loss: 1.6012287139892578
Validation loss: 2.1219618064101025

Epoch: 6| Step: 7
Training loss: 1.9882326126098633
Validation loss: 2.1078161936934277

Epoch: 6| Step: 8
Training loss: 2.154031276702881
Validation loss: 2.1245790860986196

Epoch: 6| Step: 9
Training loss: 2.600008726119995
Validation loss: 2.0749630005128923

Epoch: 6| Step: 10
Training loss: 2.64847993850708
Validation loss: 2.1043232320457377

Epoch: 6| Step: 11
Training loss: 1.9958925247192383
Validation loss: 2.101680371069139

Epoch: 6| Step: 12
Training loss: 2.478720188140869
Validation loss: 2.121444099692888

Epoch: 6| Step: 13
Training loss: 2.8168232440948486
Validation loss: 2.0887990715683147

Epoch: 242| Step: 0
Training loss: 2.3530569076538086
Validation loss: 2.096588871812308

Epoch: 6| Step: 1
Training loss: 2.4225263595581055
Validation loss: 2.085185299637497

Epoch: 6| Step: 2
Training loss: 2.64801025390625
Validation loss: 2.113094063215358

Epoch: 6| Step: 3
Training loss: 2.413135528564453
Validation loss: 2.101897962631718

Epoch: 6| Step: 4
Training loss: 2.01646089553833
Validation loss: 2.1104398414652836

Epoch: 6| Step: 5
Training loss: 1.7938627004623413
Validation loss: 2.101455762822141

Epoch: 6| Step: 6
Training loss: 2.2732534408569336
Validation loss: 2.103139031317926

Epoch: 6| Step: 7
Training loss: 2.813711643218994
Validation loss: 2.1179239583271805

Epoch: 6| Step: 8
Training loss: 1.68277907371521
Validation loss: 2.1171276095092937

Epoch: 6| Step: 9
Training loss: 1.8427187204360962
Validation loss: 2.119975564300373

Epoch: 6| Step: 10
Training loss: 1.9440207481384277
Validation loss: 2.0970232179087978

Epoch: 6| Step: 11
Training loss: 2.105947256088257
Validation loss: 2.112926313954015

Epoch: 6| Step: 12
Training loss: 1.9738926887512207
Validation loss: 2.116877673774637

Epoch: 6| Step: 13
Training loss: 1.883423924446106
Validation loss: 2.090513131951773

Epoch: 243| Step: 0
Training loss: 2.1589274406433105
Validation loss: 2.111702003786641

Epoch: 6| Step: 1
Training loss: 2.276068687438965
Validation loss: 2.101829468563039

Epoch: 6| Step: 2
Training loss: 1.6641876697540283
Validation loss: 2.1226156539814447

Epoch: 6| Step: 3
Training loss: 2.3102242946624756
Validation loss: 2.1027921079307474

Epoch: 6| Step: 4
Training loss: 2.4776692390441895
Validation loss: 2.098702464052426

Epoch: 6| Step: 5
Training loss: 2.717078447341919
Validation loss: 2.1000151698307326

Epoch: 6| Step: 6
Training loss: 1.3344426155090332
Validation loss: 2.0891016888362106

Epoch: 6| Step: 7
Training loss: 1.4614864587783813
Validation loss: 2.0864644576144475

Epoch: 6| Step: 8
Training loss: 2.236201286315918
Validation loss: 2.0909983855421825

Epoch: 6| Step: 9
Training loss: 2.454617738723755
Validation loss: 2.0980434635634064

Epoch: 6| Step: 10
Training loss: 2.8214330673217773
Validation loss: 2.0975395774328582

Epoch: 6| Step: 11
Training loss: 1.9312258958816528
Validation loss: 2.108126122464416

Epoch: 6| Step: 12
Training loss: 2.0985469818115234
Validation loss: 2.0950405302868096

Epoch: 6| Step: 13
Training loss: 2.4052140712738037
Validation loss: 2.1218482243117465

Epoch: 244| Step: 0
Training loss: 2.2460522651672363
Validation loss: 2.116601486359873

Epoch: 6| Step: 1
Training loss: 2.790951728820801
Validation loss: 2.090415108588434

Epoch: 6| Step: 2
Training loss: 2.208608388900757
Validation loss: 2.103090504164337

Epoch: 6| Step: 3
Training loss: 2.573521614074707
Validation loss: 2.1024553186150006

Epoch: 6| Step: 4
Training loss: 2.7479135990142822
Validation loss: 2.0866157547120125

Epoch: 6| Step: 5
Training loss: 2.696570634841919
Validation loss: 2.092052839135611

Epoch: 6| Step: 6
Training loss: 2.294611930847168
Validation loss: 2.0925312375509613

Epoch: 6| Step: 7
Training loss: 1.1373796463012695
Validation loss: 2.076973694627003

Epoch: 6| Step: 8
Training loss: 1.7059707641601562
Validation loss: 2.083900333732687

Epoch: 6| Step: 9
Training loss: 2.4006428718566895
Validation loss: 2.078993833193215

Epoch: 6| Step: 10
Training loss: 1.9600422382354736
Validation loss: 2.0921825388426423

Epoch: 6| Step: 11
Training loss: 1.7357914447784424
Validation loss: 2.0984041780553837

Epoch: 6| Step: 12
Training loss: 1.545320987701416
Validation loss: 2.069862170885968

Epoch: 6| Step: 13
Training loss: 1.6190645694732666
Validation loss: 2.0949884178817912

Epoch: 245| Step: 0
Training loss: 1.9302632808685303
Validation loss: 2.1019796620133104

Epoch: 6| Step: 1
Training loss: 2.00089430809021
Validation loss: 2.1016198794047036

Epoch: 6| Step: 2
Training loss: 1.8822505474090576
Validation loss: 2.1081174240317395

Epoch: 6| Step: 3
Training loss: 1.7838423252105713
Validation loss: 2.080004266513291

Epoch: 6| Step: 4
Training loss: 2.237717628479004
Validation loss: 2.0899865422197568

Epoch: 6| Step: 5
Training loss: 2.4371204376220703
Validation loss: 2.091394042456022

Epoch: 6| Step: 6
Training loss: 2.270848035812378
Validation loss: 2.076716233325261

Epoch: 6| Step: 7
Training loss: 2.437131881713867
Validation loss: 2.1057156952478553

Epoch: 6| Step: 8
Training loss: 1.8472802639007568
Validation loss: 2.077339413345501

Epoch: 6| Step: 9
Training loss: 2.1034326553344727
Validation loss: 2.0875129956071095

Epoch: 6| Step: 10
Training loss: 1.8998262882232666
Validation loss: 2.1113827202909734

Epoch: 6| Step: 11
Training loss: 2.063246726989746
Validation loss: 2.078323951331518

Epoch: 6| Step: 12
Training loss: 2.6336915493011475
Validation loss: 2.095849101261426

Epoch: 6| Step: 13
Training loss: 2.425516128540039
Validation loss: 2.0940997780010266

Epoch: 246| Step: 0
Training loss: 2.5809898376464844
Validation loss: 2.0924221674601235

Epoch: 6| Step: 1
Training loss: 2.374070644378662
Validation loss: 2.129750595297865

Epoch: 6| Step: 2
Training loss: 2.67790150642395
Validation loss: 2.084156605505174

Epoch: 6| Step: 3
Training loss: 1.8596304655075073
Validation loss: 2.0926072443685224

Epoch: 6| Step: 4
Training loss: 1.5589985847473145
Validation loss: 2.109003738690448

Epoch: 6| Step: 5
Training loss: 2.3267195224761963
Validation loss: 2.142529502991707

Epoch: 6| Step: 6
Training loss: 2.6880416870117188
Validation loss: 2.102765743450452

Epoch: 6| Step: 7
Training loss: 1.9668548107147217
Validation loss: 2.108168791699153

Epoch: 6| Step: 8
Training loss: 1.3386834859848022
Validation loss: 2.0911130930787776

Epoch: 6| Step: 9
Training loss: 1.743051290512085
Validation loss: 2.1056644147442234

Epoch: 6| Step: 10
Training loss: 2.518522024154663
Validation loss: 2.1170741101746917

Epoch: 6| Step: 11
Training loss: 2.1383109092712402
Validation loss: 2.1178922422470583

Epoch: 6| Step: 12
Training loss: 2.382932186126709
Validation loss: 2.0715745982303413

Epoch: 6| Step: 13
Training loss: 2.1774659156799316
Validation loss: 2.1394425258841565

Epoch: 247| Step: 0
Training loss: 2.088308572769165
Validation loss: 2.099732547677973

Epoch: 6| Step: 1
Training loss: 2.1897871494293213
Validation loss: 2.099701209734845

Epoch: 6| Step: 2
Training loss: 2.034191370010376
Validation loss: 2.1218204523927424

Epoch: 6| Step: 3
Training loss: 2.4832534790039062
Validation loss: 2.118514714702483

Epoch: 6| Step: 4
Training loss: 1.6980106830596924
Validation loss: 2.1002460192608576

Epoch: 6| Step: 5
Training loss: 2.094230890274048
Validation loss: 2.1299049777369343

Epoch: 6| Step: 6
Training loss: 1.9290775060653687
Validation loss: 2.1365472398778445

Epoch: 6| Step: 7
Training loss: 2.2669920921325684
Validation loss: 2.1083569936854865

Epoch: 6| Step: 8
Training loss: 2.1109766960144043
Validation loss: 2.117375966041319

Epoch: 6| Step: 9
Training loss: 2.377845287322998
Validation loss: 2.131063051121209

Epoch: 6| Step: 10
Training loss: 2.506516456604004
Validation loss: 2.1095783582297702

Epoch: 6| Step: 11
Training loss: 1.9151266813278198
Validation loss: 2.1225383486799014

Epoch: 6| Step: 12
Training loss: 2.1699371337890625
Validation loss: 2.1474249106581493

Epoch: 6| Step: 13
Training loss: 2.396097421646118
Validation loss: 2.096256653467814

Epoch: 248| Step: 0
Training loss: 1.706160068511963
Validation loss: 2.1175686749078895

Epoch: 6| Step: 1
Training loss: 1.799565076828003
Validation loss: 2.087377154698936

Epoch: 6| Step: 2
Training loss: 2.131399154663086
Validation loss: 2.1162079790587067

Epoch: 6| Step: 3
Training loss: 2.10967755317688
Validation loss: 2.0738749068270446

Epoch: 6| Step: 4
Training loss: 1.8945772647857666
Validation loss: 2.104010505060996

Epoch: 6| Step: 5
Training loss: 2.756234645843506
Validation loss: 2.1062998169211933

Epoch: 6| Step: 6
Training loss: 2.751856803894043
Validation loss: 2.0631946632939

Epoch: 6| Step: 7
Training loss: 2.1405069828033447
Validation loss: 2.0757927125500095

Epoch: 6| Step: 8
Training loss: 1.5754750967025757
Validation loss: 2.061878486346173

Epoch: 6| Step: 9
Training loss: 1.9232425689697266
Validation loss: 2.080976350333101

Epoch: 6| Step: 10
Training loss: 2.822791576385498
Validation loss: 2.096142038222282

Epoch: 6| Step: 11
Training loss: 2.092747688293457
Validation loss: 2.0837934786273586

Epoch: 6| Step: 12
Training loss: 2.4769856929779053
Validation loss: 2.0699472222276913

Epoch: 6| Step: 13
Training loss: 1.6781734228134155
Validation loss: 2.0923116617305304

Epoch: 249| Step: 0
Training loss: 3.0164499282836914
Validation loss: 2.066057696137377

Epoch: 6| Step: 1
Training loss: 2.8301258087158203
Validation loss: 2.0638129531696277

Epoch: 6| Step: 2
Training loss: 2.213050603866577
Validation loss: 2.087160321973985

Epoch: 6| Step: 3
Training loss: 1.5060745477676392
Validation loss: 2.079003628864083

Epoch: 6| Step: 4
Training loss: 2.1420345306396484
Validation loss: 2.0717393736685477

Epoch: 6| Step: 5
Training loss: 1.746885895729065
Validation loss: 2.0649632817955426

Epoch: 6| Step: 6
Training loss: 2.322911262512207
Validation loss: 2.087464645344724

Epoch: 6| Step: 7
Training loss: 1.696476697921753
Validation loss: 2.089836007805281

Epoch: 6| Step: 8
Training loss: 2.3917858600616455
Validation loss: 2.1021247653551

Epoch: 6| Step: 9
Training loss: 1.8625391721725464
Validation loss: 2.065666416639923

Epoch: 6| Step: 10
Training loss: 2.175290107727051
Validation loss: 2.0902171750222482

Epoch: 6| Step: 11
Training loss: 2.3381271362304688
Validation loss: 2.1245950524524977

Epoch: 6| Step: 12
Training loss: 1.63364839553833
Validation loss: 2.0964410074295534

Epoch: 6| Step: 13
Training loss: 2.500697135925293
Validation loss: 2.100351469491118

Epoch: 250| Step: 0
Training loss: 1.7389785051345825
Validation loss: 2.1004136608492945

Epoch: 6| Step: 1
Training loss: 1.4356105327606201
Validation loss: 2.0929913854086273

Epoch: 6| Step: 2
Training loss: 2.270826816558838
Validation loss: 2.0749865693430745

Epoch: 6| Step: 3
Training loss: 2.1713430881500244
Validation loss: 2.104150677240023

Epoch: 6| Step: 4
Training loss: 2.415276527404785
Validation loss: 2.0868414435335385

Epoch: 6| Step: 5
Training loss: 2.2235913276672363
Validation loss: 2.084385879578129

Epoch: 6| Step: 6
Training loss: 2.079117774963379
Validation loss: 2.1438615309294833

Epoch: 6| Step: 7
Training loss: 2.276618242263794
Validation loss: 2.11981047866165

Epoch: 6| Step: 8
Training loss: 2.940556526184082
Validation loss: 2.093285446525902

Epoch: 6| Step: 9
Training loss: 2.2876996994018555
Validation loss: 2.1137041866138415

Epoch: 6| Step: 10
Training loss: 2.196260690689087
Validation loss: 2.097765297018072

Epoch: 6| Step: 11
Training loss: 2.0999090671539307
Validation loss: 2.1336131377886702

Epoch: 6| Step: 12
Training loss: 2.031083345413208
Validation loss: 2.11359211449982

Epoch: 6| Step: 13
Training loss: 1.9896490573883057
Validation loss: 2.0915207798762987

Epoch: 251| Step: 0
Training loss: 2.2235934734344482
Validation loss: 2.117936370193317

Epoch: 6| Step: 1
Training loss: 2.0250155925750732
Validation loss: 2.123472782873338

Epoch: 6| Step: 2
Training loss: 2.2155280113220215
Validation loss: 2.0975577087812525

Epoch: 6| Step: 3
Training loss: 2.123314380645752
Validation loss: 2.1098793296403784

Epoch: 6| Step: 4
Training loss: 2.6652534008026123
Validation loss: 2.0946560495643207

Epoch: 6| Step: 5
Training loss: 2.5887889862060547
Validation loss: 2.1178989282218357

Epoch: 6| Step: 6
Training loss: 1.6775128841400146
Validation loss: 2.0971956176142537

Epoch: 6| Step: 7
Training loss: 1.895486831665039
Validation loss: 2.091723149822604

Epoch: 6| Step: 8
Training loss: 2.528383255004883
Validation loss: 2.096264095716579

Epoch: 6| Step: 9
Training loss: 2.1294074058532715
Validation loss: 2.092141528283396

Epoch: 6| Step: 10
Training loss: 2.198974609375
Validation loss: 2.0710104357811714

Epoch: 6| Step: 11
Training loss: 2.263552188873291
Validation loss: 2.0983959551780456

Epoch: 6| Step: 12
Training loss: 1.6646836996078491
Validation loss: 2.0766086860369612

Epoch: 6| Step: 13
Training loss: 1.726423740386963
Validation loss: 2.10046850609523

Epoch: 252| Step: 0
Training loss: 3.2828123569488525
Validation loss: 2.1096470945624897

Epoch: 6| Step: 1
Training loss: 1.3730863332748413
Validation loss: 2.0791122477541686

Epoch: 6| Step: 2
Training loss: 1.5825791358947754
Validation loss: 2.0772259337927705

Epoch: 6| Step: 3
Training loss: 1.7588008642196655
Validation loss: 2.0959833719397105

Epoch: 6| Step: 4
Training loss: 2.6659436225891113
Validation loss: 2.086368412099859

Epoch: 6| Step: 5
Training loss: 2.0720629692077637
Validation loss: 2.0923891477687384

Epoch: 6| Step: 6
Training loss: 1.6265873908996582
Validation loss: 2.09701665114331

Epoch: 6| Step: 7
Training loss: 2.519486665725708
Validation loss: 2.0995707742629515

Epoch: 6| Step: 8
Training loss: 2.3763530254364014
Validation loss: 2.103044843160978

Epoch: 6| Step: 9
Training loss: 1.4388498067855835
Validation loss: 2.109843452771505

Epoch: 6| Step: 10
Training loss: 2.410712718963623
Validation loss: 2.0754529173656175

Epoch: 6| Step: 11
Training loss: 2.930908203125
Validation loss: 2.095157218235795

Epoch: 6| Step: 12
Training loss: 1.7271727323532104
Validation loss: 2.083377891971219

Epoch: 6| Step: 13
Training loss: 2.308384656906128
Validation loss: 2.079562512777185

Epoch: 253| Step: 0
Training loss: 2.395138740539551
Validation loss: 2.0963191832265546

Epoch: 6| Step: 1
Training loss: 2.4203834533691406
Validation loss: 2.1228934616170902

Epoch: 6| Step: 2
Training loss: 2.0383307933807373
Validation loss: 2.116491174185148

Epoch: 6| Step: 3
Training loss: 1.4921791553497314
Validation loss: 2.093348313403386

Epoch: 6| Step: 4
Training loss: 2.473757266998291
Validation loss: 2.1227389535596295

Epoch: 6| Step: 5
Training loss: 1.9466159343719482
Validation loss: 2.0925781188472623

Epoch: 6| Step: 6
Training loss: 1.9327507019042969
Validation loss: 2.1004112202634095

Epoch: 6| Step: 7
Training loss: 1.6981902122497559
Validation loss: 2.1381355459972093

Epoch: 6| Step: 8
Training loss: 2.1340622901916504
Validation loss: 2.1177096161791074

Epoch: 6| Step: 9
Training loss: 2.276566505432129
Validation loss: 2.1275630330526702

Epoch: 6| Step: 10
Training loss: 2.1618475914001465
Validation loss: 2.11657892247682

Epoch: 6| Step: 11
Training loss: 2.596219539642334
Validation loss: 2.1111494238658617

Epoch: 6| Step: 12
Training loss: 2.4561948776245117
Validation loss: 2.1309194077727613

Epoch: 6| Step: 13
Training loss: 2.0942862033843994
Validation loss: 2.1014619168414863

Epoch: 254| Step: 0
Training loss: 2.4910435676574707
Validation loss: 2.092498807496922

Epoch: 6| Step: 1
Training loss: 2.4251818656921387
Validation loss: 2.1078428581196773

Epoch: 6| Step: 2
Training loss: 1.6367769241333008
Validation loss: 2.113451280901509

Epoch: 6| Step: 3
Training loss: 1.957874059677124
Validation loss: 2.112151571499404

Epoch: 6| Step: 4
Training loss: 1.9051425457000732
Validation loss: 2.0727961653022358

Epoch: 6| Step: 5
Training loss: 1.7004681825637817
Validation loss: 2.0792126053123066

Epoch: 6| Step: 6
Training loss: 1.5345656871795654
Validation loss: 2.0623657716217862

Epoch: 6| Step: 7
Training loss: 2.2600221633911133
Validation loss: 2.0951914274564354

Epoch: 6| Step: 8
Training loss: 2.5822248458862305
Validation loss: 2.0758790085392613

Epoch: 6| Step: 9
Training loss: 2.399325370788574
Validation loss: 2.0595040552077757

Epoch: 6| Step: 10
Training loss: 2.5021626949310303
Validation loss: 2.1000308836660078

Epoch: 6| Step: 11
Training loss: 2.866532325744629
Validation loss: 2.105058059897474

Epoch: 6| Step: 12
Training loss: 1.7010400295257568
Validation loss: 2.1027458201172533

Epoch: 6| Step: 13
Training loss: 1.6768603324890137
Validation loss: 2.0819115049095562

Epoch: 255| Step: 0
Training loss: 2.7722835540771484
Validation loss: 2.0740466143495295

Epoch: 6| Step: 1
Training loss: 2.392469882965088
Validation loss: 2.065306385358175

Epoch: 6| Step: 2
Training loss: 2.3424129486083984
Validation loss: 2.0728016591841176

Epoch: 6| Step: 3
Training loss: 1.4231373071670532
Validation loss: 2.0829468875802974

Epoch: 6| Step: 4
Training loss: 2.3727707862854004
Validation loss: 2.0932971239089966

Epoch: 6| Step: 5
Training loss: 2.2030110359191895
Validation loss: 2.106293039937173

Epoch: 6| Step: 6
Training loss: 2.1595661640167236
Validation loss: 2.1038019272588913

Epoch: 6| Step: 7
Training loss: 2.6501848697662354
Validation loss: 2.0927882284246464

Epoch: 6| Step: 8
Training loss: 2.186180353164673
Validation loss: 2.08778719235492

Epoch: 6| Step: 9
Training loss: 1.5905516147613525
Validation loss: 2.0962315272259455

Epoch: 6| Step: 10
Training loss: 1.9112324714660645
Validation loss: 2.1074751192523586

Epoch: 6| Step: 11
Training loss: 2.3642737865448
Validation loss: 2.1059981648639967

Epoch: 6| Step: 12
Training loss: 1.9489818811416626
Validation loss: 2.1083694914335847

Epoch: 6| Step: 13
Training loss: 1.5529154539108276
Validation loss: 2.098749441485251

Epoch: 256| Step: 0
Training loss: 1.782983660697937
Validation loss: 2.1162533503706737

Epoch: 6| Step: 1
Training loss: 2.6059491634368896
Validation loss: 2.0938395505310385

Epoch: 6| Step: 2
Training loss: 2.6319398880004883
Validation loss: 2.129082936112599

Epoch: 6| Step: 3
Training loss: 1.7795078754425049
Validation loss: 2.106306929742136

Epoch: 6| Step: 4
Training loss: 1.7333570718765259
Validation loss: 2.0987127493786555

Epoch: 6| Step: 5
Training loss: 2.4172232151031494
Validation loss: 2.082045098786713

Epoch: 6| Step: 6
Training loss: 1.7677899599075317
Validation loss: 2.0743267433617705

Epoch: 6| Step: 7
Training loss: 2.1841259002685547
Validation loss: 2.1113260253783195

Epoch: 6| Step: 8
Training loss: 2.629885673522949
Validation loss: 2.115890772111954

Epoch: 6| Step: 9
Training loss: 2.4000070095062256
Validation loss: 2.1305845527238745

Epoch: 6| Step: 10
Training loss: 1.8943191766738892
Validation loss: 2.118270056222075

Epoch: 6| Step: 11
Training loss: 1.8710016012191772
Validation loss: 2.078412437951693

Epoch: 6| Step: 12
Training loss: 1.3092607259750366
Validation loss: 2.1209774299334456

Epoch: 6| Step: 13
Training loss: 3.550173759460449
Validation loss: 2.1141704641362673

Epoch: 257| Step: 0
Training loss: 2.045393466949463
Validation loss: 2.093604431357435

Epoch: 6| Step: 1
Training loss: 2.568673849105835
Validation loss: 2.1146069521545083

Epoch: 6| Step: 2
Training loss: 1.8557896614074707
Validation loss: 2.0944585825807307

Epoch: 6| Step: 3
Training loss: 1.9577709436416626
Validation loss: 2.126158360512026

Epoch: 6| Step: 4
Training loss: 1.4968459606170654
Validation loss: 2.104635787266557

Epoch: 6| Step: 5
Training loss: 2.5692074298858643
Validation loss: 2.0999899666796447

Epoch: 6| Step: 6
Training loss: 2.284493923187256
Validation loss: 2.1012826324791036

Epoch: 6| Step: 7
Training loss: 2.0080459117889404
Validation loss: 2.0667464399850495

Epoch: 6| Step: 8
Training loss: 2.124716281890869
Validation loss: 2.0894680279557423

Epoch: 6| Step: 9
Training loss: 1.9591352939605713
Validation loss: 2.0688957398937595

Epoch: 6| Step: 10
Training loss: 2.2291877269744873
Validation loss: 2.062234031256809

Epoch: 6| Step: 11
Training loss: 2.7422993183135986
Validation loss: 2.0550595739836335

Epoch: 6| Step: 12
Training loss: 2.6595091819763184
Validation loss: 2.0931047111429195

Epoch: 6| Step: 13
Training loss: 1.2413878440856934
Validation loss: 2.095819378411898

Epoch: 258| Step: 0
Training loss: 1.9790329933166504
Validation loss: 2.079297216989661

Epoch: 6| Step: 1
Training loss: 2.302889823913574
Validation loss: 2.0859818330375095

Epoch: 6| Step: 2
Training loss: 2.893827438354492
Validation loss: 2.0950852209521877

Epoch: 6| Step: 3
Training loss: 1.526052713394165
Validation loss: 2.0841744497258174

Epoch: 6| Step: 4
Training loss: 1.4672057628631592
Validation loss: 2.1219657851803686

Epoch: 6| Step: 5
Training loss: 1.9770612716674805
Validation loss: 2.1240881489169214

Epoch: 6| Step: 6
Training loss: 2.3159406185150146
Validation loss: 2.1026196889979865

Epoch: 6| Step: 7
Training loss: 2.3455047607421875
Validation loss: 2.0896378101841098

Epoch: 6| Step: 8
Training loss: 2.495377540588379
Validation loss: 2.114930506675474

Epoch: 6| Step: 9
Training loss: 2.5277092456817627
Validation loss: 2.14497511617599

Epoch: 6| Step: 10
Training loss: 1.835018515586853
Validation loss: 2.1102776604314006

Epoch: 6| Step: 11
Training loss: 2.043132781982422
Validation loss: 2.123484217992393

Epoch: 6| Step: 12
Training loss: 2.0150561332702637
Validation loss: 2.094018170910497

Epoch: 6| Step: 13
Training loss: 2.218273639678955
Validation loss: 2.1476549371596305

Epoch: 259| Step: 0
Training loss: 2.200507640838623
Validation loss: 2.134230282998854

Epoch: 6| Step: 1
Training loss: 1.7149617671966553
Validation loss: 2.1228305473122546

Epoch: 6| Step: 2
Training loss: 2.4078192710876465
Validation loss: 2.1212377753309024

Epoch: 6| Step: 3
Training loss: 2.2794671058654785
Validation loss: 2.1164602220699353

Epoch: 6| Step: 4
Training loss: 1.7369203567504883
Validation loss: 2.0768071784768054

Epoch: 6| Step: 5
Training loss: 1.9247063398361206
Validation loss: 2.1149776956086517

Epoch: 6| Step: 6
Training loss: 2.383852481842041
Validation loss: 2.1145259770013953

Epoch: 6| Step: 7
Training loss: 2.8017477989196777
Validation loss: 2.102627954175395

Epoch: 6| Step: 8
Training loss: 1.8000274896621704
Validation loss: 2.1208932668932023

Epoch: 6| Step: 9
Training loss: 1.9944099187850952
Validation loss: 2.108993230327483

Epoch: 6| Step: 10
Training loss: 2.084362268447876
Validation loss: 2.0925549294358943

Epoch: 6| Step: 11
Training loss: 1.681962490081787
Validation loss: 2.085031727308868

Epoch: 6| Step: 12
Training loss: 2.374751567840576
Validation loss: 2.1084112198122087

Epoch: 6| Step: 13
Training loss: 2.8500802516937256
Validation loss: 2.09155955109545

Epoch: 260| Step: 0
Training loss: 1.5583667755126953
Validation loss: 2.0987261021009056

Epoch: 6| Step: 1
Training loss: 2.2108073234558105
Validation loss: 2.0635252229629026

Epoch: 6| Step: 2
Training loss: 2.4693851470947266
Validation loss: 2.1163309671545543

Epoch: 6| Step: 3
Training loss: 0.8140426874160767
Validation loss: 2.097730326396163

Epoch: 6| Step: 4
Training loss: 2.1559290885925293
Validation loss: 2.1042026447993454

Epoch: 6| Step: 5
Training loss: 2.6584620475769043
Validation loss: 2.084433881185388

Epoch: 6| Step: 6
Training loss: 2.0959701538085938
Validation loss: 2.1178182709601616

Epoch: 6| Step: 7
Training loss: 2.748589515686035
Validation loss: 2.1290693129262617

Epoch: 6| Step: 8
Training loss: 2.2055811882019043
Validation loss: 2.1263121686955935

Epoch: 6| Step: 9
Training loss: 2.534865379333496
Validation loss: 2.113790414666617

Epoch: 6| Step: 10
Training loss: 1.9390549659729004
Validation loss: 2.138997047178207

Epoch: 6| Step: 11
Training loss: 2.243727445602417
Validation loss: 2.104568289172265

Epoch: 6| Step: 12
Training loss: 1.8850529193878174
Validation loss: 2.1193246892703477

Epoch: 6| Step: 13
Training loss: 2.268624782562256
Validation loss: 2.121330213803117

Epoch: 261| Step: 0
Training loss: 1.7437301874160767
Validation loss: 2.115404298228602

Epoch: 6| Step: 1
Training loss: 2.092829942703247
Validation loss: 2.107293418658677

Epoch: 6| Step: 2
Training loss: 1.712751865386963
Validation loss: 2.1090211099193943

Epoch: 6| Step: 3
Training loss: 2.0251410007476807
Validation loss: 2.1030089496284403

Epoch: 6| Step: 4
Training loss: 2.3660013675689697
Validation loss: 2.0805488632571314

Epoch: 6| Step: 5
Training loss: 2.149609088897705
Validation loss: 2.0878268928938013

Epoch: 6| Step: 6
Training loss: 2.040752410888672
Validation loss: 2.084194311531641

Epoch: 6| Step: 7
Training loss: 2.289674758911133
Validation loss: 2.087087672243836

Epoch: 6| Step: 8
Training loss: 2.8361802101135254
Validation loss: 2.0893889883513093

Epoch: 6| Step: 9
Training loss: 2.7060675621032715
Validation loss: 2.088190932427683

Epoch: 6| Step: 10
Training loss: 1.6406669616699219
Validation loss: 2.095549178379838

Epoch: 6| Step: 11
Training loss: 1.8890469074249268
Validation loss: 2.1041905726155927

Epoch: 6| Step: 12
Training loss: 2.347292423248291
Validation loss: 2.0885889607091106

Epoch: 6| Step: 13
Training loss: 1.8676066398620605
Validation loss: 2.0897046468591176

Epoch: 262| Step: 0
Training loss: 2.2041208744049072
Validation loss: 2.071129587388808

Epoch: 6| Step: 1
Training loss: 2.3769636154174805
Validation loss: 2.1066649549750873

Epoch: 6| Step: 2
Training loss: 2.326803684234619
Validation loss: 2.0870302646390853

Epoch: 6| Step: 3
Training loss: 1.0036187171936035
Validation loss: 2.11133409571904

Epoch: 6| Step: 4
Training loss: 1.5415619611740112
Validation loss: 2.0955843515293573

Epoch: 6| Step: 5
Training loss: 2.509563446044922
Validation loss: 2.0883837079489105

Epoch: 6| Step: 6
Training loss: 2.0807371139526367
Validation loss: 2.081861503662602

Epoch: 6| Step: 7
Training loss: 1.9884060621261597
Validation loss: 2.1166845688255886

Epoch: 6| Step: 8
Training loss: 2.5972509384155273
Validation loss: 2.098563450638966

Epoch: 6| Step: 9
Training loss: 2.209223985671997
Validation loss: 2.118503144992295

Epoch: 6| Step: 10
Training loss: 2.241777181625366
Validation loss: 2.1004513438029955

Epoch: 6| Step: 11
Training loss: 2.7107346057891846
Validation loss: 2.093518821142053

Epoch: 6| Step: 12
Training loss: 2.0122761726379395
Validation loss: 2.1214061244841544

Epoch: 6| Step: 13
Training loss: 2.0769524574279785
Validation loss: 2.10876973213688

Epoch: 263| Step: 0
Training loss: 2.116925001144409
Validation loss: 2.111801490988783

Epoch: 6| Step: 1
Training loss: 2.6781985759735107
Validation loss: 2.0945510351529686

Epoch: 6| Step: 2
Training loss: 1.6511991024017334
Validation loss: 2.0950524114793345

Epoch: 6| Step: 3
Training loss: 1.8559322357177734
Validation loss: 2.1298456063834568

Epoch: 6| Step: 4
Training loss: 2.205245018005371
Validation loss: 2.101172128031331

Epoch: 6| Step: 5
Training loss: 1.7030854225158691
Validation loss: 2.0953324866551224

Epoch: 6| Step: 6
Training loss: 2.1019203662872314
Validation loss: 2.093655670842817

Epoch: 6| Step: 7
Training loss: 2.520932197570801
Validation loss: 2.098485057071973

Epoch: 6| Step: 8
Training loss: 2.15899395942688
Validation loss: 2.071662163221708

Epoch: 6| Step: 9
Training loss: 1.7402119636535645
Validation loss: 2.090860661639962

Epoch: 6| Step: 10
Training loss: 2.5595223903656006
Validation loss: 2.06074196164326

Epoch: 6| Step: 11
Training loss: 2.337310791015625
Validation loss: 2.0853499058754212

Epoch: 6| Step: 12
Training loss: 2.145596981048584
Validation loss: 2.088565182942216

Epoch: 6| Step: 13
Training loss: 2.1381075382232666
Validation loss: 2.0882368062132146

Epoch: 264| Step: 0
Training loss: 2.070197582244873
Validation loss: 2.068225629868046

Epoch: 6| Step: 1
Training loss: 2.59755802154541
Validation loss: 2.1118820431411907

Epoch: 6| Step: 2
Training loss: 1.2354133129119873
Validation loss: 2.085756665916853

Epoch: 6| Step: 3
Training loss: 1.040209174156189
Validation loss: 2.059136708577474

Epoch: 6| Step: 4
Training loss: 2.135143756866455
Validation loss: 2.0884302828901555

Epoch: 6| Step: 5
Training loss: 2.8067986965179443
Validation loss: 2.092473422327349

Epoch: 6| Step: 6
Training loss: 2.0345358848571777
Validation loss: 2.074143627638458

Epoch: 6| Step: 7
Training loss: 3.2340526580810547
Validation loss: 2.092491433184634

Epoch: 6| Step: 8
Training loss: 2.2635488510131836
Validation loss: 2.099246514740811

Epoch: 6| Step: 9
Training loss: 2.0108392238616943
Validation loss: 2.132213222083225

Epoch: 6| Step: 10
Training loss: 2.191445827484131
Validation loss: 2.0869482409569526

Epoch: 6| Step: 11
Training loss: 2.178231716156006
Validation loss: 2.102144992479714

Epoch: 6| Step: 12
Training loss: 2.136644124984741
Validation loss: 2.100758565369473

Epoch: 6| Step: 13
Training loss: 1.7714972496032715
Validation loss: 2.0904557858743975

Epoch: 265| Step: 0
Training loss: 2.735805034637451
Validation loss: 2.107561301159602

Epoch: 6| Step: 1
Training loss: 2.1118619441986084
Validation loss: 2.1070708408150622

Epoch: 6| Step: 2
Training loss: 1.8678632974624634
Validation loss: 2.1010521778496365

Epoch: 6| Step: 3
Training loss: 2.2850613594055176
Validation loss: 2.1085084048650597

Epoch: 6| Step: 4
Training loss: 2.069281578063965
Validation loss: 2.1248554593773297

Epoch: 6| Step: 5
Training loss: 1.6388753652572632
Validation loss: 2.122060275846912

Epoch: 6| Step: 6
Training loss: 2.3623132705688477
Validation loss: 2.1184261357912453

Epoch: 6| Step: 7
Training loss: 2.463944911956787
Validation loss: 2.125600281582084

Epoch: 6| Step: 8
Training loss: 1.9295213222503662
Validation loss: 2.1383081789939635

Epoch: 6| Step: 9
Training loss: 1.9482636451721191
Validation loss: 2.153697001036777

Epoch: 6| Step: 10
Training loss: 2.1600003242492676
Validation loss: 2.127894634841591

Epoch: 6| Step: 11
Training loss: 2.1592230796813965
Validation loss: 2.114293872669179

Epoch: 6| Step: 12
Training loss: 2.1577839851379395
Validation loss: 2.1461249833465903

Epoch: 6| Step: 13
Training loss: 1.8508306741714478
Validation loss: 2.1209935834330897

Epoch: 266| Step: 0
Training loss: 2.3042683601379395
Validation loss: 2.1405172399295274

Epoch: 6| Step: 1
Training loss: 1.8060590028762817
Validation loss: 2.12157973166435

Epoch: 6| Step: 2
Training loss: 1.8421754837036133
Validation loss: 2.1372585834995395

Epoch: 6| Step: 3
Training loss: 1.5819697380065918
Validation loss: 2.1200550858692457

Epoch: 6| Step: 4
Training loss: 1.455618143081665
Validation loss: 2.121121152754753

Epoch: 6| Step: 5
Training loss: 2.577019691467285
Validation loss: 2.097303521248602

Epoch: 6| Step: 6
Training loss: 2.2753348350524902
Validation loss: 2.0889252975422847

Epoch: 6| Step: 7
Training loss: 2.331009864807129
Validation loss: 2.073679138255376

Epoch: 6| Step: 8
Training loss: 1.7770156860351562
Validation loss: 2.0992180352569907

Epoch: 6| Step: 9
Training loss: 2.1540069580078125
Validation loss: 2.0834071085017216

Epoch: 6| Step: 10
Training loss: 2.3369336128234863
Validation loss: 2.0855086823945403

Epoch: 6| Step: 11
Training loss: 2.222663640975952
Validation loss: 2.0811249056170062

Epoch: 6| Step: 12
Training loss: 2.8881232738494873
Validation loss: 2.104165241282473

Epoch: 6| Step: 13
Training loss: 2.2496910095214844
Validation loss: 2.078843725624905

Epoch: 267| Step: 0
Training loss: 2.0746846199035645
Validation loss: 2.054774333071965

Epoch: 6| Step: 1
Training loss: 2.032452344894409
Validation loss: 2.098970023534631

Epoch: 6| Step: 2
Training loss: 2.9938650131225586
Validation loss: 2.0798736490229124

Epoch: 6| Step: 3
Training loss: 1.7031210660934448
Validation loss: 2.0979595607326877

Epoch: 6| Step: 4
Training loss: 1.4140104055404663
Validation loss: 2.080137852699526

Epoch: 6| Step: 5
Training loss: 2.4603426456451416
Validation loss: 2.0571705961740143

Epoch: 6| Step: 6
Training loss: 1.914088249206543
Validation loss: 2.1026289962953135

Epoch: 6| Step: 7
Training loss: 1.799599051475525
Validation loss: 2.046480445451634

Epoch: 6| Step: 8
Training loss: 2.008692979812622
Validation loss: 2.0946345072920605

Epoch: 6| Step: 9
Training loss: 1.7990339994430542
Validation loss: 2.0880487529180383

Epoch: 6| Step: 10
Training loss: 3.1753830909729004
Validation loss: 2.0859334994387884

Epoch: 6| Step: 11
Training loss: 2.1583034992218018
Validation loss: 2.095111600814327

Epoch: 6| Step: 12
Training loss: 1.9089789390563965
Validation loss: 2.1282648399312007

Epoch: 6| Step: 13
Training loss: 2.587244987487793
Validation loss: 2.106961193905082

Epoch: 268| Step: 0
Training loss: 2.756476879119873
Validation loss: 2.106817751802424

Epoch: 6| Step: 1
Training loss: 1.9623743295669556
Validation loss: 2.1171629134044854

Epoch: 6| Step: 2
Training loss: 1.4337106943130493
Validation loss: 2.1044151859898723

Epoch: 6| Step: 3
Training loss: 2.162041664123535
Validation loss: 2.0902189464979273

Epoch: 6| Step: 4
Training loss: 1.6517770290374756
Validation loss: 2.140270353645407

Epoch: 6| Step: 5
Training loss: 1.9597536325454712
Validation loss: 2.1283669933196037

Epoch: 6| Step: 6
Training loss: 2.2816638946533203
Validation loss: 2.126699880887103

Epoch: 6| Step: 7
Training loss: 2.5707454681396484
Validation loss: 2.117455477355629

Epoch: 6| Step: 8
Training loss: 1.4017369747161865
Validation loss: 2.1261505337171656

Epoch: 6| Step: 9
Training loss: 2.3418006896972656
Validation loss: 2.1268446778738372

Epoch: 6| Step: 10
Training loss: 2.624110221862793
Validation loss: 2.1462720491552867

Epoch: 6| Step: 11
Training loss: 2.0489611625671387
Validation loss: 2.145835248372888

Epoch: 6| Step: 12
Training loss: 2.556309938430786
Validation loss: 2.1274820719995806

Epoch: 6| Step: 13
Training loss: 1.34915030002594
Validation loss: 2.1157452060330297

Epoch: 269| Step: 0
Training loss: 2.381185531616211
Validation loss: 2.145583196352887

Epoch: 6| Step: 1
Training loss: 1.525879144668579
Validation loss: 2.142370980273011

Epoch: 6| Step: 2
Training loss: 2.283452033996582
Validation loss: 2.1195022547116844

Epoch: 6| Step: 3
Training loss: 1.4014432430267334
Validation loss: 2.1258906664386874

Epoch: 6| Step: 4
Training loss: 2.4280929565429688
Validation loss: 2.1216609029359716

Epoch: 6| Step: 5
Training loss: 2.320261001586914
Validation loss: 2.1351717300312494

Epoch: 6| Step: 6
Training loss: 2.6554672718048096
Validation loss: 2.147070653976933

Epoch: 6| Step: 7
Training loss: 2.0646934509277344
Validation loss: 2.1123755593453684

Epoch: 6| Step: 8
Training loss: 1.830848217010498
Validation loss: 2.14337182301347

Epoch: 6| Step: 9
Training loss: 1.96818208694458
Validation loss: 2.1122478182597826

Epoch: 6| Step: 10
Training loss: 2.4299237728118896
Validation loss: 2.1382569292540192

Epoch: 6| Step: 11
Training loss: 2.038684606552124
Validation loss: 2.1312072046341433

Epoch: 6| Step: 12
Training loss: 2.3908534049987793
Validation loss: 2.1062657346007643

Epoch: 6| Step: 13
Training loss: 1.5571589469909668
Validation loss: 2.1119794345671132

Epoch: 270| Step: 0
Training loss: 1.5225591659545898
Validation loss: 2.1016022492480535

Epoch: 6| Step: 1
Training loss: 2.197218418121338
Validation loss: 2.0838829214854906

Epoch: 6| Step: 2
Training loss: 2.5139918327331543
Validation loss: 2.08255511201838

Epoch: 6| Step: 3
Training loss: 2.732422113418579
Validation loss: 2.098366075946439

Epoch: 6| Step: 4
Training loss: 1.8474829196929932
Validation loss: 2.0998390323372296

Epoch: 6| Step: 5
Training loss: 2.1561336517333984
Validation loss: 2.1092095810879945

Epoch: 6| Step: 6
Training loss: 2.481873035430908
Validation loss: 2.0891919366775022

Epoch: 6| Step: 7
Training loss: 1.9848895072937012
Validation loss: 2.084976283452844

Epoch: 6| Step: 8
Training loss: 2.382831573486328
Validation loss: 2.086199301545338

Epoch: 6| Step: 9
Training loss: 2.0357837677001953
Validation loss: 2.0920368881635767

Epoch: 6| Step: 10
Training loss: 1.781450629234314
Validation loss: 2.069581877800726

Epoch: 6| Step: 11
Training loss: 2.482825756072998
Validation loss: 2.08629765946378

Epoch: 6| Step: 12
Training loss: 1.634358286857605
Validation loss: 2.0924531067571333

Epoch: 6| Step: 13
Training loss: 1.9359580278396606
Validation loss: 2.0754040184841362

Epoch: 271| Step: 0
Training loss: 2.129725456237793
Validation loss: 2.0887606297769854

Epoch: 6| Step: 1
Training loss: 1.821291446685791
Validation loss: 2.0688630560392975

Epoch: 6| Step: 2
Training loss: 1.6937992572784424
Validation loss: 2.0833142521560832

Epoch: 6| Step: 3
Training loss: 2.0789811611175537
Validation loss: 2.117782328718452

Epoch: 6| Step: 4
Training loss: 2.2507171630859375
Validation loss: 2.1075024604797363

Epoch: 6| Step: 5
Training loss: 2.475632667541504
Validation loss: 2.0929321191644155

Epoch: 6| Step: 6
Training loss: 1.8807263374328613
Validation loss: 2.1135791681146108

Epoch: 6| Step: 7
Training loss: 2.3877482414245605
Validation loss: 2.1110145007410357

Epoch: 6| Step: 8
Training loss: 2.728635549545288
Validation loss: 2.108900813646214

Epoch: 6| Step: 9
Training loss: 2.5826950073242188
Validation loss: 2.103769653586931

Epoch: 6| Step: 10
Training loss: 1.8288288116455078
Validation loss: 2.12361151941361

Epoch: 6| Step: 11
Training loss: 1.609266996383667
Validation loss: 2.105623681058166

Epoch: 6| Step: 12
Training loss: 1.9639402627944946
Validation loss: 2.1213195426489717

Epoch: 6| Step: 13
Training loss: 2.5426080226898193
Validation loss: 2.1377749340508574

Epoch: 272| Step: 0
Training loss: 1.7362546920776367
Validation loss: 2.1152290323729157

Epoch: 6| Step: 1
Training loss: 1.7645864486694336
Validation loss: 2.0959360932791107

Epoch: 6| Step: 2
Training loss: 1.9293614625930786
Validation loss: 2.123406433290051

Epoch: 6| Step: 3
Training loss: 2.727935552597046
Validation loss: 2.13746149821948

Epoch: 6| Step: 4
Training loss: 2.2079131603240967
Validation loss: 2.120362530472458

Epoch: 6| Step: 5
Training loss: 2.2458643913269043
Validation loss: 2.1047780052308114

Epoch: 6| Step: 6
Training loss: 2.08618426322937
Validation loss: 2.1311725647218767

Epoch: 6| Step: 7
Training loss: 2.1748149394989014
Validation loss: 2.103571032965055

Epoch: 6| Step: 8
Training loss: 2.5305347442626953
Validation loss: 2.132644212374123

Epoch: 6| Step: 9
Training loss: 1.3034231662750244
Validation loss: 2.142112365332983

Epoch: 6| Step: 10
Training loss: 2.1888716220855713
Validation loss: 2.1542369063182543

Epoch: 6| Step: 11
Training loss: 2.47367525100708
Validation loss: 2.1189955126854683

Epoch: 6| Step: 12
Training loss: 2.3232569694519043
Validation loss: 2.107611266515588

Epoch: 6| Step: 13
Training loss: 2.055464744567871
Validation loss: 2.1162611592200493

Epoch: 273| Step: 0
Training loss: 1.970844030380249
Validation loss: 2.132244485680775

Epoch: 6| Step: 1
Training loss: 1.4399235248565674
Validation loss: 2.097042211922266

Epoch: 6| Step: 2
Training loss: 2.0397658348083496
Validation loss: 2.087147298679557

Epoch: 6| Step: 3
Training loss: 1.8375269174575806
Validation loss: 2.1099819842205254

Epoch: 6| Step: 4
Training loss: 2.065884590148926
Validation loss: 2.1035870941736365

Epoch: 6| Step: 5
Training loss: 1.7219685316085815
Validation loss: 2.086844230210909

Epoch: 6| Step: 6
Training loss: 1.8442325592041016
Validation loss: 2.0767477379050305

Epoch: 6| Step: 7
Training loss: 2.1041791439056396
Validation loss: 2.098962983777446

Epoch: 6| Step: 8
Training loss: 2.8405044078826904
Validation loss: 2.0636807180220083

Epoch: 6| Step: 9
Training loss: 2.6630043983459473
Validation loss: 2.0713762647362164

Epoch: 6| Step: 10
Training loss: 2.209416389465332
Validation loss: 2.0767812087971675

Epoch: 6| Step: 11
Training loss: 2.9898574352264404
Validation loss: 2.079262182276736

Epoch: 6| Step: 12
Training loss: 1.7407151460647583
Validation loss: 2.073595631507135

Epoch: 6| Step: 13
Training loss: 2.168087959289551
Validation loss: 2.0753362306984524

Epoch: 274| Step: 0
Training loss: 3.185192108154297
Validation loss: 2.104769237579838

Epoch: 6| Step: 1
Training loss: 1.5627410411834717
Validation loss: 2.087583772597774

Epoch: 6| Step: 2
Training loss: 2.150580406188965
Validation loss: 2.0823453626325055

Epoch: 6| Step: 3
Training loss: 2.6099958419799805
Validation loss: 2.0839560749710246

Epoch: 6| Step: 4
Training loss: 2.4238405227661133
Validation loss: 2.0781227439962406

Epoch: 6| Step: 5
Training loss: 2.212648868560791
Validation loss: 2.096362363907599

Epoch: 6| Step: 6
Training loss: 1.984121561050415
Validation loss: 2.0847504279946767

Epoch: 6| Step: 7
Training loss: 2.051982879638672
Validation loss: 2.1136788322079565

Epoch: 6| Step: 8
Training loss: 1.5397671461105347
Validation loss: 2.100220577691191

Epoch: 6| Step: 9
Training loss: 1.630682349205017
Validation loss: 2.1134528152404295

Epoch: 6| Step: 10
Training loss: 1.804985523223877
Validation loss: 2.109635668416177

Epoch: 6| Step: 11
Training loss: 2.070742607116699
Validation loss: 2.083831823000344

Epoch: 6| Step: 12
Training loss: 2.2322399616241455
Validation loss: 2.0994076216092674

Epoch: 6| Step: 13
Training loss: 2.358494997024536
Validation loss: 2.1047636232068463

Epoch: 275| Step: 0
Training loss: 2.06048583984375
Validation loss: 2.1114013092492216

Epoch: 6| Step: 1
Training loss: 2.514887809753418
Validation loss: 2.1140295356832524

Epoch: 6| Step: 2
Training loss: 2.695587158203125
Validation loss: 2.093781917325912

Epoch: 6| Step: 3
Training loss: 2.123366355895996
Validation loss: 2.09381184270305

Epoch: 6| Step: 4
Training loss: 2.230116128921509
Validation loss: 2.066680246783841

Epoch: 6| Step: 5
Training loss: 1.8686906099319458
Validation loss: 2.113669159591839

Epoch: 6| Step: 6
Training loss: 1.762810230255127
Validation loss: 2.107565136365993

Epoch: 6| Step: 7
Training loss: 1.5047974586486816
Validation loss: 2.075349985912282

Epoch: 6| Step: 8
Training loss: 2.685150146484375
Validation loss: 2.0793487256573093

Epoch: 6| Step: 9
Training loss: 1.62184739112854
Validation loss: 2.1174521394955215

Epoch: 6| Step: 10
Training loss: 1.967439889907837
Validation loss: 2.078273827029813

Epoch: 6| Step: 11
Training loss: 2.1559343338012695
Validation loss: 2.08729068181848

Epoch: 6| Step: 12
Training loss: 1.6659059524536133
Validation loss: 2.0493942614524596

Epoch: 6| Step: 13
Training loss: 3.181367874145508
Validation loss: 2.06969524968055

Epoch: 276| Step: 0
Training loss: 2.274149179458618
Validation loss: 2.0615922609965005

Epoch: 6| Step: 1
Training loss: 2.4842751026153564
Validation loss: 2.0883941176117107

Epoch: 6| Step: 2
Training loss: 2.914118766784668
Validation loss: 2.096960348467673

Epoch: 6| Step: 3
Training loss: 2.1440184116363525
Validation loss: 2.110469666860437

Epoch: 6| Step: 4
Training loss: 1.9873650074005127
Validation loss: 2.1042532049199587

Epoch: 6| Step: 5
Training loss: 1.2875127792358398
Validation loss: 2.0983215557631625

Epoch: 6| Step: 6
Training loss: 1.9991705417633057
Validation loss: 2.078356060930478

Epoch: 6| Step: 7
Training loss: 2.076951265335083
Validation loss: 2.076056739335419

Epoch: 6| Step: 8
Training loss: 1.5706074237823486
Validation loss: 2.078739917406472

Epoch: 6| Step: 9
Training loss: 2.1715314388275146
Validation loss: 2.0969079720076693

Epoch: 6| Step: 10
Training loss: 2.8691279888153076
Validation loss: 2.1003432068773495

Epoch: 6| Step: 11
Training loss: 1.5715916156768799
Validation loss: 2.089199830127019

Epoch: 6| Step: 12
Training loss: 2.1050333976745605
Validation loss: 2.0981400359061455

Epoch: 6| Step: 13
Training loss: 2.1388707160949707
Validation loss: 2.1092856686602355

Epoch: 277| Step: 0
Training loss: 3.008690595626831
Validation loss: 2.116430905557448

Epoch: 6| Step: 1
Training loss: 2.217102527618408
Validation loss: 2.13364779180096

Epoch: 6| Step: 2
Training loss: 2.2541606426239014
Validation loss: 2.114292985649519

Epoch: 6| Step: 3
Training loss: 2.320596218109131
Validation loss: 2.14760834170926

Epoch: 6| Step: 4
Training loss: 2.085909605026245
Validation loss: 2.148038879517586

Epoch: 6| Step: 5
Training loss: 2.256410837173462
Validation loss: 2.1552487188769924

Epoch: 6| Step: 6
Training loss: 1.883873462677002
Validation loss: 2.146053937173659

Epoch: 6| Step: 7
Training loss: 2.391232490539551
Validation loss: 2.1277104231619064

Epoch: 6| Step: 8
Training loss: 2.1833443641662598
Validation loss: 2.1557067799311813

Epoch: 6| Step: 9
Training loss: 2.216958522796631
Validation loss: 2.1387454194407307

Epoch: 6| Step: 10
Training loss: 1.3416515588760376
Validation loss: 2.1327730276251353

Epoch: 6| Step: 11
Training loss: 2.057236671447754
Validation loss: 2.157963593800863

Epoch: 6| Step: 12
Training loss: 1.8236644268035889
Validation loss: 2.1571880591812955

Epoch: 6| Step: 13
Training loss: 1.625414252281189
Validation loss: 2.1691324710845947

Epoch: 278| Step: 0
Training loss: 2.3521006107330322
Validation loss: 2.1789087172477477

Epoch: 6| Step: 1
Training loss: 2.0038626194000244
Validation loss: 2.1483727398739068

Epoch: 6| Step: 2
Training loss: 1.8862935304641724
Validation loss: 2.1612978417386293

Epoch: 6| Step: 3
Training loss: 2.128101348876953
Validation loss: 2.1502936322201966

Epoch: 6| Step: 4
Training loss: 1.6819431781768799
Validation loss: 2.149635011149991

Epoch: 6| Step: 5
Training loss: 1.8883087635040283
Validation loss: 2.134977038188647

Epoch: 6| Step: 6
Training loss: 1.6701736450195312
Validation loss: 2.135249707006639

Epoch: 6| Step: 7
Training loss: 2.0131068229675293
Validation loss: 2.1293373082273748

Epoch: 6| Step: 8
Training loss: 2.0275135040283203
Validation loss: 2.1130474998104956

Epoch: 6| Step: 9
Training loss: 2.541797399520874
Validation loss: 2.133337664347823

Epoch: 6| Step: 10
Training loss: 2.323944091796875
Validation loss: 2.141087939662318

Epoch: 6| Step: 11
Training loss: 1.8797783851623535
Validation loss: 2.1183358610317273

Epoch: 6| Step: 12
Training loss: 2.5027358531951904
Validation loss: 2.103704123086827

Epoch: 6| Step: 13
Training loss: 3.241205930709839
Validation loss: 2.1169670294689875

Epoch: 279| Step: 0
Training loss: 2.01194167137146
Validation loss: 2.1022146619776243

Epoch: 6| Step: 1
Training loss: 2.6268138885498047
Validation loss: 2.0996060525217364

Epoch: 6| Step: 2
Training loss: 2.128807544708252
Validation loss: 2.0905030735077395

Epoch: 6| Step: 3
Training loss: 2.0060086250305176
Validation loss: 2.054405476457329

Epoch: 6| Step: 4
Training loss: 2.180849552154541
Validation loss: 2.1111851994709303

Epoch: 6| Step: 5
Training loss: 2.37186598777771
Validation loss: 2.060512793961392

Epoch: 6| Step: 6
Training loss: 1.8782289028167725
Validation loss: 2.0702614835513535

Epoch: 6| Step: 7
Training loss: 1.8949055671691895
Validation loss: 2.095636801053119

Epoch: 6| Step: 8
Training loss: 1.9988045692443848
Validation loss: 2.0870754616234892

Epoch: 6| Step: 9
Training loss: 1.8707818984985352
Validation loss: 2.123748771605953

Epoch: 6| Step: 10
Training loss: 2.0319020748138428
Validation loss: 2.0843116262907624

Epoch: 6| Step: 11
Training loss: 2.293592691421509
Validation loss: 2.0834045140973982

Epoch: 6| Step: 12
Training loss: 1.9041073322296143
Validation loss: 2.0908618332237325

Epoch: 6| Step: 13
Training loss: 2.7270796298980713
Validation loss: 2.103837441372615

Epoch: 280| Step: 0
Training loss: 2.3104348182678223
Validation loss: 2.083656011089202

Epoch: 6| Step: 1
Training loss: 2.0606303215026855
Validation loss: 2.087286840202988

Epoch: 6| Step: 2
Training loss: 1.8538811206817627
Validation loss: 2.091757728207496

Epoch: 6| Step: 3
Training loss: 1.4805052280426025
Validation loss: 2.091312644302204

Epoch: 6| Step: 4
Training loss: 1.6738824844360352
Validation loss: 2.0684630152999715

Epoch: 6| Step: 5
Training loss: 2.4235610961914062
Validation loss: 2.1023117547394126

Epoch: 6| Step: 6
Training loss: 2.3360860347747803
Validation loss: 2.0896512500701414

Epoch: 6| Step: 7
Training loss: 2.0770082473754883
Validation loss: 2.1081344671146844

Epoch: 6| Step: 8
Training loss: 2.679922103881836
Validation loss: 2.1156882727017967

Epoch: 6| Step: 9
Training loss: 2.1777963638305664
Validation loss: 2.131231161855882

Epoch: 6| Step: 10
Training loss: 1.8573296070098877
Validation loss: 2.117688476398427

Epoch: 6| Step: 11
Training loss: 2.0989151000976562
Validation loss: 2.079025137809015

Epoch: 6| Step: 12
Training loss: 2.4056737422943115
Validation loss: 2.1019434005983415

Epoch: 6| Step: 13
Training loss: 2.216510772705078
Validation loss: 2.1176883943619265

Epoch: 281| Step: 0
Training loss: 2.1374642848968506
Validation loss: 2.111197128090807

Epoch: 6| Step: 1
Training loss: 2.470602035522461
Validation loss: 2.095054361128038

Epoch: 6| Step: 2
Training loss: 2.2921228408813477
Validation loss: 2.104847099191399

Epoch: 6| Step: 3
Training loss: 2.2502646446228027
Validation loss: 2.1130706699945594

Epoch: 6| Step: 4
Training loss: 1.891944169998169
Validation loss: 2.0890497661405996

Epoch: 6| Step: 5
Training loss: 1.9302480220794678
Validation loss: 2.1426031820235716

Epoch: 6| Step: 6
Training loss: 1.7816855907440186
Validation loss: 2.111602780639484

Epoch: 6| Step: 7
Training loss: 1.6297920942306519
Validation loss: 2.141080146194786

Epoch: 6| Step: 8
Training loss: 3.053823709487915
Validation loss: 2.094799872367613

Epoch: 6| Step: 9
Training loss: 2.050300121307373
Validation loss: 2.1090637432631625

Epoch: 6| Step: 10
Training loss: 1.7159826755523682
Validation loss: 2.109606509567589

Epoch: 6| Step: 11
Training loss: 2.2540252208709717
Validation loss: 2.093592128446025

Epoch: 6| Step: 12
Training loss: 2.139759063720703
Validation loss: 2.0964014825000556

Epoch: 6| Step: 13
Training loss: 2.2068121433258057
Validation loss: 2.120361722925658

Epoch: 282| Step: 0
Training loss: 2.409876823425293
Validation loss: 2.1054585415829896

Epoch: 6| Step: 1
Training loss: 2.0052521228790283
Validation loss: 2.09712839382951

Epoch: 6| Step: 2
Training loss: 2.3760316371917725
Validation loss: 2.087393404335104

Epoch: 6| Step: 3
Training loss: 2.0969910621643066
Validation loss: 2.1155866448597243

Epoch: 6| Step: 4
Training loss: 2.00064754486084
Validation loss: 2.072235186894735

Epoch: 6| Step: 5
Training loss: 1.7484021186828613
Validation loss: 2.1135837249858405

Epoch: 6| Step: 6
Training loss: 2.479984760284424
Validation loss: 2.0910727747025026

Epoch: 6| Step: 7
Training loss: 2.525648832321167
Validation loss: 2.088097590272145

Epoch: 6| Step: 8
Training loss: 2.5791077613830566
Validation loss: 2.123428749781783

Epoch: 6| Step: 9
Training loss: 2.3077008724212646
Validation loss: 2.0776288586278118

Epoch: 6| Step: 10
Training loss: 2.056143283843994
Validation loss: 2.096697420202276

Epoch: 6| Step: 11
Training loss: 1.528433084487915
Validation loss: 2.1318068504333496

Epoch: 6| Step: 12
Training loss: 1.5813393592834473
Validation loss: 2.076236435162124

Epoch: 6| Step: 13
Training loss: 1.636657953262329
Validation loss: 2.0809746942212506

Epoch: 283| Step: 0
Training loss: 1.8405455350875854
Validation loss: 2.102905358037641

Epoch: 6| Step: 1
Training loss: 2.057746171951294
Validation loss: 2.0982097118131575

Epoch: 6| Step: 2
Training loss: 2.368335247039795
Validation loss: 2.1041384768742386

Epoch: 6| Step: 3
Training loss: 2.251011848449707
Validation loss: 2.0874675320040796

Epoch: 6| Step: 4
Training loss: 2.3460280895233154
Validation loss: 2.0929563763321086

Epoch: 6| Step: 5
Training loss: 2.5832924842834473
Validation loss: 2.0890792505715483

Epoch: 6| Step: 6
Training loss: 1.7670326232910156
Validation loss: 2.0974195464964835

Epoch: 6| Step: 7
Training loss: 2.3080718517303467
Validation loss: 2.089864392434397

Epoch: 6| Step: 8
Training loss: 2.4227304458618164
Validation loss: 2.1061721450539044

Epoch: 6| Step: 9
Training loss: 1.5867877006530762
Validation loss: 2.0688659862805436

Epoch: 6| Step: 10
Training loss: 2.0337185859680176
Validation loss: 2.0769142796916347

Epoch: 6| Step: 11
Training loss: 2.4107608795166016
Validation loss: 2.0889776778477493

Epoch: 6| Step: 12
Training loss: 1.5235472917556763
Validation loss: 2.065166700270868

Epoch: 6| Step: 13
Training loss: 2.210287570953369
Validation loss: 2.0892877553098943

Epoch: 284| Step: 0
Training loss: 1.49168062210083
Validation loss: 2.1000185781909573

Epoch: 6| Step: 1
Training loss: 2.474606990814209
Validation loss: 2.106639487769014

Epoch: 6| Step: 2
Training loss: 1.8194636106491089
Validation loss: 2.1232264772538216

Epoch: 6| Step: 3
Training loss: 2.6752429008483887
Validation loss: 2.1323962647427797

Epoch: 6| Step: 4
Training loss: 2.6039345264434814
Validation loss: 2.123219126014299

Epoch: 6| Step: 5
Training loss: 1.734006404876709
Validation loss: 2.105919873842629

Epoch: 6| Step: 6
Training loss: 1.5511322021484375
Validation loss: 2.122536866895614

Epoch: 6| Step: 7
Training loss: 1.580521821975708
Validation loss: 2.1204940580552623

Epoch: 6| Step: 8
Training loss: 2.0157899856567383
Validation loss: 2.118085256186865

Epoch: 6| Step: 9
Training loss: 1.6215882301330566
Validation loss: 2.1275589363549345

Epoch: 6| Step: 10
Training loss: 2.584949016571045
Validation loss: 2.1253453172663206

Epoch: 6| Step: 11
Training loss: 2.8175203800201416
Validation loss: 2.1420804018615396

Epoch: 6| Step: 12
Training loss: 2.2178916931152344
Validation loss: 2.1364818311506704

Epoch: 6| Step: 13
Training loss: 2.591444730758667
Validation loss: 2.1580026329204602

Epoch: 285| Step: 0
Training loss: 1.8219752311706543
Validation loss: 2.150915668856713

Epoch: 6| Step: 1
Training loss: 1.675709843635559
Validation loss: 2.150006781342209

Epoch: 6| Step: 2
Training loss: 2.569395065307617
Validation loss: 2.1315592976026636

Epoch: 6| Step: 3
Training loss: 3.1422905921936035
Validation loss: 2.1322754506141908

Epoch: 6| Step: 4
Training loss: 1.6432034969329834
Validation loss: 2.0964446631811

Epoch: 6| Step: 5
Training loss: 1.4344139099121094
Validation loss: 2.110789837375764

Epoch: 6| Step: 6
Training loss: 1.9196107387542725
Validation loss: 2.1178740327076246

Epoch: 6| Step: 7
Training loss: 2.1776366233825684
Validation loss: 2.0889347919853787

Epoch: 6| Step: 8
Training loss: 2.716108798980713
Validation loss: 2.10486368210085

Epoch: 6| Step: 9
Training loss: 2.3001089096069336
Validation loss: 2.112907789086783

Epoch: 6| Step: 10
Training loss: 2.046617031097412
Validation loss: 2.0994062449342463

Epoch: 6| Step: 11
Training loss: 1.1613489389419556
Validation loss: 2.102219707222395

Epoch: 6| Step: 12
Training loss: 2.2195277214050293
Validation loss: 2.1281418556808145

Epoch: 6| Step: 13
Training loss: 2.9564943313598633
Validation loss: 2.1078147580546718

Epoch: 286| Step: 0
Training loss: 2.0447020530700684
Validation loss: 2.101556142171224

Epoch: 6| Step: 1
Training loss: 2.0347914695739746
Validation loss: 2.1096644478459514

Epoch: 6| Step: 2
Training loss: 1.6669807434082031
Validation loss: 2.1307883749726

Epoch: 6| Step: 3
Training loss: 2.3269503116607666
Validation loss: 2.1035194345699844

Epoch: 6| Step: 4
Training loss: 1.7649458646774292
Validation loss: 2.1083519663862003

Epoch: 6| Step: 5
Training loss: 2.0648958683013916
Validation loss: 2.1133890280159573

Epoch: 6| Step: 6
Training loss: 1.8423490524291992
Validation loss: 2.128147609772221

Epoch: 6| Step: 7
Training loss: 3.0296525955200195
Validation loss: 2.1123810455363285

Epoch: 6| Step: 8
Training loss: 2.6621670722961426
Validation loss: 2.135990100522195

Epoch: 6| Step: 9
Training loss: 1.588958740234375
Validation loss: 2.146816211362039

Epoch: 6| Step: 10
Training loss: 2.172309398651123
Validation loss: 2.134377623117098

Epoch: 6| Step: 11
Training loss: 2.188859462738037
Validation loss: 2.124721578372422

Epoch: 6| Step: 12
Training loss: 2.2560696601867676
Validation loss: 2.1417436753549883

Epoch: 6| Step: 13
Training loss: 1.690676212310791
Validation loss: 2.1608188793223393

Epoch: 287| Step: 0
Training loss: 1.8826138973236084
Validation loss: 2.115597214750064

Epoch: 6| Step: 1
Training loss: 2.329850912094116
Validation loss: 2.1568047487607567

Epoch: 6| Step: 2
Training loss: 2.191649913787842
Validation loss: 2.137346210018281

Epoch: 6| Step: 3
Training loss: 1.572028636932373
Validation loss: 2.115501883209393

Epoch: 6| Step: 4
Training loss: 2.3155529499053955
Validation loss: 2.1277672718929987

Epoch: 6| Step: 5
Training loss: 1.9283065795898438
Validation loss: 2.101428013975902

Epoch: 6| Step: 6
Training loss: 2.2765846252441406
Validation loss: 2.1269959134440266

Epoch: 6| Step: 7
Training loss: 1.6862293481826782
Validation loss: 2.129117214551536

Epoch: 6| Step: 8
Training loss: 2.4718010425567627
Validation loss: 2.0986038497699204

Epoch: 6| Step: 9
Training loss: 2.424807548522949
Validation loss: 2.1003130200088664

Epoch: 6| Step: 10
Training loss: 2.5713462829589844
Validation loss: 2.0936889161345777

Epoch: 6| Step: 11
Training loss: 2.0065581798553467
Validation loss: 2.114356833119546

Epoch: 6| Step: 12
Training loss: 2.376528739929199
Validation loss: 2.0408866469578077

Epoch: 6| Step: 13
Training loss: 1.3586513996124268
Validation loss: 2.113683392924647

Epoch: 288| Step: 0
Training loss: 2.1973838806152344
Validation loss: 2.108272465326453

Epoch: 6| Step: 1
Training loss: 2.0762124061584473
Validation loss: 2.078002046513301

Epoch: 6| Step: 2
Training loss: 1.5007853507995605
Validation loss: 2.1049914718956075

Epoch: 6| Step: 3
Training loss: 3.0484561920166016
Validation loss: 2.098060166963967

Epoch: 6| Step: 4
Training loss: 2.0399773120880127
Validation loss: 2.070794105529785

Epoch: 6| Step: 5
Training loss: 2.15756893157959
Validation loss: 2.1167307874207855

Epoch: 6| Step: 6
Training loss: 2.211942195892334
Validation loss: 2.0971384894463325

Epoch: 6| Step: 7
Training loss: 1.2081395387649536
Validation loss: 2.0603476057770433

Epoch: 6| Step: 8
Training loss: 2.026777744293213
Validation loss: 2.086544753402792

Epoch: 6| Step: 9
Training loss: 1.9321606159210205
Validation loss: 2.11608543959997

Epoch: 6| Step: 10
Training loss: 1.7381350994110107
Validation loss: 2.093204980255455

Epoch: 6| Step: 11
Training loss: 2.027477264404297
Validation loss: 2.08257636972653

Epoch: 6| Step: 12
Training loss: 3.094905138015747
Validation loss: 2.1340129196002917

Epoch: 6| Step: 13
Training loss: 2.3425912857055664
Validation loss: 2.1408853556520198

Epoch: 289| Step: 0
Training loss: 1.9192869663238525
Validation loss: 2.101334377001691

Epoch: 6| Step: 1
Training loss: 2.2289669513702393
Validation loss: 2.086718528501449

Epoch: 6| Step: 2
Training loss: 1.7162810564041138
Validation loss: 2.1268961468050556

Epoch: 6| Step: 3
Training loss: 1.4778093099594116
Validation loss: 2.067056179046631

Epoch: 6| Step: 4
Training loss: 2.738323211669922
Validation loss: 2.0924096415119786

Epoch: 6| Step: 5
Training loss: 1.6693769693374634
Validation loss: 2.1033037503560386

Epoch: 6| Step: 6
Training loss: 2.2787585258483887
Validation loss: 2.086992415048743

Epoch: 6| Step: 7
Training loss: 2.4758872985839844
Validation loss: 2.0687091542828466

Epoch: 6| Step: 8
Training loss: 1.885465145111084
Validation loss: 2.0906388477612565

Epoch: 6| Step: 9
Training loss: 2.7982192039489746
Validation loss: 2.110382492824267

Epoch: 6| Step: 10
Training loss: 2.272517442703247
Validation loss: 2.1009648282040834

Epoch: 6| Step: 11
Training loss: 2.05855131149292
Validation loss: 2.1126480653721798

Epoch: 6| Step: 12
Training loss: 2.0840306282043457
Validation loss: 2.1100643886032926

Epoch: 6| Step: 13
Training loss: 2.0806961059570312
Validation loss: 2.094524363035797

Epoch: 290| Step: 0
Training loss: 1.9291863441467285
Validation loss: 2.0796459182616203

Epoch: 6| Step: 1
Training loss: 2.8211779594421387
Validation loss: 2.0994192925832604

Epoch: 6| Step: 2
Training loss: 2.5432586669921875
Validation loss: 2.11019197458862

Epoch: 6| Step: 3
Training loss: 2.0676960945129395
Validation loss: 2.080599482341479

Epoch: 6| Step: 4
Training loss: 2.1835978031158447
Validation loss: 2.1003261253397953

Epoch: 6| Step: 5
Training loss: 1.178046464920044
Validation loss: 2.123259654609106

Epoch: 6| Step: 6
Training loss: 1.6450719833374023
Validation loss: 2.0653156824009393

Epoch: 6| Step: 7
Training loss: 2.208489179611206
Validation loss: 2.0969422991557787

Epoch: 6| Step: 8
Training loss: 1.719154715538025
Validation loss: 2.113206330166068

Epoch: 6| Step: 9
Training loss: 2.1425633430480957
Validation loss: 2.1088534760218796

Epoch: 6| Step: 10
Training loss: 1.7320924997329712
Validation loss: 2.0968092308249524

Epoch: 6| Step: 11
Training loss: 1.8126412630081177
Validation loss: 2.1067447213716406

Epoch: 6| Step: 12
Training loss: 2.7581100463867188
Validation loss: 2.1004205365334787

Epoch: 6| Step: 13
Training loss: 3.008852005004883
Validation loss: 2.1033555153877503

Epoch: 291| Step: 0
Training loss: 2.6493921279907227
Validation loss: 2.1381539888279413

Epoch: 6| Step: 1
Training loss: 1.5909111499786377
Validation loss: 2.141203136854274

Epoch: 6| Step: 2
Training loss: 2.704744577407837
Validation loss: 2.107398684306811

Epoch: 6| Step: 3
Training loss: 1.9294772148132324
Validation loss: 2.135359502607776

Epoch: 6| Step: 4
Training loss: 2.125497817993164
Validation loss: 2.1331563970094085

Epoch: 6| Step: 5
Training loss: 1.6462057828903198
Validation loss: 2.1569345510134132

Epoch: 6| Step: 6
Training loss: 2.1430654525756836
Validation loss: 2.1380142755405878

Epoch: 6| Step: 7
Training loss: 1.899078130722046
Validation loss: 2.132919870397096

Epoch: 6| Step: 8
Training loss: 2.120337963104248
Validation loss: 2.153353785955778

Epoch: 6| Step: 9
Training loss: 2.1361918449401855
Validation loss: 2.147312602689189

Epoch: 6| Step: 10
Training loss: 2.1291799545288086
Validation loss: 2.155696213886302

Epoch: 6| Step: 11
Training loss: 2.00093674659729
Validation loss: 2.1371757291978404

Epoch: 6| Step: 12
Training loss: 2.1809864044189453
Validation loss: 2.1609129059699272

Epoch: 6| Step: 13
Training loss: 2.0428006649017334
Validation loss: 2.130163413222118

Epoch: 292| Step: 0
Training loss: 1.3010368347167969
Validation loss: 2.1295944208739908

Epoch: 6| Step: 1
Training loss: 1.6075682640075684
Validation loss: 2.1180646137524675

Epoch: 6| Step: 2
Training loss: 2.5858616828918457
Validation loss: 2.132716335276122

Epoch: 6| Step: 3
Training loss: 3.3817968368530273
Validation loss: 2.119784849946217

Epoch: 6| Step: 4
Training loss: 2.369187355041504
Validation loss: 2.1269852986899753

Epoch: 6| Step: 5
Training loss: 1.260124921798706
Validation loss: 2.0960973257659585

Epoch: 6| Step: 6
Training loss: 1.9588614702224731
Validation loss: 2.0973468237025763

Epoch: 6| Step: 7
Training loss: 1.5490219593048096
Validation loss: 2.07380937504512

Epoch: 6| Step: 8
Training loss: 2.0608320236206055
Validation loss: 2.0693925452488724

Epoch: 6| Step: 9
Training loss: 1.6713972091674805
Validation loss: 2.091610513707643

Epoch: 6| Step: 10
Training loss: 2.543212413787842
Validation loss: 2.1219189218295518

Epoch: 6| Step: 11
Training loss: 2.003232479095459
Validation loss: 2.094121312582365

Epoch: 6| Step: 12
Training loss: 2.9861669540405273
Validation loss: 2.1072523388811337

Epoch: 6| Step: 13
Training loss: 1.955859661102295
Validation loss: 2.0954753096385668

Epoch: 293| Step: 0
Training loss: 1.9479644298553467
Validation loss: 2.091443519438467

Epoch: 6| Step: 1
Training loss: 2.4253363609313965
Validation loss: 2.1277440337724585

Epoch: 6| Step: 2
Training loss: 1.8762476444244385
Validation loss: 2.1025843671573106

Epoch: 6| Step: 3
Training loss: 1.7561718225479126
Validation loss: 2.087235302053472

Epoch: 6| Step: 4
Training loss: 1.9305424690246582
Validation loss: 2.090584061479056

Epoch: 6| Step: 5
Training loss: 2.126359224319458
Validation loss: 2.1036373517846547

Epoch: 6| Step: 6
Training loss: 2.5151491165161133
Validation loss: 2.1045885803878948

Epoch: 6| Step: 7
Training loss: 2.3635222911834717
Validation loss: 2.1095901509766937

Epoch: 6| Step: 8
Training loss: 1.945838451385498
Validation loss: 2.1009300793370893

Epoch: 6| Step: 9
Training loss: 1.8788073062896729
Validation loss: 2.130154070033822

Epoch: 6| Step: 10
Training loss: 2.210989236831665
Validation loss: 2.0949787478293143

Epoch: 6| Step: 11
Training loss: 2.0735883712768555
Validation loss: 2.0917704874469387

Epoch: 6| Step: 12
Training loss: 2.1165215969085693
Validation loss: 2.1293469603343675

Epoch: 6| Step: 13
Training loss: 1.6019461154937744
Validation loss: 2.1431828814168132

Epoch: 294| Step: 0
Training loss: 1.9693818092346191
Validation loss: 2.123287926438034

Epoch: 6| Step: 1
Training loss: 2.2896649837493896
Validation loss: 2.1160538363200363

Epoch: 6| Step: 2
Training loss: 2.4672248363494873
Validation loss: 2.0853045550725793

Epoch: 6| Step: 3
Training loss: 1.8710269927978516
Validation loss: 2.1216711305802867

Epoch: 6| Step: 4
Training loss: 2.2144410610198975
Validation loss: 2.096170489506055

Epoch: 6| Step: 5
Training loss: 1.9149197340011597
Validation loss: 2.1122768207262923

Epoch: 6| Step: 6
Training loss: 2.0089383125305176
Validation loss: 2.1142038350464194

Epoch: 6| Step: 7
Training loss: 2.0368523597717285
Validation loss: 2.1389076761020127

Epoch: 6| Step: 8
Training loss: 2.2349467277526855
Validation loss: 2.12327072440937

Epoch: 6| Step: 9
Training loss: 2.243107557296753
Validation loss: 2.130349446368474

Epoch: 6| Step: 10
Training loss: 2.3596222400665283
Validation loss: 2.130270169627282

Epoch: 6| Step: 11
Training loss: 1.8249801397323608
Validation loss: 2.1263559582412883

Epoch: 6| Step: 12
Training loss: 1.8460495471954346
Validation loss: 2.1151348570341706

Epoch: 6| Step: 13
Training loss: 2.113809823989868
Validation loss: 2.127439101537069

Epoch: 295| Step: 0
Training loss: 1.6735560894012451
Validation loss: 2.114327249988433

Epoch: 6| Step: 1
Training loss: 1.8176138401031494
Validation loss: 2.1042216465037358

Epoch: 6| Step: 2
Training loss: 1.9442553520202637
Validation loss: 2.083129104747567

Epoch: 6| Step: 3
Training loss: 2.008885145187378
Validation loss: 2.111140081959386

Epoch: 6| Step: 4
Training loss: 1.8136992454528809
Validation loss: 2.1268266298437632

Epoch: 6| Step: 5
Training loss: 2.4713363647460938
Validation loss: 2.118090578304824

Epoch: 6| Step: 6
Training loss: 2.070448637008667
Validation loss: 2.1191809638853996

Epoch: 6| Step: 7
Training loss: 2.726346015930176
Validation loss: 2.120359992468229

Epoch: 6| Step: 8
Training loss: 2.8560142517089844
Validation loss: 2.1301237485742055

Epoch: 6| Step: 9
Training loss: 2.0593442916870117
Validation loss: 2.120832239427874

Epoch: 6| Step: 10
Training loss: 1.764214277267456
Validation loss: 2.0976627437017297

Epoch: 6| Step: 11
Training loss: 1.9808698892593384
Validation loss: 2.1236610771507345

Epoch: 6| Step: 12
Training loss: 1.8559603691101074
Validation loss: 2.1070747708761566

Epoch: 6| Step: 13
Training loss: 2.335550308227539
Validation loss: 2.1129068302851852

Epoch: 296| Step: 0
Training loss: 2.5374035835266113
Validation loss: 2.0984555303409533

Epoch: 6| Step: 1
Training loss: 2.2139651775360107
Validation loss: 2.1029814097189132

Epoch: 6| Step: 2
Training loss: 2.0773801803588867
Validation loss: 2.1058499736170613

Epoch: 6| Step: 3
Training loss: 2.213602066040039
Validation loss: 2.133339515296362

Epoch: 6| Step: 4
Training loss: 2.073668956756592
Validation loss: 2.1135165178647606

Epoch: 6| Step: 5
Training loss: 1.7785027027130127
Validation loss: 2.104873234225858

Epoch: 6| Step: 6
Training loss: 2.0273215770721436
Validation loss: 2.109177181797643

Epoch: 6| Step: 7
Training loss: 1.6578352451324463
Validation loss: 2.0935275157292685

Epoch: 6| Step: 8
Training loss: 2.1484718322753906
Validation loss: 2.101151248460175

Epoch: 6| Step: 9
Training loss: 2.257180690765381
Validation loss: 2.086762830775271

Epoch: 6| Step: 10
Training loss: 1.9676369428634644
Validation loss: 2.101417996550119

Epoch: 6| Step: 11
Training loss: 2.322023630142212
Validation loss: 2.085104224502399

Epoch: 6| Step: 12
Training loss: 2.2587549686431885
Validation loss: 2.104204454729634

Epoch: 6| Step: 13
Training loss: 1.4649487733840942
Validation loss: 2.064064223279235

Epoch: 297| Step: 0
Training loss: 1.8958905935287476
Validation loss: 2.0933651744678454

Epoch: 6| Step: 1
Training loss: 1.641507863998413
Validation loss: 2.1020326742561917

Epoch: 6| Step: 2
Training loss: 2.218655586242676
Validation loss: 2.104270549230678

Epoch: 6| Step: 3
Training loss: 1.9930018186569214
Validation loss: 2.103412130827545

Epoch: 6| Step: 4
Training loss: 2.4268364906311035
Validation loss: 2.0941943814677577

Epoch: 6| Step: 5
Training loss: 2.701371669769287
Validation loss: 2.099857678977392

Epoch: 6| Step: 6
Training loss: 2.219435453414917
Validation loss: 2.0948774019877114

Epoch: 6| Step: 7
Training loss: 1.7559788227081299
Validation loss: 2.0689702700543147

Epoch: 6| Step: 8
Training loss: 2.028172731399536
Validation loss: 2.089166038779802

Epoch: 6| Step: 9
Training loss: 2.2180044651031494
Validation loss: 2.097651791828935

Epoch: 6| Step: 10
Training loss: 1.8305214643478394
Validation loss: 2.076187536280642

Epoch: 6| Step: 11
Training loss: 2.279015302658081
Validation loss: 2.0912724951262116

Epoch: 6| Step: 12
Training loss: 1.7033116817474365
Validation loss: 2.0768094421714864

Epoch: 6| Step: 13
Training loss: 2.057440996170044
Validation loss: 2.104331890741984

Epoch: 298| Step: 0
Training loss: 2.7923367023468018
Validation loss: 2.0800062482075026

Epoch: 6| Step: 1
Training loss: 1.5312057733535767
Validation loss: 2.0958648317603656

Epoch: 6| Step: 2
Training loss: 2.0585758686065674
Validation loss: 2.1219141047487975

Epoch: 6| Step: 3
Training loss: 1.7768443822860718
Validation loss: 2.091844097260506

Epoch: 6| Step: 4
Training loss: 2.099618911743164
Validation loss: 2.1041903316333728

Epoch: 6| Step: 5
Training loss: 1.7013638019561768
Validation loss: 2.1023609253668014

Epoch: 6| Step: 6
Training loss: 2.202996253967285
Validation loss: 2.1277039589420443

Epoch: 6| Step: 7
Training loss: 2.6713578701019287
Validation loss: 2.1132629225330968

Epoch: 6| Step: 8
Training loss: 1.8092156648635864
Validation loss: 2.1321373421658754

Epoch: 6| Step: 9
Training loss: 1.9742686748504639
Validation loss: 2.127122576518725

Epoch: 6| Step: 10
Training loss: 2.0329856872558594
Validation loss: 2.1247055569002704

Epoch: 6| Step: 11
Training loss: 1.9878767728805542
Validation loss: 2.0945626407541256

Epoch: 6| Step: 12
Training loss: 2.275430679321289
Validation loss: 2.1216196090944353

Epoch: 6| Step: 13
Training loss: 2.627974271774292
Validation loss: 2.1165562701481644

Epoch: 299| Step: 0
Training loss: 1.7290935516357422
Validation loss: 2.1330098157287924

Epoch: 6| Step: 1
Training loss: 1.9226313829421997
Validation loss: 2.1237948568918372

Epoch: 6| Step: 2
Training loss: 2.154897451400757
Validation loss: 2.103002499508601

Epoch: 6| Step: 3
Training loss: 2.5724825859069824
Validation loss: 2.1369013094132945

Epoch: 6| Step: 4
Training loss: 2.0042684078216553
Validation loss: 2.131053137522872

Epoch: 6| Step: 5
Training loss: 1.8853318691253662
Validation loss: 2.107569512500558

Epoch: 6| Step: 6
Training loss: 1.8018431663513184
Validation loss: 2.090202990398612

Epoch: 6| Step: 7
Training loss: 2.396228790283203
Validation loss: 2.101023474047261

Epoch: 6| Step: 8
Training loss: 2.605809450149536
Validation loss: 2.114107507531361

Epoch: 6| Step: 9
Training loss: 2.1459338665008545
Validation loss: 2.093242411972374

Epoch: 6| Step: 10
Training loss: 1.916618824005127
Validation loss: 2.09591793885795

Epoch: 6| Step: 11
Training loss: 2.3552181720733643
Validation loss: 2.1239674501521613

Epoch: 6| Step: 12
Training loss: 2.189401149749756
Validation loss: 2.1314632302971295

Epoch: 6| Step: 13
Training loss: 1.2924288511276245
Validation loss: 2.0775619296617407

Epoch: 300| Step: 0
Training loss: 2.195828437805176
Validation loss: 2.1123925152645318

Epoch: 6| Step: 1
Training loss: 2.094139575958252
Validation loss: 2.0755355537578626

Epoch: 6| Step: 2
Training loss: 1.8633977174758911
Validation loss: 2.1067247493292696

Epoch: 6| Step: 3
Training loss: 2.0097272396087646
Validation loss: 2.100185853178783

Epoch: 6| Step: 4
Training loss: 1.184472918510437
Validation loss: 2.1211507935677805

Epoch: 6| Step: 5
Training loss: 1.9421977996826172
Validation loss: 2.123481471051452

Epoch: 6| Step: 6
Training loss: 1.9044854640960693
Validation loss: 2.09210410810286

Epoch: 6| Step: 7
Training loss: 1.7704832553863525
Validation loss: 2.104738530292306

Epoch: 6| Step: 8
Training loss: 2.190126419067383
Validation loss: 2.1012816249683337

Epoch: 6| Step: 9
Training loss: 2.5904433727264404
Validation loss: 2.1250507626482236

Epoch: 6| Step: 10
Training loss: 1.9371899366378784
Validation loss: 2.1086972336615286

Epoch: 6| Step: 11
Training loss: 2.3996880054473877
Validation loss: 2.137265487383771

Epoch: 6| Step: 12
Training loss: 2.8326826095581055
Validation loss: 2.107304508968066

Epoch: 6| Step: 13
Training loss: 2.423621416091919
Validation loss: 2.086219310760498

Epoch: 301| Step: 0
Training loss: 2.427838087081909
Validation loss: 2.1134138389300277

Epoch: 6| Step: 1
Training loss: 2.149343729019165
Validation loss: 2.1535865824709655

Epoch: 6| Step: 2
Training loss: 1.76658296585083
Validation loss: 2.1245058762129916

Epoch: 6| Step: 3
Training loss: 1.914640188217163
Validation loss: 2.1401870071247058

Epoch: 6| Step: 4
Training loss: 1.838712453842163
Validation loss: 2.1310825194081953

Epoch: 6| Step: 5
Training loss: 2.0365498065948486
Validation loss: 2.1039415405642603

Epoch: 6| Step: 6
Training loss: 2.242766857147217
Validation loss: 2.10845713333417

Epoch: 6| Step: 7
Training loss: 2.2641375064849854
Validation loss: 2.0856530691987727

Epoch: 6| Step: 8
Training loss: 1.6069788932800293
Validation loss: 2.0921064205067132

Epoch: 6| Step: 9
Training loss: 2.2932705879211426
Validation loss: 2.099562647522137

Epoch: 6| Step: 10
Training loss: 1.9267568588256836
Validation loss: 2.1117159602462605

Epoch: 6| Step: 11
Training loss: 2.3076915740966797
Validation loss: 2.1100468122830955

Epoch: 6| Step: 12
Training loss: 2.046891212463379
Validation loss: 2.1281074618780487

Epoch: 6| Step: 13
Training loss: 2.6344854831695557
Validation loss: 2.1138450432849187

Epoch: 302| Step: 0
Training loss: 1.0223846435546875
Validation loss: 2.1103569166634673

Epoch: 6| Step: 1
Training loss: 2.537010908126831
Validation loss: 2.112270942298315

Epoch: 6| Step: 2
Training loss: 2.0854086875915527
Validation loss: 2.1035037297074513

Epoch: 6| Step: 3
Training loss: 2.876805543899536
Validation loss: 2.141189057339904

Epoch: 6| Step: 4
Training loss: 1.9456473588943481
Validation loss: 2.1144357650510726

Epoch: 6| Step: 5
Training loss: 2.3051676750183105
Validation loss: 2.105964381207702

Epoch: 6| Step: 6
Training loss: 1.8736690282821655
Validation loss: 2.1215433459128104

Epoch: 6| Step: 7
Training loss: 2.0276265144348145
Validation loss: 2.130710248024233

Epoch: 6| Step: 8
Training loss: 2.6422581672668457
Validation loss: 2.106102708847292

Epoch: 6| Step: 9
Training loss: 1.9417001008987427
Validation loss: 2.1180102902074016

Epoch: 6| Step: 10
Training loss: 1.430379033088684
Validation loss: 2.1280129199386923

Epoch: 6| Step: 11
Training loss: 2.5304360389709473
Validation loss: 2.1141566281677573

Epoch: 6| Step: 12
Training loss: 1.6953678131103516
Validation loss: 2.129516960472189

Epoch: 6| Step: 13
Training loss: 2.744641065597534
Validation loss: 2.0954705848488757

Epoch: 303| Step: 0
Training loss: 2.527923583984375
Validation loss: 2.1358788218549503

Epoch: 6| Step: 1
Training loss: 2.67236065864563
Validation loss: 2.1168666975472563

Epoch: 6| Step: 2
Training loss: 1.7031798362731934
Validation loss: 2.127934435362457

Epoch: 6| Step: 3
Training loss: 2.4772706031799316
Validation loss: 2.110089011089776

Epoch: 6| Step: 4
Training loss: 2.086970806121826
Validation loss: 2.104882796605428

Epoch: 6| Step: 5
Training loss: 1.7426782846450806
Validation loss: 2.1167513196186354

Epoch: 6| Step: 6
Training loss: 2.066206693649292
Validation loss: 2.11834208426937

Epoch: 6| Step: 7
Training loss: 1.4013631343841553
Validation loss: 2.0795899398865236

Epoch: 6| Step: 8
Training loss: 1.7787173986434937
Validation loss: 2.1165161260994534

Epoch: 6| Step: 9
Training loss: 2.1042442321777344
Validation loss: 2.1245420517459994

Epoch: 6| Step: 10
Training loss: 2.419168472290039
Validation loss: 2.104166248793243

Epoch: 6| Step: 11
Training loss: 2.1168124675750732
Validation loss: 2.12561200511071

Epoch: 6| Step: 12
Training loss: 2.3481850624084473
Validation loss: 2.0842350529086207

Epoch: 6| Step: 13
Training loss: 1.5697485208511353
Validation loss: 2.0894122944083264

Epoch: 304| Step: 0
Training loss: 2.6143860816955566
Validation loss: 2.1075525283813477

Epoch: 6| Step: 1
Training loss: 1.941402554512024
Validation loss: 2.1264626031280844

Epoch: 6| Step: 2
Training loss: 2.2020204067230225
Validation loss: 2.1171236717572777

Epoch: 6| Step: 3
Training loss: 1.7676572799682617
Validation loss: 2.105001926422119

Epoch: 6| Step: 4
Training loss: 2.4666359424591064
Validation loss: 2.1285402236446256

Epoch: 6| Step: 5
Training loss: 2.5861806869506836
Validation loss: 2.1427027230621665

Epoch: 6| Step: 6
Training loss: 2.1650304794311523
Validation loss: 2.1075347085152902

Epoch: 6| Step: 7
Training loss: 2.2209720611572266
Validation loss: 2.10755185158022

Epoch: 6| Step: 8
Training loss: 1.7386016845703125
Validation loss: 2.1170803372577955

Epoch: 6| Step: 9
Training loss: 1.72615385055542
Validation loss: 2.1157057144308604

Epoch: 6| Step: 10
Training loss: 1.72966730594635
Validation loss: 2.138887507941133

Epoch: 6| Step: 11
Training loss: 2.1024346351623535
Validation loss: 2.1508205847073625

Epoch: 6| Step: 12
Training loss: 2.0383620262145996
Validation loss: 2.10185944649481

Epoch: 6| Step: 13
Training loss: 1.4588263034820557
Validation loss: 2.129524473221071

Epoch: 305| Step: 0
Training loss: 2.0548815727233887
Validation loss: 2.102443587395453

Epoch: 6| Step: 1
Training loss: 2.1077380180358887
Validation loss: 2.0673362849861063

Epoch: 6| Step: 2
Training loss: 2.0845022201538086
Validation loss: 2.100198076617333

Epoch: 6| Step: 3
Training loss: 2.2161049842834473
Validation loss: 2.1092201637965378

Epoch: 6| Step: 4
Training loss: 2.0899248123168945
Validation loss: 2.089597653317195

Epoch: 6| Step: 5
Training loss: 1.769251823425293
Validation loss: 2.098104912747619

Epoch: 6| Step: 6
Training loss: 2.8191587924957275
Validation loss: 2.113457642575746

Epoch: 6| Step: 7
Training loss: 1.3252596855163574
Validation loss: 2.0749821560357207

Epoch: 6| Step: 8
Training loss: 1.396915078163147
Validation loss: 2.0973693478491997

Epoch: 6| Step: 9
Training loss: 2.055640459060669
Validation loss: 2.087920217103856

Epoch: 6| Step: 10
Training loss: 1.6492085456848145
Validation loss: 2.1222967742591776

Epoch: 6| Step: 11
Training loss: 2.8388426303863525
Validation loss: 2.1259579709781113

Epoch: 6| Step: 12
Training loss: 2.7976431846618652
Validation loss: 2.0845955520547848

Epoch: 6| Step: 13
Training loss: 2.035684823989868
Validation loss: 2.0866820299497215

Epoch: 306| Step: 0
Training loss: 1.2818431854248047
Validation loss: 2.069356893980375

Epoch: 6| Step: 1
Training loss: 2.3401131629943848
Validation loss: 2.0961358316483034

Epoch: 6| Step: 2
Training loss: 2.921417236328125
Validation loss: 2.0881773271868305

Epoch: 6| Step: 3
Training loss: 1.7306427955627441
Validation loss: 2.0862259890443537

Epoch: 6| Step: 4
Training loss: 2.3213093280792236
Validation loss: 2.0915956343373945

Epoch: 6| Step: 5
Training loss: 1.9597405195236206
Validation loss: 2.118654183162156

Epoch: 6| Step: 6
Training loss: 1.5462499856948853
Validation loss: 2.121207762789983

Epoch: 6| Step: 7
Training loss: 2.2140867710113525
Validation loss: 2.1300847581637803

Epoch: 6| Step: 8
Training loss: 1.9902337789535522
Validation loss: 2.131958261612923

Epoch: 6| Step: 9
Training loss: 2.1810522079467773
Validation loss: 2.1379932254873295

Epoch: 6| Step: 10
Training loss: 1.599931240081787
Validation loss: 2.121404519645117

Epoch: 6| Step: 11
Training loss: 1.7794902324676514
Validation loss: 2.13345012100794

Epoch: 6| Step: 12
Training loss: 2.9899086952209473
Validation loss: 2.095158282146659

Epoch: 6| Step: 13
Training loss: 2.6079213619232178
Validation loss: 2.1377374049155944

Epoch: 307| Step: 0
Training loss: 2.0579569339752197
Validation loss: 2.136765485168785

Epoch: 6| Step: 1
Training loss: 1.725862979888916
Validation loss: 2.116778576245872

Epoch: 6| Step: 2
Training loss: 2.7360434532165527
Validation loss: 2.1166720313410603

Epoch: 6| Step: 3
Training loss: 2.1014750003814697
Validation loss: 2.124907192363534

Epoch: 6| Step: 4
Training loss: 2.267134666442871
Validation loss: 2.103599812394829

Epoch: 6| Step: 5
Training loss: 2.061859607696533
Validation loss: 2.1317570568412862

Epoch: 6| Step: 6
Training loss: 1.913123369216919
Validation loss: 2.126285437614687

Epoch: 6| Step: 7
Training loss: 1.7666890621185303
Validation loss: 2.1233677210346347

Epoch: 6| Step: 8
Training loss: 2.000494956970215
Validation loss: 2.1067001332518873

Epoch: 6| Step: 9
Training loss: 1.9393301010131836
Validation loss: 2.0992743251144246

Epoch: 6| Step: 10
Training loss: 2.244442939758301
Validation loss: 2.107852707626999

Epoch: 6| Step: 11
Training loss: 2.1977922916412354
Validation loss: 2.086620121873835

Epoch: 6| Step: 12
Training loss: 1.7543108463287354
Validation loss: 2.081236062511321

Epoch: 6| Step: 13
Training loss: 2.1741747856140137
Validation loss: 2.1058044920685473

Epoch: 308| Step: 0
Training loss: 1.6035168170928955
Validation loss: 2.0842318893760763

Epoch: 6| Step: 1
Training loss: 2.86367130279541
Validation loss: 2.1184847303616103

Epoch: 6| Step: 2
Training loss: 2.7417004108428955
Validation loss: 2.1184262203913864

Epoch: 6| Step: 3
Training loss: 1.662916898727417
Validation loss: 2.1026179969951673

Epoch: 6| Step: 4
Training loss: 1.7014433145523071
Validation loss: 2.11905014130377

Epoch: 6| Step: 5
Training loss: 2.0216569900512695
Validation loss: 2.125722913331883

Epoch: 6| Step: 6
Training loss: 1.58369779586792
Validation loss: 2.1068140998963387

Epoch: 6| Step: 7
Training loss: 2.0012149810791016
Validation loss: 2.0985139826292634

Epoch: 6| Step: 8
Training loss: 2.1461071968078613
Validation loss: 2.0638504899958128

Epoch: 6| Step: 9
Training loss: 3.042605400085449
Validation loss: 2.1188110395144393

Epoch: 6| Step: 10
Training loss: 2.481252670288086
Validation loss: 2.0909064251889466

Epoch: 6| Step: 11
Training loss: 1.6323626041412354
Validation loss: 2.092549418890348

Epoch: 6| Step: 12
Training loss: 1.8436650037765503
Validation loss: 2.1127241734535462

Epoch: 6| Step: 13
Training loss: 1.977866291999817
Validation loss: 2.0868912563529065

Epoch: 309| Step: 0
Training loss: 2.1645917892456055
Validation loss: 2.0931485827251146

Epoch: 6| Step: 1
Training loss: 2.332156181335449
Validation loss: 2.091805399105113

Epoch: 6| Step: 2
Training loss: 2.1048898696899414
Validation loss: 2.1036720686061408

Epoch: 6| Step: 3
Training loss: 2.265589952468872
Validation loss: 2.0915662011792584

Epoch: 6| Step: 4
Training loss: 1.7194910049438477
Validation loss: 2.0880545339276715

Epoch: 6| Step: 5
Training loss: 2.4845492839813232
Validation loss: 2.0979976384870467

Epoch: 6| Step: 6
Training loss: 2.2120766639709473
Validation loss: 2.117546222543204

Epoch: 6| Step: 7
Training loss: 1.6955084800720215
Validation loss: 2.1217321606092554

Epoch: 6| Step: 8
Training loss: 1.7308646440505981
Validation loss: 2.0853034373252624

Epoch: 6| Step: 9
Training loss: 2.08601713180542
Validation loss: 2.1031163046436925

Epoch: 6| Step: 10
Training loss: 2.003507137298584
Validation loss: 2.111872793525778

Epoch: 6| Step: 11
Training loss: 2.537909507751465
Validation loss: 2.118697286933981

Epoch: 6| Step: 12
Training loss: 1.539355993270874
Validation loss: 2.10697425821776

Epoch: 6| Step: 13
Training loss: 2.0327842235565186
Validation loss: 2.1204390064362557

Epoch: 310| Step: 0
Training loss: 1.654554009437561
Validation loss: 2.101939344918856

Epoch: 6| Step: 1
Training loss: 1.7494738101959229
Validation loss: 2.117612208089521

Epoch: 6| Step: 2
Training loss: 1.6533973217010498
Validation loss: 2.1082663125889276

Epoch: 6| Step: 3
Training loss: 2.257737636566162
Validation loss: 2.104429773105088

Epoch: 6| Step: 4
Training loss: 2.3149971961975098
Validation loss: 2.094039768301031

Epoch: 6| Step: 5
Training loss: 1.7610803842544556
Validation loss: 2.1189107510351364

Epoch: 6| Step: 6
Training loss: 2.506138324737549
Validation loss: 2.1002952437247

Epoch: 6| Step: 7
Training loss: 2.1002609729766846
Validation loss: 2.097234546497304

Epoch: 6| Step: 8
Training loss: 1.776822805404663
Validation loss: 2.0852335934997885

Epoch: 6| Step: 9
Training loss: 2.365286350250244
Validation loss: 2.1128519401755383

Epoch: 6| Step: 10
Training loss: 2.1824021339416504
Validation loss: 2.11243337456898

Epoch: 6| Step: 11
Training loss: 2.286721706390381
Validation loss: 2.127120970397867

Epoch: 6| Step: 12
Training loss: 1.6868598461151123
Validation loss: 2.111903193176434

Epoch: 6| Step: 13
Training loss: 2.6978938579559326
Validation loss: 2.091038365517893

Epoch: 311| Step: 0
Training loss: 1.9770967960357666
Validation loss: 2.1374617827835904

Epoch: 6| Step: 1
Training loss: 2.5877859592437744
Validation loss: 2.136157779283421

Epoch: 6| Step: 2
Training loss: 1.9676299095153809
Validation loss: 2.095980535271347

Epoch: 6| Step: 3
Training loss: 1.7167052030563354
Validation loss: 2.0992200143875612

Epoch: 6| Step: 4
Training loss: 1.3835420608520508
Validation loss: 2.1029185966778825

Epoch: 6| Step: 5
Training loss: 1.59140145778656
Validation loss: 2.1083896801035893

Epoch: 6| Step: 6
Training loss: 3.1995654106140137
Validation loss: 2.086760967008529

Epoch: 6| Step: 7
Training loss: 2.102402925491333
Validation loss: 2.118400642948766

Epoch: 6| Step: 8
Training loss: 1.9427449703216553
Validation loss: 2.1143141895212154

Epoch: 6| Step: 9
Training loss: 2.4825797080993652
Validation loss: 2.105550104571927

Epoch: 6| Step: 10
Training loss: 1.934562087059021
Validation loss: 2.0898933692645003

Epoch: 6| Step: 11
Training loss: 2.0556271076202393
Validation loss: 2.1266618441509944

Epoch: 6| Step: 12
Training loss: 1.8804963827133179
Validation loss: 2.1084987835217546

Epoch: 6| Step: 13
Training loss: 1.8737406730651855
Validation loss: 2.100544430876291

Epoch: 312| Step: 0
Training loss: 1.9786067008972168
Validation loss: 2.113417856154903

Epoch: 6| Step: 1
Training loss: 1.7182598114013672
Validation loss: 2.0857535459661998

Epoch: 6| Step: 2
Training loss: 2.1466283798217773
Validation loss: 2.0902700334466915

Epoch: 6| Step: 3
Training loss: 2.0483436584472656
Validation loss: 2.1247926065998692

Epoch: 6| Step: 4
Training loss: 2.108889102935791
Validation loss: 2.0946194484669673

Epoch: 6| Step: 5
Training loss: 1.7066051959991455
Validation loss: 2.113718496855869

Epoch: 6| Step: 6
Training loss: 2.083158254623413
Validation loss: 2.100624197272844

Epoch: 6| Step: 7
Training loss: 2.307278871536255
Validation loss: 2.106978393370105

Epoch: 6| Step: 8
Training loss: 2.4266867637634277
Validation loss: 2.1416306316211657

Epoch: 6| Step: 9
Training loss: 2.729097843170166
Validation loss: 2.1310194359030774

Epoch: 6| Step: 10
Training loss: 1.9969720840454102
Validation loss: 2.141501686906302

Epoch: 6| Step: 11
Training loss: 1.92506742477417
Validation loss: 2.0883217780820784

Epoch: 6| Step: 12
Training loss: 2.0325706005096436
Validation loss: 2.0923709459202264

Epoch: 6| Step: 13
Training loss: 1.6398671865463257
Validation loss: 2.1078044906739266

Epoch: 313| Step: 0
Training loss: 2.0508060455322266
Validation loss: 2.106358525573566

Epoch: 6| Step: 1
Training loss: 2.122255802154541
Validation loss: 2.127827330302167

Epoch: 6| Step: 2
Training loss: 2.055278778076172
Validation loss: 2.1192067515465522

Epoch: 6| Step: 3
Training loss: 1.8262171745300293
Validation loss: 2.097937135286229

Epoch: 6| Step: 4
Training loss: 2.290170669555664
Validation loss: 2.0836751153392177

Epoch: 6| Step: 5
Training loss: 1.679105520248413
Validation loss: 2.1256404076853106

Epoch: 6| Step: 6
Training loss: 2.3350343704223633
Validation loss: 2.1340366422489123

Epoch: 6| Step: 7
Training loss: 2.0113253593444824
Validation loss: 2.1241142621604343

Epoch: 6| Step: 8
Training loss: 2.2032201290130615
Validation loss: 2.071129261806447

Epoch: 6| Step: 9
Training loss: 2.1971731185913086
Validation loss: 2.111178195604714

Epoch: 6| Step: 10
Training loss: 2.383535861968994
Validation loss: 2.08573450708902

Epoch: 6| Step: 11
Training loss: 2.035999059677124
Validation loss: 2.108080945989137

Epoch: 6| Step: 12
Training loss: 1.6400392055511475
Validation loss: 2.0970499105350946

Epoch: 6| Step: 13
Training loss: 1.7562366724014282
Validation loss: 2.0944189692056305

Epoch: 314| Step: 0
Training loss: 2.518383026123047
Validation loss: 2.126931821146319

Epoch: 6| Step: 1
Training loss: 1.8945415019989014
Validation loss: 2.1035833833038167

Epoch: 6| Step: 2
Training loss: 2.310385227203369
Validation loss: 2.078080379834739

Epoch: 6| Step: 3
Training loss: 1.8582289218902588
Validation loss: 2.089646549635036

Epoch: 6| Step: 4
Training loss: 2.373663902282715
Validation loss: 2.1129256115164807

Epoch: 6| Step: 5
Training loss: 1.942122220993042
Validation loss: 2.105479663418185

Epoch: 6| Step: 6
Training loss: 1.7135705947875977
Validation loss: 2.1020761087376583

Epoch: 6| Step: 7
Training loss: 2.2433128356933594
Validation loss: 2.055713584346156

Epoch: 6| Step: 8
Training loss: 2.559427261352539
Validation loss: 2.1056975497994372

Epoch: 6| Step: 9
Training loss: 1.7759931087493896
Validation loss: 2.10139859619961

Epoch: 6| Step: 10
Training loss: 1.9240435361862183
Validation loss: 2.0737282473553895

Epoch: 6| Step: 11
Training loss: 2.4878945350646973
Validation loss: 2.1116080066209197

Epoch: 6| Step: 12
Training loss: 1.5837680101394653
Validation loss: 2.08052509702662

Epoch: 6| Step: 13
Training loss: 1.657230257987976
Validation loss: 2.0863760645671556

Epoch: 315| Step: 0
Training loss: 2.4223268032073975
Validation loss: 2.089000827522688

Epoch: 6| Step: 1
Training loss: 2.301560401916504
Validation loss: 2.07763885682629

Epoch: 6| Step: 2
Training loss: 1.701393961906433
Validation loss: 2.1313017209370932

Epoch: 6| Step: 3
Training loss: 1.9037361145019531
Validation loss: 2.139479770455309

Epoch: 6| Step: 4
Training loss: 2.7740941047668457
Validation loss: 2.1180793854498092

Epoch: 6| Step: 5
Training loss: 2.1073386669158936
Validation loss: 2.1308979462551814

Epoch: 6| Step: 6
Training loss: 2.461273193359375
Validation loss: 2.142097529544625

Epoch: 6| Step: 7
Training loss: 1.8668653964996338
Validation loss: 2.1532383721361876

Epoch: 6| Step: 8
Training loss: 1.8443069458007812
Validation loss: 2.1558368744388705

Epoch: 6| Step: 9
Training loss: 1.4894317388534546
Validation loss: 2.140506306002217

Epoch: 6| Step: 10
Training loss: 1.6473021507263184
Validation loss: 2.1356268236714024

Epoch: 6| Step: 11
Training loss: 2.489621162414551
Validation loss: 2.108526378549555

Epoch: 6| Step: 12
Training loss: 1.893641471862793
Validation loss: 2.1421147725915395

Epoch: 6| Step: 13
Training loss: 1.8794631958007812
Validation loss: 2.1107305749770133

Epoch: 316| Step: 0
Training loss: 1.9640730619430542
Validation loss: 2.1231470133668635

Epoch: 6| Step: 1
Training loss: 2.3163790702819824
Validation loss: 2.128859886559107

Epoch: 6| Step: 2
Training loss: 3.0688748359680176
Validation loss: 2.139870607724754

Epoch: 6| Step: 3
Training loss: 2.460291862487793
Validation loss: 2.100952943166097

Epoch: 6| Step: 4
Training loss: 1.9249742031097412
Validation loss: 2.093448151824295

Epoch: 6| Step: 5
Training loss: 2.811734199523926
Validation loss: 2.10111423718032

Epoch: 6| Step: 6
Training loss: 1.8298940658569336
Validation loss: 2.1066997410148702

Epoch: 6| Step: 7
Training loss: 1.8232693672180176
Validation loss: 2.1035485318911973

Epoch: 6| Step: 8
Training loss: 1.811038851737976
Validation loss: 2.083829119641294

Epoch: 6| Step: 9
Training loss: 1.6400625705718994
Validation loss: 2.076472067063855

Epoch: 6| Step: 10
Training loss: 2.0514872074127197
Validation loss: 2.081843637651013

Epoch: 6| Step: 11
Training loss: 2.0247435569763184
Validation loss: 2.0872722389877483

Epoch: 6| Step: 12
Training loss: 1.3130500316619873
Validation loss: 2.0767277620171987

Epoch: 6| Step: 13
Training loss: 1.75288724899292
Validation loss: 2.098755831359535

Epoch: 317| Step: 0
Training loss: 1.7771867513656616
Validation loss: 2.1179386108152327

Epoch: 6| Step: 1
Training loss: 1.7235019207000732
Validation loss: 2.0993145896542456

Epoch: 6| Step: 2
Training loss: 1.7508888244628906
Validation loss: 2.137393528415311

Epoch: 6| Step: 3
Training loss: 2.237468719482422
Validation loss: 2.1278339611586703

Epoch: 6| Step: 4
Training loss: 1.652435302734375
Validation loss: 2.1107543245438607

Epoch: 6| Step: 5
Training loss: 2.458247661590576
Validation loss: 2.0901504742201937

Epoch: 6| Step: 6
Training loss: 2.0050950050354004
Validation loss: 2.0791024161923315

Epoch: 6| Step: 7
Training loss: 2.346095561981201
Validation loss: 2.0936994962794806

Epoch: 6| Step: 8
Training loss: 1.9264947175979614
Validation loss: 2.1097335610338437

Epoch: 6| Step: 9
Training loss: 2.73614501953125
Validation loss: 2.1033975616578133

Epoch: 6| Step: 10
Training loss: 1.618747591972351
Validation loss: 2.10677186904415

Epoch: 6| Step: 11
Training loss: 2.6224679946899414
Validation loss: 2.1041091001161965

Epoch: 6| Step: 12
Training loss: 2.2900710105895996
Validation loss: 2.123737391605172

Epoch: 6| Step: 13
Training loss: 1.748734951019287
Validation loss: 2.1132252498339583

Epoch: 318| Step: 0
Training loss: 1.7531981468200684
Validation loss: 2.097095713820509

Epoch: 6| Step: 1
Training loss: 1.9706506729125977
Validation loss: 2.1058418007307154

Epoch: 6| Step: 2
Training loss: 1.8128890991210938
Validation loss: 2.09909555732563

Epoch: 6| Step: 3
Training loss: 1.6229881048202515
Validation loss: 2.1203096784571165

Epoch: 6| Step: 4
Training loss: 2.9013960361480713
Validation loss: 2.1100193095463577

Epoch: 6| Step: 5
Training loss: 2.090437650680542
Validation loss: 2.1218969796293523

Epoch: 6| Step: 6
Training loss: 2.211521625518799
Validation loss: 2.15576200075047

Epoch: 6| Step: 7
Training loss: 1.8071346282958984
Validation loss: 2.1278473830992177

Epoch: 6| Step: 8
Training loss: 2.4950175285339355
Validation loss: 2.142386946626889

Epoch: 6| Step: 9
Training loss: 1.7276510000228882
Validation loss: 2.159266077062135

Epoch: 6| Step: 10
Training loss: 2.2776613235473633
Validation loss: 2.1306642229839037

Epoch: 6| Step: 11
Training loss: 1.768062949180603
Validation loss: 2.138829310735067

Epoch: 6| Step: 12
Training loss: 2.3618664741516113
Validation loss: 2.155990785168063

Epoch: 6| Step: 13
Training loss: 1.6674740314483643
Validation loss: 2.158789138640127

Epoch: 319| Step: 0
Training loss: 2.760896682739258
Validation loss: 2.1418796713634203

Epoch: 6| Step: 1
Training loss: 2.387427806854248
Validation loss: 2.1392396624370287

Epoch: 6| Step: 2
Training loss: 2.0698530673980713
Validation loss: 2.1341652434359313

Epoch: 6| Step: 3
Training loss: 2.682549476623535
Validation loss: 2.1548328168930544

Epoch: 6| Step: 4
Training loss: 1.4287008047103882
Validation loss: 2.133084220270957

Epoch: 6| Step: 5
Training loss: 2.239142894744873
Validation loss: 2.1444968664517967

Epoch: 6| Step: 6
Training loss: 1.7930470705032349
Validation loss: 2.1515013210235105

Epoch: 6| Step: 7
Training loss: 1.6986087560653687
Validation loss: 2.1421400449609243

Epoch: 6| Step: 8
Training loss: 1.8643136024475098
Validation loss: 2.15022954633159

Epoch: 6| Step: 9
Training loss: 1.8150279521942139
Validation loss: 2.1592523385119695

Epoch: 6| Step: 10
Training loss: 2.0313308238983154
Validation loss: 2.1192583678871073

Epoch: 6| Step: 11
Training loss: 1.3654589653015137
Validation loss: 2.1361542337684223

Epoch: 6| Step: 12
Training loss: 2.2564830780029297
Validation loss: 2.1199378531466246

Epoch: 6| Step: 13
Training loss: 2.652723550796509
Validation loss: 2.1250621913581766

Epoch: 320| Step: 0
Training loss: 1.9588305950164795
Validation loss: 2.1028028585577525

Epoch: 6| Step: 1
Training loss: 1.6281170845031738
Validation loss: 2.0803803090126283

Epoch: 6| Step: 2
Training loss: 2.4040069580078125
Validation loss: 2.1327165993311072

Epoch: 6| Step: 3
Training loss: 2.153186798095703
Validation loss: 2.0960636420916487

Epoch: 6| Step: 4
Training loss: 1.720358967781067
Validation loss: 2.0866007189596854

Epoch: 6| Step: 5
Training loss: 1.8540635108947754
Validation loss: 2.0762127009771203

Epoch: 6| Step: 6
Training loss: 1.6655707359313965
Validation loss: 2.0977398580120457

Epoch: 6| Step: 7
Training loss: 2.2499043941497803
Validation loss: 2.0743553766640286

Epoch: 6| Step: 8
Training loss: 2.5721096992492676
Validation loss: 2.107991582603865

Epoch: 6| Step: 9
Training loss: 2.325636625289917
Validation loss: 2.0939586060021513

Epoch: 6| Step: 10
Training loss: 2.2033474445343018
Validation loss: 2.0693679650624595

Epoch: 6| Step: 11
Training loss: 1.4763623476028442
Validation loss: 2.0768214271914576

Epoch: 6| Step: 12
Training loss: 2.3090810775756836
Validation loss: 2.096925042008841

Epoch: 6| Step: 13
Training loss: 1.8963826894760132
Validation loss: 2.0932394330219557

Epoch: 321| Step: 0
Training loss: 2.390575885772705
Validation loss: 2.0961847920571604

Epoch: 6| Step: 1
Training loss: 2.1568593978881836
Validation loss: 2.0691451539275465

Epoch: 6| Step: 2
Training loss: 2.2962353229522705
Validation loss: 2.106183241772395

Epoch: 6| Step: 3
Training loss: 1.7123792171478271
Validation loss: 2.0850635818255845

Epoch: 6| Step: 4
Training loss: 2.0813350677490234
Validation loss: 2.102662285168966

Epoch: 6| Step: 5
Training loss: 1.9359643459320068
Validation loss: 2.1095313897696872

Epoch: 6| Step: 6
Training loss: 1.6956374645233154
Validation loss: 2.1544170430911485

Epoch: 6| Step: 7
Training loss: 2.603153705596924
Validation loss: 2.1166555484135947

Epoch: 6| Step: 8
Training loss: 2.185980796813965
Validation loss: 2.1576561620158534

Epoch: 6| Step: 9
Training loss: 1.6089341640472412
Validation loss: 2.116379563526441

Epoch: 6| Step: 10
Training loss: 2.0161309242248535
Validation loss: 2.1066258107462237

Epoch: 6| Step: 11
Training loss: 2.1035237312316895
Validation loss: 2.1422714187252905

Epoch: 6| Step: 12
Training loss: 1.9811193943023682
Validation loss: 2.112891063895277

Epoch: 6| Step: 13
Training loss: 2.3138535022735596
Validation loss: 2.113220243043797

Epoch: 322| Step: 0
Training loss: 2.1148645877838135
Validation loss: 2.0806291846818823

Epoch: 6| Step: 1
Training loss: 2.62296199798584
Validation loss: 2.1023975213368735

Epoch: 6| Step: 2
Training loss: 1.8105359077453613
Validation loss: 2.113560615047332

Epoch: 6| Step: 3
Training loss: 2.161930561065674
Validation loss: 2.11907119904795

Epoch: 6| Step: 4
Training loss: 1.8828787803649902
Validation loss: 2.1200062818424676

Epoch: 6| Step: 5
Training loss: 2.605947256088257
Validation loss: 2.065892140070597

Epoch: 6| Step: 6
Training loss: 1.6384778022766113
Validation loss: 2.112847625568349

Epoch: 6| Step: 7
Training loss: 2.009669780731201
Validation loss: 2.073916632642028

Epoch: 6| Step: 8
Training loss: 2.181133270263672
Validation loss: 2.0833377069042576

Epoch: 6| Step: 9
Training loss: 2.2140073776245117
Validation loss: 2.090996365393362

Epoch: 6| Step: 10
Training loss: 2.1114888191223145
Validation loss: 2.0943735658481555

Epoch: 6| Step: 11
Training loss: 1.5468984842300415
Validation loss: 2.1198805429602183

Epoch: 6| Step: 12
Training loss: 1.7402830123901367
Validation loss: 2.096707954201647

Epoch: 6| Step: 13
Training loss: 2.0455162525177
Validation loss: 2.109563624987038

Epoch: 323| Step: 0
Training loss: 2.4169015884399414
Validation loss: 2.097853523428722

Epoch: 6| Step: 1
Training loss: 2.362227201461792
Validation loss: 2.092292470316733

Epoch: 6| Step: 2
Training loss: 1.8268613815307617
Validation loss: 2.1047497923656175

Epoch: 6| Step: 3
Training loss: 2.211139678955078
Validation loss: 2.1072691794364684

Epoch: 6| Step: 4
Training loss: 1.0744893550872803
Validation loss: 2.1536094193817465

Epoch: 6| Step: 5
Training loss: 2.0451500415802
Validation loss: 2.129397734518974

Epoch: 6| Step: 6
Training loss: 2.1467514038085938
Validation loss: 2.1213061758266982

Epoch: 6| Step: 7
Training loss: 2.2188620567321777
Validation loss: 2.1482960319006317

Epoch: 6| Step: 8
Training loss: 2.007936477661133
Validation loss: 2.1336304308265768

Epoch: 6| Step: 9
Training loss: 1.9475605487823486
Validation loss: 2.1246806883042857

Epoch: 6| Step: 10
Training loss: 2.513901948928833
Validation loss: 2.1288936907245266

Epoch: 6| Step: 11
Training loss: 2.4097213745117188
Validation loss: 2.12913260152263

Epoch: 6| Step: 12
Training loss: 1.709369421005249
Validation loss: 2.120176053816272

Epoch: 6| Step: 13
Training loss: 1.756533145904541
Validation loss: 2.1450832377197924

Epoch: 324| Step: 0
Training loss: 1.524200201034546
Validation loss: 2.13207124381937

Epoch: 6| Step: 1
Training loss: 1.4563114643096924
Validation loss: 2.131027058888507

Epoch: 6| Step: 2
Training loss: 2.0260727405548096
Validation loss: 2.119365776738813

Epoch: 6| Step: 3
Training loss: 2.420269012451172
Validation loss: 2.116989204960485

Epoch: 6| Step: 4
Training loss: 1.7702007293701172
Validation loss: 2.0875378988122426

Epoch: 6| Step: 5
Training loss: 2.1694722175598145
Validation loss: 2.1198014277283863

Epoch: 6| Step: 6
Training loss: 2.3425893783569336
Validation loss: 2.1395762787070325

Epoch: 6| Step: 7
Training loss: 2.1680078506469727
Validation loss: 2.115568040519632

Epoch: 6| Step: 8
Training loss: 2.2654671669006348
Validation loss: 2.0828697117426063

Epoch: 6| Step: 9
Training loss: 2.5428223609924316
Validation loss: 2.1000748142119376

Epoch: 6| Step: 10
Training loss: 1.7641633749008179
Validation loss: 2.130716457161852

Epoch: 6| Step: 11
Training loss: 2.683892250061035
Validation loss: 2.108033070000269

Epoch: 6| Step: 12
Training loss: 2.027595043182373
Validation loss: 2.122179162117743

Epoch: 6| Step: 13
Training loss: 1.1081289052963257
Validation loss: 2.130085619547034

Epoch: 325| Step: 0
Training loss: 1.9823551177978516
Validation loss: 2.070981248732536

Epoch: 6| Step: 1
Training loss: 2.223109006881714
Validation loss: 2.105365614737234

Epoch: 6| Step: 2
Training loss: 1.9804073572158813
Validation loss: 2.1067889505817043

Epoch: 6| Step: 3
Training loss: 1.8125771284103394
Validation loss: 2.092827399571737

Epoch: 6| Step: 4
Training loss: 2.543184280395508
Validation loss: 2.08115832267269

Epoch: 6| Step: 5
Training loss: 1.8977383375167847
Validation loss: 2.1329291635943997

Epoch: 6| Step: 6
Training loss: 2.304675340652466
Validation loss: 2.1133161514036116

Epoch: 6| Step: 7
Training loss: 1.9906318187713623
Validation loss: 2.0906971270038235

Epoch: 6| Step: 8
Training loss: 2.540639877319336
Validation loss: 2.1005546405751216

Epoch: 6| Step: 9
Training loss: 1.8010401725769043
Validation loss: 2.1182106899958786

Epoch: 6| Step: 10
Training loss: 1.3952479362487793
Validation loss: 2.1098594742436565

Epoch: 6| Step: 11
Training loss: 2.2817420959472656
Validation loss: 2.0845626784909155

Epoch: 6| Step: 12
Training loss: 2.051635503768921
Validation loss: 2.1040884089726273

Epoch: 6| Step: 13
Training loss: 1.5311312675476074
Validation loss: 2.106431914914039

Epoch: 326| Step: 0
Training loss: 1.8441119194030762
Validation loss: 2.1367657722965365

Epoch: 6| Step: 1
Training loss: 1.9141756296157837
Validation loss: 2.0963602860768638

Epoch: 6| Step: 2
Training loss: 1.6455087661743164
Validation loss: 2.1032032838431736

Epoch: 6| Step: 3
Training loss: 2.00870680809021
Validation loss: 2.117369149320869

Epoch: 6| Step: 4
Training loss: 2.222980499267578
Validation loss: 2.104928174326497

Epoch: 6| Step: 5
Training loss: 2.688214063644409
Validation loss: 2.1013371790609052

Epoch: 6| Step: 6
Training loss: 1.7672481536865234
Validation loss: 2.12120133830655

Epoch: 6| Step: 7
Training loss: 1.6032803058624268
Validation loss: 2.1000950797911613

Epoch: 6| Step: 8
Training loss: 1.8729592561721802
Validation loss: 2.1043667498455254

Epoch: 6| Step: 9
Training loss: 2.786238431930542
Validation loss: 2.1304588087143435

Epoch: 6| Step: 10
Training loss: 1.4846514463424683
Validation loss: 2.1507272425518242

Epoch: 6| Step: 11
Training loss: 2.3230319023132324
Validation loss: 2.1071401539669243

Epoch: 6| Step: 12
Training loss: 2.092442274093628
Validation loss: 2.1007719680827153

Epoch: 6| Step: 13
Training loss: 2.8687734603881836
Validation loss: 2.103152472485778

Epoch: 327| Step: 0
Training loss: 1.625519037246704
Validation loss: 2.127436776315012

Epoch: 6| Step: 1
Training loss: 1.843503475189209
Validation loss: 2.104698592616666

Epoch: 6| Step: 2
Training loss: 2.3562397956848145
Validation loss: 2.106958655900853

Epoch: 6| Step: 3
Training loss: 2.1469626426696777
Validation loss: 2.126619128770726

Epoch: 6| Step: 4
Training loss: 1.6783429384231567
Validation loss: 2.1247493067095355

Epoch: 6| Step: 5
Training loss: 2.273055076599121
Validation loss: 2.1078642183734524

Epoch: 6| Step: 6
Training loss: 2.495436906814575
Validation loss: 2.1200851919830486

Epoch: 6| Step: 7
Training loss: 2.4792604446411133
Validation loss: 2.1513398719090286

Epoch: 6| Step: 8
Training loss: 1.984591007232666
Validation loss: 2.144382639597821

Epoch: 6| Step: 9
Training loss: 2.1355953216552734
Validation loss: 2.1326737185960174

Epoch: 6| Step: 10
Training loss: 1.7657802104949951
Validation loss: 2.1141930203283987

Epoch: 6| Step: 11
Training loss: 2.309856414794922
Validation loss: 2.1230965763010006

Epoch: 6| Step: 12
Training loss: 1.1852481365203857
Validation loss: 2.1481300874422957

Epoch: 6| Step: 13
Training loss: 2.3817384243011475
Validation loss: 2.1041863964449976

Epoch: 328| Step: 0
Training loss: 2.0570247173309326
Validation loss: 2.1095680395762124

Epoch: 6| Step: 1
Training loss: 2.196228504180908
Validation loss: 2.093969870639104

Epoch: 6| Step: 2
Training loss: 1.9312007427215576
Validation loss: 2.0880580973881546

Epoch: 6| Step: 3
Training loss: 1.7594040632247925
Validation loss: 2.0884922614661594

Epoch: 6| Step: 4
Training loss: 2.2659707069396973
Validation loss: 2.104925783731604

Epoch: 6| Step: 5
Training loss: 1.86885666847229
Validation loss: 2.106422434570969

Epoch: 6| Step: 6
Training loss: 1.8831160068511963
Validation loss: 2.1386762793346117

Epoch: 6| Step: 7
Training loss: 1.939603567123413
Validation loss: 2.1212690825103433

Epoch: 6| Step: 8
Training loss: 2.062978982925415
Validation loss: 2.106760832571214

Epoch: 6| Step: 9
Training loss: 2.2852818965911865
Validation loss: 2.1039488430946105

Epoch: 6| Step: 10
Training loss: 1.7830572128295898
Validation loss: 2.1300429990214687

Epoch: 6| Step: 11
Training loss: 2.7904653549194336
Validation loss: 2.112701851834533

Epoch: 6| Step: 12
Training loss: 1.4031692743301392
Validation loss: 2.107177793338735

Epoch: 6| Step: 13
Training loss: 2.7610902786254883
Validation loss: 2.137612506907473

Epoch: 329| Step: 0
Training loss: 2.4924726486206055
Validation loss: 2.0936545043863277

Epoch: 6| Step: 1
Training loss: 2.291856288909912
Validation loss: 2.1128025875296643

Epoch: 6| Step: 2
Training loss: 1.9014956951141357
Validation loss: 2.0846606492996216

Epoch: 6| Step: 3
Training loss: 1.6594172716140747
Validation loss: 2.0885941161904285

Epoch: 6| Step: 4
Training loss: 2.596191883087158
Validation loss: 2.0950238268862487

Epoch: 6| Step: 5
Training loss: 1.7596726417541504
Validation loss: 2.1119566963564966

Epoch: 6| Step: 6
Training loss: 2.5187768936157227
Validation loss: 2.142170088265532

Epoch: 6| Step: 7
Training loss: 1.6683106422424316
Validation loss: 2.124581843294123

Epoch: 6| Step: 8
Training loss: 2.422682285308838
Validation loss: 2.1769460144863335

Epoch: 6| Step: 9
Training loss: 1.884965181350708
Validation loss: 2.1464177587980866

Epoch: 6| Step: 10
Training loss: 2.280980110168457
Validation loss: 2.1442847879984046

Epoch: 6| Step: 11
Training loss: 1.2962582111358643
Validation loss: 2.1467131183993433

Epoch: 6| Step: 12
Training loss: 2.0434885025024414
Validation loss: 2.121761452767157

Epoch: 6| Step: 13
Training loss: 2.1068005561828613
Validation loss: 2.1648064890215473

Epoch: 330| Step: 0
Training loss: 1.1504764556884766
Validation loss: 2.1339445985773557

Epoch: 6| Step: 1
Training loss: 2.260730743408203
Validation loss: 2.146120960994433

Epoch: 6| Step: 2
Training loss: 2.1199347972869873
Validation loss: 2.125121207647426

Epoch: 6| Step: 3
Training loss: 2.1880545616149902
Validation loss: 2.142785992673648

Epoch: 6| Step: 4
Training loss: 2.4516191482543945
Validation loss: 2.128390904395811

Epoch: 6| Step: 5
Training loss: 1.5453522205352783
Validation loss: 2.121419346460732

Epoch: 6| Step: 6
Training loss: 2.5885696411132812
Validation loss: 2.1465349197387695

Epoch: 6| Step: 7
Training loss: 2.5992584228515625
Validation loss: 2.1078061596039803

Epoch: 6| Step: 8
Training loss: 1.5382192134857178
Validation loss: 2.100434103319722

Epoch: 6| Step: 9
Training loss: 2.108895778656006
Validation loss: 2.1266293500059392

Epoch: 6| Step: 10
Training loss: 2.3265652656555176
Validation loss: 2.1202250552433792

Epoch: 6| Step: 11
Training loss: 1.656541347503662
Validation loss: 2.1378717268666914

Epoch: 6| Step: 12
Training loss: 2.4011738300323486
Validation loss: 2.109356105968516

Epoch: 6| Step: 13
Training loss: 1.3953648805618286
Validation loss: 2.109560465299955

Epoch: 331| Step: 0
Training loss: 1.6997840404510498
Validation loss: 2.1068241211675827

Epoch: 6| Step: 1
Training loss: 1.4891142845153809
Validation loss: 2.070604775541572

Epoch: 6| Step: 2
Training loss: 1.771431803703308
Validation loss: 2.079041229781284

Epoch: 6| Step: 3
Training loss: 2.1312408447265625
Validation loss: 2.1011652895199355

Epoch: 6| Step: 4
Training loss: 2.6521925926208496
Validation loss: 2.097140665977232

Epoch: 6| Step: 5
Training loss: 2.0524749755859375
Validation loss: 2.061754357430243

Epoch: 6| Step: 6
Training loss: 1.8087927103042603
Validation loss: 2.0693878204591813

Epoch: 6| Step: 7
Training loss: 1.229722499847412
Validation loss: 2.0609704679058445

Epoch: 6| Step: 8
Training loss: 2.4690310955047607
Validation loss: 2.0805955753531507

Epoch: 6| Step: 9
Training loss: 2.170044422149658
Validation loss: 2.0822873423176427

Epoch: 6| Step: 10
Training loss: 2.0997281074523926
Validation loss: 2.074070599771315

Epoch: 6| Step: 11
Training loss: 2.3757917881011963
Validation loss: 2.0735484297557543

Epoch: 6| Step: 12
Training loss: 2.438778877258301
Validation loss: 2.073107886058028

Epoch: 6| Step: 13
Training loss: 2.3268065452575684
Validation loss: 2.100879184661373

Epoch: 332| Step: 0
Training loss: 2.1679630279541016
Validation loss: 2.0688342163639684

Epoch: 6| Step: 1
Training loss: 1.8724784851074219
Validation loss: 2.0917858308361423

Epoch: 6| Step: 2
Training loss: 1.2901031970977783
Validation loss: 2.0524478855953423

Epoch: 6| Step: 3
Training loss: 1.8087191581726074
Validation loss: 2.090172320283869

Epoch: 6| Step: 4
Training loss: 1.807633876800537
Validation loss: 2.1059531691253826

Epoch: 6| Step: 5
Training loss: 1.7870371341705322
Validation loss: 2.1029633578433784

Epoch: 6| Step: 6
Training loss: 1.9292279481887817
Validation loss: 2.125567138835948

Epoch: 6| Step: 7
Training loss: 2.0794460773468018
Validation loss: 2.11817656793902

Epoch: 6| Step: 8
Training loss: 2.105872631072998
Validation loss: 2.1371260945514967

Epoch: 6| Step: 9
Training loss: 2.8724069595336914
Validation loss: 2.11061611483174

Epoch: 6| Step: 10
Training loss: 2.1348636150360107
Validation loss: 2.123808968451715

Epoch: 6| Step: 11
Training loss: 2.1302335262298584
Validation loss: 2.1247152051618023

Epoch: 6| Step: 12
Training loss: 1.966679334640503
Validation loss: 2.1115403739354943

Epoch: 6| Step: 13
Training loss: 3.195941686630249
Validation loss: 2.1255160454780824

Epoch: 333| Step: 0
Training loss: 1.7130296230316162
Validation loss: 2.1162986037551716

Epoch: 6| Step: 1
Training loss: 1.595210313796997
Validation loss: 2.146197090866745

Epoch: 6| Step: 2
Training loss: 2.049384355545044
Validation loss: 2.114781077190112

Epoch: 6| Step: 3
Training loss: 2.0461299419403076
Validation loss: 2.122197038383894

Epoch: 6| Step: 4
Training loss: 2.4375715255737305
Validation loss: 2.1007091358143795

Epoch: 6| Step: 5
Training loss: 1.8822427988052368
Validation loss: 2.138670854671027

Epoch: 6| Step: 6
Training loss: 2.156162738800049
Validation loss: 2.113342046737671

Epoch: 6| Step: 7
Training loss: 1.9647142887115479
Validation loss: 2.1130891781981274

Epoch: 6| Step: 8
Training loss: 2.5375232696533203
Validation loss: 2.149335768914992

Epoch: 6| Step: 9
Training loss: 1.6901638507843018
Validation loss: 2.1299044547542447

Epoch: 6| Step: 10
Training loss: 2.6758666038513184
Validation loss: 2.1210531726960213

Epoch: 6| Step: 11
Training loss: 1.652463436126709
Validation loss: 2.1027057158049716

Epoch: 6| Step: 12
Training loss: 2.4235384464263916
Validation loss: 2.1255291725999568

Epoch: 6| Step: 13
Training loss: 1.1709591150283813
Validation loss: 2.1241119856475503

Epoch: 334| Step: 0
Training loss: 1.600801944732666
Validation loss: 2.135370954390495

Epoch: 6| Step: 1
Training loss: 1.8859964609146118
Validation loss: 2.1040036127131474

Epoch: 6| Step: 2
Training loss: 2.7060701847076416
Validation loss: 2.0771009281117427

Epoch: 6| Step: 3
Training loss: 1.4204480648040771
Validation loss: 2.1204395255734845

Epoch: 6| Step: 4
Training loss: 2.4464850425720215
Validation loss: 2.0821404610910723

Epoch: 6| Step: 5
Training loss: 1.9282797574996948
Validation loss: 2.1146158044056227

Epoch: 6| Step: 6
Training loss: 2.6306419372558594
Validation loss: 2.1067994563810286

Epoch: 6| Step: 7
Training loss: 1.9534261226654053
Validation loss: 2.0990953817162463

Epoch: 6| Step: 8
Training loss: 2.2729616165161133
Validation loss: 2.1041719708391415

Epoch: 6| Step: 9
Training loss: 1.4815539121627808
Validation loss: 2.0796601439035065

Epoch: 6| Step: 10
Training loss: 2.1203737258911133
Validation loss: 2.0612584211493052

Epoch: 6| Step: 11
Training loss: 1.4274464845657349
Validation loss: 2.085535649330385

Epoch: 6| Step: 12
Training loss: 2.184296131134033
Validation loss: 2.086429158846537

Epoch: 6| Step: 13
Training loss: 2.5921144485473633
Validation loss: 2.0964959385574504

Epoch: 335| Step: 0
Training loss: 1.6309359073638916
Validation loss: 2.100555701922345

Epoch: 6| Step: 1
Training loss: 1.8011518716812134
Validation loss: 2.108536917676208

Epoch: 6| Step: 2
Training loss: 2.0187501907348633
Validation loss: 2.112013345123619

Epoch: 6| Step: 3
Training loss: 1.774389624595642
Validation loss: 2.10011282531164

Epoch: 6| Step: 4
Training loss: 2.460597038269043
Validation loss: 2.1076824921433643

Epoch: 6| Step: 5
Training loss: 2.5378365516662598
Validation loss: 2.09385153811465

Epoch: 6| Step: 6
Training loss: 2.339078903198242
Validation loss: 2.1024417620833202

Epoch: 6| Step: 7
Training loss: 1.1733207702636719
Validation loss: 2.100862718397571

Epoch: 6| Step: 8
Training loss: 2.06215763092041
Validation loss: 2.1498185588467504

Epoch: 6| Step: 9
Training loss: 2.6594252586364746
Validation loss: 2.1380807994514384

Epoch: 6| Step: 10
Training loss: 1.4214446544647217
Validation loss: 2.108148697883852

Epoch: 6| Step: 11
Training loss: 2.6530110836029053
Validation loss: 2.1181797929989394

Epoch: 6| Step: 12
Training loss: 1.657376766204834
Validation loss: 2.1175470326536443

Epoch: 6| Step: 13
Training loss: 2.1577768325805664
Validation loss: 2.1204975253792218

Epoch: 336| Step: 0
Training loss: 1.2711232900619507
Validation loss: 2.123360005758142

Epoch: 6| Step: 1
Training loss: 2.156230926513672
Validation loss: 2.131669041930988

Epoch: 6| Step: 2
Training loss: 2.2939867973327637
Validation loss: 2.129177729288737

Epoch: 6| Step: 3
Training loss: 2.2007246017456055
Validation loss: 2.1400326336583784

Epoch: 6| Step: 4
Training loss: 2.208202600479126
Validation loss: 2.1378354564789803

Epoch: 6| Step: 5
Training loss: 1.7162930965423584
Validation loss: 2.110627906296843

Epoch: 6| Step: 6
Training loss: 1.1009962558746338
Validation loss: 2.1197726290713073

Epoch: 6| Step: 7
Training loss: 3.37546968460083
Validation loss: 2.0782222363256637

Epoch: 6| Step: 8
Training loss: 2.1680097579956055
Validation loss: 2.0962282547386746

Epoch: 6| Step: 9
Training loss: 1.9137935638427734
Validation loss: 2.1174239702122186

Epoch: 6| Step: 10
Training loss: 2.1404869556427
Validation loss: 2.070230476317867

Epoch: 6| Step: 11
Training loss: 2.083533763885498
Validation loss: 2.097496026305742

Epoch: 6| Step: 12
Training loss: 1.2097800970077515
Validation loss: 2.099188902044809

Epoch: 6| Step: 13
Training loss: 2.783897638320923
Validation loss: 2.105175043946953

Epoch: 337| Step: 0
Training loss: 2.501915693283081
Validation loss: 2.0674620546320432

Epoch: 6| Step: 1
Training loss: 1.8018840551376343
Validation loss: 2.1179026121734292

Epoch: 6| Step: 2
Training loss: 2.6534488201141357
Validation loss: 2.08925485867326

Epoch: 6| Step: 3
Training loss: 1.5440436601638794
Validation loss: 2.0802170961133895

Epoch: 6| Step: 4
Training loss: 1.6292884349822998
Validation loss: 2.079661847442709

Epoch: 6| Step: 5
Training loss: 2.4826419353485107
Validation loss: 2.0947816346281316

Epoch: 6| Step: 6
Training loss: 1.6429038047790527
Validation loss: 2.082621475701691

Epoch: 6| Step: 7
Training loss: 1.7688562870025635
Validation loss: 2.0800684421293196

Epoch: 6| Step: 8
Training loss: 1.9368737936019897
Validation loss: 2.069548401781308

Epoch: 6| Step: 9
Training loss: 2.2210354804992676
Validation loss: 2.1052316901504353

Epoch: 6| Step: 10
Training loss: 2.154261589050293
Validation loss: 2.0849869558888097

Epoch: 6| Step: 11
Training loss: 2.7244019508361816
Validation loss: 2.100429077302256

Epoch: 6| Step: 12
Training loss: 1.7543227672576904
Validation loss: 2.0748195507193126

Epoch: 6| Step: 13
Training loss: 1.5098084211349487
Validation loss: 2.0880833415574926

Epoch: 338| Step: 0
Training loss: 2.1908020973205566
Validation loss: 2.0820381859297394

Epoch: 6| Step: 1
Training loss: 2.217242956161499
Validation loss: 2.0621221962795464

Epoch: 6| Step: 2
Training loss: 1.7000751495361328
Validation loss: 2.092335815070778

Epoch: 6| Step: 3
Training loss: 1.904022455215454
Validation loss: 2.108837971123316

Epoch: 6| Step: 4
Training loss: 1.6165344715118408
Validation loss: 2.092092155128397

Epoch: 6| Step: 5
Training loss: 2.3059988021850586
Validation loss: 2.1178947341057563

Epoch: 6| Step: 6
Training loss: 1.8239812850952148
Validation loss: 2.11228266069966

Epoch: 6| Step: 7
Training loss: 1.3820383548736572
Validation loss: 2.0860281439237696

Epoch: 6| Step: 8
Training loss: 2.0052051544189453
Validation loss: 2.1179364265934115

Epoch: 6| Step: 9
Training loss: 2.6036078929901123
Validation loss: 2.1208898431511334

Epoch: 6| Step: 10
Training loss: 1.7750147581100464
Validation loss: 2.093955073305356

Epoch: 6| Step: 11
Training loss: 2.1679553985595703
Validation loss: 2.0632823051944857

Epoch: 6| Step: 12
Training loss: 1.977777123451233
Validation loss: 2.0925403961571316

Epoch: 6| Step: 13
Training loss: 3.303550958633423
Validation loss: 2.10316990780574

Epoch: 339| Step: 0
Training loss: 1.4587373733520508
Validation loss: 2.075766255778651

Epoch: 6| Step: 1
Training loss: 2.6528539657592773
Validation loss: 2.1015829757977555

Epoch: 6| Step: 2
Training loss: 2.5641274452209473
Validation loss: 2.100801947296307

Epoch: 6| Step: 3
Training loss: 1.097381353378296
Validation loss: 2.1110579980316984

Epoch: 6| Step: 4
Training loss: 2.2972145080566406
Validation loss: 2.101921760907737

Epoch: 6| Step: 5
Training loss: 1.7950208187103271
Validation loss: 2.0740145714052263

Epoch: 6| Step: 6
Training loss: 2.1435160636901855
Validation loss: 2.082804327370018

Epoch: 6| Step: 7
Training loss: 2.029804229736328
Validation loss: 2.118311838437152

Epoch: 6| Step: 8
Training loss: 1.4326245784759521
Validation loss: 2.110581208300847

Epoch: 6| Step: 9
Training loss: 2.335278034210205
Validation loss: 2.099574004450152

Epoch: 6| Step: 10
Training loss: 2.2463364601135254
Validation loss: 2.1229040699620403

Epoch: 6| Step: 11
Training loss: 2.5999457836151123
Validation loss: 2.107609127157478

Epoch: 6| Step: 12
Training loss: 1.5379822254180908
Validation loss: 2.1085015240535943

Epoch: 6| Step: 13
Training loss: 2.0566365718841553
Validation loss: 2.1308385813108055

Epoch: 340| Step: 0
Training loss: 2.0613021850585938
Validation loss: 2.1302282707665556

Epoch: 6| Step: 1
Training loss: 1.8099924325942993
Validation loss: 2.1456668325649795

Epoch: 6| Step: 2
Training loss: 2.548367977142334
Validation loss: 2.1224646965662637

Epoch: 6| Step: 3
Training loss: 2.2323408126831055
Validation loss: 2.1273238607632217

Epoch: 6| Step: 4
Training loss: 1.9888942241668701
Validation loss: 2.140666557896522

Epoch: 6| Step: 5
Training loss: 1.1973425149917603
Validation loss: 2.1170916980312717

Epoch: 6| Step: 6
Training loss: 2.530601978302002
Validation loss: 2.13003473384406

Epoch: 6| Step: 7
Training loss: 2.018226146697998
Validation loss: 2.094676753526093

Epoch: 6| Step: 8
Training loss: 1.4449483156204224
Validation loss: 2.0905228327679377

Epoch: 6| Step: 9
Training loss: 2.5842607021331787
Validation loss: 2.0556539271467473

Epoch: 6| Step: 10
Training loss: 1.9121944904327393
Validation loss: 2.1241927018729587

Epoch: 6| Step: 11
Training loss: 2.0363242626190186
Validation loss: 2.0888213137144684

Epoch: 6| Step: 12
Training loss: 1.5192738771438599
Validation loss: 2.0739543130320888

Epoch: 6| Step: 13
Training loss: 2.3156614303588867
Validation loss: 2.08483785967673

Epoch: 341| Step: 0
Training loss: 2.389523983001709
Validation loss: 2.0899672662058184

Epoch: 6| Step: 1
Training loss: 2.4899792671203613
Validation loss: 2.0845307047649095

Epoch: 6| Step: 2
Training loss: 1.5407347679138184
Validation loss: 2.077862819035848

Epoch: 6| Step: 3
Training loss: 2.0657405853271484
Validation loss: 2.091344928228727

Epoch: 6| Step: 4
Training loss: 1.4261796474456787
Validation loss: 2.0857536177481375

Epoch: 6| Step: 5
Training loss: 1.6692862510681152
Validation loss: 2.0864703283515027

Epoch: 6| Step: 6
Training loss: 2.1300137042999268
Validation loss: 2.1002312975545085

Epoch: 6| Step: 7
Training loss: 2.242859363555908
Validation loss: 2.1154724705603813

Epoch: 6| Step: 8
Training loss: 2.4980287551879883
Validation loss: 2.113684118434947

Epoch: 6| Step: 9
Training loss: 2.294315814971924
Validation loss: 2.097273180561681

Epoch: 6| Step: 10
Training loss: 1.7515641450881958
Validation loss: 2.0971882343292236

Epoch: 6| Step: 11
Training loss: 1.9672003984451294
Validation loss: 2.0861189262841338

Epoch: 6| Step: 12
Training loss: 2.063217878341675
Validation loss: 2.109960781630649

Epoch: 6| Step: 13
Training loss: 1.5985522270202637
Validation loss: 2.110999502161498

Epoch: 342| Step: 0
Training loss: 1.2263314723968506
Validation loss: 2.0732247983255694

Epoch: 6| Step: 1
Training loss: 2.148554563522339
Validation loss: 2.1013127911475395

Epoch: 6| Step: 2
Training loss: 1.5795178413391113
Validation loss: 2.143937107055418

Epoch: 6| Step: 3
Training loss: 2.1371207237243652
Validation loss: 2.083782443436243

Epoch: 6| Step: 4
Training loss: 1.4646066427230835
Validation loss: 2.0950015539764077

Epoch: 6| Step: 5
Training loss: 2.406721830368042
Validation loss: 2.1111916060088785

Epoch: 6| Step: 6
Training loss: 2.417637825012207
Validation loss: 2.095399357939279

Epoch: 6| Step: 7
Training loss: 2.195876121520996
Validation loss: 2.126311840549592

Epoch: 6| Step: 8
Training loss: 2.1041202545166016
Validation loss: 2.095105032767019

Epoch: 6| Step: 9
Training loss: 2.6666884422302246
Validation loss: 2.061970897900161

Epoch: 6| Step: 10
Training loss: 1.74677312374115
Validation loss: 2.0831068997742026

Epoch: 6| Step: 11
Training loss: 2.568983316421509
Validation loss: 2.131545377034013

Epoch: 6| Step: 12
Training loss: 1.4977012872695923
Validation loss: 2.1341736547408567

Epoch: 6| Step: 13
Training loss: 1.5676571130752563
Validation loss: 2.088137376692987

Epoch: 343| Step: 0
Training loss: 1.463218092918396
Validation loss: 2.1001907843415455

Epoch: 6| Step: 1
Training loss: 1.8448233604431152
Validation loss: 2.1066462762894167

Epoch: 6| Step: 2
Training loss: 2.001254081726074
Validation loss: 2.107481807790777

Epoch: 6| Step: 3
Training loss: 2.2625086307525635
Validation loss: 2.08575197445449

Epoch: 6| Step: 4
Training loss: 1.8284443616867065
Validation loss: 2.09926869792323

Epoch: 6| Step: 5
Training loss: 2.0041680335998535
Validation loss: 2.1083166240363993

Epoch: 6| Step: 6
Training loss: 1.9633188247680664
Validation loss: 2.061595550147436

Epoch: 6| Step: 7
Training loss: 2.1892590522766113
Validation loss: 2.074706974849906

Epoch: 6| Step: 8
Training loss: 2.144134283065796
Validation loss: 2.077649410052966

Epoch: 6| Step: 9
Training loss: 2.6240499019622803
Validation loss: 2.063635790219871

Epoch: 6| Step: 10
Training loss: 2.2324352264404297
Validation loss: 2.1227225360049995

Epoch: 6| Step: 11
Training loss: 1.902864933013916
Validation loss: 2.1188594551496607

Epoch: 6| Step: 12
Training loss: 1.7140681743621826
Validation loss: 2.109603915163266

Epoch: 6| Step: 13
Training loss: 1.7151432037353516
Validation loss: 2.0854786890809254

Epoch: 344| Step: 0
Training loss: 1.8011994361877441
Validation loss: 2.0916108982537382

Epoch: 6| Step: 1
Training loss: 1.8176965713500977
Validation loss: 2.0687169374958163

Epoch: 6| Step: 2
Training loss: 1.844675064086914
Validation loss: 2.09399063100097

Epoch: 6| Step: 3
Training loss: 1.7333664894104004
Validation loss: 2.069205255918605

Epoch: 6| Step: 4
Training loss: 2.094229221343994
Validation loss: 2.1199478192995955

Epoch: 6| Step: 5
Training loss: 2.0477519035339355
Validation loss: 2.1059074350582656

Epoch: 6| Step: 6
Training loss: 2.00020694732666
Validation loss: 2.1083714487732097

Epoch: 6| Step: 7
Training loss: 2.232564687728882
Validation loss: 2.0992647088984007

Epoch: 6| Step: 8
Training loss: 2.3530638217926025
Validation loss: 2.082225949533524

Epoch: 6| Step: 9
Training loss: 2.2293689250946045
Validation loss: 2.0526427479200464

Epoch: 6| Step: 10
Training loss: 1.9069814682006836
Validation loss: 2.097670747387794

Epoch: 6| Step: 11
Training loss: 2.2358431816101074
Validation loss: 2.096476995816795

Epoch: 6| Step: 12
Training loss: 1.9251209497451782
Validation loss: 2.1139320532480874

Epoch: 6| Step: 13
Training loss: 1.7859681844711304
Validation loss: 2.108017034428094

Epoch: 345| Step: 0
Training loss: 2.1424708366394043
Validation loss: 2.076366860379455

Epoch: 6| Step: 1
Training loss: 2.775826930999756
Validation loss: 2.094793519666118

Epoch: 6| Step: 2
Training loss: 1.9982664585113525
Validation loss: 2.129804483024023

Epoch: 6| Step: 3
Training loss: 2.1758108139038086
Validation loss: 2.097582055676368

Epoch: 6| Step: 4
Training loss: 1.479056477546692
Validation loss: 2.140788916618593

Epoch: 6| Step: 5
Training loss: 1.7219785451889038
Validation loss: 2.1168734591494323

Epoch: 6| Step: 6
Training loss: 1.8910735845565796
Validation loss: 2.1210690467588362

Epoch: 6| Step: 7
Training loss: 2.017427921295166
Validation loss: 2.077157566624303

Epoch: 6| Step: 8
Training loss: 1.3145169019699097
Validation loss: 2.0836906612560315

Epoch: 6| Step: 9
Training loss: 2.0542001724243164
Validation loss: 2.1135358579697145

Epoch: 6| Step: 10
Training loss: 2.684908866882324
Validation loss: 2.0951605740413872

Epoch: 6| Step: 11
Training loss: 2.390561103820801
Validation loss: 2.111333336881412

Epoch: 6| Step: 12
Training loss: 2.0154190063476562
Validation loss: 2.105407868662188

Epoch: 6| Step: 13
Training loss: 1.0482568740844727
Validation loss: 2.1091657684695337

Epoch: 346| Step: 0
Training loss: 1.2949471473693848
Validation loss: 2.0852319194424536

Epoch: 6| Step: 1
Training loss: 1.6417732238769531
Validation loss: 2.048357691816104

Epoch: 6| Step: 2
Training loss: 2.224658966064453
Validation loss: 2.106377968224146

Epoch: 6| Step: 3
Training loss: 2.292302131652832
Validation loss: 2.070192716454947

Epoch: 6| Step: 4
Training loss: 2.373466730117798
Validation loss: 2.081998958382555

Epoch: 6| Step: 5
Training loss: 2.292924404144287
Validation loss: 2.116567539912398

Epoch: 6| Step: 6
Training loss: 1.5713202953338623
Validation loss: 2.083100454781645

Epoch: 6| Step: 7
Training loss: 2.3008980751037598
Validation loss: 2.1002776494590183

Epoch: 6| Step: 8
Training loss: 2.1513888835906982
Validation loss: 2.1356803858152

Epoch: 6| Step: 9
Training loss: 2.2096424102783203
Validation loss: 2.127472769829535

Epoch: 6| Step: 10
Training loss: 2.56177020072937
Validation loss: 2.1129237298042542

Epoch: 6| Step: 11
Training loss: 0.9269962310791016
Validation loss: 2.0998462400128766

Epoch: 6| Step: 12
Training loss: 2.5891482830047607
Validation loss: 2.0554823029425835

Epoch: 6| Step: 13
Training loss: 1.688547134399414
Validation loss: 2.0650629023069977

Epoch: 347| Step: 0
Training loss: 2.600966453552246
Validation loss: 2.113284751933108

Epoch: 6| Step: 1
Training loss: 1.5102267265319824
Validation loss: 2.0966614677060034

Epoch: 6| Step: 2
Training loss: 2.1545627117156982
Validation loss: 2.1226669703760455

Epoch: 6| Step: 3
Training loss: 1.3127937316894531
Validation loss: 2.139306317093552

Epoch: 6| Step: 4
Training loss: 1.7660430669784546
Validation loss: 2.1049630847028507

Epoch: 6| Step: 5
Training loss: 1.9985575675964355
Validation loss: 2.1313396089820453

Epoch: 6| Step: 6
Training loss: 2.3371453285217285
Validation loss: 2.1098503220465874

Epoch: 6| Step: 7
Training loss: 1.6429376602172852
Validation loss: 2.111357827340403

Epoch: 6| Step: 8
Training loss: 2.9000911712646484
Validation loss: 2.1157503563870668

Epoch: 6| Step: 9
Training loss: 1.6053105592727661
Validation loss: 2.0826790050793718

Epoch: 6| Step: 10
Training loss: 1.8896558284759521
Validation loss: 2.087391702077722

Epoch: 6| Step: 11
Training loss: 1.863457441329956
Validation loss: 2.102844181881156

Epoch: 6| Step: 12
Training loss: 2.0441362857818604
Validation loss: 2.0909866056134625

Epoch: 6| Step: 13
Training loss: 3.0091264247894287
Validation loss: 2.0758507328648723

Epoch: 348| Step: 0
Training loss: 2.8510940074920654
Validation loss: 2.1261581925935644

Epoch: 6| Step: 1
Training loss: 1.6799328327178955
Validation loss: 2.0801145094697193

Epoch: 6| Step: 2
Training loss: 1.8853306770324707
Validation loss: 2.087315818314911

Epoch: 6| Step: 3
Training loss: 2.005707263946533
Validation loss: 2.105008735451647

Epoch: 6| Step: 4
Training loss: 2.222503185272217
Validation loss: 2.0906759154412056

Epoch: 6| Step: 5
Training loss: 1.9066804647445679
Validation loss: 2.105577766254384

Epoch: 6| Step: 6
Training loss: 1.882938027381897
Validation loss: 2.079192840924827

Epoch: 6| Step: 7
Training loss: 1.9111547470092773
Validation loss: 2.0901344540298625

Epoch: 6| Step: 8
Training loss: 2.1480751037597656
Validation loss: 2.0624697067404307

Epoch: 6| Step: 9
Training loss: 1.6460150480270386
Validation loss: 2.1338306127055997

Epoch: 6| Step: 10
Training loss: 2.5445456504821777
Validation loss: 2.100955201733497

Epoch: 6| Step: 11
Training loss: 2.289707660675049
Validation loss: 2.1335580528423352

Epoch: 6| Step: 12
Training loss: 1.6937189102172852
Validation loss: 2.0938096187447988

Epoch: 6| Step: 13
Training loss: 1.088761806488037
Validation loss: 2.080467724031018

Epoch: 349| Step: 0
Training loss: 2.013002872467041
Validation loss: 2.0765242243325837

Epoch: 6| Step: 1
Training loss: 2.1161129474639893
Validation loss: 2.100782181627007

Epoch: 6| Step: 2
Training loss: 2.739513874053955
Validation loss: 2.1151450475056968

Epoch: 6| Step: 3
Training loss: 1.6145484447479248
Validation loss: 2.1026991644213275

Epoch: 6| Step: 4
Training loss: 1.7403520345687866
Validation loss: 2.1356612636196997

Epoch: 6| Step: 5
Training loss: 2.2382516860961914
Validation loss: 2.0972915669923187

Epoch: 6| Step: 6
Training loss: 1.8361310958862305
Validation loss: 2.107252792645526

Epoch: 6| Step: 7
Training loss: 1.8567824363708496
Validation loss: 2.1212876048139346

Epoch: 6| Step: 8
Training loss: 1.982244849205017
Validation loss: 2.090473218630719

Epoch: 6| Step: 9
Training loss: 2.0226306915283203
Validation loss: 2.090252145644157

Epoch: 6| Step: 10
Training loss: 1.737634301185608
Validation loss: 2.1056868363452215

Epoch: 6| Step: 11
Training loss: 2.5834789276123047
Validation loss: 2.1061314254678707

Epoch: 6| Step: 12
Training loss: 1.532642126083374
Validation loss: 2.119054881475305

Epoch: 6| Step: 13
Training loss: 2.6443324089050293
Validation loss: 2.096233706320486

Epoch: 350| Step: 0
Training loss: 2.159395456314087
Validation loss: 2.0717743237813315

Epoch: 6| Step: 1
Training loss: 1.8781174421310425
Validation loss: 2.0970023370558217

Epoch: 6| Step: 2
Training loss: 2.0925235748291016
Validation loss: 2.0851290200346257

Epoch: 6| Step: 3
Training loss: 2.3220198154449463
Validation loss: 2.085051458369019

Epoch: 6| Step: 4
Training loss: 1.8613345623016357
Validation loss: 2.131214790446784

Epoch: 6| Step: 5
Training loss: 1.0026668310165405
Validation loss: 2.12302396118

Epoch: 6| Step: 6
Training loss: 1.1681653261184692
Validation loss: 2.080820316909462

Epoch: 6| Step: 7
Training loss: 3.4239790439605713
Validation loss: 2.0613106271272064

Epoch: 6| Step: 8
Training loss: 2.311023712158203
Validation loss: 2.122959356154165

Epoch: 6| Step: 9
Training loss: 1.4639194011688232
Validation loss: 2.1190436655475247

Epoch: 6| Step: 10
Training loss: 1.7671589851379395
Validation loss: 2.1433538467653337

Epoch: 6| Step: 11
Training loss: 2.7250635623931885
Validation loss: 2.0652964448416107

Epoch: 6| Step: 12
Training loss: 1.6999540328979492
Validation loss: 2.103521466255188

Epoch: 6| Step: 13
Training loss: 2.006645441055298
Validation loss: 2.1208691404711817

Epoch: 351| Step: 0
Training loss: 2.745131492614746
Validation loss: 2.093312530107396

Epoch: 6| Step: 1
Training loss: 2.352517604827881
Validation loss: 2.1077585784337853

Epoch: 6| Step: 2
Training loss: 1.6943132877349854
Validation loss: 2.0918265158130276

Epoch: 6| Step: 3
Training loss: 1.9096096754074097
Validation loss: 2.07165127928539

Epoch: 6| Step: 4
Training loss: 2.2117857933044434
Validation loss: 2.0842723359343824

Epoch: 6| Step: 5
Training loss: 2.3355374336242676
Validation loss: 2.1132945399130545

Epoch: 6| Step: 6
Training loss: 1.9385144710540771
Validation loss: 2.076040683254119

Epoch: 6| Step: 7
Training loss: 1.7278692722320557
Validation loss: 2.092321950902221

Epoch: 6| Step: 8
Training loss: 1.884322166442871
Validation loss: 2.1044008783114854

Epoch: 6| Step: 9
Training loss: 1.6577320098876953
Validation loss: 2.083310329785911

Epoch: 6| Step: 10
Training loss: 2.126230001449585
Validation loss: 2.0900203412578953

Epoch: 6| Step: 11
Training loss: 1.9527770280838013
Validation loss: 2.107090301411126

Epoch: 6| Step: 12
Training loss: 2.117941379547119
Validation loss: 2.0639105432776996

Epoch: 6| Step: 13
Training loss: 1.32155179977417
Validation loss: 2.1090162492567495

Epoch: 352| Step: 0
Training loss: 2.1059207916259766
Validation loss: 2.0925330295357654

Epoch: 6| Step: 1
Training loss: 1.9553278684616089
Validation loss: 2.090955741943852

Epoch: 6| Step: 2
Training loss: 1.678030014038086
Validation loss: 2.101623935084189

Epoch: 6| Step: 3
Training loss: 2.3961453437805176
Validation loss: 2.080731740561865

Epoch: 6| Step: 4
Training loss: 1.5263043642044067
Validation loss: 2.086807986741425

Epoch: 6| Step: 5
Training loss: 1.6315722465515137
Validation loss: 2.110017879034883

Epoch: 6| Step: 6
Training loss: 2.543173313140869
Validation loss: 2.089501223256511

Epoch: 6| Step: 7
Training loss: 1.5415762662887573
Validation loss: 2.0441479759831584

Epoch: 6| Step: 8
Training loss: 1.6296777725219727
Validation loss: 2.093179646358695

Epoch: 6| Step: 9
Training loss: 1.9580494165420532
Validation loss: 2.0829993960677937

Epoch: 6| Step: 10
Training loss: 2.767606258392334
Validation loss: 2.0589327632739978

Epoch: 6| Step: 11
Training loss: 1.6228266954421997
Validation loss: 2.047494214068177

Epoch: 6| Step: 12
Training loss: 2.5251412391662598
Validation loss: 2.0731566311210714

Epoch: 6| Step: 13
Training loss: 2.293508768081665
Validation loss: 2.089770581132622

Epoch: 353| Step: 0
Training loss: 2.5312089920043945
Validation loss: 2.0793452942243187

Epoch: 6| Step: 1
Training loss: 2.2370102405548096
Validation loss: 2.0798205034707182

Epoch: 6| Step: 2
Training loss: 1.9514744281768799
Validation loss: 2.075968917979989

Epoch: 6| Step: 3
Training loss: 1.8614943027496338
Validation loss: 2.1155753904773342

Epoch: 6| Step: 4
Training loss: 1.3274929523468018
Validation loss: 2.1396005307474444

Epoch: 6| Step: 5
Training loss: 1.6761786937713623
Validation loss: 2.106363110644843

Epoch: 6| Step: 6
Training loss: 2.3535614013671875
Validation loss: 2.0757725264436457

Epoch: 6| Step: 7
Training loss: 1.5767990350723267
Validation loss: 2.079872513330111

Epoch: 6| Step: 8
Training loss: 2.5162150859832764
Validation loss: 2.101121382046771

Epoch: 6| Step: 9
Training loss: 2.0199880599975586
Validation loss: 2.088972722330401

Epoch: 6| Step: 10
Training loss: 2.0728089809417725
Validation loss: 2.1198704601615987

Epoch: 6| Step: 11
Training loss: 1.4495251178741455
Validation loss: 2.120143469943795

Epoch: 6| Step: 12
Training loss: 1.8824106454849243
Validation loss: 2.1394807446387505

Epoch: 6| Step: 13
Training loss: 2.7331886291503906
Validation loss: 2.1280400419747956

Epoch: 354| Step: 0
Training loss: 2.3917412757873535
Validation loss: 2.122561882900935

Epoch: 6| Step: 1
Training loss: 2.220010995864868
Validation loss: 2.109613800561556

Epoch: 6| Step: 2
Training loss: 2.1686513423919678
Validation loss: 2.102495724155057

Epoch: 6| Step: 3
Training loss: 2.3704965114593506
Validation loss: 2.1162209510803223

Epoch: 6| Step: 4
Training loss: 1.5267009735107422
Validation loss: 2.0988842774462957

Epoch: 6| Step: 5
Training loss: 1.7069494724273682
Validation loss: 2.063662500791652

Epoch: 6| Step: 6
Training loss: 1.8808289766311646
Validation loss: 2.069394419270177

Epoch: 6| Step: 7
Training loss: 2.4084300994873047
Validation loss: 2.078016132436773

Epoch: 6| Step: 8
Training loss: 1.6990172863006592
Validation loss: 2.069870174572032

Epoch: 6| Step: 9
Training loss: 1.537818431854248
Validation loss: 2.071749048848306

Epoch: 6| Step: 10
Training loss: 2.304067611694336
Validation loss: 2.0713855630608013

Epoch: 6| Step: 11
Training loss: 1.9288406372070312
Validation loss: 2.100355707189088

Epoch: 6| Step: 12
Training loss: 2.3558409214019775
Validation loss: 2.121250437152001

Epoch: 6| Step: 13
Training loss: 1.819947600364685
Validation loss: 2.0729459203699583

Epoch: 355| Step: 0
Training loss: 2.1672861576080322
Validation loss: 2.109616502638786

Epoch: 6| Step: 1
Training loss: 2.027578830718994
Validation loss: 2.0732969494276148

Epoch: 6| Step: 2
Training loss: 2.389148235321045
Validation loss: 2.0803291336182625

Epoch: 6| Step: 3
Training loss: 1.856819987297058
Validation loss: 2.1093862723278742

Epoch: 6| Step: 4
Training loss: 1.7504255771636963
Validation loss: 2.1082033495749197

Epoch: 6| Step: 5
Training loss: 2.4247984886169434
Validation loss: 2.065879665395265

Epoch: 6| Step: 6
Training loss: 1.4384117126464844
Validation loss: 2.1095500787099204

Epoch: 6| Step: 7
Training loss: 2.6173453330993652
Validation loss: 2.0808261209918606

Epoch: 6| Step: 8
Training loss: 1.546386480331421
Validation loss: 2.07799461836456

Epoch: 6| Step: 9
Training loss: 1.7272837162017822
Validation loss: 2.110276937484741

Epoch: 6| Step: 10
Training loss: 2.1741888523101807
Validation loss: 2.0840770403544107

Epoch: 6| Step: 11
Training loss: 1.3613476753234863
Validation loss: 2.07596706703145

Epoch: 6| Step: 12
Training loss: 2.3286454677581787
Validation loss: 2.0737294779028943

Epoch: 6| Step: 13
Training loss: 2.5225448608398438
Validation loss: 2.090793542964484

Epoch: 356| Step: 0
Training loss: 1.8957197666168213
Validation loss: 2.0781356801268873

Epoch: 6| Step: 1
Training loss: 1.2066550254821777
Validation loss: 2.0848741633917696

Epoch: 6| Step: 2
Training loss: 2.890927314758301
Validation loss: 2.116453924486714

Epoch: 6| Step: 3
Training loss: 2.2716102600097656
Validation loss: 2.1500863644384567

Epoch: 6| Step: 4
Training loss: 3.036783218383789
Validation loss: 2.1475115181297384

Epoch: 6| Step: 5
Training loss: 2.1021132469177246
Validation loss: 2.1157695554917857

Epoch: 6| Step: 6
Training loss: 1.9929070472717285
Validation loss: 2.142678104421144

Epoch: 6| Step: 7
Training loss: 1.126523733139038
Validation loss: 2.1262298373765844

Epoch: 6| Step: 8
Training loss: 2.279991626739502
Validation loss: 2.112461349015595

Epoch: 6| Step: 9
Training loss: 1.887099027633667
Validation loss: 2.1368463680308354

Epoch: 6| Step: 10
Training loss: 1.954408049583435
Validation loss: 2.130435250138724

Epoch: 6| Step: 11
Training loss: 1.6510841846466064
Validation loss: 2.1409984660404984

Epoch: 6| Step: 12
Training loss: 2.371448040008545
Validation loss: 2.1079618059178835

Epoch: 6| Step: 13
Training loss: 1.104085922241211
Validation loss: 2.1237447107991865

Epoch: 357| Step: 0
Training loss: 1.804962158203125
Validation loss: 2.0926483600370345

Epoch: 6| Step: 1
Training loss: 2.529540777206421
Validation loss: 2.106411321188814

Epoch: 6| Step: 2
Training loss: 2.064812183380127
Validation loss: 2.1094742718563286

Epoch: 6| Step: 3
Training loss: 1.6192371845245361
Validation loss: 2.113224019286453

Epoch: 6| Step: 4
Training loss: 1.7240285873413086
Validation loss: 2.1061488889878794

Epoch: 6| Step: 5
Training loss: 2.0814008712768555
Validation loss: 2.0904977526716007

Epoch: 6| Step: 6
Training loss: 1.3118338584899902
Validation loss: 2.087421537727438

Epoch: 6| Step: 7
Training loss: 2.1825551986694336
Validation loss: 2.08399002270032

Epoch: 6| Step: 8
Training loss: 2.05729341506958
Validation loss: 2.12781633100202

Epoch: 6| Step: 9
Training loss: 2.0720295906066895
Validation loss: 2.109821763089908

Epoch: 6| Step: 10
Training loss: 2.0144309997558594
Validation loss: 2.088245745628111

Epoch: 6| Step: 11
Training loss: 1.9463229179382324
Validation loss: 2.085403170636905

Epoch: 6| Step: 12
Training loss: 2.1709609031677246
Validation loss: 2.079370411493445

Epoch: 6| Step: 13
Training loss: 2.402552843093872
Validation loss: 2.0823996015774306

Epoch: 358| Step: 0
Training loss: 2.056816577911377
Validation loss: 2.0708286223873014

Epoch: 6| Step: 1
Training loss: 2.365488052368164
Validation loss: 2.1372144299168743

Epoch: 6| Step: 2
Training loss: 1.8719565868377686
Validation loss: 2.0902122310412827

Epoch: 6| Step: 3
Training loss: 1.815535306930542
Validation loss: 2.129391289526416

Epoch: 6| Step: 4
Training loss: 2.440253973007202
Validation loss: 2.102580870351484

Epoch: 6| Step: 5
Training loss: 1.460073709487915
Validation loss: 2.0959571164141417

Epoch: 6| Step: 6
Training loss: 2.4385643005371094
Validation loss: 2.1291773319244385

Epoch: 6| Step: 7
Training loss: 2.0461502075195312
Validation loss: 2.0955392442723757

Epoch: 6| Step: 8
Training loss: 2.3357701301574707
Validation loss: 2.0971615955393803

Epoch: 6| Step: 9
Training loss: 2.0286662578582764
Validation loss: 2.0431700752627466

Epoch: 6| Step: 10
Training loss: 1.881165623664856
Validation loss: 2.0861848054393644

Epoch: 6| Step: 11
Training loss: 1.996476173400879
Validation loss: 2.0875771891686226

Epoch: 6| Step: 12
Training loss: 1.7954201698303223
Validation loss: 2.0902468337807605

Epoch: 6| Step: 13
Training loss: 1.024132490158081
Validation loss: 2.0745913777300107

Epoch: 359| Step: 0
Training loss: 2.5553808212280273
Validation loss: 2.105673743832496

Epoch: 6| Step: 1
Training loss: 1.60882568359375
Validation loss: 2.0907878157913045

Epoch: 6| Step: 2
Training loss: 1.793665885925293
Validation loss: 2.0991257108667845

Epoch: 6| Step: 3
Training loss: 1.8748197555541992
Validation loss: 2.079794704273183

Epoch: 6| Step: 4
Training loss: 2.4909298419952393
Validation loss: 2.0708713070038827

Epoch: 6| Step: 5
Training loss: 1.5719659328460693
Validation loss: 2.081275905332258

Epoch: 6| Step: 6
Training loss: 2.0709493160247803
Validation loss: 2.0602670138882053

Epoch: 6| Step: 7
Training loss: 2.0436670780181885
Validation loss: 2.096377649614888

Epoch: 6| Step: 8
Training loss: 2.2053892612457275
Validation loss: 2.0864086022941013

Epoch: 6| Step: 9
Training loss: 1.5247364044189453
Validation loss: 2.09087465142691

Epoch: 6| Step: 10
Training loss: 2.2138664722442627
Validation loss: 2.0864563257463518

Epoch: 6| Step: 11
Training loss: 2.1063103675842285
Validation loss: 2.0864237098283667

Epoch: 6| Step: 12
Training loss: 1.5072038173675537
Validation loss: 2.084613456520983

Epoch: 6| Step: 13
Training loss: 2.2129554748535156
Validation loss: 2.0892213993175055

Epoch: 360| Step: 0
Training loss: 1.841915488243103
Validation loss: 2.089168563965828

Epoch: 6| Step: 1
Training loss: 2.151641845703125
Validation loss: 2.07936450230178

Epoch: 6| Step: 2
Training loss: 2.0096940994262695
Validation loss: 2.081351564776513

Epoch: 6| Step: 3
Training loss: 2.0734364986419678
Validation loss: 2.073118516193923

Epoch: 6| Step: 4
Training loss: 1.8722591400146484
Validation loss: 2.0963832486060356

Epoch: 6| Step: 5
Training loss: 2.8397879600524902
Validation loss: 2.092025415871733

Epoch: 6| Step: 6
Training loss: 1.7489889860153198
Validation loss: 2.1018243066726194

Epoch: 6| Step: 7
Training loss: 2.0720057487487793
Validation loss: 2.10272539559231

Epoch: 6| Step: 8
Training loss: 1.3849834203720093
Validation loss: 2.105744041422362

Epoch: 6| Step: 9
Training loss: 1.935031771659851
Validation loss: 2.090203618490568

Epoch: 6| Step: 10
Training loss: 2.183511734008789
Validation loss: 2.088029148758099

Epoch: 6| Step: 11
Training loss: 1.5398794412612915
Validation loss: 2.0854877489869312

Epoch: 6| Step: 12
Training loss: 2.0130133628845215
Validation loss: 2.1042578438276887

Epoch: 6| Step: 13
Training loss: 2.640535354614258
Validation loss: 2.120184777885355

Epoch: 361| Step: 0
Training loss: 1.9090008735656738
Validation loss: 2.0944353816329793

Epoch: 6| Step: 1
Training loss: 2.0261621475219727
Validation loss: 2.097246253362266

Epoch: 6| Step: 2
Training loss: 2.396756649017334
Validation loss: 2.1123362164343558

Epoch: 6| Step: 3
Training loss: 2.463839054107666
Validation loss: 2.0742903396647465

Epoch: 6| Step: 4
Training loss: 2.375908613204956
Validation loss: 2.069248658354564

Epoch: 6| Step: 5
Training loss: 1.4233746528625488
Validation loss: 2.088635770223474

Epoch: 6| Step: 6
Training loss: 1.6023080348968506
Validation loss: 2.081693390364288

Epoch: 6| Step: 7
Training loss: 1.9063470363616943
Validation loss: 2.082919848862515

Epoch: 6| Step: 8
Training loss: 1.5712780952453613
Validation loss: 2.0454001785606466

Epoch: 6| Step: 9
Training loss: 1.8746203184127808
Validation loss: 2.0929178909588884

Epoch: 6| Step: 10
Training loss: 1.350209355354309
Validation loss: 2.1047891878312632

Epoch: 6| Step: 11
Training loss: 2.196845293045044
Validation loss: 2.093759952052947

Epoch: 6| Step: 12
Training loss: 2.5485501289367676
Validation loss: 2.061273587647305

Epoch: 6| Step: 13
Training loss: 2.129910469055176
Validation loss: 2.110626014330054

Epoch: 362| Step: 0
Training loss: 1.3339455127716064
Validation loss: 2.0885019148549726

Epoch: 6| Step: 1
Training loss: 2.3088626861572266
Validation loss: 2.1266895058334514

Epoch: 6| Step: 2
Training loss: 2.4916157722473145
Validation loss: 2.095070012154118

Epoch: 6| Step: 3
Training loss: 2.390423536300659
Validation loss: 2.1196458275600145

Epoch: 6| Step: 4
Training loss: 1.890317678451538
Validation loss: 2.1231090586672545

Epoch: 6| Step: 5
Training loss: 2.2411410808563232
Validation loss: 2.1143317581504903

Epoch: 6| Step: 6
Training loss: 1.4732290506362915
Validation loss: 2.123482588798769

Epoch: 6| Step: 7
Training loss: 2.168476104736328
Validation loss: 2.090703538669053

Epoch: 6| Step: 8
Training loss: 1.3114330768585205
Validation loss: 2.1184804234453427

Epoch: 6| Step: 9
Training loss: 2.051240921020508
Validation loss: 2.142935334995229

Epoch: 6| Step: 10
Training loss: 2.100850820541382
Validation loss: 2.0931428132518644

Epoch: 6| Step: 11
Training loss: 2.0076446533203125
Validation loss: 2.1022957473672848

Epoch: 6| Step: 12
Training loss: 1.8454135656356812
Validation loss: 2.1333855787913003

Epoch: 6| Step: 13
Training loss: 2.502535581588745
Validation loss: 2.0986372040164087

Epoch: 363| Step: 0
Training loss: 2.4743738174438477
Validation loss: 2.133592446645101

Epoch: 6| Step: 1
Training loss: 2.254089117050171
Validation loss: 2.1271924664897304

Epoch: 6| Step: 2
Training loss: 1.7484691143035889
Validation loss: 2.114187991747292

Epoch: 6| Step: 3
Training loss: 2.083914279937744
Validation loss: 2.0844986361842

Epoch: 6| Step: 4
Training loss: 2.2654788494110107
Validation loss: 2.1157120812323784

Epoch: 6| Step: 5
Training loss: 1.876033067703247
Validation loss: 2.110457651076778

Epoch: 6| Step: 6
Training loss: 1.7393885850906372
Validation loss: 2.0760502225609234

Epoch: 6| Step: 7
Training loss: 1.6908588409423828
Validation loss: 2.0713593780353503

Epoch: 6| Step: 8
Training loss: 1.9934802055358887
Validation loss: 2.0811861708600032

Epoch: 6| Step: 9
Training loss: 1.7883950471878052
Validation loss: 2.099592839517901

Epoch: 6| Step: 10
Training loss: 1.487792730331421
Validation loss: 2.0911889742779475

Epoch: 6| Step: 11
Training loss: 1.5196553468704224
Validation loss: 2.075576884772188

Epoch: 6| Step: 12
Training loss: 2.750105619430542
Validation loss: 2.100288462895219

Epoch: 6| Step: 13
Training loss: 2.29054856300354
Validation loss: 2.0679573243664158

Epoch: 364| Step: 0
Training loss: 2.270071268081665
Validation loss: 2.0697675494737524

Epoch: 6| Step: 1
Training loss: 1.9849720001220703
Validation loss: 2.058856230910106

Epoch: 6| Step: 2
Training loss: 1.7266077995300293
Validation loss: 2.062689754270738

Epoch: 6| Step: 3
Training loss: 1.301698923110962
Validation loss: 2.0679169111354376

Epoch: 6| Step: 4
Training loss: 2.448962688446045
Validation loss: 2.089602303761308

Epoch: 6| Step: 5
Training loss: 2.0808627605438232
Validation loss: 2.052370548248291

Epoch: 6| Step: 6
Training loss: 1.2719346284866333
Validation loss: 2.0584202094744612

Epoch: 6| Step: 7
Training loss: 1.7406229972839355
Validation loss: 2.1235695603073284

Epoch: 6| Step: 8
Training loss: 1.5806312561035156
Validation loss: 2.091892801305299

Epoch: 6| Step: 9
Training loss: 1.812677264213562
Validation loss: 2.074286873622607

Epoch: 6| Step: 10
Training loss: 2.020099639892578
Validation loss: 2.061235340692664

Epoch: 6| Step: 11
Training loss: 2.3626153469085693
Validation loss: 2.074875388094174

Epoch: 6| Step: 12
Training loss: 2.6476316452026367
Validation loss: 2.075063633662398

Epoch: 6| Step: 13
Training loss: 3.1045169830322266
Validation loss: 2.1106114823331117

Epoch: 365| Step: 0
Training loss: 1.951134443283081
Validation loss: 2.0898536110437043

Epoch: 6| Step: 1
Training loss: 1.9219263792037964
Validation loss: 2.0533780179997927

Epoch: 6| Step: 2
Training loss: 2.395007371902466
Validation loss: 2.1108019890323764

Epoch: 6| Step: 3
Training loss: 1.350414752960205
Validation loss: 2.084197026427074

Epoch: 6| Step: 4
Training loss: 2.6715762615203857
Validation loss: 2.09273793876812

Epoch: 6| Step: 5
Training loss: 1.2613799571990967
Validation loss: 2.073457730713711

Epoch: 6| Step: 6
Training loss: 1.7364319562911987
Validation loss: 2.1230819943130657

Epoch: 6| Step: 7
Training loss: 2.1815543174743652
Validation loss: 2.1074693484972884

Epoch: 6| Step: 8
Training loss: 1.9043278694152832
Validation loss: 2.0685117103720225

Epoch: 6| Step: 9
Training loss: 2.4198503494262695
Validation loss: 2.1025993542004655

Epoch: 6| Step: 10
Training loss: 1.93595552444458
Validation loss: 2.0783114715289046

Epoch: 6| Step: 11
Training loss: 1.910335898399353
Validation loss: 2.088672476430093

Epoch: 6| Step: 12
Training loss: 2.1219322681427
Validation loss: 2.1070539720596804

Epoch: 6| Step: 13
Training loss: 1.5160068273544312
Validation loss: 2.0916943627019084

Epoch: 366| Step: 0
Training loss: 1.550569772720337
Validation loss: 2.1078654848119265

Epoch: 6| Step: 1
Training loss: 2.2719178199768066
Validation loss: 2.064830856938516

Epoch: 6| Step: 2
Training loss: 2.2314584255218506
Validation loss: 2.1355296309276293

Epoch: 6| Step: 3
Training loss: 1.1840109825134277
Validation loss: 2.0827474850480274

Epoch: 6| Step: 4
Training loss: 2.323289632797241
Validation loss: 2.1237330923798265

Epoch: 6| Step: 5
Training loss: 1.9945098161697388
Validation loss: 2.1204223299539215

Epoch: 6| Step: 6
Training loss: 1.9423956871032715
Validation loss: 2.0810159047444663

Epoch: 6| Step: 7
Training loss: 1.610245704650879
Validation loss: 2.1133188124625915

Epoch: 6| Step: 8
Training loss: 2.0680081844329834
Validation loss: 2.1453723164014917

Epoch: 6| Step: 9
Training loss: 1.6776115894317627
Validation loss: 2.116537086425289

Epoch: 6| Step: 10
Training loss: 2.057140588760376
Validation loss: 2.1242712723311556

Epoch: 6| Step: 11
Training loss: 2.292227268218994
Validation loss: 2.1047195362788376

Epoch: 6| Step: 12
Training loss: 2.3587186336517334
Validation loss: 2.0863491399313814

Epoch: 6| Step: 13
Training loss: 2.128945827484131
Validation loss: 2.08721556714786

Epoch: 367| Step: 0
Training loss: 2.2510199546813965
Validation loss: 2.0462187182518745

Epoch: 6| Step: 1
Training loss: 1.768198013305664
Validation loss: 2.126583558256908

Epoch: 6| Step: 2
Training loss: 1.5588388442993164
Validation loss: 2.059840476641091

Epoch: 6| Step: 3
Training loss: 2.21907639503479
Validation loss: 2.079607945616527

Epoch: 6| Step: 4
Training loss: 1.1787720918655396
Validation loss: 2.0788955585930937

Epoch: 6| Step: 5
Training loss: 2.2526347637176514
Validation loss: 2.0814323784202657

Epoch: 6| Step: 6
Training loss: 1.810258388519287
Validation loss: 2.0700597122151363

Epoch: 6| Step: 7
Training loss: 1.7854063510894775
Validation loss: 2.05121023167846

Epoch: 6| Step: 8
Training loss: 2.5890817642211914
Validation loss: 2.0272476609035204

Epoch: 6| Step: 9
Training loss: 2.5115652084350586
Validation loss: 2.081579624965627

Epoch: 6| Step: 10
Training loss: 2.4618396759033203
Validation loss: 2.084010639498311

Epoch: 6| Step: 11
Training loss: 1.5852766036987305
Validation loss: 2.11013457082933

Epoch: 6| Step: 12
Training loss: 1.6438226699829102
Validation loss: 2.1196206667089976

Epoch: 6| Step: 13
Training loss: 2.1055192947387695
Validation loss: 2.089968212189213

Epoch: 368| Step: 0
Training loss: 1.9135260581970215
Validation loss: 2.0675881883149505

Epoch: 6| Step: 1
Training loss: 1.8150384426116943
Validation loss: 2.1127242990719375

Epoch: 6| Step: 2
Training loss: 2.0349507331848145
Validation loss: 2.1140775936906055

Epoch: 6| Step: 3
Training loss: 2.1756513118743896
Validation loss: 2.1008250136529245

Epoch: 6| Step: 4
Training loss: 1.852778434753418
Validation loss: 2.1170368617580784

Epoch: 6| Step: 5
Training loss: 1.9025061130523682
Validation loss: 2.121770533182288

Epoch: 6| Step: 6
Training loss: 2.5343000888824463
Validation loss: 2.125739054013324

Epoch: 6| Step: 7
Training loss: 1.660969614982605
Validation loss: 2.1069427215924827

Epoch: 6| Step: 8
Training loss: 1.3674911260604858
Validation loss: 2.136681725901942

Epoch: 6| Step: 9
Training loss: 2.0809860229492188
Validation loss: 2.1012554835247736

Epoch: 6| Step: 10
Training loss: 2.153581142425537
Validation loss: 2.1293266691187376

Epoch: 6| Step: 11
Training loss: 2.084913969039917
Validation loss: 2.120200962148687

Epoch: 6| Step: 12
Training loss: 2.120496988296509
Validation loss: 2.1022910917958906

Epoch: 6| Step: 13
Training loss: 1.8872898817062378
Validation loss: 2.091562440318446

Epoch: 369| Step: 0
Training loss: 2.0979061126708984
Validation loss: 2.1059579285242225

Epoch: 6| Step: 1
Training loss: 2.5581748485565186
Validation loss: 2.0837494045175533

Epoch: 6| Step: 2
Training loss: 1.8494739532470703
Validation loss: 2.1175075987333893

Epoch: 6| Step: 3
Training loss: 2.195018768310547
Validation loss: 2.1162855496970554

Epoch: 6| Step: 4
Training loss: 2.3901195526123047
Validation loss: 2.0923338679857153

Epoch: 6| Step: 5
Training loss: 2.2266407012939453
Validation loss: 2.102224173084382

Epoch: 6| Step: 6
Training loss: 1.409148931503296
Validation loss: 2.064888979799004

Epoch: 6| Step: 7
Training loss: 1.7586277723312378
Validation loss: 2.061037401999197

Epoch: 6| Step: 8
Training loss: 1.8472850322723389
Validation loss: 2.0900378791234826

Epoch: 6| Step: 9
Training loss: 1.8955473899841309
Validation loss: 2.0732936602766796

Epoch: 6| Step: 10
Training loss: 1.9808851480484009
Validation loss: 2.082120035284309

Epoch: 6| Step: 11
Training loss: 1.7100634574890137
Validation loss: 2.1002405035880303

Epoch: 6| Step: 12
Training loss: 2.0806503295898438
Validation loss: 2.1046335902265323

Epoch: 6| Step: 13
Training loss: 1.6680378913879395
Validation loss: 2.1072459925887403

Epoch: 370| Step: 0
Training loss: 2.2506022453308105
Validation loss: 2.093361403352471

Epoch: 6| Step: 1
Training loss: 2.0746450424194336
Validation loss: 2.102618525105138

Epoch: 6| Step: 2
Training loss: 2.116464853286743
Validation loss: 2.1125541758793656

Epoch: 6| Step: 3
Training loss: 1.929072380065918
Validation loss: 2.103634262597689

Epoch: 6| Step: 4
Training loss: 1.8618226051330566
Validation loss: 2.13988390404691

Epoch: 6| Step: 5
Training loss: 1.9294466972351074
Validation loss: 2.07099352728936

Epoch: 6| Step: 6
Training loss: 1.3498032093048096
Validation loss: 2.1199464233972694

Epoch: 6| Step: 7
Training loss: 1.6362096071243286
Validation loss: 2.088677239674394

Epoch: 6| Step: 8
Training loss: 2.1550445556640625
Validation loss: 2.14254008313661

Epoch: 6| Step: 9
Training loss: 1.683307409286499
Validation loss: 2.156814721322829

Epoch: 6| Step: 10
Training loss: 2.3406872749328613
Validation loss: 2.097846814381179

Epoch: 6| Step: 11
Training loss: 1.8362584114074707
Validation loss: 2.1148745641913465

Epoch: 6| Step: 12
Training loss: 2.708433151245117
Validation loss: 2.1148445913868565

Epoch: 6| Step: 13
Training loss: 1.7683947086334229
Validation loss: 2.099865130198899

Epoch: 371| Step: 0
Training loss: 1.75465726852417
Validation loss: 2.0979893105004424

Epoch: 6| Step: 1
Training loss: 1.5953166484832764
Validation loss: 2.111667130583076

Epoch: 6| Step: 2
Training loss: 2.17452335357666
Validation loss: 2.0844101931459162

Epoch: 6| Step: 3
Training loss: 2.0975828170776367
Validation loss: 2.0850674131865143

Epoch: 6| Step: 4
Training loss: 1.7030584812164307
Validation loss: 2.083110620898585

Epoch: 6| Step: 5
Training loss: 1.9560375213623047
Validation loss: 2.118365157035089

Epoch: 6| Step: 6
Training loss: 1.8419954776763916
Validation loss: 2.0601532074712936

Epoch: 6| Step: 7
Training loss: 1.9714794158935547
Validation loss: 2.0819903753137075

Epoch: 6| Step: 8
Training loss: 2.2006497383117676
Validation loss: 2.0830616130623767

Epoch: 6| Step: 9
Training loss: 1.7583128213882446
Validation loss: 2.090193807437856

Epoch: 6| Step: 10
Training loss: 2.3005521297454834
Validation loss: 2.058949721756802

Epoch: 6| Step: 11
Training loss: 2.298095703125
Validation loss: 2.06342726625422

Epoch: 6| Step: 12
Training loss: 2.1531543731689453
Validation loss: 2.1130318628844393

Epoch: 6| Step: 13
Training loss: 2.038058280944824
Validation loss: 2.0887695653464204

Epoch: 372| Step: 0
Training loss: 1.7532932758331299
Validation loss: 2.0834288276651853

Epoch: 6| Step: 1
Training loss: 2.1468732357025146
Validation loss: 2.0967332470801567

Epoch: 6| Step: 2
Training loss: 1.8913335800170898
Validation loss: 2.1132826036022556

Epoch: 6| Step: 3
Training loss: 1.8772144317626953
Validation loss: 2.1152924106967066

Epoch: 6| Step: 4
Training loss: 1.291623592376709
Validation loss: 2.1250561206571517

Epoch: 6| Step: 5
Training loss: 2.1829159259796143
Validation loss: 2.097224981554093

Epoch: 6| Step: 6
Training loss: 1.3244560956954956
Validation loss: 2.1305738110696115

Epoch: 6| Step: 7
Training loss: 2.0468192100524902
Validation loss: 2.086196671250046

Epoch: 6| Step: 8
Training loss: 3.2389259338378906
Validation loss: 2.0668849765613513

Epoch: 6| Step: 9
Training loss: 1.8000881671905518
Validation loss: 2.082837591889084

Epoch: 6| Step: 10
Training loss: 1.821841835975647
Validation loss: 2.104639135381227

Epoch: 6| Step: 11
Training loss: 1.8172956705093384
Validation loss: 2.087556113478958

Epoch: 6| Step: 12
Training loss: 2.3709535598754883
Validation loss: 2.132016404982536

Epoch: 6| Step: 13
Training loss: 2.2529165744781494
Validation loss: 2.0973437986066266

Epoch: 373| Step: 0
Training loss: 2.537363290786743
Validation loss: 2.089634482578565

Epoch: 6| Step: 1
Training loss: 1.774526834487915
Validation loss: 2.1040217594433854

Epoch: 6| Step: 2
Training loss: 2.255018949508667
Validation loss: 2.111094726029263

Epoch: 6| Step: 3
Training loss: 1.972615122795105
Validation loss: 2.0894248331746748

Epoch: 6| Step: 4
Training loss: 2.4568939208984375
Validation loss: 2.0932939642219135

Epoch: 6| Step: 5
Training loss: 1.7536050081253052
Validation loss: 2.0770896429656656

Epoch: 6| Step: 6
Training loss: 1.6742198467254639
Validation loss: 2.1380840322022796

Epoch: 6| Step: 7
Training loss: 2.2566041946411133
Validation loss: 2.082049972267561

Epoch: 6| Step: 8
Training loss: 1.5293546915054321
Validation loss: 2.0744607269123034

Epoch: 6| Step: 9
Training loss: 1.9524102210998535
Validation loss: 2.0424827452628844

Epoch: 6| Step: 10
Training loss: 2.0182158946990967
Validation loss: 2.089484663419826

Epoch: 6| Step: 11
Training loss: 1.9219310283660889
Validation loss: 2.054215121012862

Epoch: 6| Step: 12
Training loss: 2.155696153640747
Validation loss: 2.119291602924306

Epoch: 6| Step: 13
Training loss: 1.1171879768371582
Validation loss: 2.0756806224905033

Epoch: 374| Step: 0
Training loss: 1.5828081369400024
Validation loss: 2.0628097595707064

Epoch: 6| Step: 1
Training loss: 2.231274127960205
Validation loss: 2.108103334262807

Epoch: 6| Step: 2
Training loss: 2.0489563941955566
Validation loss: 2.0909567186909337

Epoch: 6| Step: 3
Training loss: 2.06650447845459
Validation loss: 2.1217123898126746

Epoch: 6| Step: 4
Training loss: 2.0396862030029297
Validation loss: 2.132343033308624

Epoch: 6| Step: 5
Training loss: 2.303176164627075
Validation loss: 2.1442514055518695

Epoch: 6| Step: 6
Training loss: 1.7912307977676392
Validation loss: 2.099612146295527

Epoch: 6| Step: 7
Training loss: 2.578212261199951
Validation loss: 2.1321508974157353

Epoch: 6| Step: 8
Training loss: 1.8369221687316895
Validation loss: 2.1330380567940335

Epoch: 6| Step: 9
Training loss: 1.4773290157318115
Validation loss: 2.0653513477694605

Epoch: 6| Step: 10
Training loss: 1.9702495336532593
Validation loss: 2.1080793180773334

Epoch: 6| Step: 11
Training loss: 1.5786082744598389
Validation loss: 2.101416157137963

Epoch: 6| Step: 12
Training loss: 2.425361156463623
Validation loss: 2.1227649950212046

Epoch: 6| Step: 13
Training loss: 1.220460295677185
Validation loss: 2.1158923128599763

Epoch: 375| Step: 0
Training loss: 2.284088611602783
Validation loss: 2.151564339155792

Epoch: 6| Step: 1
Training loss: 1.3705867528915405
Validation loss: 2.127275178509374

Epoch: 6| Step: 2
Training loss: 1.8728432655334473
Validation loss: 2.1174102073074668

Epoch: 6| Step: 3
Training loss: 1.7587765455245972
Validation loss: 2.088009657398347

Epoch: 6| Step: 4
Training loss: 2.088900566101074
Validation loss: 2.123844963248058

Epoch: 6| Step: 5
Training loss: 2.149197578430176
Validation loss: 2.089017311731974

Epoch: 6| Step: 6
Training loss: 1.6016714572906494
Validation loss: 2.086009984375328

Epoch: 6| Step: 7
Training loss: 1.9030567407608032
Validation loss: 2.1124182516528713

Epoch: 6| Step: 8
Training loss: 2.0594234466552734
Validation loss: 2.132396674925281

Epoch: 6| Step: 9
Training loss: 1.9514002799987793
Validation loss: 2.0843764838351997

Epoch: 6| Step: 10
Training loss: 1.9383409023284912
Validation loss: 2.089747210984589

Epoch: 6| Step: 11
Training loss: 2.000967502593994
Validation loss: 2.0640478505883166

Epoch: 6| Step: 12
Training loss: 2.2720770835876465
Validation loss: 2.11288853614561

Epoch: 6| Step: 13
Training loss: 2.238637924194336
Validation loss: 2.095479731918663

Epoch: 376| Step: 0
Training loss: 1.0698275566101074
Validation loss: 2.0886377980632167

Epoch: 6| Step: 1
Training loss: 2.3157200813293457
Validation loss: 2.0942915229387182

Epoch: 6| Step: 2
Training loss: 1.9306235313415527
Validation loss: 2.0894393715807187

Epoch: 6| Step: 3
Training loss: 1.7784172296524048
Validation loss: 2.0868769217562932

Epoch: 6| Step: 4
Training loss: 2.852950096130371
Validation loss: 2.077034410609994

Epoch: 6| Step: 5
Training loss: 2.0629849433898926
Validation loss: 2.0770215321612615

Epoch: 6| Step: 6
Training loss: 3.022876739501953
Validation loss: 2.081342815071024

Epoch: 6| Step: 7
Training loss: 1.5235258340835571
Validation loss: 2.0808025816435456

Epoch: 6| Step: 8
Training loss: 1.9874874353408813
Validation loss: 2.056466207709364

Epoch: 6| Step: 9
Training loss: 2.2076029777526855
Validation loss: 2.07478045007234

Epoch: 6| Step: 10
Training loss: 1.5961188077926636
Validation loss: 2.0880186826952043

Epoch: 6| Step: 11
Training loss: 1.5003492832183838
Validation loss: 2.1157263043106243

Epoch: 6| Step: 12
Training loss: 1.8846094608306885
Validation loss: 2.1132052739461265

Epoch: 6| Step: 13
Training loss: 1.2585686445236206
Validation loss: 2.0764026206026793

Epoch: 377| Step: 0
Training loss: 1.6748180389404297
Validation loss: 2.0759001073016914

Epoch: 6| Step: 1
Training loss: 1.6444268226623535
Validation loss: 2.08141654281206

Epoch: 6| Step: 2
Training loss: 1.842519998550415
Validation loss: 2.130346723782119

Epoch: 6| Step: 3
Training loss: 1.8861472606658936
Validation loss: 2.076798108316237

Epoch: 6| Step: 4
Training loss: 1.58502995967865
Validation loss: 2.1248190556803057

Epoch: 6| Step: 5
Training loss: 2.1704020500183105
Validation loss: 2.060353393195778

Epoch: 6| Step: 6
Training loss: 1.6669065952301025
Validation loss: 2.1204855544592744

Epoch: 6| Step: 7
Training loss: 1.6627954244613647
Validation loss: 2.057268475973478

Epoch: 6| Step: 8
Training loss: 2.2736997604370117
Validation loss: 2.0914874128116074

Epoch: 6| Step: 9
Training loss: 2.387373208999634
Validation loss: 2.0650094093814975

Epoch: 6| Step: 10
Training loss: 2.0943870544433594
Validation loss: 2.0987008694679505

Epoch: 6| Step: 11
Training loss: 2.369645595550537
Validation loss: 2.095584943730344

Epoch: 6| Step: 12
Training loss: 2.043501615524292
Validation loss: 2.084366862491895

Epoch: 6| Step: 13
Training loss: 2.8455753326416016
Validation loss: 2.1093905536077355

Epoch: 378| Step: 0
Training loss: 2.280045509338379
Validation loss: 2.0886331091644945

Epoch: 6| Step: 1
Training loss: 1.699497103691101
Validation loss: 2.06819958712465

Epoch: 6| Step: 2
Training loss: 1.7626128196716309
Validation loss: 2.092286261179114

Epoch: 6| Step: 3
Training loss: 2.1421918869018555
Validation loss: 2.1379087099464993

Epoch: 6| Step: 4
Training loss: 2.0889148712158203
Validation loss: 2.0866163674221245

Epoch: 6| Step: 5
Training loss: 1.8354198932647705
Validation loss: 2.0739669799804688

Epoch: 6| Step: 6
Training loss: 1.6889654397964478
Validation loss: 2.0763094194473757

Epoch: 6| Step: 7
Training loss: 2.3781895637512207
Validation loss: 2.100018773027646

Epoch: 6| Step: 8
Training loss: 2.205009698867798
Validation loss: 2.114816996359056

Epoch: 6| Step: 9
Training loss: 1.4172377586364746
Validation loss: 2.082401406380438

Epoch: 6| Step: 10
Training loss: 2.08685302734375
Validation loss: 2.1104461146939184

Epoch: 6| Step: 11
Training loss: 1.9348130226135254
Validation loss: 2.1343561667267994

Epoch: 6| Step: 12
Training loss: 1.9212028980255127
Validation loss: 2.124160720456031

Epoch: 6| Step: 13
Training loss: 1.8500404357910156
Validation loss: 2.1010154857430408

Epoch: 379| Step: 0
Training loss: 1.385286808013916
Validation loss: 2.1058364978400608

Epoch: 6| Step: 1
Training loss: 2.2251620292663574
Validation loss: 2.07592369920464

Epoch: 6| Step: 2
Training loss: 2.011692762374878
Validation loss: 2.0768017743223455

Epoch: 6| Step: 3
Training loss: 1.8035231828689575
Validation loss: 2.0875651118575886

Epoch: 6| Step: 4
Training loss: 1.56088125705719
Validation loss: 2.0523424815106135

Epoch: 6| Step: 5
Training loss: 1.557229995727539
Validation loss: 2.043879831990888

Epoch: 6| Step: 6
Training loss: 1.995708703994751
Validation loss: 2.062492839751705

Epoch: 6| Step: 7
Training loss: 2.5727503299713135
Validation loss: 2.0883236623579458

Epoch: 6| Step: 8
Training loss: 2.5298473834991455
Validation loss: 2.0781441632137505

Epoch: 6| Step: 9
Training loss: 2.071155071258545
Validation loss: 2.0575691410290298

Epoch: 6| Step: 10
Training loss: 1.514947772026062
Validation loss: 2.046697549922492

Epoch: 6| Step: 11
Training loss: 1.7164390087127686
Validation loss: 2.0867064973359466

Epoch: 6| Step: 12
Training loss: 2.0620546340942383
Validation loss: 2.074230400464868

Epoch: 6| Step: 13
Training loss: 2.7579407691955566
Validation loss: 2.0777649418000252

Epoch: 380| Step: 0
Training loss: 2.142477512359619
Validation loss: 2.0930158912494616

Epoch: 6| Step: 1
Training loss: 2.3739542961120605
Validation loss: 2.065694014231364

Epoch: 6| Step: 2
Training loss: 1.9785053730010986
Validation loss: 2.10097982165634

Epoch: 6| Step: 3
Training loss: 2.2994751930236816
Validation loss: 2.0872783071251324

Epoch: 6| Step: 4
Training loss: 1.7498542070388794
Validation loss: 2.114697772969482

Epoch: 6| Step: 5
Training loss: 2.006169557571411
Validation loss: 2.093889726105557

Epoch: 6| Step: 6
Training loss: 1.3962749242782593
Validation loss: 2.097469599016251

Epoch: 6| Step: 7
Training loss: 1.7380218505859375
Validation loss: 2.0844766478384695

Epoch: 6| Step: 8
Training loss: 2.4326629638671875
Validation loss: 2.0573130217931603

Epoch: 6| Step: 9
Training loss: 2.173046827316284
Validation loss: 2.049740472147542

Epoch: 6| Step: 10
Training loss: 1.752121925354004
Validation loss: 2.108230153719584

Epoch: 6| Step: 11
Training loss: 1.605644702911377
Validation loss: 2.090726390961678

Epoch: 6| Step: 12
Training loss: 1.5997769832611084
Validation loss: 2.112241562976632

Epoch: 6| Step: 13
Training loss: 2.286322832107544
Validation loss: 2.1092945350113737

Epoch: 381| Step: 0
Training loss: 1.8988226652145386
Validation loss: 2.108997183461343

Epoch: 6| Step: 1
Training loss: 1.5617775917053223
Validation loss: 2.081380601852171

Epoch: 6| Step: 2
Training loss: 1.4236443042755127
Validation loss: 2.1001666207467355

Epoch: 6| Step: 3
Training loss: 1.7541394233703613
Validation loss: 2.084701914941111

Epoch: 6| Step: 4
Training loss: 1.164609432220459
Validation loss: 2.0673011656730407

Epoch: 6| Step: 5
Training loss: 1.9197754859924316
Validation loss: 2.0779619191282537

Epoch: 6| Step: 6
Training loss: 1.9303324222564697
Validation loss: 2.1289791522487516

Epoch: 6| Step: 7
Training loss: 2.0576000213623047
Validation loss: 2.0594772779813377

Epoch: 6| Step: 8
Training loss: 2.5737123489379883
Validation loss: 2.1265786104304816

Epoch: 6| Step: 9
Training loss: 1.5989090204238892
Validation loss: 2.1213241443839124

Epoch: 6| Step: 10
Training loss: 2.302124500274658
Validation loss: 2.0727019207451933

Epoch: 6| Step: 11
Training loss: 3.104677677154541
Validation loss: 2.0865601724193943

Epoch: 6| Step: 12
Training loss: 2.0594589710235596
Validation loss: 2.0922132179301274

Epoch: 6| Step: 13
Training loss: 1.896890640258789
Validation loss: 2.0722840345034035

Epoch: 382| Step: 0
Training loss: 2.002105236053467
Validation loss: 2.1204019438835884

Epoch: 6| Step: 1
Training loss: 2.9036502838134766
Validation loss: 2.0903120604894494

Epoch: 6| Step: 2
Training loss: 2.0598740577697754
Validation loss: 2.105630733633554

Epoch: 6| Step: 3
Training loss: 1.9966069459915161
Validation loss: 2.1073954579650716

Epoch: 6| Step: 4
Training loss: 1.6725822687149048
Validation loss: 2.0844640859993557

Epoch: 6| Step: 5
Training loss: 2.156716823577881
Validation loss: 2.10624034174027

Epoch: 6| Step: 6
Training loss: 2.27594256401062
Validation loss: 2.1126046693453224

Epoch: 6| Step: 7
Training loss: 1.9653277397155762
Validation loss: 2.064768978344497

Epoch: 6| Step: 8
Training loss: 1.8033396005630493
Validation loss: 2.0732352784884873

Epoch: 6| Step: 9
Training loss: 2.448960781097412
Validation loss: 2.057835499445597

Epoch: 6| Step: 10
Training loss: 2.017251968383789
Validation loss: 2.061971925920056

Epoch: 6| Step: 11
Training loss: 1.3890042304992676
Validation loss: 2.0638016334144016

Epoch: 6| Step: 12
Training loss: 1.2862907648086548
Validation loss: 2.0747889088046167

Epoch: 6| Step: 13
Training loss: 0.9191733598709106
Validation loss: 2.1048644755476262

Epoch: 383| Step: 0
Training loss: 2.1416819095611572
Validation loss: 2.0953245162963867

Epoch: 6| Step: 1
Training loss: 1.387343406677246
Validation loss: 2.130369391492618

Epoch: 6| Step: 2
Training loss: 1.8975266218185425
Validation loss: 2.099887219808435

Epoch: 6| Step: 3
Training loss: 1.979397177696228
Validation loss: 2.131868939245901

Epoch: 6| Step: 4
Training loss: 1.8947522640228271
Validation loss: 2.0856892908773115

Epoch: 6| Step: 5
Training loss: 2.0489425659179688
Validation loss: 2.1073457707640944

Epoch: 6| Step: 6
Training loss: 1.7886021137237549
Validation loss: 2.13988915053747

Epoch: 6| Step: 7
Training loss: 2.2605173587799072
Validation loss: 2.155679977068337

Epoch: 6| Step: 8
Training loss: 1.3922425508499146
Validation loss: 2.1410344287913334

Epoch: 6| Step: 9
Training loss: 1.6662307977676392
Validation loss: 2.1441257102515108

Epoch: 6| Step: 10
Training loss: 2.0088539123535156
Validation loss: 2.1389682574938704

Epoch: 6| Step: 11
Training loss: 2.274996042251587
Validation loss: 2.1351426032281693

Epoch: 6| Step: 12
Training loss: 2.384955406188965
Validation loss: 2.1604591646502094

Epoch: 6| Step: 13
Training loss: 2.695068120956421
Validation loss: 2.1410702159327846

Epoch: 384| Step: 0
Training loss: 1.6181970834732056
Validation loss: 2.1314707366369103

Epoch: 6| Step: 1
Training loss: 1.628420352935791
Validation loss: 2.128870597449682

Epoch: 6| Step: 2
Training loss: 1.6637392044067383
Validation loss: 2.1086168161002536

Epoch: 6| Step: 3
Training loss: 2.2550878524780273
Validation loss: 2.1083125478477887

Epoch: 6| Step: 4
Training loss: 1.5293691158294678
Validation loss: 2.0756998369770665

Epoch: 6| Step: 5
Training loss: 1.821941614151001
Validation loss: 2.1122584958230295

Epoch: 6| Step: 6
Training loss: 2.2061963081359863
Validation loss: 2.0803354350469445

Epoch: 6| Step: 7
Training loss: 2.6431117057800293
Validation loss: 2.062374481590845

Epoch: 6| Step: 8
Training loss: 1.8094549179077148
Validation loss: 2.0686794211787563

Epoch: 6| Step: 9
Training loss: 1.6712260246276855
Validation loss: 2.0869292161797963

Epoch: 6| Step: 10
Training loss: 2.197384834289551
Validation loss: 2.091042128942346

Epoch: 6| Step: 11
Training loss: 2.1471757888793945
Validation loss: 2.0598334599566717

Epoch: 6| Step: 12
Training loss: 1.8368871212005615
Validation loss: 2.0493282733425016

Epoch: 6| Step: 13
Training loss: 2.490139961242676
Validation loss: 2.0741397770502235

Epoch: 385| Step: 0
Training loss: 2.6168510913848877
Validation loss: 2.04722527534731

Epoch: 6| Step: 1
Training loss: 1.0611495971679688
Validation loss: 2.0944864198725712

Epoch: 6| Step: 2
Training loss: 1.806833267211914
Validation loss: 2.0697367819406653

Epoch: 6| Step: 3
Training loss: 1.8205876350402832
Validation loss: 2.0837641877512776

Epoch: 6| Step: 4
Training loss: 2.101332664489746
Validation loss: 2.0480797098528956

Epoch: 6| Step: 5
Training loss: 1.972409725189209
Validation loss: 2.0919784576662126

Epoch: 6| Step: 6
Training loss: 1.8181922435760498
Validation loss: 2.080199804357303

Epoch: 6| Step: 7
Training loss: 2.0348687171936035
Validation loss: 2.132943866073444

Epoch: 6| Step: 8
Training loss: 2.471372127532959
Validation loss: 2.0537027261590444

Epoch: 6| Step: 9
Training loss: 2.058359384536743
Validation loss: 2.1239209790383615

Epoch: 6| Step: 10
Training loss: 2.0781073570251465
Validation loss: 2.085799427442653

Epoch: 6| Step: 11
Training loss: 1.5321143865585327
Validation loss: 2.0747620444143973

Epoch: 6| Step: 12
Training loss: 1.8415451049804688
Validation loss: 2.0656662525669223

Epoch: 6| Step: 13
Training loss: 2.072080612182617
Validation loss: 2.0744719364309825

Epoch: 386| Step: 0
Training loss: 1.8692021369934082
Validation loss: 2.0763399754801104

Epoch: 6| Step: 1
Training loss: 2.289740562438965
Validation loss: 2.109494916854366

Epoch: 6| Step: 2
Training loss: 1.363581657409668
Validation loss: 2.0918770682427192

Epoch: 6| Step: 3
Training loss: 2.1656718254089355
Validation loss: 2.0938839515050254

Epoch: 6| Step: 4
Training loss: 1.8800956010818481
Validation loss: 2.1002072031779955

Epoch: 6| Step: 5
Training loss: 1.6027852296829224
Validation loss: 2.0777095722895798

Epoch: 6| Step: 6
Training loss: 1.6092634201049805
Validation loss: 2.092315880201196

Epoch: 6| Step: 7
Training loss: 2.840334415435791
Validation loss: 2.084694944402223

Epoch: 6| Step: 8
Training loss: 1.4253078699111938
Validation loss: 2.040820103819652

Epoch: 6| Step: 9
Training loss: 2.1188180446624756
Validation loss: 2.1275865006190475

Epoch: 6| Step: 10
Training loss: 1.6907545328140259
Validation loss: 2.1159651048721804

Epoch: 6| Step: 11
Training loss: 2.1451334953308105
Validation loss: 2.0864804111501223

Epoch: 6| Step: 12
Training loss: 2.062882423400879
Validation loss: 2.124697995442216

Epoch: 6| Step: 13
Training loss: 1.9299182891845703
Validation loss: 2.0969707581304733

Epoch: 387| Step: 0
Training loss: 1.9416511058807373
Validation loss: 2.0840385857448784

Epoch: 6| Step: 1
Training loss: 1.9835517406463623
Validation loss: 2.058181808840844

Epoch: 6| Step: 2
Training loss: 2.1604833602905273
Validation loss: 2.0996934752310477

Epoch: 6| Step: 3
Training loss: 1.7347428798675537
Validation loss: 2.0833741900741414

Epoch: 6| Step: 4
Training loss: 2.950866222381592
Validation loss: 2.1280554340731714

Epoch: 6| Step: 5
Training loss: 2.5631580352783203
Validation loss: 2.0857940489246

Epoch: 6| Step: 6
Training loss: 1.6653425693511963
Validation loss: 2.145760349048081

Epoch: 6| Step: 7
Training loss: 1.7115755081176758
Validation loss: 2.089437892360072

Epoch: 6| Step: 8
Training loss: 2.291780710220337
Validation loss: 2.086917172196091

Epoch: 6| Step: 9
Training loss: 1.6064118146896362
Validation loss: 2.092747826730051

Epoch: 6| Step: 10
Training loss: 1.8376858234405518
Validation loss: 2.0882098982411046

Epoch: 6| Step: 11
Training loss: 1.0278949737548828
Validation loss: 2.0790560309604933

Epoch: 6| Step: 12
Training loss: 1.7786519527435303
Validation loss: 2.0765824907569477

Epoch: 6| Step: 13
Training loss: 1.8248716592788696
Validation loss: 2.103903611501058

Epoch: 388| Step: 0
Training loss: 1.955834150314331
Validation loss: 2.109173044081657

Epoch: 6| Step: 1
Training loss: 2.2819347381591797
Validation loss: 2.064161132740718

Epoch: 6| Step: 2
Training loss: 1.4307713508605957
Validation loss: 2.085676808511057

Epoch: 6| Step: 3
Training loss: 2.395775318145752
Validation loss: 2.119492478268121

Epoch: 6| Step: 4
Training loss: 2.18031907081604
Validation loss: 2.0687388194504606

Epoch: 6| Step: 5
Training loss: 1.4580881595611572
Validation loss: 2.090358708494453

Epoch: 6| Step: 6
Training loss: 1.559412956237793
Validation loss: 2.101560213232553

Epoch: 6| Step: 7
Training loss: 2.2464849948883057
Validation loss: 2.1007020524753037

Epoch: 6| Step: 8
Training loss: 2.217980146408081
Validation loss: 2.1233331964861963

Epoch: 6| Step: 9
Training loss: 2.319154977798462
Validation loss: 2.070390970476212

Epoch: 6| Step: 10
Training loss: 1.376739263534546
Validation loss: 2.0767835109464583

Epoch: 6| Step: 11
Training loss: 2.4235002994537354
Validation loss: 2.075629629114623

Epoch: 6| Step: 12
Training loss: 1.9985971450805664
Validation loss: 2.0827676634634695

Epoch: 6| Step: 13
Training loss: 0.9802908301353455
Validation loss: 2.0801801450790895

Epoch: 389| Step: 0
Training loss: 1.78010892868042
Validation loss: 2.1272381736386206

Epoch: 6| Step: 1
Training loss: 1.859142541885376
Validation loss: 2.072740267681819

Epoch: 6| Step: 2
Training loss: 2.044199228286743
Validation loss: 2.0784912006829375

Epoch: 6| Step: 3
Training loss: 1.0731990337371826
Validation loss: 2.06268229792195

Epoch: 6| Step: 4
Training loss: 2.6995320320129395
Validation loss: 2.062542441070721

Epoch: 6| Step: 5
Training loss: 1.9065049886703491
Validation loss: 2.0740820259176274

Epoch: 6| Step: 6
Training loss: 1.3450062274932861
Validation loss: 2.0696727434794107

Epoch: 6| Step: 7
Training loss: 2.056013345718384
Validation loss: 2.094504807585029

Epoch: 6| Step: 8
Training loss: 1.4183077812194824
Validation loss: 2.115592336141935

Epoch: 6| Step: 9
Training loss: 2.4081246852874756
Validation loss: 2.1089277434092697

Epoch: 6| Step: 10
Training loss: 2.4877841472625732
Validation loss: 2.138645610501689

Epoch: 6| Step: 11
Training loss: 2.4940218925476074
Validation loss: 2.0925274536173832

Epoch: 6| Step: 12
Training loss: 1.8603599071502686
Validation loss: 2.108273926601615

Epoch: 6| Step: 13
Training loss: 1.7534259557724
Validation loss: 2.1091540000771962

Epoch: 390| Step: 0
Training loss: 1.9159996509552002
Validation loss: 2.093600018050081

Epoch: 6| Step: 1
Training loss: 1.5229154825210571
Validation loss: 2.099193496088828

Epoch: 6| Step: 2
Training loss: 1.3594249486923218
Validation loss: 2.1054675091979322

Epoch: 6| Step: 3
Training loss: 2.250380277633667
Validation loss: 2.1109853752197756

Epoch: 6| Step: 4
Training loss: 2.171243906021118
Validation loss: 2.1232971734898065

Epoch: 6| Step: 5
Training loss: 2.1987109184265137
Validation loss: 2.0723087223627235

Epoch: 6| Step: 6
Training loss: 2.3934226036071777
Validation loss: 2.04504180723621

Epoch: 6| Step: 7
Training loss: 1.9914088249206543
Validation loss: 2.0963054856946393

Epoch: 6| Step: 8
Training loss: 1.8420993089675903
Validation loss: 2.1172228115861134

Epoch: 6| Step: 9
Training loss: 2.1204280853271484
Validation loss: 2.0545168051155667

Epoch: 6| Step: 10
Training loss: 1.8724006414413452
Validation loss: 2.098744466740598

Epoch: 6| Step: 11
Training loss: 1.3260400295257568
Validation loss: 2.0689214480820524

Epoch: 6| Step: 12
Training loss: 2.3500661849975586
Validation loss: 2.1017665529763825

Epoch: 6| Step: 13
Training loss: 1.7160181999206543
Validation loss: 2.1077235514117825

Epoch: 391| Step: 0
Training loss: 1.8726990222930908
Validation loss: 2.0990949420518774

Epoch: 6| Step: 1
Training loss: 1.4342825412750244
Validation loss: 2.0982674629457536

Epoch: 6| Step: 2
Training loss: 2.2305350303649902
Validation loss: 2.08335845060246

Epoch: 6| Step: 3
Training loss: 1.9196324348449707
Validation loss: 2.0386370625547183

Epoch: 6| Step: 4
Training loss: 1.869929552078247
Validation loss: 2.0834339998101674

Epoch: 6| Step: 5
Training loss: 2.03483247756958
Validation loss: 2.0868294931227163

Epoch: 6| Step: 6
Training loss: 2.738323211669922
Validation loss: 2.0871725390034337

Epoch: 6| Step: 7
Training loss: 2.138883113861084
Validation loss: 2.0836726132259575

Epoch: 6| Step: 8
Training loss: 1.6564533710479736
Validation loss: 2.0674301219242874

Epoch: 6| Step: 9
Training loss: 1.7845678329467773
Validation loss: 2.07567415442518

Epoch: 6| Step: 10
Training loss: 2.514218807220459
Validation loss: 2.0973715051527946

Epoch: 6| Step: 11
Training loss: 0.8909905552864075
Validation loss: 2.1046133220836682

Epoch: 6| Step: 12
Training loss: 1.7516295909881592
Validation loss: 2.093738379016999

Epoch: 6| Step: 13
Training loss: 2.1232194900512695
Validation loss: 2.0360711774518414

Epoch: 392| Step: 0
Training loss: 1.9989144802093506
Validation loss: 2.0978435739394157

Epoch: 6| Step: 1
Training loss: 1.8113126754760742
Validation loss: 2.0798995443569717

Epoch: 6| Step: 2
Training loss: 1.77480947971344
Validation loss: 2.1258412509836178

Epoch: 6| Step: 3
Training loss: 1.6814117431640625
Validation loss: 2.042570011590117

Epoch: 6| Step: 4
Training loss: 2.685713529586792
Validation loss: 2.0558512595392044

Epoch: 6| Step: 5
Training loss: 2.0460100173950195
Validation loss: 2.089035539216893

Epoch: 6| Step: 6
Training loss: 1.7141889333724976
Validation loss: 2.112647774398968

Epoch: 6| Step: 7
Training loss: 1.865802526473999
Validation loss: 2.067488653685457

Epoch: 6| Step: 8
Training loss: 1.9534382820129395
Validation loss: 2.0596397358884095

Epoch: 6| Step: 9
Training loss: 2.0980582237243652
Validation loss: 2.107882494567543

Epoch: 6| Step: 10
Training loss: 2.022282123565674
Validation loss: 2.0917801344266502

Epoch: 6| Step: 11
Training loss: 1.7778096199035645
Validation loss: 2.0888107156240814

Epoch: 6| Step: 12
Training loss: 1.7955414056777954
Validation loss: 2.0827882212977253

Epoch: 6| Step: 13
Training loss: 1.29813814163208
Validation loss: 2.082590926078058

Epoch: 393| Step: 0
Training loss: 0.7917495965957642
Validation loss: 2.0399489800135293

Epoch: 6| Step: 1
Training loss: 1.6321367025375366
Validation loss: 2.096699745424332

Epoch: 6| Step: 2
Training loss: 1.852764368057251
Validation loss: 2.109970259410079

Epoch: 6| Step: 3
Training loss: 1.9966349601745605
Validation loss: 2.0739684104919434

Epoch: 6| Step: 4
Training loss: 2.996323585510254
Validation loss: 2.090605043595837

Epoch: 6| Step: 5
Training loss: 1.852278232574463
Validation loss: 2.0731469533776723

Epoch: 6| Step: 6
Training loss: 2.24684739112854
Validation loss: 2.1010041365059475

Epoch: 6| Step: 7
Training loss: 2.11063814163208
Validation loss: 2.056220227672208

Epoch: 6| Step: 8
Training loss: 2.2009012699127197
Validation loss: 2.0665055859473442

Epoch: 6| Step: 9
Training loss: 2.0409374237060547
Validation loss: 2.0681304931640625

Epoch: 6| Step: 10
Training loss: 2.0258140563964844
Validation loss: 2.082455386397659

Epoch: 6| Step: 11
Training loss: 1.8234279155731201
Validation loss: 2.0941035901346514

Epoch: 6| Step: 12
Training loss: 1.6641849279403687
Validation loss: 2.0972654588760866

Epoch: 6| Step: 13
Training loss: 1.6758242845535278
Validation loss: 2.0581561634617467

Epoch: 394| Step: 0
Training loss: 2.172867774963379
Validation loss: 2.1075654081119004

Epoch: 6| Step: 1
Training loss: 3.017798662185669
Validation loss: 2.1035547487197386

Epoch: 6| Step: 2
Training loss: 1.5914934873580933
Validation loss: 2.1190395688497894

Epoch: 6| Step: 3
Training loss: 1.5547711849212646
Validation loss: 2.0774498601113596

Epoch: 6| Step: 4
Training loss: 2.048032283782959
Validation loss: 2.103778136673794

Epoch: 6| Step: 5
Training loss: 1.8494462966918945
Validation loss: 2.1227631876545567

Epoch: 6| Step: 6
Training loss: 1.5112745761871338
Validation loss: 2.0879615096635717

Epoch: 6| Step: 7
Training loss: 1.9716157913208008
Validation loss: 2.115063560906277

Epoch: 6| Step: 8
Training loss: 1.641416072845459
Validation loss: 2.1038311219984487

Epoch: 6| Step: 9
Training loss: 1.5653865337371826
Validation loss: 2.081554271841562

Epoch: 6| Step: 10
Training loss: 2.0200254917144775
Validation loss: 2.084270287585515

Epoch: 6| Step: 11
Training loss: 1.9332491159439087
Validation loss: 2.1205138903792187

Epoch: 6| Step: 12
Training loss: 2.0890920162200928
Validation loss: 2.0760487048856673

Epoch: 6| Step: 13
Training loss: 1.780251383781433
Validation loss: 2.0646317620431223

Epoch: 395| Step: 0
Training loss: 1.8270184993743896
Validation loss: 2.1023018513956377

Epoch: 6| Step: 1
Training loss: 2.2573845386505127
Validation loss: 2.112400383077642

Epoch: 6| Step: 2
Training loss: 2.075183153152466
Validation loss: 2.122183261379119

Epoch: 6| Step: 3
Training loss: 2.37361478805542
Validation loss: 2.0750580000621017

Epoch: 6| Step: 4
Training loss: 1.8514747619628906
Validation loss: 2.08945587629913

Epoch: 6| Step: 5
Training loss: 1.6449005603790283
Validation loss: 2.0920202501358522

Epoch: 6| Step: 6
Training loss: 1.9154391288757324
Validation loss: 2.0850964156530236

Epoch: 6| Step: 7
Training loss: 1.6823370456695557
Validation loss: 2.1007660922183784

Epoch: 6| Step: 8
Training loss: 1.9864017963409424
Validation loss: 2.0698226344200874

Epoch: 6| Step: 9
Training loss: 1.9916722774505615
Validation loss: 2.054326762435257

Epoch: 6| Step: 10
Training loss: 1.5549323558807373
Validation loss: 2.118039322155778

Epoch: 6| Step: 11
Training loss: 1.3180975914001465
Validation loss: 2.1149739296205583

Epoch: 6| Step: 12
Training loss: 2.4652106761932373
Validation loss: 2.097767412021596

Epoch: 6| Step: 13
Training loss: 2.175166606903076
Validation loss: 2.091971566600184

Epoch: 396| Step: 0
Training loss: 1.5204578638076782
Validation loss: 2.12723784805626

Epoch: 6| Step: 1
Training loss: 2.561614513397217
Validation loss: 2.0844500757032827

Epoch: 6| Step: 2
Training loss: 2.2121996879577637
Validation loss: 2.067793551311698

Epoch: 6| Step: 3
Training loss: 2.150937795639038
Validation loss: 2.080995482783164

Epoch: 6| Step: 4
Training loss: 2.3732433319091797
Validation loss: 2.075980911972702

Epoch: 6| Step: 5
Training loss: 1.5668046474456787
Validation loss: 2.0618505682996524

Epoch: 6| Step: 6
Training loss: 1.8208441734313965
Validation loss: 2.063328896799395

Epoch: 6| Step: 7
Training loss: 1.6627886295318604
Validation loss: 2.0858806461416264

Epoch: 6| Step: 8
Training loss: 1.527544617652893
Validation loss: 2.072532187225998

Epoch: 6| Step: 9
Training loss: 2.6248416900634766
Validation loss: 2.0668265306821434

Epoch: 6| Step: 10
Training loss: 2.0117268562316895
Validation loss: 2.07353219421961

Epoch: 6| Step: 11
Training loss: 0.9029688835144043
Validation loss: 2.043827105593938

Epoch: 6| Step: 12
Training loss: 1.7577381134033203
Validation loss: 2.0981879644496466

Epoch: 6| Step: 13
Training loss: 2.9043960571289062
Validation loss: 2.120150858356107

Epoch: 397| Step: 0
Training loss: 2.0632195472717285
Validation loss: 2.1022172640728694

Epoch: 6| Step: 1
Training loss: 2.6122241020202637
Validation loss: 2.0988933296613794

Epoch: 6| Step: 2
Training loss: 1.1648211479187012
Validation loss: 2.080957926729674

Epoch: 6| Step: 3
Training loss: 2.174495220184326
Validation loss: 2.0820185651061354

Epoch: 6| Step: 4
Training loss: 2.275099515914917
Validation loss: 2.0938001525017524

Epoch: 6| Step: 5
Training loss: 1.509197473526001
Validation loss: 2.0539566650185535

Epoch: 6| Step: 6
Training loss: 2.138672351837158
Validation loss: 2.111007158474256

Epoch: 6| Step: 7
Training loss: 1.8585493564605713
Validation loss: 2.106155727499275

Epoch: 6| Step: 8
Training loss: 2.310232162475586
Validation loss: 2.1203412984007146

Epoch: 6| Step: 9
Training loss: 1.7318315505981445
Validation loss: 2.107970071095292

Epoch: 6| Step: 10
Training loss: 1.4520466327667236
Validation loss: 2.0974627874230825

Epoch: 6| Step: 11
Training loss: 1.924382209777832
Validation loss: 2.08155478969697

Epoch: 6| Step: 12
Training loss: 1.3719362020492554
Validation loss: 2.089489052372594

Epoch: 6| Step: 13
Training loss: 2.3875670433044434
Validation loss: 2.0993648729016705

Epoch: 398| Step: 0
Training loss: 1.4768922328948975
Validation loss: 2.0740627268309235

Epoch: 6| Step: 1
Training loss: 2.9904673099517822
Validation loss: 2.0783610754115607

Epoch: 6| Step: 2
Training loss: 1.3486602306365967
Validation loss: 2.0780558534847793

Epoch: 6| Step: 3
Training loss: 2.081775426864624
Validation loss: 2.1054008135231594

Epoch: 6| Step: 4
Training loss: 1.7452664375305176
Validation loss: 2.0673046265878985

Epoch: 6| Step: 5
Training loss: 1.3137545585632324
Validation loss: 2.0309348913931076

Epoch: 6| Step: 6
Training loss: 1.6021974086761475
Validation loss: 2.0783959665606098

Epoch: 6| Step: 7
Training loss: 1.883907437324524
Validation loss: 2.082609509909025

Epoch: 6| Step: 8
Training loss: 1.5190696716308594
Validation loss: 2.101979803013545

Epoch: 6| Step: 9
Training loss: 2.6811466217041016
Validation loss: 2.1041960177883023

Epoch: 6| Step: 10
Training loss: 1.544881820678711
Validation loss: 2.087711949502268

Epoch: 6| Step: 11
Training loss: 1.7420169115066528
Validation loss: 2.071698029836019

Epoch: 6| Step: 12
Training loss: 2.695701837539673
Validation loss: 2.0836317257214616

Epoch: 6| Step: 13
Training loss: 1.9896913766860962
Validation loss: 2.068680076188939

Epoch: 399| Step: 0
Training loss: 1.5964305400848389
Validation loss: 2.0947946912498883

Epoch: 6| Step: 1
Training loss: 1.6796238422393799
Validation loss: 2.0986941937477357

Epoch: 6| Step: 2
Training loss: 2.880098342895508
Validation loss: 2.0859240485775854

Epoch: 6| Step: 3
Training loss: 1.592198371887207
Validation loss: 2.0957137564177155

Epoch: 6| Step: 4
Training loss: 1.8050615787506104
Validation loss: 2.10037164790656

Epoch: 6| Step: 5
Training loss: 2.3135035037994385
Validation loss: 2.1027056427412134

Epoch: 6| Step: 6
Training loss: 2.081362724304199
Validation loss: 2.11244624148133

Epoch: 6| Step: 7
Training loss: 1.9984657764434814
Validation loss: 2.0791978118240193

Epoch: 6| Step: 8
Training loss: 1.8168096542358398
Validation loss: 2.087536270900439

Epoch: 6| Step: 9
Training loss: 1.572852373123169
Validation loss: 2.101507067680359

Epoch: 6| Step: 10
Training loss: 1.9347648620605469
Validation loss: 2.1020460846603557

Epoch: 6| Step: 11
Training loss: 1.5194683074951172
Validation loss: 2.1071870429541475

Epoch: 6| Step: 12
Training loss: 2.261514663696289
Validation loss: 2.0800513785372496

Epoch: 6| Step: 13
Training loss: 1.7482430934906006
Validation loss: 2.1138590407627884

Epoch: 400| Step: 0
Training loss: 1.5045950412750244
Validation loss: 2.133201760630454

Epoch: 6| Step: 1
Training loss: 1.6732758283615112
Validation loss: 2.0966220184039046

Epoch: 6| Step: 2
Training loss: 2.386774778366089
Validation loss: 2.1206146388925533

Epoch: 6| Step: 3
Training loss: 2.5664267539978027
Validation loss: 2.127320458812098

Epoch: 6| Step: 4
Training loss: 1.7414612770080566
Validation loss: 2.09821327014636

Epoch: 6| Step: 5
Training loss: 1.514514446258545
Validation loss: 2.1004127533205095

Epoch: 6| Step: 6
Training loss: 2.143576145172119
Validation loss: 2.0700677364103255

Epoch: 6| Step: 7
Training loss: 2.2920961380004883
Validation loss: 2.0939672147074053

Epoch: 6| Step: 8
Training loss: 1.9025568962097168
Validation loss: 2.076719186639273

Epoch: 6| Step: 9
Training loss: 1.8762019872665405
Validation loss: 2.0661505755557807

Epoch: 6| Step: 10
Training loss: 1.8639945983886719
Validation loss: 2.1086257337242045

Epoch: 6| Step: 11
Training loss: 1.8118606805801392
Validation loss: 2.095719929664366

Epoch: 6| Step: 12
Training loss: 1.6214807033538818
Validation loss: 2.053333411934555

Epoch: 6| Step: 13
Training loss: 2.0208210945129395
Validation loss: 2.1024389036240114

Testing loss: 1.9517848385704888

Epoch: 1| Step: 0
Training loss: 4.1775054931640625
Validation loss: 5.768967843824817

Epoch: 6| Step: 1
Training loss: 4.7172675132751465
Validation loss: 5.765423979810489

Epoch: 6| Step: 2
Training loss: 4.8817853927612305
Validation loss: 5.758555925020608

Epoch: 6| Step: 3
Training loss: 5.9214606285095215
Validation loss: 5.75202735265096

Epoch: 6| Step: 4
Training loss: 5.867508411407471
Validation loss: 5.748020797647456

Epoch: 6| Step: 5
Training loss: 5.456331253051758
Validation loss: 5.742045525581606

Epoch: 6| Step: 6
Training loss: 6.185341835021973
Validation loss: 5.738328749133695

Epoch: 6| Step: 7
Training loss: 5.657702445983887
Validation loss: 5.733545626363447

Epoch: 6| Step: 8
Training loss: 5.728513717651367
Validation loss: 5.725869865827663

Epoch: 6| Step: 9
Training loss: 6.900259494781494
Validation loss: 5.723103420708769

Epoch: 6| Step: 10
Training loss: 4.443140029907227
Validation loss: 5.7163698801430325

Epoch: 6| Step: 11
Training loss: 5.601045608520508
Validation loss: 5.714018698661558

Epoch: 6| Step: 12
Training loss: 6.271419048309326
Validation loss: 5.707619349161784

Epoch: 6| Step: 13
Training loss: 5.683512210845947
Validation loss: 5.700317490485407

Epoch: 2| Step: 0
Training loss: 7.593541145324707
Validation loss: 5.6961304705630065

Epoch: 6| Step: 1
Training loss: 5.388962745666504
Validation loss: 5.690343031319239

Epoch: 6| Step: 2
Training loss: 6.31657600402832
Validation loss: 5.684848611072828

Epoch: 6| Step: 3
Training loss: 4.888360977172852
Validation loss: 5.679518699645996

Epoch: 6| Step: 4
Training loss: 4.669005393981934
Validation loss: 5.676924787541871

Epoch: 6| Step: 5
Training loss: 4.7877702713012695
Validation loss: 5.6682859902740805

Epoch: 6| Step: 6
Training loss: 6.173298358917236
Validation loss: 5.6637828426976355

Epoch: 6| Step: 7
Training loss: 6.533018112182617
Validation loss: 5.660649509840114

Epoch: 6| Step: 8
Training loss: 5.161992073059082
Validation loss: 5.65845376189037

Epoch: 6| Step: 9
Training loss: 4.612613201141357
Validation loss: 5.6497783609615855

Epoch: 6| Step: 10
Training loss: 4.850949764251709
Validation loss: 5.644096615493939

Epoch: 6| Step: 11
Training loss: 4.896697044372559
Validation loss: 5.6427788734436035

Epoch: 6| Step: 12
Training loss: 4.836910247802734
Validation loss: 5.634204228719075

Epoch: 6| Step: 13
Training loss: 5.8185954093933105
Validation loss: 5.627450332846693

Epoch: 3| Step: 0
Training loss: 4.275876998901367
Validation loss: 5.6245634017452115

Epoch: 6| Step: 1
Training loss: 5.200693130493164
Validation loss: 5.617805378411406

Epoch: 6| Step: 2
Training loss: 5.062453746795654
Validation loss: 5.611415473363733

Epoch: 6| Step: 3
Training loss: 5.882315635681152
Validation loss: 5.608742898510348

Epoch: 6| Step: 4
Training loss: 6.190053462982178
Validation loss: 5.601209743048555

Epoch: 6| Step: 5
Training loss: 5.64260196685791
Validation loss: 5.59851953547488

Epoch: 6| Step: 6
Training loss: 4.83823823928833
Validation loss: 5.589075226937571

Epoch: 6| Step: 7
Training loss: 5.008180141448975
Validation loss: 5.584449024610622

Epoch: 6| Step: 8
Training loss: 5.126103401184082
Validation loss: 5.576013052335349

Epoch: 6| Step: 9
Training loss: 6.584939956665039
Validation loss: 5.575169142856393

Epoch: 6| Step: 10
Training loss: 6.236795902252197
Validation loss: 5.567556991372057

Epoch: 6| Step: 11
Training loss: 5.759123802185059
Validation loss: 5.5598002659377235

Epoch: 6| Step: 12
Training loss: 4.465592861175537
Validation loss: 5.553905271714734

Epoch: 6| Step: 13
Training loss: 4.693760871887207
Validation loss: 5.547152790971982

Epoch: 4| Step: 0
Training loss: 6.6662211418151855
Validation loss: 5.541900957784345

Epoch: 6| Step: 1
Training loss: 4.9710869789123535
Validation loss: 5.537625969097179

Epoch: 6| Step: 2
Training loss: 4.661436557769775
Validation loss: 5.528622724676645

Epoch: 6| Step: 3
Training loss: 4.951473712921143
Validation loss: 5.519168889650735

Epoch: 6| Step: 4
Training loss: 4.782461166381836
Validation loss: 5.513463430507208

Epoch: 6| Step: 5
Training loss: 4.490567207336426
Validation loss: 5.505800995775449

Epoch: 6| Step: 6
Training loss: 5.372791290283203
Validation loss: 5.50266222799978

Epoch: 6| Step: 7
Training loss: 5.876400947570801
Validation loss: 5.491994883424493

Epoch: 6| Step: 8
Training loss: 4.472885608673096
Validation loss: 5.486792918174498

Epoch: 6| Step: 9
Training loss: 5.825016975402832
Validation loss: 5.479390903185773

Epoch: 6| Step: 10
Training loss: 5.269012451171875
Validation loss: 5.469933120153284

Epoch: 6| Step: 11
Training loss: 5.466311454772949
Validation loss: 5.462482565192766

Epoch: 6| Step: 12
Training loss: 6.188356399536133
Validation loss: 5.4561154047648115

Epoch: 6| Step: 13
Training loss: 4.680220127105713
Validation loss: 5.4472041847885295

Epoch: 5| Step: 0
Training loss: 4.822283744812012
Validation loss: 5.436914361933226

Epoch: 6| Step: 1
Training loss: 4.166748523712158
Validation loss: 5.428486044688891

Epoch: 6| Step: 2
Training loss: 5.123462677001953
Validation loss: 5.418531515265024

Epoch: 6| Step: 3
Training loss: 4.418360233306885
Validation loss: 5.414899749140585

Epoch: 6| Step: 4
Training loss: 5.201282978057861
Validation loss: 5.4072701520817255

Epoch: 6| Step: 5
Training loss: 5.276252746582031
Validation loss: 5.398389001046458

Epoch: 6| Step: 6
Training loss: 5.183207988739014
Validation loss: 5.386777154860958

Epoch: 6| Step: 7
Training loss: 5.959107875823975
Validation loss: 5.378812800171555

Epoch: 6| Step: 8
Training loss: 6.601213455200195
Validation loss: 5.3725654284159345

Epoch: 6| Step: 9
Training loss: 4.369932174682617
Validation loss: 5.361380187414026

Epoch: 6| Step: 10
Training loss: 6.090822696685791
Validation loss: 5.351970226533951

Epoch: 6| Step: 11
Training loss: 4.865472316741943
Validation loss: 5.347088716363394

Epoch: 6| Step: 12
Training loss: 5.688509941101074
Validation loss: 5.334839585006878

Epoch: 6| Step: 13
Training loss: 4.093295574188232
Validation loss: 5.329931336064493

Epoch: 6| Step: 0
Training loss: 4.838251113891602
Validation loss: 5.320002786574825

Epoch: 6| Step: 1
Training loss: 5.299757957458496
Validation loss: 5.309456825256348

Epoch: 6| Step: 2
Training loss: 4.992486476898193
Validation loss: 5.300297388466456

Epoch: 6| Step: 3
Training loss: 5.674506664276123
Validation loss: 5.293527751840571

Epoch: 6| Step: 4
Training loss: 4.136231422424316
Validation loss: 5.283094836819556

Epoch: 6| Step: 5
Training loss: 4.76702880859375
Validation loss: 5.265560662874612

Epoch: 6| Step: 6
Training loss: 6.21613883972168
Validation loss: 5.261033858022382

Epoch: 6| Step: 7
Training loss: 4.599728107452393
Validation loss: 5.249180065688266

Epoch: 6| Step: 8
Training loss: 5.873583793640137
Validation loss: 5.242797902835313

Epoch: 6| Step: 9
Training loss: 3.7870066165924072
Validation loss: 5.228228281903011

Epoch: 6| Step: 10
Training loss: 5.502864837646484
Validation loss: 5.222073078155518

Epoch: 6| Step: 11
Training loss: 4.757284164428711
Validation loss: 5.210735782500236

Epoch: 6| Step: 12
Training loss: 5.197504997253418
Validation loss: 5.200735040890273

Epoch: 6| Step: 13
Training loss: 4.695613861083984
Validation loss: 5.193081332791236

Epoch: 7| Step: 0
Training loss: 4.857893943786621
Validation loss: 5.17810933820663

Epoch: 6| Step: 1
Training loss: 5.285259246826172
Validation loss: 5.1727654908293035

Epoch: 6| Step: 2
Training loss: 5.284977912902832
Validation loss: 5.158177180956769

Epoch: 6| Step: 3
Training loss: 4.849453449249268
Validation loss: 5.150323165360318

Epoch: 6| Step: 4
Training loss: 5.266416549682617
Validation loss: 5.136121108967771

Epoch: 6| Step: 5
Training loss: 5.876619338989258
Validation loss: 5.124537724320606

Epoch: 6| Step: 6
Training loss: 4.875251293182373
Validation loss: 5.111047098713536

Epoch: 6| Step: 7
Training loss: 5.779590606689453
Validation loss: 5.104321684888614

Epoch: 6| Step: 8
Training loss: 3.545238494873047
Validation loss: 5.089301914297124

Epoch: 6| Step: 9
Training loss: 5.7993011474609375
Validation loss: 5.077659042932654

Epoch: 6| Step: 10
Training loss: 4.515409469604492
Validation loss: 5.0671679537783385

Epoch: 6| Step: 11
Training loss: 4.17218017578125
Validation loss: 5.055298579636441

Epoch: 6| Step: 12
Training loss: 3.6154048442840576
Validation loss: 5.041969981244815

Epoch: 6| Step: 13
Training loss: 4.456581115722656
Validation loss: 5.029398902770011

Epoch: 8| Step: 0
Training loss: 5.40811824798584
Validation loss: 5.022278426795878

Epoch: 6| Step: 1
Training loss: 3.954946517944336
Validation loss: 5.006307622437836

Epoch: 6| Step: 2
Training loss: 4.416070938110352
Validation loss: 4.994642714018463

Epoch: 6| Step: 3
Training loss: 5.890755653381348
Validation loss: 4.976648320433914

Epoch: 6| Step: 4
Training loss: 5.064950942993164
Validation loss: 4.964137410604826

Epoch: 6| Step: 5
Training loss: 4.93225622177124
Validation loss: 4.9528545769312045

Epoch: 6| Step: 6
Training loss: 4.37123966217041
Validation loss: 4.943139691506663

Epoch: 6| Step: 7
Training loss: 3.9914865493774414
Validation loss: 4.92538107082408

Epoch: 6| Step: 8
Training loss: 5.035346984863281
Validation loss: 4.910406076779929

Epoch: 6| Step: 9
Training loss: 4.772702217102051
Validation loss: 4.898367492101526

Epoch: 6| Step: 10
Training loss: 4.317251205444336
Validation loss: 4.886601586495677

Epoch: 6| Step: 11
Training loss: 4.422774791717529
Validation loss: 4.870627064858714

Epoch: 6| Step: 12
Training loss: 5.149746894836426
Validation loss: 4.860576937275548

Epoch: 6| Step: 13
Training loss: 3.796309471130371
Validation loss: 4.845765365067349

Epoch: 9| Step: 0
Training loss: 4.507047176361084
Validation loss: 4.8324496464062765

Epoch: 6| Step: 1
Training loss: 4.454497337341309
Validation loss: 4.819561394312048

Epoch: 6| Step: 2
Training loss: 4.9530439376831055
Validation loss: 4.801440997790265

Epoch: 6| Step: 3
Training loss: 4.5653839111328125
Validation loss: 4.789884885152181

Epoch: 6| Step: 4
Training loss: 5.280801773071289
Validation loss: 4.7724427048878

Epoch: 6| Step: 5
Training loss: 3.792423725128174
Validation loss: 4.755440014664845

Epoch: 6| Step: 6
Training loss: 4.382586479187012
Validation loss: 4.74090906881517

Epoch: 6| Step: 7
Training loss: 4.960516929626465
Validation loss: 4.726919527976744

Epoch: 6| Step: 8
Training loss: 5.9398884773254395
Validation loss: 4.716921288480041

Epoch: 6| Step: 9
Training loss: 4.799999713897705
Validation loss: 4.695449280482467

Epoch: 6| Step: 10
Training loss: 3.2450499534606934
Validation loss: 4.680572338001703

Epoch: 6| Step: 11
Training loss: 3.788064956665039
Validation loss: 4.66512873864943

Epoch: 6| Step: 12
Training loss: 4.71949577331543
Validation loss: 4.651993033706501

Epoch: 6| Step: 13
Training loss: 3.0655605792999268
Validation loss: 4.635164173700476

Epoch: 10| Step: 0
Training loss: 2.8855934143066406
Validation loss: 4.615248300695932

Epoch: 6| Step: 1
Training loss: 3.28804874420166
Validation loss: 4.601517518361409

Epoch: 6| Step: 2
Training loss: 4.9220051765441895
Validation loss: 4.58373632225939

Epoch: 6| Step: 3
Training loss: 5.0505266189575195
Validation loss: 4.569449014561151

Epoch: 6| Step: 4
Training loss: 5.147641181945801
Validation loss: 4.550197406481671

Epoch: 6| Step: 5
Training loss: 3.537053346633911
Validation loss: 4.532237411827169

Epoch: 6| Step: 6
Training loss: 3.9036481380462646
Validation loss: 4.514596985232446

Epoch: 6| Step: 7
Training loss: 4.132761001586914
Validation loss: 4.5002958672021025

Epoch: 6| Step: 8
Training loss: 3.790544033050537
Validation loss: 4.4856684746280795

Epoch: 6| Step: 9
Training loss: 5.388829708099365
Validation loss: 4.466740669742707

Epoch: 6| Step: 10
Training loss: 4.98883056640625
Validation loss: 4.447954536766134

Epoch: 6| Step: 11
Training loss: 3.0958690643310547
Validation loss: 4.422104107436313

Epoch: 6| Step: 12
Training loss: 4.64299201965332
Validation loss: 4.402699057773877

Epoch: 6| Step: 13
Training loss: 5.606081008911133
Validation loss: 4.391117911184987

Epoch: 11| Step: 0
Training loss: 3.941281795501709
Validation loss: 4.367440551839849

Epoch: 6| Step: 1
Training loss: 3.8549816608428955
Validation loss: 4.354456563149729

Epoch: 6| Step: 2
Training loss: 4.324160099029541
Validation loss: 4.327430494369999

Epoch: 6| Step: 3
Training loss: 3.183142900466919
Validation loss: 4.308146579291231

Epoch: 6| Step: 4
Training loss: 4.253007411956787
Validation loss: 4.291690980234454

Epoch: 6| Step: 5
Training loss: 3.3354954719543457
Validation loss: 4.273355196881038

Epoch: 6| Step: 6
Training loss: 3.8074169158935547
Validation loss: 4.2531396906862975

Epoch: 6| Step: 7
Training loss: 3.680891990661621
Validation loss: 4.225862097996537

Epoch: 6| Step: 8
Training loss: 4.127999305725098
Validation loss: 4.212507212033835

Epoch: 6| Step: 9
Training loss: 4.657385349273682
Validation loss: 4.1906808166093725

Epoch: 6| Step: 10
Training loss: 4.401039123535156
Validation loss: 4.17771779593601

Epoch: 6| Step: 11
Training loss: 4.487729072570801
Validation loss: 4.151039523463095

Epoch: 6| Step: 12
Training loss: 4.018298149108887
Validation loss: 4.131325824286348

Epoch: 6| Step: 13
Training loss: 4.5774993896484375
Validation loss: 4.115208410447644

Epoch: 12| Step: 0
Training loss: 4.23582124710083
Validation loss: 4.092279857204806

Epoch: 6| Step: 1
Training loss: 3.6942689418792725
Validation loss: 4.074574588447489

Epoch: 6| Step: 2
Training loss: 4.038835048675537
Validation loss: 4.054085467451362

Epoch: 6| Step: 3
Training loss: 3.5025205612182617
Validation loss: 4.041518108819121

Epoch: 6| Step: 4
Training loss: 4.150559425354004
Validation loss: 4.014626933682349

Epoch: 6| Step: 5
Training loss: 3.1618313789367676
Validation loss: 3.9985824861834125

Epoch: 6| Step: 6
Training loss: 4.065302848815918
Validation loss: 3.984320050926619

Epoch: 6| Step: 7
Training loss: 2.521906852722168
Validation loss: 3.9562752759584816

Epoch: 6| Step: 8
Training loss: 3.279176712036133
Validation loss: 3.941821849474343

Epoch: 6| Step: 9
Training loss: 4.786012649536133
Validation loss: 3.928160518728277

Epoch: 6| Step: 10
Training loss: 4.150263786315918
Validation loss: 3.904700494581653

Epoch: 6| Step: 11
Training loss: 3.108700752258301
Validation loss: 3.8804862037781747

Epoch: 6| Step: 12
Training loss: 4.514549255371094
Validation loss: 3.8533440559141097

Epoch: 6| Step: 13
Training loss: 3.8668878078460693
Validation loss: 3.847340419728269

Epoch: 13| Step: 0
Training loss: 2.870086431503296
Validation loss: 3.8121311690217707

Epoch: 6| Step: 1
Training loss: 4.364047050476074
Validation loss: 3.804437627074539

Epoch: 6| Step: 2
Training loss: 3.8239834308624268
Validation loss: 3.7882310036690003

Epoch: 6| Step: 3
Training loss: 4.641695976257324
Validation loss: 3.756231769438713

Epoch: 6| Step: 4
Training loss: 4.340092182159424
Validation loss: 3.736677236454461

Epoch: 6| Step: 5
Training loss: 3.5361111164093018
Validation loss: 3.71884379207447

Epoch: 6| Step: 6
Training loss: 3.4397454261779785
Validation loss: 3.6999364822141585

Epoch: 6| Step: 7
Training loss: 4.223357200622559
Validation loss: 3.6728011228704966

Epoch: 6| Step: 8
Training loss: 2.4046318531036377
Validation loss: 3.6615003693488335

Epoch: 6| Step: 9
Training loss: 2.9201831817626953
Validation loss: 3.644332298668482

Epoch: 6| Step: 10
Training loss: 4.1706976890563965
Validation loss: 3.626344293676397

Epoch: 6| Step: 11
Training loss: 2.0836455821990967
Validation loss: 3.6080637798514417

Epoch: 6| Step: 12
Training loss: 2.926875591278076
Validation loss: 3.5778336909509476

Epoch: 6| Step: 13
Training loss: 4.4116106033325195
Validation loss: 3.5661397211013304

Epoch: 14| Step: 0
Training loss: 3.534109115600586
Validation loss: 3.5266096027948524

Epoch: 6| Step: 1
Training loss: 4.142492771148682
Validation loss: 3.5199413043196484

Epoch: 6| Step: 2
Training loss: 2.264474630355835
Validation loss: 3.48024324704242

Epoch: 6| Step: 3
Training loss: 3.9193215370178223
Validation loss: 3.4675854944413707

Epoch: 6| Step: 4
Training loss: 4.452866554260254
Validation loss: 3.4453781394548315

Epoch: 6| Step: 5
Training loss: 3.147463083267212
Validation loss: 3.422852357228597

Epoch: 6| Step: 6
Training loss: 2.5825552940368652
Validation loss: 3.40398593102732

Epoch: 6| Step: 7
Training loss: 2.965264320373535
Validation loss: 3.3916688760121665

Epoch: 6| Step: 8
Training loss: 2.2990636825561523
Validation loss: 3.363242615935623

Epoch: 6| Step: 9
Training loss: 3.9993479251861572
Validation loss: 3.3420288588411067

Epoch: 6| Step: 10
Training loss: 2.4831442832946777
Validation loss: 3.3199881815141246

Epoch: 6| Step: 11
Training loss: 3.258350372314453
Validation loss: 3.308244287326772

Epoch: 6| Step: 12
Training loss: 2.8472743034362793
Validation loss: 3.2805882115517893

Epoch: 6| Step: 13
Training loss: 5.114487648010254
Validation loss: 3.2536944368834138

Epoch: 15| Step: 0
Training loss: 3.1131300926208496
Validation loss: 3.237592443343132

Epoch: 6| Step: 1
Training loss: 3.665480613708496
Validation loss: 3.213115543447515

Epoch: 6| Step: 2
Training loss: 2.454395055770874
Validation loss: 3.190731592075799

Epoch: 6| Step: 3
Training loss: 2.4516026973724365
Validation loss: 3.164364707085394

Epoch: 6| Step: 4
Training loss: 2.794433832168579
Validation loss: 3.152294866500362

Epoch: 6| Step: 5
Training loss: 2.5354185104370117
Validation loss: 3.129197161684754

Epoch: 6| Step: 6
Training loss: 3.4870972633361816
Validation loss: 3.112755749815254

Epoch: 6| Step: 7
Training loss: 2.7158820629119873
Validation loss: 3.090066776480726

Epoch: 6| Step: 8
Training loss: 3.576695442199707
Validation loss: 3.0715044057497414

Epoch: 6| Step: 9
Training loss: 2.704925298690796
Validation loss: 3.044225056966146

Epoch: 6| Step: 10
Training loss: 3.580811023712158
Validation loss: 3.02231183872428

Epoch: 6| Step: 11
Training loss: 3.6497457027435303
Validation loss: 3.0196285606712423

Epoch: 6| Step: 12
Training loss: 3.1562271118164062
Validation loss: 2.9888340427029516

Epoch: 6| Step: 13
Training loss: 3.1600840091705322
Validation loss: 2.967003773617488

Epoch: 16| Step: 0
Training loss: 2.764514923095703
Validation loss: 2.940726318667012

Epoch: 6| Step: 1
Training loss: 2.2057528495788574
Validation loss: 2.929928702692832

Epoch: 6| Step: 2
Training loss: 3.3793931007385254
Validation loss: 2.907613415871897

Epoch: 6| Step: 3
Training loss: 3.180817127227783
Validation loss: 2.879173594136392

Epoch: 6| Step: 4
Training loss: 3.6965885162353516
Validation loss: 2.864718929413826

Epoch: 6| Step: 5
Training loss: 3.0089492797851562
Validation loss: 2.8286040008708997

Epoch: 6| Step: 6
Training loss: 2.4163925647735596
Validation loss: 2.8157103446222123

Epoch: 6| Step: 7
Training loss: 2.5104479789733887
Validation loss: 2.796744433782434

Epoch: 6| Step: 8
Training loss: 2.767289400100708
Validation loss: 2.776737146480109

Epoch: 6| Step: 9
Training loss: 2.1156604290008545
Validation loss: 2.7588926463998775

Epoch: 6| Step: 10
Training loss: 2.845595359802246
Validation loss: 2.737817665582062

Epoch: 6| Step: 11
Training loss: 3.3116190433502197
Validation loss: 2.7436858889877156

Epoch: 6| Step: 12
Training loss: 2.7452406883239746
Validation loss: 2.700056532377838

Epoch: 6| Step: 13
Training loss: 3.6333301067352295
Validation loss: 2.6930892659771826

Epoch: 17| Step: 0
Training loss: 2.952815055847168
Validation loss: 2.6770505443696053

Epoch: 6| Step: 1
Training loss: 2.9594857692718506
Validation loss: 2.6602028031503

Epoch: 6| Step: 2
Training loss: 2.0428361892700195
Validation loss: 2.6581405260229625

Epoch: 6| Step: 3
Training loss: 2.4877772331237793
Validation loss: 2.6422867262235252

Epoch: 6| Step: 4
Training loss: 3.0475988388061523
Validation loss: 2.6162235583028486

Epoch: 6| Step: 5
Training loss: 3.0280158519744873
Validation loss: 2.596994169296757

Epoch: 6| Step: 6
Training loss: 3.434072732925415
Validation loss: 2.6033233647705405

Epoch: 6| Step: 7
Training loss: 2.76690673828125
Validation loss: 2.580614818039761

Epoch: 6| Step: 8
Training loss: 2.609175682067871
Validation loss: 2.564127447784588

Epoch: 6| Step: 9
Training loss: 2.392338752746582
Validation loss: 2.5518056423433366

Epoch: 6| Step: 10
Training loss: 1.9389617443084717
Validation loss: 2.523449646529331

Epoch: 6| Step: 11
Training loss: 2.9584569931030273
Validation loss: 2.513995732030561

Epoch: 6| Step: 12
Training loss: 2.845402240753174
Validation loss: 2.4974545663402927

Epoch: 6| Step: 13
Training loss: 2.292300224304199
Validation loss: 2.4957097217600834

Epoch: 18| Step: 0
Training loss: 2.4138009548187256
Validation loss: 2.4670695309997885

Epoch: 6| Step: 1
Training loss: 2.6294169425964355
Validation loss: 2.455235876062865

Epoch: 6| Step: 2
Training loss: 2.6656723022460938
Validation loss: 2.4442681804780038

Epoch: 6| Step: 3
Training loss: 3.1722118854522705
Validation loss: 2.4278613649388796

Epoch: 6| Step: 4
Training loss: 2.443056583404541
Validation loss: 2.40843447818551

Epoch: 6| Step: 5
Training loss: 2.689944267272949
Validation loss: 2.3995862853142524

Epoch: 6| Step: 6
Training loss: 2.908208131790161
Validation loss: 2.3959964834233767

Epoch: 6| Step: 7
Training loss: 2.347080945968628
Validation loss: 2.3682742016289824

Epoch: 6| Step: 8
Training loss: 2.53842830657959
Validation loss: 2.3519268933162896

Epoch: 6| Step: 9
Training loss: 2.379503011703491
Validation loss: 2.3487917761648855

Epoch: 6| Step: 10
Training loss: 2.304879665374756
Validation loss: 2.3232635221173688

Epoch: 6| Step: 11
Training loss: 2.51953125
Validation loss: 2.3218929767608643

Epoch: 6| Step: 12
Training loss: 2.7052388191223145
Validation loss: 2.286528315595401

Epoch: 6| Step: 13
Training loss: 2.2162466049194336
Validation loss: 2.2829017664796565

Epoch: 19| Step: 0
Training loss: 2.174355983734131
Validation loss: 2.2834453390490626

Epoch: 6| Step: 1
Training loss: 2.418592929840088
Validation loss: 2.2642318510240123

Epoch: 6| Step: 2
Training loss: 3.011014461517334
Validation loss: 2.2616199293444232

Epoch: 6| Step: 3
Training loss: 2.6457080841064453
Validation loss: 2.2572123889000184

Epoch: 6| Step: 4
Training loss: 2.887343406677246
Validation loss: 2.2375584263955393

Epoch: 6| Step: 5
Training loss: 2.0530314445495605
Validation loss: 2.2343919354100383

Epoch: 6| Step: 6
Training loss: 2.5360031127929688
Validation loss: 2.2137523210176857

Epoch: 6| Step: 7
Training loss: 3.2120299339294434
Validation loss: 2.2135369034223658

Epoch: 6| Step: 8
Training loss: 2.2990665435791016
Validation loss: 2.208925513811009

Epoch: 6| Step: 9
Training loss: 2.5462942123413086
Validation loss: 2.2065762473690893

Epoch: 6| Step: 10
Training loss: 2.125532388687134
Validation loss: 2.196043478545322

Epoch: 6| Step: 11
Training loss: 1.707764983177185
Validation loss: 2.1916419844473563

Epoch: 6| Step: 12
Training loss: 2.5184273719787598
Validation loss: 2.1666522000425603

Epoch: 6| Step: 13
Training loss: 2.642949342727661
Validation loss: 2.1860345050852787

Epoch: 20| Step: 0
Training loss: 2.6592679023742676
Validation loss: 2.168888666296518

Epoch: 6| Step: 1
Training loss: 2.935093402862549
Validation loss: 2.189506148779264

Epoch: 6| Step: 2
Training loss: 1.8563088178634644
Validation loss: 2.16722809883856

Epoch: 6| Step: 3
Training loss: 2.5298328399658203
Validation loss: 2.1574140876852055

Epoch: 6| Step: 4
Training loss: 2.025266647338867
Validation loss: 2.160508232731973

Epoch: 6| Step: 5
Training loss: 2.8547236919403076
Validation loss: 2.1464978712861256

Epoch: 6| Step: 6
Training loss: 2.2385852336883545
Validation loss: 2.1294474319745134

Epoch: 6| Step: 7
Training loss: 2.252239227294922
Validation loss: 2.14427714450385

Epoch: 6| Step: 8
Training loss: 2.788090229034424
Validation loss: 2.149967647367908

Epoch: 6| Step: 9
Training loss: 2.4766335487365723
Validation loss: 2.1596218950004986

Epoch: 6| Step: 10
Training loss: 2.0738086700439453
Validation loss: 2.143847337333105

Epoch: 6| Step: 11
Training loss: 2.4738094806671143
Validation loss: 2.164369372911351

Epoch: 6| Step: 12
Training loss: 2.3522071838378906
Validation loss: 2.132517240380728

Epoch: 6| Step: 13
Training loss: 2.6846110820770264
Validation loss: 2.147290406688567

Epoch: 21| Step: 0
Training loss: 2.708240509033203
Validation loss: 2.1398274949801865

Epoch: 6| Step: 1
Training loss: 2.409406900405884
Validation loss: 2.1445318050281976

Epoch: 6| Step: 2
Training loss: 1.9442651271820068
Validation loss: 2.1346169876795944

Epoch: 6| Step: 3
Training loss: 2.634032726287842
Validation loss: 2.1351151658642675

Epoch: 6| Step: 4
Training loss: 2.2276129722595215
Validation loss: 2.1174726460569646

Epoch: 6| Step: 5
Training loss: 2.029625177383423
Validation loss: 2.112489063252685

Epoch: 6| Step: 6
Training loss: 2.32767653465271
Validation loss: 2.1316748485770276

Epoch: 6| Step: 7
Training loss: 2.727973222732544
Validation loss: 2.1316657745709984

Epoch: 6| Step: 8
Training loss: 2.7717723846435547
Validation loss: 2.1308677875867454

Epoch: 6| Step: 9
Training loss: 2.3440191745758057
Validation loss: 2.1284832339132986

Epoch: 6| Step: 10
Training loss: 2.143934726715088
Validation loss: 2.101540219399237

Epoch: 6| Step: 11
Training loss: 2.6497766971588135
Validation loss: 2.095680823890112

Epoch: 6| Step: 12
Training loss: 2.624502182006836
Validation loss: 2.121333432453935

Epoch: 6| Step: 13
Training loss: 2.6536359786987305
Validation loss: 2.0975292190428703

Epoch: 22| Step: 0
Training loss: 2.744321346282959
Validation loss: 2.0987885459776847

Epoch: 6| Step: 1
Training loss: 2.4460978507995605
Validation loss: 2.097661279862927

Epoch: 6| Step: 2
Training loss: 1.7502511739730835
Validation loss: 2.1113938170094646

Epoch: 6| Step: 3
Training loss: 2.1583480834960938
Validation loss: 2.0856657566562777

Epoch: 6| Step: 4
Training loss: 2.5700480937957764
Validation loss: 2.0938039364353305

Epoch: 6| Step: 5
Training loss: 1.5982564687728882
Validation loss: 2.076328201960492

Epoch: 6| Step: 6
Training loss: 1.7396284341812134
Validation loss: 2.1041786991139895

Epoch: 6| Step: 7
Training loss: 3.1503024101257324
Validation loss: 2.1046125145368677

Epoch: 6| Step: 8
Training loss: 2.4260406494140625
Validation loss: 2.111592559404271

Epoch: 6| Step: 9
Training loss: 2.8374252319335938
Validation loss: 2.099769087247951

Epoch: 6| Step: 10
Training loss: 2.8620338439941406
Validation loss: 2.0907962501689954

Epoch: 6| Step: 11
Training loss: 3.223720073699951
Validation loss: 2.1112091989927393

Epoch: 6| Step: 12
Training loss: 1.9244189262390137
Validation loss: 2.099962217833406

Epoch: 6| Step: 13
Training loss: 2.5178112983703613
Validation loss: 2.1008068284680768

Epoch: 23| Step: 0
Training loss: 2.940530300140381
Validation loss: 2.0999516748612925

Epoch: 6| Step: 1
Training loss: 2.930445432662964
Validation loss: 2.070198946101691

Epoch: 6| Step: 2
Training loss: 2.18410062789917
Validation loss: 2.0979976807871172

Epoch: 6| Step: 3
Training loss: 2.355807304382324
Validation loss: 2.0958392286813385

Epoch: 6| Step: 4
Training loss: 2.6250410079956055
Validation loss: 2.090319207919541

Epoch: 6| Step: 5
Training loss: 2.0255961418151855
Validation loss: 2.086221630855273

Epoch: 6| Step: 6
Training loss: 2.465693473815918
Validation loss: 2.080955591253055

Epoch: 6| Step: 7
Training loss: 2.328862190246582
Validation loss: 2.075356129677065

Epoch: 6| Step: 8
Training loss: 2.2227797508239746
Validation loss: 2.0623567681158743

Epoch: 6| Step: 9
Training loss: 2.230107307434082
Validation loss: 2.0610942366302654

Epoch: 6| Step: 10
Training loss: 2.6447110176086426
Validation loss: 2.0684108605948825

Epoch: 6| Step: 11
Training loss: 2.4550535678863525
Validation loss: 2.0650341485136297

Epoch: 6| Step: 12
Training loss: 2.4891300201416016
Validation loss: 2.064799001139979

Epoch: 6| Step: 13
Training loss: 1.8529036045074463
Validation loss: 2.045938117529756

Epoch: 24| Step: 0
Training loss: 2.7154176235198975
Validation loss: 2.0596489111582437

Epoch: 6| Step: 1
Training loss: 2.3924312591552734
Validation loss: 2.051485211618485

Epoch: 6| Step: 2
Training loss: 2.660849094390869
Validation loss: 2.0589026404965307

Epoch: 6| Step: 3
Training loss: 1.917548418045044
Validation loss: 2.0753850847162227

Epoch: 6| Step: 4
Training loss: 2.741241216659546
Validation loss: 2.057167750532909

Epoch: 6| Step: 5
Training loss: 2.0839762687683105
Validation loss: 2.075883447483022

Epoch: 6| Step: 6
Training loss: 3.285404682159424
Validation loss: 2.0906141855383433

Epoch: 6| Step: 7
Training loss: 2.1329450607299805
Validation loss: 2.07008381043711

Epoch: 6| Step: 8
Training loss: 2.384493350982666
Validation loss: 2.061011986065936

Epoch: 6| Step: 9
Training loss: 2.6752591133117676
Validation loss: 2.078244114434847

Epoch: 6| Step: 10
Training loss: 2.3674709796905518
Validation loss: 2.0802548367490052

Epoch: 6| Step: 11
Training loss: 1.995072841644287
Validation loss: 2.1040343289734214

Epoch: 6| Step: 12
Training loss: 2.4841175079345703
Validation loss: 2.0860358002365276

Epoch: 6| Step: 13
Training loss: 2.0270957946777344
Validation loss: 2.0623574743988695

Epoch: 25| Step: 0
Training loss: 2.251455307006836
Validation loss: 2.092120343639005

Epoch: 6| Step: 1
Training loss: 2.614931106567383
Validation loss: 2.0889624370041715

Epoch: 6| Step: 2
Training loss: 1.6776313781738281
Validation loss: 2.068495055680634

Epoch: 6| Step: 3
Training loss: 1.7733395099639893
Validation loss: 2.0714622851341002

Epoch: 6| Step: 4
Training loss: 2.368797779083252
Validation loss: 2.086287595892465

Epoch: 6| Step: 5
Training loss: 2.3405771255493164
Validation loss: 2.0774267001818587

Epoch: 6| Step: 6
Training loss: 2.2506203651428223
Validation loss: 2.080992915297067

Epoch: 6| Step: 7
Training loss: 2.4575557708740234
Validation loss: 2.0637720631014917

Epoch: 6| Step: 8
Training loss: 2.5001940727233887
Validation loss: 2.0620871987394107

Epoch: 6| Step: 9
Training loss: 2.9280166625976562
Validation loss: 2.0646571651581795

Epoch: 6| Step: 10
Training loss: 3.0023531913757324
Validation loss: 2.0652902151948664

Epoch: 6| Step: 11
Training loss: 2.243833541870117
Validation loss: 2.057285319092453

Epoch: 6| Step: 12
Training loss: 2.8331198692321777
Validation loss: 2.0581689303921116

Epoch: 6| Step: 13
Training loss: 2.7658531665802
Validation loss: 2.077953586014368

Epoch: 26| Step: 0
Training loss: 2.659000873565674
Validation loss: 2.0780505570032264

Epoch: 6| Step: 1
Training loss: 2.694434642791748
Validation loss: 2.0698291460673013

Epoch: 6| Step: 2
Training loss: 2.642899990081787
Validation loss: 2.09072539114183

Epoch: 6| Step: 3
Training loss: 2.95892333984375
Validation loss: 2.0628662596466723

Epoch: 6| Step: 4
Training loss: 2.3716859817504883
Validation loss: 2.0565004835846605

Epoch: 6| Step: 5
Training loss: 1.8645710945129395
Validation loss: 2.0476056580902426

Epoch: 6| Step: 6
Training loss: 2.469594955444336
Validation loss: 2.065006932904643

Epoch: 6| Step: 7
Training loss: 2.0239572525024414
Validation loss: 2.061773564225884

Epoch: 6| Step: 8
Training loss: 3.2766013145446777
Validation loss: 2.0555249824318835

Epoch: 6| Step: 9
Training loss: 1.9394630193710327
Validation loss: 2.038960728594052

Epoch: 6| Step: 10
Training loss: 2.6512815952301025
Validation loss: 2.043622998781102

Epoch: 6| Step: 11
Training loss: 2.108933210372925
Validation loss: 2.0670080274663944

Epoch: 6| Step: 12
Training loss: 1.9849625825881958
Validation loss: 2.069459704942601

Epoch: 6| Step: 13
Training loss: 2.186328887939453
Validation loss: 2.0522866325993694

Epoch: 27| Step: 0
Training loss: 2.486114740371704
Validation loss: 2.0678728498438352

Epoch: 6| Step: 1
Training loss: 1.8869487047195435
Validation loss: 2.0641744828993276

Epoch: 6| Step: 2
Training loss: 1.9236886501312256
Validation loss: 2.0596342420065277

Epoch: 6| Step: 3
Training loss: 2.935288906097412
Validation loss: 2.0673297348842827

Epoch: 6| Step: 4
Training loss: 2.8996365070343018
Validation loss: 2.067591465929503

Epoch: 6| Step: 5
Training loss: 2.349684953689575
Validation loss: 2.072895029539703

Epoch: 6| Step: 6
Training loss: 1.8098981380462646
Validation loss: 2.0684148970470635

Epoch: 6| Step: 7
Training loss: 2.131456136703491
Validation loss: 2.0656271237199024

Epoch: 6| Step: 8
Training loss: 2.656373977661133
Validation loss: 2.0614175540144726

Epoch: 6| Step: 9
Training loss: 2.4066367149353027
Validation loss: 2.0734422873425227

Epoch: 6| Step: 10
Training loss: 3.2311654090881348
Validation loss: 2.082086491328414

Epoch: 6| Step: 11
Training loss: 1.8545706272125244
Validation loss: 2.04874659610051

Epoch: 6| Step: 12
Training loss: 2.7713184356689453
Validation loss: 2.059188844055258

Epoch: 6| Step: 13
Training loss: 2.297351121902466
Validation loss: 2.0680511856591828

Epoch: 28| Step: 0
Training loss: 2.729224681854248
Validation loss: 2.0823333135215183

Epoch: 6| Step: 1
Training loss: 2.1043200492858887
Validation loss: 2.0659564272049935

Epoch: 6| Step: 2
Training loss: 2.5547242164611816
Validation loss: 2.083444805555446

Epoch: 6| Step: 3
Training loss: 2.2810189723968506
Validation loss: 2.0924552627789077

Epoch: 6| Step: 4
Training loss: 2.710437059402466
Validation loss: 2.0663306072194088

Epoch: 6| Step: 5
Training loss: 2.1988589763641357
Validation loss: 2.055494095689507

Epoch: 6| Step: 6
Training loss: 1.7837986946105957
Validation loss: 2.0679441434080883

Epoch: 6| Step: 7
Training loss: 2.9976656436920166
Validation loss: 2.0986177895658757

Epoch: 6| Step: 8
Training loss: 2.929300308227539
Validation loss: 2.0961174118903374

Epoch: 6| Step: 9
Training loss: 2.0850813388824463
Validation loss: 2.0709712710431827

Epoch: 6| Step: 10
Training loss: 2.5929293632507324
Validation loss: 2.074639767728826

Epoch: 6| Step: 11
Training loss: 2.345111846923828
Validation loss: 2.0660783449808755

Epoch: 6| Step: 12
Training loss: 1.84212327003479
Validation loss: 2.0628978360083794

Epoch: 6| Step: 13
Training loss: 2.5220043659210205
Validation loss: 2.085552102775984

Epoch: 29| Step: 0
Training loss: 2.1319961547851562
Validation loss: 2.089219961115109

Epoch: 6| Step: 1
Training loss: 1.6221498250961304
Validation loss: 2.0877740639512257

Epoch: 6| Step: 2
Training loss: 2.067697048187256
Validation loss: 2.0770053555888515

Epoch: 6| Step: 3
Training loss: 3.2005984783172607
Validation loss: 2.0812106388871388

Epoch: 6| Step: 4
Training loss: 2.6570448875427246
Validation loss: 2.093646046935871

Epoch: 6| Step: 5
Training loss: 1.8875148296356201
Validation loss: 2.0712380486149944

Epoch: 6| Step: 6
Training loss: 2.4001731872558594
Validation loss: 2.0788019113643195

Epoch: 6| Step: 7
Training loss: 2.181772232055664
Validation loss: 2.064455762986214

Epoch: 6| Step: 8
Training loss: 2.9325504302978516
Validation loss: 2.08548717857689

Epoch: 6| Step: 9
Training loss: 1.7487109899520874
Validation loss: 2.1091440851970384

Epoch: 6| Step: 10
Training loss: 2.610384702682495
Validation loss: 2.077626330878145

Epoch: 6| Step: 11
Training loss: 2.1502535343170166
Validation loss: 2.0625292408850884

Epoch: 6| Step: 12
Training loss: 2.9854843616485596
Validation loss: 2.085681212845669

Epoch: 6| Step: 13
Training loss: 3.4442529678344727
Validation loss: 2.0836715082968436

Epoch: 30| Step: 0
Training loss: 2.3977303504943848
Validation loss: 2.0683372661631596

Epoch: 6| Step: 1
Training loss: 2.2032909393310547
Validation loss: 2.0753157113188054

Epoch: 6| Step: 2
Training loss: 2.9522738456726074
Validation loss: 2.059670394466769

Epoch: 6| Step: 3
Training loss: 3.048767566680908
Validation loss: 2.0870637932131366

Epoch: 6| Step: 4
Training loss: 2.182802200317383
Validation loss: 2.0901311828244116

Epoch: 6| Step: 5
Training loss: 2.584674835205078
Validation loss: 2.0600518795751754

Epoch: 6| Step: 6
Training loss: 1.743864893913269
Validation loss: 2.066371738269765

Epoch: 6| Step: 7
Training loss: 1.6804943084716797
Validation loss: 2.087618371491791

Epoch: 6| Step: 8
Training loss: 2.546821355819702
Validation loss: 2.060752958379766

Epoch: 6| Step: 9
Training loss: 2.6598973274230957
Validation loss: 2.065815043705766

Epoch: 6| Step: 10
Training loss: 2.3173792362213135
Validation loss: 2.0699426602291804

Epoch: 6| Step: 11
Training loss: 2.557037115097046
Validation loss: 2.0679540275245585

Epoch: 6| Step: 12
Training loss: 2.507887363433838
Validation loss: 2.0828983911903958

Epoch: 6| Step: 13
Training loss: 2.1692442893981934
Validation loss: 2.0676317471329884

Epoch: 31| Step: 0
Training loss: 2.302830219268799
Validation loss: 2.078776200612386

Epoch: 6| Step: 1
Training loss: 2.9448351860046387
Validation loss: 2.0638191443617626

Epoch: 6| Step: 2
Training loss: 1.7394764423370361
Validation loss: 2.08267665678455

Epoch: 6| Step: 3
Training loss: 2.386591911315918
Validation loss: 2.0839361939378964

Epoch: 6| Step: 4
Training loss: 2.4253592491149902
Validation loss: 2.0711068543054725

Epoch: 6| Step: 5
Training loss: 2.0080721378326416
Validation loss: 2.0809105570598314

Epoch: 6| Step: 6
Training loss: 2.800809860229492
Validation loss: 2.063832954693866

Epoch: 6| Step: 7
Training loss: 2.2737717628479004
Validation loss: 2.0775393747514292

Epoch: 6| Step: 8
Training loss: 1.8348885774612427
Validation loss: 2.0587295460444626

Epoch: 6| Step: 9
Training loss: 2.6691551208496094
Validation loss: 2.0951326431766635

Epoch: 6| Step: 10
Training loss: 2.424485206604004
Validation loss: 2.0796361943726898

Epoch: 6| Step: 11
Training loss: 2.2552363872528076
Validation loss: 2.0828963184869416

Epoch: 6| Step: 12
Training loss: 2.474601984024048
Validation loss: 2.0758405398297053

Epoch: 6| Step: 13
Training loss: 3.4732229709625244
Validation loss: 2.058893285771852

Epoch: 32| Step: 0
Training loss: 2.4093306064605713
Validation loss: 2.1031935394451184

Epoch: 6| Step: 1
Training loss: 1.780641794204712
Validation loss: 2.0836046434217885

Epoch: 6| Step: 2
Training loss: 1.9321025609970093
Validation loss: 2.0621755110320223

Epoch: 6| Step: 3
Training loss: 2.932384729385376
Validation loss: 2.0862859859261462

Epoch: 6| Step: 4
Training loss: 2.640317440032959
Validation loss: 2.079884559877457

Epoch: 6| Step: 5
Training loss: 2.636955976486206
Validation loss: 2.0851779983889673

Epoch: 6| Step: 6
Training loss: 1.876147747039795
Validation loss: 2.072907486269551

Epoch: 6| Step: 7
Training loss: 1.925925374031067
Validation loss: 2.090675461676813

Epoch: 6| Step: 8
Training loss: 2.417382001876831
Validation loss: 2.0839566017991755

Epoch: 6| Step: 9
Training loss: 2.806140184402466
Validation loss: 2.084424723861038

Epoch: 6| Step: 10
Training loss: 2.228163242340088
Validation loss: 2.090037876559842

Epoch: 6| Step: 11
Training loss: 2.6183323860168457
Validation loss: 2.0757828963700162

Epoch: 6| Step: 12
Training loss: 2.1024699211120605
Validation loss: 2.0864521508575766

Epoch: 6| Step: 13
Training loss: 3.63615083694458
Validation loss: 2.1031586995688816

Epoch: 33| Step: 0
Training loss: 2.1929988861083984
Validation loss: 2.101928352027811

Epoch: 6| Step: 1
Training loss: 2.0759356021881104
Validation loss: 2.0825818225901616

Epoch: 6| Step: 2
Training loss: 2.191432476043701
Validation loss: 2.084072919302089

Epoch: 6| Step: 3
Training loss: 2.113570213317871
Validation loss: 2.0748858580025296

Epoch: 6| Step: 4
Training loss: 2.5012102127075195
Validation loss: 2.0849729686655025

Epoch: 6| Step: 5
Training loss: 2.6631836891174316
Validation loss: 2.0856616445766982

Epoch: 6| Step: 6
Training loss: 3.075624704360962
Validation loss: 2.085366898967374

Epoch: 6| Step: 7
Training loss: 2.2192959785461426
Validation loss: 2.073543139683303

Epoch: 6| Step: 8
Training loss: 2.2900333404541016
Validation loss: 2.056345688399448

Epoch: 6| Step: 9
Training loss: 2.2376351356506348
Validation loss: 2.059976549558742

Epoch: 6| Step: 10
Training loss: 2.4948387145996094
Validation loss: 2.073655628388928

Epoch: 6| Step: 11
Training loss: 2.507270574569702
Validation loss: 2.067410510073426

Epoch: 6| Step: 12
Training loss: 3.011478900909424
Validation loss: 2.0711945103060816

Epoch: 6| Step: 13
Training loss: 1.6537445783615112
Validation loss: 2.0659777272132134

Epoch: 34| Step: 0
Training loss: 1.911076545715332
Validation loss: 2.1125019699014644

Epoch: 6| Step: 1
Training loss: 2.027527332305908
Validation loss: 2.0885318966322046

Epoch: 6| Step: 2
Training loss: 2.317905902862549
Validation loss: 2.0740309684507308

Epoch: 6| Step: 3
Training loss: 2.987530469894409
Validation loss: 2.080888658441523

Epoch: 6| Step: 4
Training loss: 2.7357728481292725
Validation loss: 2.052282366701352

Epoch: 6| Step: 5
Training loss: 2.3545377254486084
Validation loss: 2.0557177169348604

Epoch: 6| Step: 6
Training loss: 2.013124942779541
Validation loss: 2.081862138163659

Epoch: 6| Step: 7
Training loss: 2.486546039581299
Validation loss: 2.067606205581337

Epoch: 6| Step: 8
Training loss: 2.3689913749694824
Validation loss: 2.07101373775031

Epoch: 6| Step: 9
Training loss: 2.555776596069336
Validation loss: 2.067331947306151

Epoch: 6| Step: 10
Training loss: 2.426964044570923
Validation loss: 2.0726061815856607

Epoch: 6| Step: 11
Training loss: 2.441774845123291
Validation loss: 2.0636748780486402

Epoch: 6| Step: 12
Training loss: 2.652470111846924
Validation loss: 2.0758257348050355

Epoch: 6| Step: 13
Training loss: 1.8909236192703247
Validation loss: 2.0729276211031022

Epoch: 35| Step: 0
Training loss: 2.2118260860443115
Validation loss: 2.0694281875446277

Epoch: 6| Step: 1
Training loss: 3.098651170730591
Validation loss: 2.0790292985977663

Epoch: 6| Step: 2
Training loss: 1.622831106185913
Validation loss: 2.085068384806315

Epoch: 6| Step: 3
Training loss: 2.644935131072998
Validation loss: 2.0710909430698683

Epoch: 6| Step: 4
Training loss: 2.7447586059570312
Validation loss: 2.069289121576535

Epoch: 6| Step: 5
Training loss: 2.4854543209075928
Validation loss: 2.069752853403809

Epoch: 6| Step: 6
Training loss: 2.0163638591766357
Validation loss: 2.0716989399284444

Epoch: 6| Step: 7
Training loss: 2.4578778743743896
Validation loss: 2.0824014935442197

Epoch: 6| Step: 8
Training loss: 2.091775417327881
Validation loss: 2.077952697712888

Epoch: 6| Step: 9
Training loss: 2.0741257667541504
Validation loss: 2.0759105631100234

Epoch: 6| Step: 10
Training loss: 2.929441213607788
Validation loss: 2.0627766052881875

Epoch: 6| Step: 11
Training loss: 1.9165352582931519
Validation loss: 2.0554538978043424

Epoch: 6| Step: 12
Training loss: 2.6237082481384277
Validation loss: 2.0743495751452703

Epoch: 6| Step: 13
Training loss: 2.4932548999786377
Validation loss: 2.075437776504024

Epoch: 36| Step: 0
Training loss: 2.6262869834899902
Validation loss: 2.068257377993676

Epoch: 6| Step: 1
Training loss: 2.6623449325561523
Validation loss: 2.077278639680596

Epoch: 6| Step: 2
Training loss: 1.9087644815444946
Validation loss: 2.0747193649250972

Epoch: 6| Step: 3
Training loss: 3.3273911476135254
Validation loss: 2.0731116443552

Epoch: 6| Step: 4
Training loss: 2.1602625846862793
Validation loss: 2.0680417245434177

Epoch: 6| Step: 5
Training loss: 2.6123690605163574
Validation loss: 2.072428023943337

Epoch: 6| Step: 6
Training loss: 2.617039680480957
Validation loss: 2.0892233182025213

Epoch: 6| Step: 7
Training loss: 1.8444567918777466
Validation loss: 2.091951980385729

Epoch: 6| Step: 8
Training loss: 2.6501011848449707
Validation loss: 2.087363022629933

Epoch: 6| Step: 9
Training loss: 1.5613970756530762
Validation loss: 2.0765030127699657

Epoch: 6| Step: 10
Training loss: 2.4382176399230957
Validation loss: 2.075638835148145

Epoch: 6| Step: 11
Training loss: 1.9865334033966064
Validation loss: 2.091092373735161

Epoch: 6| Step: 12
Training loss: 2.6095833778381348
Validation loss: 2.074188957932175

Epoch: 6| Step: 13
Training loss: 2.3444857597351074
Validation loss: 2.0865288934400006

Epoch: 37| Step: 0
Training loss: 2.4415392875671387
Validation loss: 2.068844687554144

Epoch: 6| Step: 1
Training loss: 2.287306070327759
Validation loss: 2.0772520342180805

Epoch: 6| Step: 2
Training loss: 2.0333399772644043
Validation loss: 2.070259932548769

Epoch: 6| Step: 3
Training loss: 1.6679517030715942
Validation loss: 2.0709082747018464

Epoch: 6| Step: 4
Training loss: 2.0322861671447754
Validation loss: 2.040881683749537

Epoch: 6| Step: 5
Training loss: 3.0157406330108643
Validation loss: 2.075345431604693

Epoch: 6| Step: 6
Training loss: 2.197598934173584
Validation loss: 2.0555840846030944

Epoch: 6| Step: 7
Training loss: 2.647341728210449
Validation loss: 2.063025689894153

Epoch: 6| Step: 8
Training loss: 2.763357639312744
Validation loss: 2.041947280206988

Epoch: 6| Step: 9
Training loss: 2.253338575363159
Validation loss: 2.054463532663161

Epoch: 6| Step: 10
Training loss: 2.4668002128601074
Validation loss: 2.053722047036694

Epoch: 6| Step: 11
Training loss: 2.0876107215881348
Validation loss: 2.050581109139227

Epoch: 6| Step: 12
Training loss: 2.6570286750793457
Validation loss: 2.0588084446486605

Epoch: 6| Step: 13
Training loss: 3.153388261795044
Validation loss: 2.0667872223802792

Epoch: 38| Step: 0
Training loss: 2.0043816566467285
Validation loss: 2.047544684461368

Epoch: 6| Step: 1
Training loss: 2.3678226470947266
Validation loss: 2.029467293011245

Epoch: 6| Step: 2
Training loss: 2.1529102325439453
Validation loss: 2.0589278359566965

Epoch: 6| Step: 3
Training loss: 1.9628781080245972
Validation loss: 2.062026031555668

Epoch: 6| Step: 4
Training loss: 2.0246694087982178
Validation loss: 2.0546394009743967

Epoch: 6| Step: 5
Training loss: 2.926273822784424
Validation loss: 2.066796833469022

Epoch: 6| Step: 6
Training loss: 3.2356319427490234
Validation loss: 2.069426272505073

Epoch: 6| Step: 7
Training loss: 2.1085917949676514
Validation loss: 2.081743212156398

Epoch: 6| Step: 8
Training loss: 2.477292776107788
Validation loss: 2.0661389699546238

Epoch: 6| Step: 9
Training loss: 2.400726795196533
Validation loss: 2.072843506772031

Epoch: 6| Step: 10
Training loss: 2.7283480167388916
Validation loss: 2.0834198408229376

Epoch: 6| Step: 11
Training loss: 1.9720113277435303
Validation loss: 2.0718162008511123

Epoch: 6| Step: 12
Training loss: 2.6561269760131836
Validation loss: 2.0483399424501645

Epoch: 6| Step: 13
Training loss: 2.157970666885376
Validation loss: 2.0903401605544554

Epoch: 39| Step: 0
Training loss: 2.6717443466186523
Validation loss: 2.063372649172301

Epoch: 6| Step: 1
Training loss: 2.5973494052886963
Validation loss: 2.075636321498502

Epoch: 6| Step: 2
Training loss: 2.634463310241699
Validation loss: 2.0438721372235205

Epoch: 6| Step: 3
Training loss: 2.8401222229003906
Validation loss: 2.0580181972954863

Epoch: 6| Step: 4
Training loss: 2.742166042327881
Validation loss: 2.0680705283277776

Epoch: 6| Step: 5
Training loss: 1.7760123014450073
Validation loss: 2.096285864871035

Epoch: 6| Step: 6
Training loss: 2.8073198795318604
Validation loss: 2.0617691855276785

Epoch: 6| Step: 7
Training loss: 2.079228639602661
Validation loss: 2.047175266409433

Epoch: 6| Step: 8
Training loss: 1.64101243019104
Validation loss: 2.0925989715001916

Epoch: 6| Step: 9
Training loss: 2.1444129943847656
Validation loss: 2.0453157540290587

Epoch: 6| Step: 10
Training loss: 1.9496033191680908
Validation loss: 2.061559174650459

Epoch: 6| Step: 11
Training loss: 3.148904800415039
Validation loss: 2.056508051451816

Epoch: 6| Step: 12
Training loss: 2.351492166519165
Validation loss: 2.0750242228149087

Epoch: 6| Step: 13
Training loss: 1.2980973720550537
Validation loss: 2.0550076448789207

Epoch: 40| Step: 0
Training loss: 3.3048062324523926
Validation loss: 2.0614823115769254

Epoch: 6| Step: 1
Training loss: 2.058392286300659
Validation loss: 2.0728797938234065

Epoch: 6| Step: 2
Training loss: 2.443552017211914
Validation loss: 2.041680080916292

Epoch: 6| Step: 3
Training loss: 2.085658073425293
Validation loss: 2.044619530759832

Epoch: 6| Step: 4
Training loss: 2.3269102573394775
Validation loss: 2.0377743782535678

Epoch: 6| Step: 5
Training loss: 2.0277678966522217
Validation loss: 2.056238769203104

Epoch: 6| Step: 6
Training loss: 2.497013568878174
Validation loss: 2.0542892281727125

Epoch: 6| Step: 7
Training loss: 2.090812921524048
Validation loss: 2.057501221215853

Epoch: 6| Step: 8
Training loss: 2.4600725173950195
Validation loss: 2.0562029410434026

Epoch: 6| Step: 9
Training loss: 2.6736466884613037
Validation loss: 2.053684378183016

Epoch: 6| Step: 10
Training loss: 2.2972402572631836
Validation loss: 2.0379098487156693

Epoch: 6| Step: 11
Training loss: 2.6373167037963867
Validation loss: 2.0334941943486533

Epoch: 6| Step: 12
Training loss: 1.555863857269287
Validation loss: 2.065139970471782

Epoch: 6| Step: 13
Training loss: 2.5462844371795654
Validation loss: 2.0548490093600367

Epoch: 41| Step: 0
Training loss: 2.3165481090545654
Validation loss: 2.0334369431259813

Epoch: 6| Step: 1
Training loss: 2.1809144020080566
Validation loss: 2.0557429226495887

Epoch: 6| Step: 2
Training loss: 2.166726589202881
Validation loss: 2.046312515453626

Epoch: 6| Step: 3
Training loss: 2.3987975120544434
Validation loss: 2.0536382332924874

Epoch: 6| Step: 4
Training loss: 2.4075725078582764
Validation loss: 2.060245398552187

Epoch: 6| Step: 5
Training loss: 2.9326372146606445
Validation loss: 2.052817419011106

Epoch: 6| Step: 6
Training loss: 3.0627496242523193
Validation loss: 2.056110676898751

Epoch: 6| Step: 7
Training loss: 2.0015578269958496
Validation loss: 2.040141087706371

Epoch: 6| Step: 8
Training loss: 1.809063196182251
Validation loss: 2.0476224217363583

Epoch: 6| Step: 9
Training loss: 2.2575831413269043
Validation loss: 2.048136390665526

Epoch: 6| Step: 10
Training loss: 2.2336344718933105
Validation loss: 2.0658641528057795

Epoch: 6| Step: 11
Training loss: 2.8056633472442627
Validation loss: 2.0692848851603847

Epoch: 6| Step: 12
Training loss: 2.6988744735717773
Validation loss: 2.060552412463773

Epoch: 6| Step: 13
Training loss: 1.341931700706482
Validation loss: 2.064324411012793

Epoch: 42| Step: 0
Training loss: 2.4545021057128906
Validation loss: 2.0729162898114932

Epoch: 6| Step: 1
Training loss: 2.7747788429260254
Validation loss: 2.059316394149616

Epoch: 6| Step: 2
Training loss: 2.073728084564209
Validation loss: 2.0444530107641734

Epoch: 6| Step: 3
Training loss: 2.4221792221069336
Validation loss: 2.061102032661438

Epoch: 6| Step: 4
Training loss: 2.1853485107421875
Validation loss: 2.0514776886150403

Epoch: 6| Step: 5
Training loss: 2.749391794204712
Validation loss: 2.043344309253077

Epoch: 6| Step: 6
Training loss: 1.8844821453094482
Validation loss: 2.0508592231299287

Epoch: 6| Step: 7
Training loss: 2.5198493003845215
Validation loss: 2.067235013490082

Epoch: 6| Step: 8
Training loss: 2.6046876907348633
Validation loss: 2.0450608422679286

Epoch: 6| Step: 9
Training loss: 2.2858572006225586
Validation loss: 2.058958653480776

Epoch: 6| Step: 10
Training loss: 1.940767526626587
Validation loss: 2.0651983778963805

Epoch: 6| Step: 11
Training loss: 2.3587446212768555
Validation loss: 2.0514705975850425

Epoch: 6| Step: 12
Training loss: 2.111699104309082
Validation loss: 2.0407046220635854

Epoch: 6| Step: 13
Training loss: 2.9191784858703613
Validation loss: 2.0497262554783977

Epoch: 43| Step: 0
Training loss: 2.29529070854187
Validation loss: 2.057162482251403

Epoch: 6| Step: 1
Training loss: 3.2554197311401367
Validation loss: 2.0597498263082197

Epoch: 6| Step: 2
Training loss: 2.2508130073547363
Validation loss: 2.0511918375569005

Epoch: 6| Step: 3
Training loss: 1.3507137298583984
Validation loss: 2.0732412722802933

Epoch: 6| Step: 4
Training loss: 2.1066641807556152
Validation loss: 2.0559108077838855

Epoch: 6| Step: 5
Training loss: 2.2588653564453125
Validation loss: 2.0601232282577024

Epoch: 6| Step: 6
Training loss: 2.9801454544067383
Validation loss: 2.0512785578286774

Epoch: 6| Step: 7
Training loss: 2.086341619491577
Validation loss: 2.039297060299945

Epoch: 6| Step: 8
Training loss: 2.3995018005371094
Validation loss: 2.0366677866187146

Epoch: 6| Step: 9
Training loss: 2.8025476932525635
Validation loss: 2.0469905138015747

Epoch: 6| Step: 10
Training loss: 1.984514832496643
Validation loss: 2.0570008844457646

Epoch: 6| Step: 11
Training loss: 2.045849323272705
Validation loss: 2.0372435303144556

Epoch: 6| Step: 12
Training loss: 2.313509464263916
Validation loss: 2.038244357673071

Epoch: 6| Step: 13
Training loss: 2.920353412628174
Validation loss: 2.040057525839857

Epoch: 44| Step: 0
Training loss: 2.376293420791626
Validation loss: 2.0560357647557415

Epoch: 6| Step: 1
Training loss: 2.6758875846862793
Validation loss: 2.0597857018952728

Epoch: 6| Step: 2
Training loss: 2.7506327629089355
Validation loss: 2.0472534882125033

Epoch: 6| Step: 3
Training loss: 2.11921763420105
Validation loss: 2.03617529074351

Epoch: 6| Step: 4
Training loss: 2.1237385272979736
Validation loss: 2.0350304470267346

Epoch: 6| Step: 5
Training loss: 2.8661088943481445
Validation loss: 2.044923097856583

Epoch: 6| Step: 6
Training loss: 2.6380252838134766
Validation loss: 2.06380493410172

Epoch: 6| Step: 7
Training loss: 1.7213823795318604
Validation loss: 2.0389656520658925

Epoch: 6| Step: 8
Training loss: 2.2830212116241455
Validation loss: 2.0745072698080413

Epoch: 6| Step: 9
Training loss: 2.4098148345947266
Validation loss: 2.0659984824477986

Epoch: 6| Step: 10
Training loss: 2.3647167682647705
Validation loss: 2.0590118438966813

Epoch: 6| Step: 11
Training loss: 2.3983161449432373
Validation loss: 2.0695976800816034

Epoch: 6| Step: 12
Training loss: 2.1842684745788574
Validation loss: 2.0658751174967778

Epoch: 6| Step: 13
Training loss: 1.825417160987854
Validation loss: 2.061641541860437

Epoch: 45| Step: 0
Training loss: 2.048316478729248
Validation loss: 2.0493445614332795

Epoch: 6| Step: 1
Training loss: 2.9805052280426025
Validation loss: 2.0524146300490185

Epoch: 6| Step: 2
Training loss: 2.7009801864624023
Validation loss: 2.0824548467513053

Epoch: 6| Step: 3
Training loss: 1.8923686742782593
Validation loss: 2.0844440998569613

Epoch: 6| Step: 4
Training loss: 2.4226202964782715
Validation loss: 2.0914822650212113

Epoch: 6| Step: 5
Training loss: 2.5872678756713867
Validation loss: 2.0683329643741732

Epoch: 6| Step: 6
Training loss: 1.7673203945159912
Validation loss: 2.0827752031305784

Epoch: 6| Step: 7
Training loss: 2.8149523735046387
Validation loss: 2.0479379007893224

Epoch: 6| Step: 8
Training loss: 2.039121150970459
Validation loss: 2.055326484864758

Epoch: 6| Step: 9
Training loss: 2.3831920623779297
Validation loss: 2.0746697264332927

Epoch: 6| Step: 10
Training loss: 2.2172679901123047
Validation loss: 2.061143047066145

Epoch: 6| Step: 11
Training loss: 2.223021984100342
Validation loss: 2.06920959103492

Epoch: 6| Step: 12
Training loss: 2.5693140029907227
Validation loss: 2.0652802836510444

Epoch: 6| Step: 13
Training loss: 2.2266123294830322
Validation loss: 2.064202941874022

Epoch: 46| Step: 0
Training loss: 2.172339677810669
Validation loss: 2.078809074176255

Epoch: 6| Step: 1
Training loss: 2.6218957901000977
Validation loss: 2.058289615056848

Epoch: 6| Step: 2
Training loss: 2.1091599464416504
Validation loss: 2.066359968595607

Epoch: 6| Step: 3
Training loss: 2.2286128997802734
Validation loss: 2.087937343505121

Epoch: 6| Step: 4
Training loss: 2.9762072563171387
Validation loss: 2.0761242220478673

Epoch: 6| Step: 5
Training loss: 1.694205403327942
Validation loss: 2.068073081713851

Epoch: 6| Step: 6
Training loss: 2.610873222351074
Validation loss: 2.0779640469499814

Epoch: 6| Step: 7
Training loss: 1.7513936758041382
Validation loss: 2.0674321061821392

Epoch: 6| Step: 8
Training loss: 2.321197032928467
Validation loss: 2.080316317978726

Epoch: 6| Step: 9
Training loss: 1.9915827512741089
Validation loss: 2.0749658000084663

Epoch: 6| Step: 10
Training loss: 2.2563700675964355
Validation loss: 2.0678874369590514

Epoch: 6| Step: 11
Training loss: 2.9026379585266113
Validation loss: 2.078581101150923

Epoch: 6| Step: 12
Training loss: 2.6460556983947754
Validation loss: 2.0745655669960925

Epoch: 6| Step: 13
Training loss: 3.001291513442993
Validation loss: 2.0936243982725244

Epoch: 47| Step: 0
Training loss: 2.761629104614258
Validation loss: 2.0741776740679176

Epoch: 6| Step: 1
Training loss: 2.406726837158203
Validation loss: 2.1013013342375397

Epoch: 6| Step: 2
Training loss: 2.7538938522338867
Validation loss: 2.0614183564339914

Epoch: 6| Step: 3
Training loss: 2.422172784805298
Validation loss: 2.0538346882789367

Epoch: 6| Step: 4
Training loss: 2.071760654449463
Validation loss: 2.0691176152998403

Epoch: 6| Step: 5
Training loss: 2.354199171066284
Validation loss: 2.0641954919343353

Epoch: 6| Step: 6
Training loss: 1.8740198612213135
Validation loss: 2.055288427619524

Epoch: 6| Step: 7
Training loss: 2.9368507862091064
Validation loss: 2.068233464353828

Epoch: 6| Step: 8
Training loss: 2.220149278640747
Validation loss: 2.084353470033215

Epoch: 6| Step: 9
Training loss: 2.580498218536377
Validation loss: 2.066711087380686

Epoch: 6| Step: 10
Training loss: 1.4880930185317993
Validation loss: 2.063304465304139

Epoch: 6| Step: 11
Training loss: 2.2372140884399414
Validation loss: 2.0676941307642127

Epoch: 6| Step: 12
Training loss: 2.62068772315979
Validation loss: 2.0928463307760095

Epoch: 6| Step: 13
Training loss: 1.82503080368042
Validation loss: 2.0638046213375625

Epoch: 48| Step: 0
Training loss: 2.178333282470703
Validation loss: 2.063903295865623

Epoch: 6| Step: 1
Training loss: 2.592886447906494
Validation loss: 2.06243404265373

Epoch: 6| Step: 2
Training loss: 1.6662896871566772
Validation loss: 2.0624957341019825

Epoch: 6| Step: 3
Training loss: 1.7515443563461304
Validation loss: 2.0783061622291483

Epoch: 6| Step: 4
Training loss: 2.7222020626068115
Validation loss: 2.0770009499724194

Epoch: 6| Step: 5
Training loss: 2.6202001571655273
Validation loss: 2.066653654139529

Epoch: 6| Step: 6
Training loss: 1.7659245729446411
Validation loss: 2.0711877704948507

Epoch: 6| Step: 7
Training loss: 2.874382257461548
Validation loss: 2.0708312526825936

Epoch: 6| Step: 8
Training loss: 2.280059337615967
Validation loss: 2.041515939979143

Epoch: 6| Step: 9
Training loss: 2.270399570465088
Validation loss: 2.0602321368391796

Epoch: 6| Step: 10
Training loss: 2.5281224250793457
Validation loss: 2.0519364226248955

Epoch: 6| Step: 11
Training loss: 3.0909323692321777
Validation loss: 2.0528294758130143

Epoch: 6| Step: 12
Training loss: 2.270629405975342
Validation loss: 2.0602948255436395

Epoch: 6| Step: 13
Training loss: 1.8909791707992554
Validation loss: 2.05593035041645

Epoch: 49| Step: 0
Training loss: 2.5551257133483887
Validation loss: 2.0727376348228863

Epoch: 6| Step: 1
Training loss: 1.9656291007995605
Validation loss: 2.0629510725698164

Epoch: 6| Step: 2
Training loss: 2.3647267818450928
Validation loss: 2.058549032416395

Epoch: 6| Step: 3
Training loss: 1.8457884788513184
Validation loss: 2.0670260639600855

Epoch: 6| Step: 4
Training loss: 2.2539119720458984
Validation loss: 2.0515629373570925

Epoch: 6| Step: 5
Training loss: 1.8552707433700562
Validation loss: 2.0576105784344416

Epoch: 6| Step: 6
Training loss: 2.7040789127349854
Validation loss: 2.0399745689925326

Epoch: 6| Step: 7
Training loss: 3.2390248775482178
Validation loss: 2.0627511009093253

Epoch: 6| Step: 8
Training loss: 2.012270927429199
Validation loss: 2.0580528936078473

Epoch: 6| Step: 9
Training loss: 3.1887850761413574
Validation loss: 2.057861312743156

Epoch: 6| Step: 10
Training loss: 1.8237513303756714
Validation loss: 2.057946374339442

Epoch: 6| Step: 11
Training loss: 2.012547731399536
Validation loss: 2.0343132172861407

Epoch: 6| Step: 12
Training loss: 2.7676992416381836
Validation loss: 2.0451347776638564

Epoch: 6| Step: 13
Training loss: 1.9974528551101685
Validation loss: 2.0535019187517065

Epoch: 50| Step: 0
Training loss: 3.395749568939209
Validation loss: 2.0523780212607434

Epoch: 6| Step: 1
Training loss: 2.0610690116882324
Validation loss: 2.0580723324129657

Epoch: 6| Step: 2
Training loss: 2.4304792881011963
Validation loss: 2.060304041831724

Epoch: 6| Step: 3
Training loss: 2.444176197052002
Validation loss: 2.0557366160936255

Epoch: 6| Step: 4
Training loss: 2.355740547180176
Validation loss: 2.044520979286522

Epoch: 6| Step: 5
Training loss: 2.400752305984497
Validation loss: 2.0435047790568364

Epoch: 6| Step: 6
Training loss: 1.9992027282714844
Validation loss: 2.048655581730668

Epoch: 6| Step: 7
Training loss: 1.8213096857070923
Validation loss: 2.038903174861785

Epoch: 6| Step: 8
Training loss: 1.9881203174591064
Validation loss: 2.039025242610644

Epoch: 6| Step: 9
Training loss: 2.5497889518737793
Validation loss: 2.0474058274299867

Epoch: 6| Step: 10
Training loss: 2.371736526489258
Validation loss: 2.0468887026591966

Epoch: 6| Step: 11
Training loss: 2.1535897254943848
Validation loss: 2.064689620848625

Epoch: 6| Step: 12
Training loss: 2.629526138305664
Validation loss: 2.0423575408997072

Epoch: 6| Step: 13
Training loss: 1.9329895973205566
Validation loss: 2.063491138078833

Epoch: 51| Step: 0
Training loss: 2.4453933238983154
Validation loss: 2.0492491773379746

Epoch: 6| Step: 1
Training loss: 2.1307101249694824
Validation loss: 2.0636828048254854

Epoch: 6| Step: 2
Training loss: 2.9946646690368652
Validation loss: 2.074161551331961

Epoch: 6| Step: 3
Training loss: 2.3928720951080322
Validation loss: 2.0589901503696235

Epoch: 6| Step: 4
Training loss: 2.0494470596313477
Validation loss: 2.0560491623417025

Epoch: 6| Step: 5
Training loss: 2.2056188583374023
Validation loss: 2.080165240072435

Epoch: 6| Step: 6
Training loss: 1.8031524419784546
Validation loss: 2.069707760246851

Epoch: 6| Step: 7
Training loss: 2.7398111820220947
Validation loss: 2.0759517761968795

Epoch: 6| Step: 8
Training loss: 2.399602174758911
Validation loss: 2.094602410511304

Epoch: 6| Step: 9
Training loss: 1.9623594284057617
Validation loss: 2.0805629120078137

Epoch: 6| Step: 10
Training loss: 2.208648681640625
Validation loss: 2.0707130355219685

Epoch: 6| Step: 11
Training loss: 2.865739345550537
Validation loss: 2.0744937286582044

Epoch: 6| Step: 12
Training loss: 2.235123634338379
Validation loss: 2.082736833121187

Epoch: 6| Step: 13
Training loss: 1.9110777378082275
Validation loss: 2.0675620827623593

Epoch: 52| Step: 0
Training loss: 2.6509766578674316
Validation loss: 2.0880945651761946

Epoch: 6| Step: 1
Training loss: 2.412522554397583
Validation loss: 2.0640177649836384

Epoch: 6| Step: 2
Training loss: 2.372645854949951
Validation loss: 2.0887297199618433

Epoch: 6| Step: 3
Training loss: 1.7801827192306519
Validation loss: 2.0975193413355018

Epoch: 6| Step: 4
Training loss: 1.978705883026123
Validation loss: 2.0613881054744927

Epoch: 6| Step: 5
Training loss: 2.2309889793395996
Validation loss: 2.082834432202001

Epoch: 6| Step: 6
Training loss: 1.7819864749908447
Validation loss: 2.0629440904945455

Epoch: 6| Step: 7
Training loss: 2.782264232635498
Validation loss: 2.0681265836120932

Epoch: 6| Step: 8
Training loss: 2.688157081604004
Validation loss: 2.071637192080098

Epoch: 6| Step: 9
Training loss: 1.653024673461914
Validation loss: 2.0674340314762567

Epoch: 6| Step: 10
Training loss: 1.7006678581237793
Validation loss: 2.0648949197543565

Epoch: 6| Step: 11
Training loss: 2.988401412963867
Validation loss: 2.064128138685739

Epoch: 6| Step: 12
Training loss: 2.6123156547546387
Validation loss: 2.0766185265715404

Epoch: 6| Step: 13
Training loss: 3.3735122680664062
Validation loss: 2.0650854315809024

Epoch: 53| Step: 0
Training loss: 2.1689186096191406
Validation loss: 2.059370735640167

Epoch: 6| Step: 1
Training loss: 2.3475241661071777
Validation loss: 2.061193830223494

Epoch: 6| Step: 2
Training loss: 1.898160457611084
Validation loss: 2.0543648901806084

Epoch: 6| Step: 3
Training loss: 1.7650821208953857
Validation loss: 2.054626468689211

Epoch: 6| Step: 4
Training loss: 2.1913487911224365
Validation loss: 2.0532580652544574

Epoch: 6| Step: 5
Training loss: 2.2502243518829346
Validation loss: 2.0502478820021435

Epoch: 6| Step: 6
Training loss: 2.3619730472564697
Validation loss: 2.044616501818421

Epoch: 6| Step: 7
Training loss: 1.8585227727890015
Validation loss: 2.0550975517560075

Epoch: 6| Step: 8
Training loss: 1.766474962234497
Validation loss: 2.0346962303243656

Epoch: 6| Step: 9
Training loss: 2.770554780960083
Validation loss: 2.0403220576624714

Epoch: 6| Step: 10
Training loss: 2.7135701179504395
Validation loss: 2.0536838603276077

Epoch: 6| Step: 11
Training loss: 3.0672216415405273
Validation loss: 2.0427687834667903

Epoch: 6| Step: 12
Training loss: 3.0650534629821777
Validation loss: 2.035869045924115

Epoch: 6| Step: 13
Training loss: 2.1639463901519775
Validation loss: 2.0728586694245696

Epoch: 54| Step: 0
Training loss: 2.81211519241333
Validation loss: 2.0704705971543507

Epoch: 6| Step: 1
Training loss: 2.97082257270813
Validation loss: 2.0680297267052437

Epoch: 6| Step: 2
Training loss: 2.5752158164978027
Validation loss: 2.076669522511062

Epoch: 6| Step: 3
Training loss: 1.6233415603637695
Validation loss: 2.07944469810814

Epoch: 6| Step: 4
Training loss: 2.0494887828826904
Validation loss: 2.0928111794174358

Epoch: 6| Step: 5
Training loss: 2.104374885559082
Validation loss: 2.088881041413994

Epoch: 6| Step: 6
Training loss: 2.0217394828796387
Validation loss: 2.1029183377501783

Epoch: 6| Step: 7
Training loss: 1.7500988245010376
Validation loss: 2.089226871408442

Epoch: 6| Step: 8
Training loss: 2.3160107135772705
Validation loss: 2.082869452814902

Epoch: 6| Step: 9
Training loss: 3.131598949432373
Validation loss: 2.106704642695765

Epoch: 6| Step: 10
Training loss: 3.1354541778564453
Validation loss: 2.1047883161934475

Epoch: 6| Step: 11
Training loss: 1.8366620540618896
Validation loss: 2.100258351654135

Epoch: 6| Step: 12
Training loss: 1.9534460306167603
Validation loss: 2.100849114438539

Epoch: 6| Step: 13
Training loss: 2.185234546661377
Validation loss: 2.088530389211511

Epoch: 55| Step: 0
Training loss: 2.8753814697265625
Validation loss: 2.097997393659366

Epoch: 6| Step: 1
Training loss: 1.7838162183761597
Validation loss: 2.0768369782355522

Epoch: 6| Step: 2
Training loss: 2.0183629989624023
Validation loss: 2.096164975115048

Epoch: 6| Step: 3
Training loss: 2.051649808883667
Validation loss: 2.088426725838774

Epoch: 6| Step: 4
Training loss: 2.0698535442352295
Validation loss: 2.0944825321115474

Epoch: 6| Step: 5
Training loss: 2.0144896507263184
Validation loss: 2.0869280189596195

Epoch: 6| Step: 6
Training loss: 3.010563373565674
Validation loss: 2.086974045281769

Epoch: 6| Step: 7
Training loss: 1.7372244596481323
Validation loss: 2.0782429402874363

Epoch: 6| Step: 8
Training loss: 2.427222967147827
Validation loss: 2.067291928875831

Epoch: 6| Step: 9
Training loss: 2.492312431335449
Validation loss: 2.0771922655003046

Epoch: 6| Step: 10
Training loss: 3.128464698791504
Validation loss: 2.0854222851414836

Epoch: 6| Step: 11
Training loss: 2.8863959312438965
Validation loss: 2.078799836097225

Epoch: 6| Step: 12
Training loss: 1.991762399673462
Validation loss: 2.0982342253449144

Epoch: 6| Step: 13
Training loss: 1.888732671737671
Validation loss: 2.0875161899033414

Epoch: 56| Step: 0
Training loss: 2.454713821411133
Validation loss: 2.0859421209622453

Epoch: 6| Step: 1
Training loss: 2.3911242485046387
Validation loss: 2.0816606808734197

Epoch: 6| Step: 2
Training loss: 2.640685558319092
Validation loss: 2.0954099803842525

Epoch: 6| Step: 3
Training loss: 2.6346042156219482
Validation loss: 2.081757839008044

Epoch: 6| Step: 4
Training loss: 2.5416665077209473
Validation loss: 2.065372718277798

Epoch: 6| Step: 5
Training loss: 2.5207631587982178
Validation loss: 2.067073222129576

Epoch: 6| Step: 6
Training loss: 2.437680244445801
Validation loss: 2.069188669163694

Epoch: 6| Step: 7
Training loss: 2.550976276397705
Validation loss: 2.0543796349597234

Epoch: 6| Step: 8
Training loss: 2.3220062255859375
Validation loss: 2.0734323224713727

Epoch: 6| Step: 9
Training loss: 1.8522430658340454
Validation loss: 2.032538196092011

Epoch: 6| Step: 10
Training loss: 1.2029610872268677
Validation loss: 2.049004872639974

Epoch: 6| Step: 11
Training loss: 2.0521156787872314
Validation loss: 2.0676750316414783

Epoch: 6| Step: 12
Training loss: 2.3891754150390625
Validation loss: 2.068287628953175

Epoch: 6| Step: 13
Training loss: 2.404496431350708
Validation loss: 2.0427766153889317

Epoch: 57| Step: 0
Training loss: 2.319220781326294
Validation loss: 2.0575578161465224

Epoch: 6| Step: 1
Training loss: 2.9454145431518555
Validation loss: 2.0609372508141304

Epoch: 6| Step: 2
Training loss: 2.2550270557403564
Validation loss: 2.0328560503580237

Epoch: 6| Step: 3
Training loss: 1.9944651126861572
Validation loss: 2.0436514731376403

Epoch: 6| Step: 4
Training loss: 2.1854777336120605
Validation loss: 2.0488477804327525

Epoch: 6| Step: 5
Training loss: 1.65755033493042
Validation loss: 2.048895658985261

Epoch: 6| Step: 6
Training loss: 2.498417377471924
Validation loss: 2.0422208078445925

Epoch: 6| Step: 7
Training loss: 2.301084280014038
Validation loss: 2.0522257768979637

Epoch: 6| Step: 8
Training loss: 2.0917446613311768
Validation loss: 2.0720759553294026

Epoch: 6| Step: 9
Training loss: 1.9431980848312378
Validation loss: 2.062088825369394

Epoch: 6| Step: 10
Training loss: 2.8480494022369385
Validation loss: 2.036453163751992

Epoch: 6| Step: 11
Training loss: 2.1340856552124023
Validation loss: 2.048688268148771

Epoch: 6| Step: 12
Training loss: 2.9263322353363037
Validation loss: 2.0323894998078704

Epoch: 6| Step: 13
Training loss: 2.545301675796509
Validation loss: 2.040849201140865

Epoch: 58| Step: 0
Training loss: 2.2032337188720703
Validation loss: 2.0540839677215903

Epoch: 6| Step: 1
Training loss: 1.7551219463348389
Validation loss: 2.046522776285807

Epoch: 6| Step: 2
Training loss: 2.070955514907837
Validation loss: 2.0539818707332818

Epoch: 6| Step: 3
Training loss: 2.462747812271118
Validation loss: 2.0266985970158733

Epoch: 6| Step: 4
Training loss: 2.7423672676086426
Validation loss: 2.04495959256285

Epoch: 6| Step: 5
Training loss: 2.0371553897857666
Validation loss: 2.0577315258723434

Epoch: 6| Step: 6
Training loss: 1.956714391708374
Validation loss: 2.047824894228289

Epoch: 6| Step: 7
Training loss: 2.0045888423919678
Validation loss: 2.041879371930194

Epoch: 6| Step: 8
Training loss: 2.212162971496582
Validation loss: 2.0739853792293097

Epoch: 6| Step: 9
Training loss: 2.4034314155578613
Validation loss: 2.0536945199453704

Epoch: 6| Step: 10
Training loss: 4.116947650909424
Validation loss: 2.054203068056414

Epoch: 6| Step: 11
Training loss: 2.2739005088806152
Validation loss: 2.045753998141135

Epoch: 6| Step: 12
Training loss: 1.9472261667251587
Validation loss: 2.053916541478967

Epoch: 6| Step: 13
Training loss: 2.4741647243499756
Validation loss: 2.061348151135188

Epoch: 59| Step: 0
Training loss: 1.9999992847442627
Validation loss: 2.0507956807331373

Epoch: 6| Step: 1
Training loss: 2.8336048126220703
Validation loss: 2.0479751953514675

Epoch: 6| Step: 2
Training loss: 1.996619701385498
Validation loss: 2.0644997524958786

Epoch: 6| Step: 3
Training loss: 1.8644156455993652
Validation loss: 2.05632625087615

Epoch: 6| Step: 4
Training loss: 2.6205711364746094
Validation loss: 2.068657919924746

Epoch: 6| Step: 5
Training loss: 2.533053398132324
Validation loss: 2.074712348240678

Epoch: 6| Step: 6
Training loss: 2.238217353820801
Validation loss: 2.0709159233236827

Epoch: 6| Step: 7
Training loss: 2.541393280029297
Validation loss: 2.0588541056520198

Epoch: 6| Step: 8
Training loss: 2.1902916431427
Validation loss: 2.0727146107663392

Epoch: 6| Step: 9
Training loss: 2.7747604846954346
Validation loss: 2.0658786732663392

Epoch: 6| Step: 10
Training loss: 2.8739895820617676
Validation loss: 2.0777892028131792

Epoch: 6| Step: 11
Training loss: 1.4897396564483643
Validation loss: 2.0743799324958556

Epoch: 6| Step: 12
Training loss: 2.0940470695495605
Validation loss: 2.067901667728219

Epoch: 6| Step: 13
Training loss: 2.1926674842834473
Validation loss: 2.067907617938134

Epoch: 60| Step: 0
Training loss: 2.8332793712615967
Validation loss: 2.0612235761457876

Epoch: 6| Step: 1
Training loss: 0.9698747396469116
Validation loss: 2.0774580919614403

Epoch: 6| Step: 2
Training loss: 2.5364227294921875
Validation loss: 2.082477205543108

Epoch: 6| Step: 3
Training loss: 2.4637646675109863
Validation loss: 2.0693418569462274

Epoch: 6| Step: 4
Training loss: 2.5946145057678223
Validation loss: 2.0691603460619525

Epoch: 6| Step: 5
Training loss: 2.005610942840576
Validation loss: 2.059746511520878

Epoch: 6| Step: 6
Training loss: 2.163438320159912
Validation loss: 2.068238463453067

Epoch: 6| Step: 7
Training loss: 2.370858907699585
Validation loss: 2.0622679264314714

Epoch: 6| Step: 8
Training loss: 2.6112565994262695
Validation loss: 2.0721771845253567

Epoch: 6| Step: 9
Training loss: 2.6426191329956055
Validation loss: 2.0830654277596423

Epoch: 6| Step: 10
Training loss: 2.3565008640289307
Validation loss: 2.0768982300194363

Epoch: 6| Step: 11
Training loss: 2.4733567237854004
Validation loss: 2.0717434652390017

Epoch: 6| Step: 12
Training loss: 2.005297899246216
Validation loss: 2.063868896935576

Epoch: 6| Step: 13
Training loss: 2.5295681953430176
Validation loss: 2.0745555470066686

Epoch: 61| Step: 0
Training loss: 2.247581958770752
Validation loss: 2.088626039925442

Epoch: 6| Step: 1
Training loss: 2.2952892780303955
Validation loss: 2.0935710553200013

Epoch: 6| Step: 2
Training loss: 2.4638118743896484
Validation loss: 2.0746492134627474

Epoch: 6| Step: 3
Training loss: 2.5282113552093506
Validation loss: 2.0716712423550185

Epoch: 6| Step: 4
Training loss: 2.0613720417022705
Validation loss: 2.066517545330909

Epoch: 6| Step: 5
Training loss: 1.4759454727172852
Validation loss: 2.0709834662816857

Epoch: 6| Step: 6
Training loss: 2.655892848968506
Validation loss: 2.0926600912565827

Epoch: 6| Step: 7
Training loss: 3.3416178226470947
Validation loss: 2.1048487950396795

Epoch: 6| Step: 8
Training loss: 2.62923526763916
Validation loss: 2.0709573889291413

Epoch: 6| Step: 9
Training loss: 2.1036295890808105
Validation loss: 2.0873412598845777

Epoch: 6| Step: 10
Training loss: 1.323171854019165
Validation loss: 2.0712785695188787

Epoch: 6| Step: 11
Training loss: 2.014479637145996
Validation loss: 2.0933659871419272

Epoch: 6| Step: 12
Training loss: 3.0909781455993652
Validation loss: 2.0714468725265993

Epoch: 6| Step: 13
Training loss: 2.0556724071502686
Validation loss: 2.0585214912250476

Epoch: 62| Step: 0
Training loss: 2.9347476959228516
Validation loss: 2.0740968591423443

Epoch: 6| Step: 1
Training loss: 2.385387897491455
Validation loss: 2.0775087725731636

Epoch: 6| Step: 2
Training loss: 2.3344521522521973
Validation loss: 2.0615339638084493

Epoch: 6| Step: 3
Training loss: 2.1044788360595703
Validation loss: 2.0557220981967066

Epoch: 6| Step: 4
Training loss: 2.3948934078216553
Validation loss: 2.0546911403697026

Epoch: 6| Step: 5
Training loss: 1.8895899057388306
Validation loss: 2.0909308489932807

Epoch: 6| Step: 6
Training loss: 2.5737414360046387
Validation loss: 2.0752970364785965

Epoch: 6| Step: 7
Training loss: 2.1941161155700684
Validation loss: 2.0716595239536737

Epoch: 6| Step: 8
Training loss: 1.8368711471557617
Validation loss: 2.059631424565469

Epoch: 6| Step: 9
Training loss: 1.9436105489730835
Validation loss: 2.047208930856438

Epoch: 6| Step: 10
Training loss: 2.7296876907348633
Validation loss: 2.073065279632486

Epoch: 6| Step: 11
Training loss: 2.253129243850708
Validation loss: 2.048086130490867

Epoch: 6| Step: 12
Training loss: 2.13638973236084
Validation loss: 2.057312245010048

Epoch: 6| Step: 13
Training loss: 2.8621017932891846
Validation loss: 2.0860228461603962

Epoch: 63| Step: 0
Training loss: 1.8865169286727905
Validation loss: 2.06669278811383

Epoch: 6| Step: 1
Training loss: 2.168931007385254
Validation loss: 2.0607045568445677

Epoch: 6| Step: 2
Training loss: 2.052886486053467
Validation loss: 2.056874054734425

Epoch: 6| Step: 3
Training loss: 1.9503273963928223
Validation loss: 2.0686908742432952

Epoch: 6| Step: 4
Training loss: 2.886542797088623
Validation loss: 2.056159265579716

Epoch: 6| Step: 5
Training loss: 1.8585903644561768
Validation loss: 2.052120686859213

Epoch: 6| Step: 6
Training loss: 2.744662046432495
Validation loss: 2.05489783517776

Epoch: 6| Step: 7
Training loss: 2.530189037322998
Validation loss: 2.045072396596273

Epoch: 6| Step: 8
Training loss: 2.7174274921417236
Validation loss: 2.0536003856248755

Epoch: 6| Step: 9
Training loss: 1.958355188369751
Validation loss: 2.0399926131771458

Epoch: 6| Step: 10
Training loss: 2.5090582370758057
Validation loss: 2.0325677279503114

Epoch: 6| Step: 11
Training loss: 1.626325011253357
Validation loss: 2.0471289465504308

Epoch: 6| Step: 12
Training loss: 2.6334927082061768
Validation loss: 2.054865106459587

Epoch: 6| Step: 13
Training loss: 3.046229124069214
Validation loss: 2.0419149975622854

Epoch: 64| Step: 0
Training loss: 2.4603641033172607
Validation loss: 2.0527911878401235

Epoch: 6| Step: 1
Training loss: 1.8710557222366333
Validation loss: 2.0694209644871373

Epoch: 6| Step: 2
Training loss: 2.4351696968078613
Validation loss: 2.075625144025331

Epoch: 6| Step: 3
Training loss: 2.9418087005615234
Validation loss: 2.051125082918393

Epoch: 6| Step: 4
Training loss: 2.159453868865967
Validation loss: 2.07257818406628

Epoch: 6| Step: 5
Training loss: 1.8334561586380005
Validation loss: 2.075867509329191

Epoch: 6| Step: 6
Training loss: 2.0312376022338867
Validation loss: 2.070657459638452

Epoch: 6| Step: 7
Training loss: 2.309208869934082
Validation loss: 2.082391567127679

Epoch: 6| Step: 8
Training loss: 2.596362352371216
Validation loss: 2.075298378544469

Epoch: 6| Step: 9
Training loss: 2.5606958866119385
Validation loss: 2.0682171365266204

Epoch: 6| Step: 10
Training loss: 2.636303424835205
Validation loss: 2.0777417741796023

Epoch: 6| Step: 11
Training loss: 2.0366897583007812
Validation loss: 2.090353681195167

Epoch: 6| Step: 12
Training loss: 2.241199016571045
Validation loss: 2.079721990452018

Epoch: 6| Step: 13
Training loss: 2.383211851119995
Validation loss: 2.089729857701127

Epoch: 65| Step: 0
Training loss: 2.702457904815674
Validation loss: 2.0682734763750465

Epoch: 6| Step: 1
Training loss: 2.3571250438690186
Validation loss: 2.080962270818731

Epoch: 6| Step: 2
Training loss: 2.3839473724365234
Validation loss: 2.058523202455172

Epoch: 6| Step: 3
Training loss: 2.9586143493652344
Validation loss: 2.0545536779588267

Epoch: 6| Step: 4
Training loss: 1.9740140438079834
Validation loss: 2.0849450326734975

Epoch: 6| Step: 5
Training loss: 1.9462196826934814
Validation loss: 2.0730706927596882

Epoch: 6| Step: 6
Training loss: 2.837371826171875
Validation loss: 2.0440389238378054

Epoch: 6| Step: 7
Training loss: 2.112031936645508
Validation loss: 2.0840716579908967

Epoch: 6| Step: 8
Training loss: 2.7334117889404297
Validation loss: 2.059408792885401

Epoch: 6| Step: 9
Training loss: 2.6628172397613525
Validation loss: 2.03801586551051

Epoch: 6| Step: 10
Training loss: 2.08322811126709
Validation loss: 2.061073495495704

Epoch: 6| Step: 11
Training loss: 1.7300777435302734
Validation loss: 2.069958024127509

Epoch: 6| Step: 12
Training loss: 1.3528530597686768
Validation loss: 2.082702262427217

Epoch: 6| Step: 13
Training loss: 2.5985207557678223
Validation loss: 2.0734853385597147

Epoch: 66| Step: 0
Training loss: 2.207063913345337
Validation loss: 2.064777562695165

Epoch: 6| Step: 1
Training loss: 2.253781318664551
Validation loss: 2.103706762354861

Epoch: 6| Step: 2
Training loss: 2.4798786640167236
Validation loss: 2.071965174008441

Epoch: 6| Step: 3
Training loss: 2.914435863494873
Validation loss: 2.0662767425660165

Epoch: 6| Step: 4
Training loss: 1.2882237434387207
Validation loss: 2.080519760808637

Epoch: 6| Step: 5
Training loss: 2.449465036392212
Validation loss: 2.0796046667201544

Epoch: 6| Step: 6
Training loss: 2.219890594482422
Validation loss: 2.0926305119709303

Epoch: 6| Step: 7
Training loss: 2.489077091217041
Validation loss: 2.073335983419931

Epoch: 6| Step: 8
Training loss: 2.392324447631836
Validation loss: 2.075239004627351

Epoch: 6| Step: 9
Training loss: 2.0993380546569824
Validation loss: 2.0807019202939925

Epoch: 6| Step: 10
Training loss: 2.9295501708984375
Validation loss: 2.0780249898151686

Epoch: 6| Step: 11
Training loss: 2.389223337173462
Validation loss: 2.059041743637413

Epoch: 6| Step: 12
Training loss: 1.8573458194732666
Validation loss: 2.0979804274856404

Epoch: 6| Step: 13
Training loss: 2.290944814682007
Validation loss: 2.065048998402011

Epoch: 67| Step: 0
Training loss: 2.3363521099090576
Validation loss: 2.0889784725763465

Epoch: 6| Step: 1
Training loss: 1.8114964962005615
Validation loss: 2.077198556674424

Epoch: 6| Step: 2
Training loss: 2.677523612976074
Validation loss: 2.09454075495402

Epoch: 6| Step: 3
Training loss: 2.6460485458374023
Validation loss: 2.088474340336297

Epoch: 6| Step: 4
Training loss: 2.0397143363952637
Validation loss: 2.08790309070259

Epoch: 6| Step: 5
Training loss: 2.482182741165161
Validation loss: 2.1001174885739564

Epoch: 6| Step: 6
Training loss: 1.9880962371826172
Validation loss: 2.095092576037171

Epoch: 6| Step: 7
Training loss: 2.350008010864258
Validation loss: 2.083280227517569

Epoch: 6| Step: 8
Training loss: 2.7905139923095703
Validation loss: 2.080178224912254

Epoch: 6| Step: 9
Training loss: 2.280855894088745
Validation loss: 2.0884085867994573

Epoch: 6| Step: 10
Training loss: 2.085892915725708
Validation loss: 2.08252380740258

Epoch: 6| Step: 11
Training loss: 2.4683704376220703
Validation loss: 2.0719856164788686

Epoch: 6| Step: 12
Training loss: 1.7742705345153809
Validation loss: 2.0626940881052325

Epoch: 6| Step: 13
Training loss: 2.5212242603302
Validation loss: 2.0871463860234907

Epoch: 68| Step: 0
Training loss: 2.177536725997925
Validation loss: 2.1080097024158766

Epoch: 6| Step: 1
Training loss: 1.9100087881088257
Validation loss: 2.0731748329695834

Epoch: 6| Step: 2
Training loss: 2.7440690994262695
Validation loss: 2.0856018809862036

Epoch: 6| Step: 3
Training loss: 1.8504047393798828
Validation loss: 2.062966533886489

Epoch: 6| Step: 4
Training loss: 3.1018552780151367
Validation loss: 2.0648949274452786

Epoch: 6| Step: 5
Training loss: 1.7974448204040527
Validation loss: 2.072702379636867

Epoch: 6| Step: 6
Training loss: 2.1301751136779785
Validation loss: 2.069960412158761

Epoch: 6| Step: 7
Training loss: 2.1640963554382324
Validation loss: 2.052806294092568

Epoch: 6| Step: 8
Training loss: 2.171351432800293
Validation loss: 2.0542405036187943

Epoch: 6| Step: 9
Training loss: 2.524123191833496
Validation loss: 2.0677731588322628

Epoch: 6| Step: 10
Training loss: 1.9559733867645264
Validation loss: 2.0817851417808124

Epoch: 6| Step: 11
Training loss: 2.2394661903381348
Validation loss: 2.062687885376715

Epoch: 6| Step: 12
Training loss: 3.177511215209961
Validation loss: 2.0499982680043867

Epoch: 6| Step: 13
Training loss: 2.24666690826416
Validation loss: 2.0628396054749847

Epoch: 69| Step: 0
Training loss: 2.2521800994873047
Validation loss: 2.0590596173399236

Epoch: 6| Step: 1
Training loss: 2.739671230316162
Validation loss: 2.0748580117379465

Epoch: 6| Step: 2
Training loss: 1.8573229312896729
Validation loss: 2.047854241504464

Epoch: 6| Step: 3
Training loss: 2.022520065307617
Validation loss: 2.0794942378997803

Epoch: 6| Step: 4
Training loss: 2.291673183441162
Validation loss: 2.058436765465685

Epoch: 6| Step: 5
Training loss: 2.083277940750122
Validation loss: 2.0670255179046304

Epoch: 6| Step: 6
Training loss: 3.087697982788086
Validation loss: 2.071149044139411

Epoch: 6| Step: 7
Training loss: 2.4035983085632324
Validation loss: 2.0416610574209564

Epoch: 6| Step: 8
Training loss: 2.2747132778167725
Validation loss: 2.057806363669775

Epoch: 6| Step: 9
Training loss: 1.8317573070526123
Validation loss: 2.066845081185782

Epoch: 6| Step: 10
Training loss: 2.9136433601379395
Validation loss: 2.0559763446930917

Epoch: 6| Step: 11
Training loss: 2.2931723594665527
Validation loss: 2.0501934225841234

Epoch: 6| Step: 12
Training loss: 1.8437659740447998
Validation loss: 2.0588990090995707

Epoch: 6| Step: 13
Training loss: 2.3070476055145264
Validation loss: 2.0687578673003824

Epoch: 70| Step: 0
Training loss: 2.766414165496826
Validation loss: 2.0583051878918885

Epoch: 6| Step: 1
Training loss: 2.1714093685150146
Validation loss: 2.051487133067141

Epoch: 6| Step: 2
Training loss: 2.1234936714172363
Validation loss: 2.066323567462224

Epoch: 6| Step: 3
Training loss: 2.486614942550659
Validation loss: 2.0629956183894986

Epoch: 6| Step: 4
Training loss: 1.9536843299865723
Validation loss: 2.065405373932213

Epoch: 6| Step: 5
Training loss: 2.9168789386749268
Validation loss: 2.0642704976502286

Epoch: 6| Step: 6
Training loss: 1.8195524215698242
Validation loss: 2.0697380522246003

Epoch: 6| Step: 7
Training loss: 2.523672103881836
Validation loss: 2.0779023580653693

Epoch: 6| Step: 8
Training loss: 1.9748624563217163
Validation loss: 2.0727123291261735

Epoch: 6| Step: 9
Training loss: 1.8299076557159424
Validation loss: 2.073535637188983

Epoch: 6| Step: 10
Training loss: 1.9223651885986328
Validation loss: 2.0715888405358918

Epoch: 6| Step: 11
Training loss: 2.1113436222076416
Validation loss: 2.0656235371866534

Epoch: 6| Step: 12
Training loss: 3.1726937294006348
Validation loss: 2.0771763670829033

Epoch: 6| Step: 13
Training loss: 2.329848289489746
Validation loss: 2.0604570027320617

Epoch: 71| Step: 0
Training loss: 1.6825315952301025
Validation loss: 2.07897440848812

Epoch: 6| Step: 1
Training loss: 2.58174991607666
Validation loss: 2.0611515493803125

Epoch: 6| Step: 2
Training loss: 2.528261423110962
Validation loss: 2.0854821102593535

Epoch: 6| Step: 3
Training loss: 2.4155139923095703
Validation loss: 2.1050489230822493

Epoch: 6| Step: 4
Training loss: 2.298415422439575
Validation loss: 2.0908842996884416

Epoch: 6| Step: 5
Training loss: 2.454019069671631
Validation loss: 2.08130133023826

Epoch: 6| Step: 6
Training loss: 2.519160270690918
Validation loss: 2.080670610550911

Epoch: 6| Step: 7
Training loss: 2.2452385425567627
Validation loss: 2.0767935450359056

Epoch: 6| Step: 8
Training loss: 2.0284640789031982
Validation loss: 2.0938521572338638

Epoch: 6| Step: 9
Training loss: 2.3860180377960205
Validation loss: 2.1099956163796048

Epoch: 6| Step: 10
Training loss: 2.0320682525634766
Validation loss: 2.067469490471707

Epoch: 6| Step: 11
Training loss: 3.0877325534820557
Validation loss: 2.0693927682856077

Epoch: 6| Step: 12
Training loss: 1.9084832668304443
Validation loss: 2.113533099492391

Epoch: 6| Step: 13
Training loss: 1.7389158010482788
Validation loss: 2.0689621484407814

Epoch: 72| Step: 0
Training loss: 2.1705265045166016
Validation loss: 2.082819346458681

Epoch: 6| Step: 1
Training loss: 2.346562385559082
Validation loss: 2.1000135996008433

Epoch: 6| Step: 2
Training loss: 1.9871726036071777
Validation loss: 2.0707924853089037

Epoch: 6| Step: 3
Training loss: 2.0050177574157715
Validation loss: 2.0936318853850007

Epoch: 6| Step: 4
Training loss: 2.276832103729248
Validation loss: 2.096380028673398

Epoch: 6| Step: 5
Training loss: 2.260233163833618
Validation loss: 2.075306882140457

Epoch: 6| Step: 6
Training loss: 2.2838621139526367
Validation loss: 2.0951017641252085

Epoch: 6| Step: 7
Training loss: 2.8618693351745605
Validation loss: 2.0662856025080525

Epoch: 6| Step: 8
Training loss: 2.7056281566619873
Validation loss: 2.0749746766141666

Epoch: 6| Step: 9
Training loss: 2.229238986968994
Validation loss: 2.080760919919578

Epoch: 6| Step: 10
Training loss: 2.4557037353515625
Validation loss: 2.046509841436981

Epoch: 6| Step: 11
Training loss: 2.4288954734802246
Validation loss: 2.029482133926884

Epoch: 6| Step: 12
Training loss: 2.0271031856536865
Validation loss: 2.05917388649397

Epoch: 6| Step: 13
Training loss: 2.0648036003112793
Validation loss: 2.047633494100263

Epoch: 73| Step: 0
Training loss: 3.1715474128723145
Validation loss: 2.0493794154095393

Epoch: 6| Step: 1
Training loss: 2.113513708114624
Validation loss: 2.0648989382610528

Epoch: 6| Step: 2
Training loss: 1.9914878606796265
Validation loss: 2.058901230494181

Epoch: 6| Step: 3
Training loss: 2.1003448963165283
Validation loss: 2.0787168382316508

Epoch: 6| Step: 4
Training loss: 2.431166410446167
Validation loss: 2.041453065410737

Epoch: 6| Step: 5
Training loss: 2.0329902172088623
Validation loss: 2.0818854531934186

Epoch: 6| Step: 6
Training loss: 2.062432289123535
Validation loss: 2.0676556479546333

Epoch: 6| Step: 7
Training loss: 2.569725513458252
Validation loss: 2.080431410061416

Epoch: 6| Step: 8
Training loss: 2.2347183227539062
Validation loss: 2.0768238652137017

Epoch: 6| Step: 9
Training loss: 2.6359853744506836
Validation loss: 2.0731635901235763

Epoch: 6| Step: 10
Training loss: 1.7929248809814453
Validation loss: 2.0738736916613836

Epoch: 6| Step: 11
Training loss: 2.317877769470215
Validation loss: 2.09825700072832

Epoch: 6| Step: 12
Training loss: 2.0734362602233887
Validation loss: 2.0960704280484106

Epoch: 6| Step: 13
Training loss: 2.9313981533050537
Validation loss: 2.081830040101082

Epoch: 74| Step: 0
Training loss: 3.1937150955200195
Validation loss: 2.078943460218368

Epoch: 6| Step: 1
Training loss: 2.2460851669311523
Validation loss: 2.0814831167139034

Epoch: 6| Step: 2
Training loss: 1.592247724533081
Validation loss: 2.06777129250188

Epoch: 6| Step: 3
Training loss: 1.740607738494873
Validation loss: 2.0863573653723604

Epoch: 6| Step: 4
Training loss: 2.307377815246582
Validation loss: 2.0902524173900647

Epoch: 6| Step: 5
Training loss: 2.6274571418762207
Validation loss: 2.089653604774065

Epoch: 6| Step: 6
Training loss: 2.1871213912963867
Validation loss: 2.0910151056064072

Epoch: 6| Step: 7
Training loss: 2.3686370849609375
Validation loss: 2.093182848345849

Epoch: 6| Step: 8
Training loss: 2.043001174926758
Validation loss: 2.1067105326601254

Epoch: 6| Step: 9
Training loss: 2.571225166320801
Validation loss: 2.094609870705553

Epoch: 6| Step: 10
Training loss: 1.9987505674362183
Validation loss: 2.10101326306661

Epoch: 6| Step: 11
Training loss: 2.6240687370300293
Validation loss: 2.073841843553769

Epoch: 6| Step: 12
Training loss: 2.432453155517578
Validation loss: 2.0799170437679497

Epoch: 6| Step: 13
Training loss: 1.8596916198730469
Validation loss: 2.0999129638876965

Epoch: 75| Step: 0
Training loss: 2.045269012451172
Validation loss: 2.0757510380078386

Epoch: 6| Step: 1
Training loss: 2.1581740379333496
Validation loss: 2.082352989463396

Epoch: 6| Step: 2
Training loss: 2.06058931350708
Validation loss: 2.0928694945509716

Epoch: 6| Step: 3
Training loss: 2.4703664779663086
Validation loss: 2.0771275643379457

Epoch: 6| Step: 4
Training loss: 2.5250372886657715
Validation loss: 2.0826285769862514

Epoch: 6| Step: 5
Training loss: 2.4893503189086914
Validation loss: 2.07001740958101

Epoch: 6| Step: 6
Training loss: 1.7613708972930908
Validation loss: 2.0774215165004937

Epoch: 6| Step: 7
Training loss: 3.527170419692993
Validation loss: 2.0674811332456526

Epoch: 6| Step: 8
Training loss: 2.4508466720581055
Validation loss: 2.0790990142412085

Epoch: 6| Step: 9
Training loss: 2.4181883335113525
Validation loss: 2.059668319199675

Epoch: 6| Step: 10
Training loss: 2.0558364391326904
Validation loss: 2.0775873725132277

Epoch: 6| Step: 11
Training loss: 2.557603359222412
Validation loss: 2.083985713220412

Epoch: 6| Step: 12
Training loss: 1.8157440423965454
Validation loss: 2.0978374173564296

Epoch: 6| Step: 13
Training loss: 1.1851234436035156
Validation loss: 2.074063161368011

Epoch: 76| Step: 0
Training loss: 2.654564380645752
Validation loss: 2.0947229580212663

Epoch: 6| Step: 1
Training loss: 2.574082374572754
Validation loss: 2.094373086447357

Epoch: 6| Step: 2
Training loss: 2.620668888092041
Validation loss: 2.0757348665627102

Epoch: 6| Step: 3
Training loss: 2.504552125930786
Validation loss: 2.0770637732680126

Epoch: 6| Step: 4
Training loss: 2.451211452484131
Validation loss: 2.0771678878415014

Epoch: 6| Step: 5
Training loss: 1.4178962707519531
Validation loss: 2.0917111981299614

Epoch: 6| Step: 6
Training loss: 3.2255430221557617
Validation loss: 2.0777841691047914

Epoch: 6| Step: 7
Training loss: 1.498681664466858
Validation loss: 2.070320954886816

Epoch: 6| Step: 8
Training loss: 2.589425563812256
Validation loss: 2.073444280573117

Epoch: 6| Step: 9
Training loss: 2.31254506111145
Validation loss: 2.070494439012261

Epoch: 6| Step: 10
Training loss: 2.550816535949707
Validation loss: 2.0596326961312243

Epoch: 6| Step: 11
Training loss: 1.6968159675598145
Validation loss: 2.068468086181148

Epoch: 6| Step: 12
Training loss: 2.1750664710998535
Validation loss: 2.0777312504347933

Epoch: 6| Step: 13
Training loss: 1.5291813611984253
Validation loss: 2.086453140422862

Epoch: 77| Step: 0
Training loss: 2.296814441680908
Validation loss: 2.0631172964649815

Epoch: 6| Step: 1
Training loss: 2.645206928253174
Validation loss: 2.0643144192234164

Epoch: 6| Step: 2
Training loss: 2.075186252593994
Validation loss: 2.0803165076881327

Epoch: 6| Step: 3
Training loss: 3.2216737270355225
Validation loss: 2.0721753976678334

Epoch: 6| Step: 4
Training loss: 2.4994146823883057
Validation loss: 2.0834007622093282

Epoch: 6| Step: 5
Training loss: 2.045745372772217
Validation loss: 2.0664957031126945

Epoch: 6| Step: 6
Training loss: 3.007039785385132
Validation loss: 2.082957850989475

Epoch: 6| Step: 7
Training loss: 1.6107442378997803
Validation loss: 2.066810155427584

Epoch: 6| Step: 8
Training loss: 1.7625125646591187
Validation loss: 2.061325523161119

Epoch: 6| Step: 9
Training loss: 2.4314374923706055
Validation loss: 2.052876745500872

Epoch: 6| Step: 10
Training loss: 1.697943925857544
Validation loss: 2.0319620998956824

Epoch: 6| Step: 11
Training loss: 1.8069733381271362
Validation loss: 2.080194088720506

Epoch: 6| Step: 12
Training loss: 2.351654529571533
Validation loss: 2.0654227784884873

Epoch: 6| Step: 13
Training loss: 2.3694167137145996
Validation loss: 2.0732639733181206

Epoch: 78| Step: 0
Training loss: 2.3391432762145996
Validation loss: 2.0639609777799217

Epoch: 6| Step: 1
Training loss: 2.260745048522949
Validation loss: 2.0882299330926712

Epoch: 6| Step: 2
Training loss: 2.3066935539245605
Validation loss: 2.0812795751838276

Epoch: 6| Step: 3
Training loss: 2.7749123573303223
Validation loss: 2.0605206066562283

Epoch: 6| Step: 4
Training loss: 2.4406094551086426
Validation loss: 2.061194815943318

Epoch: 6| Step: 5
Training loss: 2.815248489379883
Validation loss: 2.0462350281335975

Epoch: 6| Step: 6
Training loss: 1.6860709190368652
Validation loss: 2.0531332031373055

Epoch: 6| Step: 7
Training loss: 2.17659854888916
Validation loss: 2.0426414089818157

Epoch: 6| Step: 8
Training loss: 2.4114227294921875
Validation loss: 2.0738746325174966

Epoch: 6| Step: 9
Training loss: 2.0313844680786133
Validation loss: 2.048145427498766

Epoch: 6| Step: 10
Training loss: 1.7300302982330322
Validation loss: 2.0728224349278275

Epoch: 6| Step: 11
Training loss: 2.4702439308166504
Validation loss: 2.06471437023532

Epoch: 6| Step: 12
Training loss: 2.2030344009399414
Validation loss: 2.0596705431579263

Epoch: 6| Step: 13
Training loss: 1.948525309562683
Validation loss: 2.082044586058586

Epoch: 79| Step: 0
Training loss: 2.3198323249816895
Validation loss: 2.070770054735163

Epoch: 6| Step: 1
Training loss: 1.9294047355651855
Validation loss: 2.073474577678147

Epoch: 6| Step: 2
Training loss: 1.7942020893096924
Validation loss: 2.0641032418897076

Epoch: 6| Step: 3
Training loss: 2.5364162921905518
Validation loss: 2.0694278799077517

Epoch: 6| Step: 4
Training loss: 1.696815013885498
Validation loss: 2.083131177451021

Epoch: 6| Step: 5
Training loss: 2.079314708709717
Validation loss: 2.0369469273474907

Epoch: 6| Step: 6
Training loss: 1.8183175325393677
Validation loss: 2.0660313073024956

Epoch: 6| Step: 7
Training loss: 2.420747756958008
Validation loss: 2.0714615416783158

Epoch: 6| Step: 8
Training loss: 2.618046283721924
Validation loss: 2.0714630080807592

Epoch: 6| Step: 9
Training loss: 2.2419309616088867
Validation loss: 2.027163715772731

Epoch: 6| Step: 10
Training loss: 2.637336254119873
Validation loss: 2.0618226092348815

Epoch: 6| Step: 11
Training loss: 3.4790291786193848
Validation loss: 2.052726540514218

Epoch: 6| Step: 12
Training loss: 1.7562068700790405
Validation loss: 2.0650291442871094

Epoch: 6| Step: 13
Training loss: 2.5981228351593018
Validation loss: 2.050509404110652

Epoch: 80| Step: 0
Training loss: 2.232224464416504
Validation loss: 2.056975442876098

Epoch: 6| Step: 1
Training loss: 2.6519432067871094
Validation loss: 2.065489572863425

Epoch: 6| Step: 2
Training loss: 2.439624309539795
Validation loss: 2.058223937147407

Epoch: 6| Step: 3
Training loss: 2.8828208446502686
Validation loss: 2.0826792704161776

Epoch: 6| Step: 4
Training loss: 1.7458469867706299
Validation loss: 2.0584775837518836

Epoch: 6| Step: 5
Training loss: 1.1122424602508545
Validation loss: 2.0778804338106545

Epoch: 6| Step: 6
Training loss: 1.9466278553009033
Validation loss: 2.0462663609494447

Epoch: 6| Step: 7
Training loss: 2.3913397789001465
Validation loss: 2.070078061472985

Epoch: 6| Step: 8
Training loss: 2.9037723541259766
Validation loss: 2.0644009497857865

Epoch: 6| Step: 9
Training loss: 2.6835618019104004
Validation loss: 2.0703708869154736

Epoch: 6| Step: 10
Training loss: 2.0543956756591797
Validation loss: 2.0761388655631774

Epoch: 6| Step: 11
Training loss: 2.266544818878174
Validation loss: 2.066152395740632

Epoch: 6| Step: 12
Training loss: 2.4131217002868652
Validation loss: 2.0784451692335066

Epoch: 6| Step: 13
Training loss: 1.8945833444595337
Validation loss: 2.060292141411894

Epoch: 81| Step: 0
Training loss: 2.560204029083252
Validation loss: 2.0756186926236717

Epoch: 6| Step: 1
Training loss: 2.2470719814300537
Validation loss: 2.0658378793347265

Epoch: 6| Step: 2
Training loss: 1.937217116355896
Validation loss: 2.0645783588450444

Epoch: 6| Step: 3
Training loss: 2.3427484035491943
Validation loss: 2.059131828687524

Epoch: 6| Step: 4
Training loss: 2.409923553466797
Validation loss: 2.0835205354998187

Epoch: 6| Step: 5
Training loss: 1.9925353527069092
Validation loss: 2.079993837623186

Epoch: 6| Step: 6
Training loss: 2.3328211307525635
Validation loss: 2.0677333942023655

Epoch: 6| Step: 7
Training loss: 2.5594959259033203
Validation loss: 2.0803171742346978

Epoch: 6| Step: 8
Training loss: 2.347238063812256
Validation loss: 2.088710533675327

Epoch: 6| Step: 9
Training loss: 2.6405205726623535
Validation loss: 2.09090531000527

Epoch: 6| Step: 10
Training loss: 2.5028328895568848
Validation loss: 2.086184652902747

Epoch: 6| Step: 11
Training loss: 2.1851863861083984
Validation loss: 2.0841906967983452

Epoch: 6| Step: 12
Training loss: 2.2070345878601074
Validation loss: 2.0730748945666897

Epoch: 6| Step: 13
Training loss: 1.2193660736083984
Validation loss: 2.0859896957233386

Epoch: 82| Step: 0
Training loss: 2.54388427734375
Validation loss: 2.0776590685690604

Epoch: 6| Step: 1
Training loss: 2.2363314628601074
Validation loss: 2.1002182781055407

Epoch: 6| Step: 2
Training loss: 1.9017279148101807
Validation loss: 2.0574435111015075

Epoch: 6| Step: 3
Training loss: 1.4434828758239746
Validation loss: 2.0601815498003395

Epoch: 6| Step: 4
Training loss: 2.7953333854675293
Validation loss: 2.0728161565719114

Epoch: 6| Step: 5
Training loss: 2.5572352409362793
Validation loss: 2.0744542280832925

Epoch: 6| Step: 6
Training loss: 1.9461102485656738
Validation loss: 2.0875329074039253

Epoch: 6| Step: 7
Training loss: 2.631032943725586
Validation loss: 2.0703384491705124

Epoch: 6| Step: 8
Training loss: 2.156747817993164
Validation loss: 2.072710547395932

Epoch: 6| Step: 9
Training loss: 2.498473644256592
Validation loss: 2.0679183416469122

Epoch: 6| Step: 10
Training loss: 2.097811222076416
Validation loss: 2.0541752781919254

Epoch: 6| Step: 11
Training loss: 2.7255148887634277
Validation loss: 2.090769349887807

Epoch: 6| Step: 12
Training loss: 2.33154034614563
Validation loss: 2.081325207987139

Epoch: 6| Step: 13
Training loss: 1.5559887886047363
Validation loss: 2.059934252051897

Epoch: 83| Step: 0
Training loss: 2.2982869148254395
Validation loss: 2.089025023163006

Epoch: 6| Step: 1
Training loss: 2.536381721496582
Validation loss: 2.0558453670112034

Epoch: 6| Step: 2
Training loss: 2.9492132663726807
Validation loss: 2.1085129232816797

Epoch: 6| Step: 3
Training loss: 2.2763869762420654
Validation loss: 2.0927756935037594

Epoch: 6| Step: 4
Training loss: 1.9467982053756714
Validation loss: 2.0929745576714955

Epoch: 6| Step: 5
Training loss: 2.323428153991699
Validation loss: 2.0748050366678545

Epoch: 6| Step: 6
Training loss: 2.516347646713257
Validation loss: 2.08957197589259

Epoch: 6| Step: 7
Training loss: 2.997755527496338
Validation loss: 2.1041360029610257

Epoch: 6| Step: 8
Training loss: 2.268439769744873
Validation loss: 2.1052436251794138

Epoch: 6| Step: 9
Training loss: 1.231119155883789
Validation loss: 2.095162791590537

Epoch: 6| Step: 10
Training loss: 1.9845433235168457
Validation loss: 2.0819224426823277

Epoch: 6| Step: 11
Training loss: 1.950109601020813
Validation loss: 2.0906497804067468

Epoch: 6| Step: 12
Training loss: 2.5334062576293945
Validation loss: 2.071977961447931

Epoch: 6| Step: 13
Training loss: 2.0090339183807373
Validation loss: 2.078840926129331

Epoch: 84| Step: 0
Training loss: 2.1574249267578125
Validation loss: 2.09434066536606

Epoch: 6| Step: 1
Training loss: 2.3148062229156494
Validation loss: 2.0914432912744503

Epoch: 6| Step: 2
Training loss: 1.7650957107543945
Validation loss: 2.069502238304384

Epoch: 6| Step: 3
Training loss: 2.3017191886901855
Validation loss: 2.0636337187982376

Epoch: 6| Step: 4
Training loss: 2.397191047668457
Validation loss: 2.0553257375635128

Epoch: 6| Step: 5
Training loss: 2.647829532623291
Validation loss: 2.0664033402678785

Epoch: 6| Step: 6
Training loss: 3.181943416595459
Validation loss: 2.0830873391961537

Epoch: 6| Step: 7
Training loss: 2.185856819152832
Validation loss: 2.033765426246069

Epoch: 6| Step: 8
Training loss: 2.680083751678467
Validation loss: 2.053909806795018

Epoch: 6| Step: 9
Training loss: 1.9595797061920166
Validation loss: 2.0804668267567954

Epoch: 6| Step: 10
Training loss: 2.303170919418335
Validation loss: 2.0772374432574034

Epoch: 6| Step: 11
Training loss: 2.013279676437378
Validation loss: 2.042917847633362

Epoch: 6| Step: 12
Training loss: 1.8880012035369873
Validation loss: 2.0637576233956123

Epoch: 6| Step: 13
Training loss: 1.6903132200241089
Validation loss: 2.0610750413710073

Epoch: 85| Step: 0
Training loss: 2.6497254371643066
Validation loss: 2.05469927992872

Epoch: 6| Step: 1
Training loss: 2.241745948791504
Validation loss: 2.075444847024897

Epoch: 6| Step: 2
Training loss: 2.1035614013671875
Validation loss: 2.062383460742171

Epoch: 6| Step: 3
Training loss: 1.7540053129196167
Validation loss: 2.0549374447073987

Epoch: 6| Step: 4
Training loss: 2.905945062637329
Validation loss: 2.0524665924810592

Epoch: 6| Step: 5
Training loss: 2.666019916534424
Validation loss: 2.0719794419503983

Epoch: 6| Step: 6
Training loss: 1.5165789127349854
Validation loss: 2.0799994673780215

Epoch: 6| Step: 7
Training loss: 1.8790782690048218
Validation loss: 2.0748647400127944

Epoch: 6| Step: 8
Training loss: 2.380119562149048
Validation loss: 2.0582373962607434

Epoch: 6| Step: 9
Training loss: 2.5256645679473877
Validation loss: 2.0787005398863103

Epoch: 6| Step: 10
Training loss: 2.122175693511963
Validation loss: 2.0749868346798803

Epoch: 6| Step: 11
Training loss: 2.417264699935913
Validation loss: 2.0923343807138424

Epoch: 6| Step: 12
Training loss: 2.1442859172821045
Validation loss: 2.0773337425724154

Epoch: 6| Step: 13
Training loss: 2.576080799102783
Validation loss: 2.096918559843494

Epoch: 86| Step: 0
Training loss: 2.655118942260742
Validation loss: 2.1022751510784192

Epoch: 6| Step: 1
Training loss: 2.1994378566741943
Validation loss: 2.089165638851863

Epoch: 6| Step: 2
Training loss: 2.6826488971710205
Validation loss: 2.0931278710724204

Epoch: 6| Step: 3
Training loss: 2.449629306793213
Validation loss: 2.0913231859924974

Epoch: 6| Step: 4
Training loss: 2.9142842292785645
Validation loss: 2.0705543487302718

Epoch: 6| Step: 5
Training loss: 1.9327640533447266
Validation loss: 2.057286291994074

Epoch: 6| Step: 6
Training loss: 2.172795295715332
Validation loss: 2.0913066710195234

Epoch: 6| Step: 7
Training loss: 2.4265518188476562
Validation loss: 2.06612378551114

Epoch: 6| Step: 8
Training loss: 2.050485372543335
Validation loss: 2.0913847633587417

Epoch: 6| Step: 9
Training loss: 2.07532000541687
Validation loss: 2.07962437598936

Epoch: 6| Step: 10
Training loss: 2.245816230773926
Validation loss: 2.0674236666771675

Epoch: 6| Step: 11
Training loss: 1.7471510171890259
Validation loss: 2.0895912083246375

Epoch: 6| Step: 12
Training loss: 1.580340027809143
Validation loss: 2.0770091549042733

Epoch: 6| Step: 13
Training loss: 2.594550609588623
Validation loss: 2.083441408731604

Epoch: 87| Step: 0
Training loss: 2.5086159706115723
Validation loss: 2.0776568253835044

Epoch: 6| Step: 1
Training loss: 2.3159141540527344
Validation loss: 2.0771322583639495

Epoch: 6| Step: 2
Training loss: 2.6668033599853516
Validation loss: 2.0763961269009497

Epoch: 6| Step: 3
Training loss: 2.2431087493896484
Validation loss: 2.0777890861675306

Epoch: 6| Step: 4
Training loss: 2.5675878524780273
Validation loss: 2.061858795022452

Epoch: 6| Step: 5
Training loss: 1.8388643264770508
Validation loss: 2.0739670312532814

Epoch: 6| Step: 6
Training loss: 1.7583624124526978
Validation loss: 2.046851081232871

Epoch: 6| Step: 7
Training loss: 1.3744075298309326
Validation loss: 2.0484744733379734

Epoch: 6| Step: 8
Training loss: 2.3339059352874756
Validation loss: 2.0745369247210923

Epoch: 6| Step: 9
Training loss: 2.822078227996826
Validation loss: 2.055300751040059

Epoch: 6| Step: 10
Training loss: 1.7782328128814697
Validation loss: 2.0773224407626736

Epoch: 6| Step: 11
Training loss: 2.4939682483673096
Validation loss: 2.0471575939527122

Epoch: 6| Step: 12
Training loss: 1.9836846590042114
Validation loss: 2.0328790269872195

Epoch: 6| Step: 13
Training loss: 3.472808837890625
Validation loss: 2.049192770834892

Epoch: 88| Step: 0
Training loss: 3.0840160846710205
Validation loss: 2.055378367823939

Epoch: 6| Step: 1
Training loss: 2.253702402114868
Validation loss: 2.0552459352759906

Epoch: 6| Step: 2
Training loss: 2.142455577850342
Validation loss: 2.0598569672594786

Epoch: 6| Step: 3
Training loss: 2.0651931762695312
Validation loss: 2.0671785569960073

Epoch: 6| Step: 4
Training loss: 1.7299566268920898
Validation loss: 2.055488004479357

Epoch: 6| Step: 5
Training loss: 2.4567785263061523
Validation loss: 2.072702154036491

Epoch: 6| Step: 6
Training loss: 2.7866318225860596
Validation loss: 2.0601699518901047

Epoch: 6| Step: 7
Training loss: 2.229269504547119
Validation loss: 2.0577572032969487

Epoch: 6| Step: 8
Training loss: 2.0095925331115723
Validation loss: 2.089673867789648

Epoch: 6| Step: 9
Training loss: 1.8927611112594604
Validation loss: 2.104844388141427

Epoch: 6| Step: 10
Training loss: 1.7357197999954224
Validation loss: 2.067226481694047

Epoch: 6| Step: 11
Training loss: 2.0858664512634277
Validation loss: 2.069101861728135

Epoch: 6| Step: 12
Training loss: 2.77805757522583
Validation loss: 2.0700394491995535

Epoch: 6| Step: 13
Training loss: 2.6968955993652344
Validation loss: 2.0970106176150742

Epoch: 89| Step: 0
Training loss: 1.7996515035629272
Validation loss: 2.080601074362314

Epoch: 6| Step: 1
Training loss: 1.7791452407836914
Validation loss: 2.100709078132465

Epoch: 6| Step: 2
Training loss: 2.278473138809204
Validation loss: 2.0610906757334226

Epoch: 6| Step: 3
Training loss: 1.5021674633026123
Validation loss: 2.051230184493526

Epoch: 6| Step: 4
Training loss: 2.4735028743743896
Validation loss: 2.087311480634956

Epoch: 6| Step: 5
Training loss: 2.3480381965637207
Validation loss: 2.086813913878574

Epoch: 6| Step: 6
Training loss: 2.9357972145080566
Validation loss: 2.0579558969825826

Epoch: 6| Step: 7
Training loss: 1.7842929363250732
Validation loss: 2.070728672448025

Epoch: 6| Step: 8
Training loss: 2.1556203365325928
Validation loss: 2.083685726247808

Epoch: 6| Step: 9
Training loss: 2.273169994354248
Validation loss: 2.067970727079658

Epoch: 6| Step: 10
Training loss: 2.342561721801758
Validation loss: 2.0920742045166674

Epoch: 6| Step: 11
Training loss: 2.839498281478882
Validation loss: 2.1020016977863927

Epoch: 6| Step: 12
Training loss: 2.6795363426208496
Validation loss: 2.0745701020763767

Epoch: 6| Step: 13
Training loss: 2.384518623352051
Validation loss: 2.0780973408811834

Epoch: 90| Step: 0
Training loss: 2.3586111068725586
Validation loss: 2.074123333859187

Epoch: 6| Step: 1
Training loss: 2.271167278289795
Validation loss: 2.0864956045663483

Epoch: 6| Step: 2
Training loss: 2.6380867958068848
Validation loss: 2.085083770495589

Epoch: 6| Step: 3
Training loss: 2.340458869934082
Validation loss: 2.0720275832760717

Epoch: 6| Step: 4
Training loss: 2.1780810356140137
Validation loss: 2.0917227293855403

Epoch: 6| Step: 5
Training loss: 2.264909029006958
Validation loss: 2.0693680586353427

Epoch: 6| Step: 6
Training loss: 2.5608255863189697
Validation loss: 2.0865387788382908

Epoch: 6| Step: 7
Training loss: 2.552027940750122
Validation loss: 2.0778195640092254

Epoch: 6| Step: 8
Training loss: 1.9612370729446411
Validation loss: 2.09317450113194

Epoch: 6| Step: 9
Training loss: 2.3472044467926025
Validation loss: 2.0855756728879866

Epoch: 6| Step: 10
Training loss: 2.0404539108276367
Validation loss: 2.0947411931971067

Epoch: 6| Step: 11
Training loss: 1.846855640411377
Validation loss: 2.11104086009405

Epoch: 6| Step: 12
Training loss: 2.2019457817077637
Validation loss: 2.0795295879405034

Epoch: 6| Step: 13
Training loss: 1.9196913242340088
Validation loss: 2.0827388084062965

Epoch: 91| Step: 0
Training loss: 2.1424005031585693
Validation loss: 2.0857357466092674

Epoch: 6| Step: 1
Training loss: 1.9439226388931274
Validation loss: 2.0860984889409875

Epoch: 6| Step: 2
Training loss: 1.668528437614441
Validation loss: 2.096835251777403

Epoch: 6| Step: 3
Training loss: 2.5046982765197754
Validation loss: 2.094918025437222

Epoch: 6| Step: 4
Training loss: 2.288701295852661
Validation loss: 2.0781531923560688

Epoch: 6| Step: 5
Training loss: 2.401219606399536
Validation loss: 2.0931471291408745

Epoch: 6| Step: 6
Training loss: 2.1138384342193604
Validation loss: 2.0968356837508497

Epoch: 6| Step: 7
Training loss: 2.3909542560577393
Validation loss: 2.098884972192908

Epoch: 6| Step: 8
Training loss: 2.511976718902588
Validation loss: 2.0956471709794897

Epoch: 6| Step: 9
Training loss: 2.3191518783569336
Validation loss: 2.0864988603899555

Epoch: 6| Step: 10
Training loss: 2.318441390991211
Validation loss: 2.080766708620133

Epoch: 6| Step: 11
Training loss: 2.428466320037842
Validation loss: 2.0967773006808375

Epoch: 6| Step: 12
Training loss: 2.547431707382202
Validation loss: 2.090265102283929

Epoch: 6| Step: 13
Training loss: 2.0219624042510986
Validation loss: 2.092698492029662

Epoch: 92| Step: 0
Training loss: 2.2477869987487793
Validation loss: 2.0985538510866064

Epoch: 6| Step: 1
Training loss: 1.9108436107635498
Validation loss: 2.1211104726278656

Epoch: 6| Step: 2
Training loss: 1.6762526035308838
Validation loss: 2.0955709923980055

Epoch: 6| Step: 3
Training loss: 1.8846435546875
Validation loss: 2.0981751359919065

Epoch: 6| Step: 4
Training loss: 2.382902145385742
Validation loss: 2.085394795222949

Epoch: 6| Step: 5
Training loss: 2.631315231323242
Validation loss: 2.060212912098054

Epoch: 6| Step: 6
Training loss: 1.6779005527496338
Validation loss: 2.070819790645312

Epoch: 6| Step: 7
Training loss: 2.2212698459625244
Validation loss: 2.0802351761889715

Epoch: 6| Step: 8
Training loss: 2.589805841445923
Validation loss: 2.0804097216616393

Epoch: 6| Step: 9
Training loss: 2.060349941253662
Validation loss: 2.038956981833263

Epoch: 6| Step: 10
Training loss: 2.8110318183898926
Validation loss: 2.0819407150309575

Epoch: 6| Step: 11
Training loss: 2.309241771697998
Validation loss: 2.073215981965424

Epoch: 6| Step: 12
Training loss: 3.0930709838867188
Validation loss: 2.063533713740687

Epoch: 6| Step: 13
Training loss: 2.1240766048431396
Validation loss: 2.086601835425182

Epoch: 93| Step: 0
Training loss: 2.114637613296509
Validation loss: 2.069851718923097

Epoch: 6| Step: 1
Training loss: 1.8183468580245972
Validation loss: 2.067369528996047

Epoch: 6| Step: 2
Training loss: 2.0894060134887695
Validation loss: 2.0778673182251635

Epoch: 6| Step: 3
Training loss: 1.6390628814697266
Validation loss: 2.088050583357452

Epoch: 6| Step: 4
Training loss: 1.9657756090164185
Validation loss: 2.0781279481867307

Epoch: 6| Step: 5
Training loss: 2.8361124992370605
Validation loss: 2.062363302835854

Epoch: 6| Step: 6
Training loss: 2.0851690769195557
Validation loss: 2.0781946387342227

Epoch: 6| Step: 7
Training loss: 3.06288743019104
Validation loss: 2.065553024250974

Epoch: 6| Step: 8
Training loss: 1.593660593032837
Validation loss: 2.084914093376488

Epoch: 6| Step: 9
Training loss: 1.8342928886413574
Validation loss: 2.08570437277517

Epoch: 6| Step: 10
Training loss: 2.4284770488739014
Validation loss: 2.080645138217557

Epoch: 6| Step: 11
Training loss: 2.08821439743042
Validation loss: 2.084836147164786

Epoch: 6| Step: 12
Training loss: 3.5489118099212646
Validation loss: 2.071220205676171

Epoch: 6| Step: 13
Training loss: 2.7596330642700195
Validation loss: 2.0558392309373423

Epoch: 94| Step: 0
Training loss: 2.5242676734924316
Validation loss: 2.0862270837189048

Epoch: 6| Step: 1
Training loss: 3.2704906463623047
Validation loss: 2.0591150676050494

Epoch: 6| Step: 2
Training loss: 2.0428876876831055
Validation loss: 2.0563231155436528

Epoch: 6| Step: 3
Training loss: 2.1572179794311523
Validation loss: 2.090307015244679

Epoch: 6| Step: 4
Training loss: 2.6120500564575195
Validation loss: 2.0469215710957847

Epoch: 6| Step: 5
Training loss: 1.9363240003585815
Validation loss: 2.084719140042541

Epoch: 6| Step: 6
Training loss: 2.788595199584961
Validation loss: 2.0469630418285245

Epoch: 6| Step: 7
Training loss: 2.296144962310791
Validation loss: 2.059790806103778

Epoch: 6| Step: 8
Training loss: 2.209254264831543
Validation loss: 2.076139223191046

Epoch: 6| Step: 9
Training loss: 1.9199302196502686
Validation loss: 2.083506000939236

Epoch: 6| Step: 10
Training loss: 2.1740639209747314
Validation loss: 2.0534282115197953

Epoch: 6| Step: 11
Training loss: 1.8740038871765137
Validation loss: 2.082434913163544

Epoch: 6| Step: 12
Training loss: 1.701284646987915
Validation loss: 2.076669482774632

Epoch: 6| Step: 13
Training loss: 2.03265118598938
Validation loss: 2.0795960016148065

Epoch: 95| Step: 0
Training loss: 2.0383169651031494
Validation loss: 2.0572507253257175

Epoch: 6| Step: 1
Training loss: 2.411868095397949
Validation loss: 2.071169296900431

Epoch: 6| Step: 2
Training loss: 2.223468542098999
Validation loss: 2.0746779634106542

Epoch: 6| Step: 3
Training loss: 2.2762675285339355
Validation loss: 2.0854542383583645

Epoch: 6| Step: 4
Training loss: 1.9854886531829834
Validation loss: 2.063278264896844

Epoch: 6| Step: 5
Training loss: 2.097607135772705
Validation loss: 2.1011845398974676

Epoch: 6| Step: 6
Training loss: 2.4623310565948486
Validation loss: 2.0785040906680528

Epoch: 6| Step: 7
Training loss: 2.5848875045776367
Validation loss: 2.0797168311252388

Epoch: 6| Step: 8
Training loss: 2.6873722076416016
Validation loss: 2.1167547125970163

Epoch: 6| Step: 9
Training loss: 2.068430185317993
Validation loss: 2.1107552436090287

Epoch: 6| Step: 10
Training loss: 2.565753936767578
Validation loss: 2.0941655776833974

Epoch: 6| Step: 11
Training loss: 1.6599984169006348
Validation loss: 2.117247196935838

Epoch: 6| Step: 12
Training loss: 2.4597837924957275
Validation loss: 2.0961158788332375

Epoch: 6| Step: 13
Training loss: 2.102933406829834
Validation loss: 2.1050812916089128

Epoch: 96| Step: 0
Training loss: 2.0350382328033447
Validation loss: 2.1083258018698743

Epoch: 6| Step: 1
Training loss: 3.067084789276123
Validation loss: 2.114275311910978

Epoch: 6| Step: 2
Training loss: 1.8341124057769775
Validation loss: 2.0915849747196322

Epoch: 6| Step: 3
Training loss: 2.0740528106689453
Validation loss: 2.0746721657373572

Epoch: 6| Step: 4
Training loss: 2.536296844482422
Validation loss: 2.0618249806024695

Epoch: 6| Step: 5
Training loss: 2.099076271057129
Validation loss: 2.0849350421659407

Epoch: 6| Step: 6
Training loss: 1.6134047508239746
Validation loss: 2.071278836137505

Epoch: 6| Step: 7
Training loss: 2.2291550636291504
Validation loss: 2.069998418131182

Epoch: 6| Step: 8
Training loss: 2.9147186279296875
Validation loss: 2.0599472484280987

Epoch: 6| Step: 9
Training loss: 2.245001792907715
Validation loss: 2.064276849069903

Epoch: 6| Step: 10
Training loss: 2.0789432525634766
Validation loss: 2.078863710485479

Epoch: 6| Step: 11
Training loss: 1.9832922220230103
Validation loss: 2.0981720468049407

Epoch: 6| Step: 12
Training loss: 2.431694507598877
Validation loss: 2.07596956914471

Epoch: 6| Step: 13
Training loss: 2.664249897003174
Validation loss: 2.0633780084630495

Epoch: 97| Step: 0
Training loss: 2.226698398590088
Validation loss: 2.0466749411757275

Epoch: 6| Step: 1
Training loss: 1.6387114524841309
Validation loss: 2.053068099483367

Epoch: 6| Step: 2
Training loss: 1.523193597793579
Validation loss: 2.058743526858668

Epoch: 6| Step: 3
Training loss: 3.046827793121338
Validation loss: 2.067234149543188

Epoch: 6| Step: 4
Training loss: 2.615269184112549
Validation loss: 2.0850051923464705

Epoch: 6| Step: 5
Training loss: 3.1930222511291504
Validation loss: 2.0754369176844114

Epoch: 6| Step: 6
Training loss: 2.284968376159668
Validation loss: 2.076086856985605

Epoch: 6| Step: 7
Training loss: 2.329721689224243
Validation loss: 2.060899137168802

Epoch: 6| Step: 8
Training loss: 1.8162128925323486
Validation loss: 2.057948525233935

Epoch: 6| Step: 9
Training loss: 2.4058585166931152
Validation loss: 2.059111838699669

Epoch: 6| Step: 10
Training loss: 1.7119088172912598
Validation loss: 2.041410485903422

Epoch: 6| Step: 11
Training loss: 2.653104305267334
Validation loss: 2.0717523072355535

Epoch: 6| Step: 12
Training loss: 2.3931808471679688
Validation loss: 2.0697011063175816

Epoch: 6| Step: 13
Training loss: 1.219972848892212
Validation loss: 2.062547165860412

Epoch: 98| Step: 0
Training loss: 2.2029659748077393
Validation loss: 2.047113335260781

Epoch: 6| Step: 1
Training loss: 2.476689338684082
Validation loss: 2.082921137091934

Epoch: 6| Step: 2
Training loss: 2.531175136566162
Validation loss: 2.083088233906736

Epoch: 6| Step: 3
Training loss: 1.4091171026229858
Validation loss: 2.0929045190093336

Epoch: 6| Step: 4
Training loss: 1.7674343585968018
Validation loss: 2.084418532668903

Epoch: 6| Step: 5
Training loss: 3.0789129734039307
Validation loss: 2.071117780541861

Epoch: 6| Step: 6
Training loss: 2.877110481262207
Validation loss: 2.0767570887842486

Epoch: 6| Step: 7
Training loss: 2.4626035690307617
Validation loss: 2.08453457842591

Epoch: 6| Step: 8
Training loss: 2.2259182929992676
Validation loss: 2.0764827958999144

Epoch: 6| Step: 9
Training loss: 1.786405086517334
Validation loss: 2.080113027685432

Epoch: 6| Step: 10
Training loss: 2.123307704925537
Validation loss: 2.0842129338172173

Epoch: 6| Step: 11
Training loss: 2.689650774002075
Validation loss: 2.078645298557897

Epoch: 6| Step: 12
Training loss: 1.352074146270752
Validation loss: 2.0794279472802275

Epoch: 6| Step: 13
Training loss: 2.7789618968963623
Validation loss: 2.094791220080468

Epoch: 99| Step: 0
Training loss: 2.638119697570801
Validation loss: 2.093881135345787

Epoch: 6| Step: 1
Training loss: 1.738194465637207
Validation loss: 2.098536829794607

Epoch: 6| Step: 2
Training loss: 1.6871609687805176
Validation loss: 2.108863330656482

Epoch: 6| Step: 3
Training loss: 1.2142014503479004
Validation loss: 2.1000487291684715

Epoch: 6| Step: 4
Training loss: 2.3218514919281006
Validation loss: 2.1031060834084787

Epoch: 6| Step: 5
Training loss: 3.004849910736084
Validation loss: 2.094318812893283

Epoch: 6| Step: 6
Training loss: 2.7499008178710938
Validation loss: 2.088728348414103

Epoch: 6| Step: 7
Training loss: 1.7038655281066895
Validation loss: 2.0941347704138806

Epoch: 6| Step: 8
Training loss: 2.1964900493621826
Validation loss: 2.0886346627307195

Epoch: 6| Step: 9
Training loss: 2.153256416320801
Validation loss: 2.0930831945070656

Epoch: 6| Step: 10
Training loss: 2.9443883895874023
Validation loss: 2.0564562210472683

Epoch: 6| Step: 11
Training loss: 2.8931291103363037
Validation loss: 2.0693617020883868

Epoch: 6| Step: 12
Training loss: 2.29195499420166
Validation loss: 2.0839590641760055

Epoch: 6| Step: 13
Training loss: 1.7994308471679688
Validation loss: 2.0806840888915525

Epoch: 100| Step: 0
Training loss: 1.422670841217041
Validation loss: 2.086993650723529

Epoch: 6| Step: 1
Training loss: 2.1560137271881104
Validation loss: 2.0833953772821734

Epoch: 6| Step: 2
Training loss: 2.377412796020508
Validation loss: 2.1147275586282053

Epoch: 6| Step: 3
Training loss: 3.009258270263672
Validation loss: 2.096377985451811

Epoch: 6| Step: 4
Training loss: 2.540156602859497
Validation loss: 2.0902072511693484

Epoch: 6| Step: 5
Training loss: 2.1364080905914307
Validation loss: 2.0668515505329257

Epoch: 6| Step: 6
Training loss: 2.1535465717315674
Validation loss: 2.1104507266834216

Epoch: 6| Step: 7
Training loss: 1.8994388580322266
Validation loss: 2.0964965128129527

Epoch: 6| Step: 8
Training loss: 2.7685306072235107
Validation loss: 2.103666954143073

Epoch: 6| Step: 9
Training loss: 2.392436981201172
Validation loss: 2.097388421335528

Epoch: 6| Step: 10
Training loss: 2.008927822113037
Validation loss: 2.0959096211259083

Epoch: 6| Step: 11
Training loss: 1.8138070106506348
Validation loss: 2.0816463501222673

Epoch: 6| Step: 12
Training loss: 2.551004409790039
Validation loss: 2.08124178712086

Epoch: 6| Step: 13
Training loss: 2.7718396186828613
Validation loss: 2.0884213345025175

Epoch: 101| Step: 0
Training loss: 2.6055383682250977
Validation loss: 2.0740167992089384

Epoch: 6| Step: 1
Training loss: 2.375370979309082
Validation loss: 2.106281236935687

Epoch: 6| Step: 2
Training loss: 2.195530414581299
Validation loss: 2.085612484203872

Epoch: 6| Step: 3
Training loss: 1.2629239559173584
Validation loss: 2.092634185667961

Epoch: 6| Step: 4
Training loss: 2.016828775405884
Validation loss: 2.0888162710333384

Epoch: 6| Step: 5
Training loss: 2.2626147270202637
Validation loss: 2.0894917698316675

Epoch: 6| Step: 6
Training loss: 2.015883207321167
Validation loss: 2.0863914541018906

Epoch: 6| Step: 7
Training loss: 2.6510701179504395
Validation loss: 2.0829084803981166

Epoch: 6| Step: 8
Training loss: 2.358790874481201
Validation loss: 2.086312363224645

Epoch: 6| Step: 9
Training loss: 2.3156168460845947
Validation loss: 2.0880068399572886

Epoch: 6| Step: 10
Training loss: 2.464061737060547
Validation loss: 2.095123408943094

Epoch: 6| Step: 11
Training loss: 2.869688034057617
Validation loss: 2.10323759817308

Epoch: 6| Step: 12
Training loss: 2.3061017990112305
Validation loss: 2.116093917559552

Epoch: 6| Step: 13
Training loss: 1.3948854207992554
Validation loss: 2.1026308869802826

Epoch: 102| Step: 0
Training loss: 1.5835187435150146
Validation loss: 2.0976677735646567

Epoch: 6| Step: 1
Training loss: 1.8810479640960693
Validation loss: 2.106915509828957

Epoch: 6| Step: 2
Training loss: 3.125300884246826
Validation loss: 2.095176569877132

Epoch: 6| Step: 3
Training loss: 2.3723061084747314
Validation loss: 2.1155883483989264

Epoch: 6| Step: 4
Training loss: 2.061034679412842
Validation loss: 2.0976981834698747

Epoch: 6| Step: 5
Training loss: 2.349968910217285
Validation loss: 2.0986201763153076

Epoch: 6| Step: 6
Training loss: 2.6831326484680176
Validation loss: 2.0807101124076435

Epoch: 6| Step: 7
Training loss: 2.173177719116211
Validation loss: 2.0877665883751324

Epoch: 6| Step: 8
Training loss: 1.6689426898956299
Validation loss: 2.081651367166991

Epoch: 6| Step: 9
Training loss: 1.5483418703079224
Validation loss: 2.0846256286867204

Epoch: 6| Step: 10
Training loss: 2.3965206146240234
Validation loss: 2.0921284678161784

Epoch: 6| Step: 11
Training loss: 1.99922513961792
Validation loss: 2.0765055277014293

Epoch: 6| Step: 12
Training loss: 3.06868052482605
Validation loss: 2.0526581989821566

Epoch: 6| Step: 13
Training loss: 2.5017919540405273
Validation loss: 2.0835966692175916

Epoch: 103| Step: 0
Training loss: 2.9676883220672607
Validation loss: 2.0590084316909953

Epoch: 6| Step: 1
Training loss: 2.8778610229492188
Validation loss: 2.061610832009264

Epoch: 6| Step: 2
Training loss: 1.8392205238342285
Validation loss: 2.0778643149201588

Epoch: 6| Step: 3
Training loss: 2.7899062633514404
Validation loss: 2.07622940053222

Epoch: 6| Step: 4
Training loss: 1.4569790363311768
Validation loss: 2.075742374184311

Epoch: 6| Step: 5
Training loss: 1.942190170288086
Validation loss: 2.0803719348804925

Epoch: 6| Step: 6
Training loss: 2.0159573554992676
Validation loss: 2.0671098616815384

Epoch: 6| Step: 7
Training loss: 2.6165995597839355
Validation loss: 2.092307491969037

Epoch: 6| Step: 8
Training loss: 1.7143644094467163
Validation loss: 2.0931498850545576

Epoch: 6| Step: 9
Training loss: 2.5247764587402344
Validation loss: 2.0952468392669514

Epoch: 6| Step: 10
Training loss: 1.980414628982544
Validation loss: 2.0784761110941568

Epoch: 6| Step: 11
Training loss: 1.609177589416504
Validation loss: 2.084112972341558

Epoch: 6| Step: 12
Training loss: 2.0694828033447266
Validation loss: 2.060410676463958

Epoch: 6| Step: 13
Training loss: 3.1749045848846436
Validation loss: 2.083074603029477

Epoch: 104| Step: 0
Training loss: 3.0073039531707764
Validation loss: 2.0772100802390807

Epoch: 6| Step: 1
Training loss: 3.0544943809509277
Validation loss: 2.087320425177133

Epoch: 6| Step: 2
Training loss: 1.9154177904129028
Validation loss: 2.0856737449604976

Epoch: 6| Step: 3
Training loss: 3.023146867752075
Validation loss: 2.088176974686243

Epoch: 6| Step: 4
Training loss: 1.5443819761276245
Validation loss: 2.101712290958692

Epoch: 6| Step: 5
Training loss: 1.9762072563171387
Validation loss: 2.0986145978332846

Epoch: 6| Step: 6
Training loss: 2.872373580932617
Validation loss: 2.070119152786911

Epoch: 6| Step: 7
Training loss: 2.372128963470459
Validation loss: 2.0914748073906027

Epoch: 6| Step: 8
Training loss: 2.275766134262085
Validation loss: 2.085381028472736

Epoch: 6| Step: 9
Training loss: 1.7932003736495972
Validation loss: 2.094785100670271

Epoch: 6| Step: 10
Training loss: 2.3122622966766357
Validation loss: 2.0859480827085433

Epoch: 6| Step: 11
Training loss: 1.9518805742263794
Validation loss: 2.107626715014058

Epoch: 6| Step: 12
Training loss: 1.1701815128326416
Validation loss: 2.0867960247942197

Epoch: 6| Step: 13
Training loss: 1.6383053064346313
Validation loss: 2.087895581799169

Epoch: 105| Step: 0
Training loss: 2.2413830757141113
Validation loss: 2.0922275243266935

Epoch: 6| Step: 1
Training loss: 2.374758243560791
Validation loss: 2.0807606302281862

Epoch: 6| Step: 2
Training loss: 1.82300865650177
Validation loss: 2.11286340221282

Epoch: 6| Step: 3
Training loss: 3.1955628395080566
Validation loss: 2.121188857222116

Epoch: 6| Step: 4
Training loss: 2.7805838584899902
Validation loss: 2.1180375891347087

Epoch: 6| Step: 5
Training loss: 1.8764678239822388
Validation loss: 2.080739777575257

Epoch: 6| Step: 6
Training loss: 2.299774646759033
Validation loss: 2.1053618090127104

Epoch: 6| Step: 7
Training loss: 2.115907669067383
Validation loss: 2.0924844767457698

Epoch: 6| Step: 8
Training loss: 2.020064353942871
Validation loss: 2.098944781928934

Epoch: 6| Step: 9
Training loss: 2.167494535446167
Validation loss: 2.0868232686032533

Epoch: 6| Step: 10
Training loss: 2.297429084777832
Validation loss: 2.1164810785683255

Epoch: 6| Step: 11
Training loss: 2.2696661949157715
Validation loss: 2.086159239533127

Epoch: 6| Step: 12
Training loss: 1.9566947221755981
Validation loss: 2.092615891528386

Epoch: 6| Step: 13
Training loss: 1.9792101383209229
Validation loss: 2.0831043617699736

Epoch: 106| Step: 0
Training loss: 2.9593541622161865
Validation loss: 2.1020821730295816

Epoch: 6| Step: 1
Training loss: 2.6376161575317383
Validation loss: 2.103590511506604

Epoch: 6| Step: 2
Training loss: 1.3581039905548096
Validation loss: 2.1152351825468

Epoch: 6| Step: 3
Training loss: 1.949989676475525
Validation loss: 2.125151648316332

Epoch: 6| Step: 4
Training loss: 1.5795488357543945
Validation loss: 2.1131935516993203

Epoch: 6| Step: 5
Training loss: 2.1661930084228516
Validation loss: 2.1060145132003294

Epoch: 6| Step: 6
Training loss: 2.5660340785980225
Validation loss: 2.1304286449186263

Epoch: 6| Step: 7
Training loss: 3.2225775718688965
Validation loss: 2.133468138274326

Epoch: 6| Step: 8
Training loss: 2.913663864135742
Validation loss: 2.1365741760500017

Epoch: 6| Step: 9
Training loss: 1.4171488285064697
Validation loss: 2.1147016171486146

Epoch: 6| Step: 10
Training loss: 1.5873100757598877
Validation loss: 2.1263679842795096

Epoch: 6| Step: 11
Training loss: 2.1684939861297607
Validation loss: 2.1397605596050138

Epoch: 6| Step: 12
Training loss: 2.414332389831543
Validation loss: 2.1000961616475093

Epoch: 6| Step: 13
Training loss: 2.4727466106414795
Validation loss: 2.115328747739074

Epoch: 107| Step: 0
Training loss: 2.6317648887634277
Validation loss: 2.099491407794337

Epoch: 6| Step: 1
Training loss: 2.5237016677856445
Validation loss: 2.121247504347114

Epoch: 6| Step: 2
Training loss: 2.2707982063293457
Validation loss: 2.1311659812927246

Epoch: 6| Step: 3
Training loss: 2.014882802963257
Validation loss: 2.110101819038391

Epoch: 6| Step: 4
Training loss: 2.399505376815796
Validation loss: 2.1225596474063013

Epoch: 6| Step: 5
Training loss: 1.2214105129241943
Validation loss: 2.0999851444716096

Epoch: 6| Step: 6
Training loss: 2.230278968811035
Validation loss: 2.1198018930291616

Epoch: 6| Step: 7
Training loss: 2.6502492427825928
Validation loss: 2.1030679389994633

Epoch: 6| Step: 8
Training loss: 1.6508193016052246
Validation loss: 2.1001806720610587

Epoch: 6| Step: 9
Training loss: 2.5764522552490234
Validation loss: 2.093636585820106

Epoch: 6| Step: 10
Training loss: 2.558286666870117
Validation loss: 2.1045522766728557

Epoch: 6| Step: 11
Training loss: 2.191852569580078
Validation loss: 2.089223431002709

Epoch: 6| Step: 12
Training loss: 1.7065720558166504
Validation loss: 2.0975522674540037

Epoch: 6| Step: 13
Training loss: 3.1333670616149902
Validation loss: 2.0871481485264276

Epoch: 108| Step: 0
Training loss: 2.110459327697754
Validation loss: 2.0676100561695714

Epoch: 6| Step: 1
Training loss: 2.4029719829559326
Validation loss: 2.0739509572264967

Epoch: 6| Step: 2
Training loss: 2.9610023498535156
Validation loss: 2.10013553532221

Epoch: 6| Step: 3
Training loss: 1.9366992712020874
Validation loss: 2.086590261869533

Epoch: 6| Step: 4
Training loss: 2.067448616027832
Validation loss: 2.100091568885311

Epoch: 6| Step: 5
Training loss: 1.926851511001587
Validation loss: 2.0737748043511504

Epoch: 6| Step: 6
Training loss: 1.9099042415618896
Validation loss: 2.111871334814256

Epoch: 6| Step: 7
Training loss: 2.208631753921509
Validation loss: 2.088390193959718

Epoch: 6| Step: 8
Training loss: 2.154798746109009
Validation loss: 2.0859208299267675

Epoch: 6| Step: 9
Training loss: 1.572708249092102
Validation loss: 2.0718309507575086

Epoch: 6| Step: 10
Training loss: 2.2386202812194824
Validation loss: 2.088367997959096

Epoch: 6| Step: 11
Training loss: 2.140105724334717
Validation loss: 2.065596439505136

Epoch: 6| Step: 12
Training loss: 2.920823097229004
Validation loss: 2.0823160333018147

Epoch: 6| Step: 13
Training loss: 2.743619680404663
Validation loss: 2.070160271019064

Epoch: 109| Step: 0
Training loss: 1.7079055309295654
Validation loss: 2.071703931336762

Epoch: 6| Step: 1
Training loss: 1.658081293106079
Validation loss: 2.0788881791535245

Epoch: 6| Step: 2
Training loss: 2.0570077896118164
Validation loss: 2.074529145353584

Epoch: 6| Step: 3
Training loss: 2.3472483158111572
Validation loss: 2.069764639741631

Epoch: 6| Step: 4
Training loss: 2.6929025650024414
Validation loss: 2.0608521251268286

Epoch: 6| Step: 5
Training loss: 2.2754554748535156
Validation loss: 2.0588588201871483

Epoch: 6| Step: 6
Training loss: 2.0857460498809814
Validation loss: 2.0710802257701917

Epoch: 6| Step: 7
Training loss: 2.449097156524658
Validation loss: 2.0751474493293354

Epoch: 6| Step: 8
Training loss: 2.2867517471313477
Validation loss: 2.059120532005064

Epoch: 6| Step: 9
Training loss: 2.76483416557312
Validation loss: 2.0664868726525256

Epoch: 6| Step: 10
Training loss: 2.7803564071655273
Validation loss: 2.054635586277131

Epoch: 6| Step: 11
Training loss: 1.2328507900238037
Validation loss: 2.0562358748528267

Epoch: 6| Step: 12
Training loss: 2.5783393383026123
Validation loss: 2.0705373235928115

Epoch: 6| Step: 13
Training loss: 2.6434338092803955
Validation loss: 2.026454398708959

Epoch: 110| Step: 0
Training loss: 2.6132116317749023
Validation loss: 2.060824510871723

Epoch: 6| Step: 1
Training loss: 2.627143621444702
Validation loss: 2.081395233831098

Epoch: 6| Step: 2
Training loss: 2.4517910480499268
Validation loss: 2.0862716833750405

Epoch: 6| Step: 3
Training loss: 2.3766486644744873
Validation loss: 2.0974916155620287

Epoch: 6| Step: 4
Training loss: 1.64276123046875
Validation loss: 2.066915814594556

Epoch: 6| Step: 5
Training loss: 1.6523072719573975
Validation loss: 2.0831219175810456

Epoch: 6| Step: 6
Training loss: 2.702061176300049
Validation loss: 2.089059550275085

Epoch: 6| Step: 7
Training loss: 1.9178152084350586
Validation loss: 2.1196206051816224

Epoch: 6| Step: 8
Training loss: 2.542224407196045
Validation loss: 2.1172980787933513

Epoch: 6| Step: 9
Training loss: 2.3730502128601074
Validation loss: 2.137647316020022

Epoch: 6| Step: 10
Training loss: 2.2999730110168457
Validation loss: 2.1174990989828624

Epoch: 6| Step: 11
Training loss: 2.046858549118042
Validation loss: 2.1014951813605522

Epoch: 6| Step: 12
Training loss: 2.1943936347961426
Validation loss: 2.0744476933633127

Epoch: 6| Step: 13
Training loss: 1.788093090057373
Validation loss: 2.128932608071194

Epoch: 111| Step: 0
Training loss: 2.411259651184082
Validation loss: 2.1096770686487996

Epoch: 6| Step: 1
Training loss: 2.3087403774261475
Validation loss: 2.132396571097835

Epoch: 6| Step: 2
Training loss: 2.0308196544647217
Validation loss: 2.14386397792447

Epoch: 6| Step: 3
Training loss: 2.4515860080718994
Validation loss: 2.0909668681442097

Epoch: 6| Step: 4
Training loss: 2.7101783752441406
Validation loss: 2.1218093928470405

Epoch: 6| Step: 5
Training loss: 1.4907937049865723
Validation loss: 2.085010761855751

Epoch: 6| Step: 6
Training loss: 2.2258968353271484
Validation loss: 2.1122602070531538

Epoch: 6| Step: 7
Training loss: 2.3051533699035645
Validation loss: 2.119153827749273

Epoch: 6| Step: 8
Training loss: 2.6837000846862793
Validation loss: 2.1180210421162267

Epoch: 6| Step: 9
Training loss: 2.1932454109191895
Validation loss: 2.1261424454309608

Epoch: 6| Step: 10
Training loss: 1.9907435178756714
Validation loss: 2.097786498326127

Epoch: 6| Step: 11
Training loss: 1.80643630027771
Validation loss: 2.117590553017073

Epoch: 6| Step: 12
Training loss: 2.481463670730591
Validation loss: 2.0958287485184206

Epoch: 6| Step: 13
Training loss: 1.8966612815856934
Validation loss: 2.1033679964721843

Epoch: 112| Step: 0
Training loss: 2.3857250213623047
Validation loss: 2.104361880210138

Epoch: 6| Step: 1
Training loss: 1.373693585395813
Validation loss: 2.0946326947981313

Epoch: 6| Step: 2
Training loss: 2.5585970878601074
Validation loss: 2.1182679899277224

Epoch: 6| Step: 3
Training loss: 1.8373562097549438
Validation loss: 2.1219504007729153

Epoch: 6| Step: 4
Training loss: 1.6574623584747314
Validation loss: 2.088669001415212

Epoch: 6| Step: 5
Training loss: 2.603015422821045
Validation loss: 2.096847040678865

Epoch: 6| Step: 6
Training loss: 2.507148265838623
Validation loss: 2.101358970006307

Epoch: 6| Step: 7
Training loss: 3.0255682468414307
Validation loss: 2.0888818938245057

Epoch: 6| Step: 8
Training loss: 2.4354496002197266
Validation loss: 2.108944398100658

Epoch: 6| Step: 9
Training loss: 2.2411508560180664
Validation loss: 2.0965016990579586

Epoch: 6| Step: 10
Training loss: 1.9064667224884033
Validation loss: 2.111392190379481

Epoch: 6| Step: 11
Training loss: 2.800441265106201
Validation loss: 2.1056316898715113

Epoch: 6| Step: 12
Training loss: 2.0514984130859375
Validation loss: 2.0816635483054706

Epoch: 6| Step: 13
Training loss: 1.4847581386566162
Validation loss: 2.11804687335927

Epoch: 113| Step: 0
Training loss: 1.9889730215072632
Validation loss: 2.113349906859859

Epoch: 6| Step: 1
Training loss: 1.751194715499878
Validation loss: 2.0700825375895344

Epoch: 6| Step: 2
Training loss: 1.458561658859253
Validation loss: 2.081845155326269

Epoch: 6| Step: 3
Training loss: 2.8423309326171875
Validation loss: 2.08824114261135

Epoch: 6| Step: 4
Training loss: 1.9599032402038574
Validation loss: 2.0966111357494066

Epoch: 6| Step: 5
Training loss: 3.1071393489837646
Validation loss: 2.0928778674012873

Epoch: 6| Step: 6
Training loss: 2.101370334625244
Validation loss: 2.1148303452358452

Epoch: 6| Step: 7
Training loss: 1.8795632123947144
Validation loss: 2.0772176634880806

Epoch: 6| Step: 8
Training loss: 2.0952625274658203
Validation loss: 2.0772382418314614

Epoch: 6| Step: 9
Training loss: 2.921436309814453
Validation loss: 2.09028091866483

Epoch: 6| Step: 10
Training loss: 2.9701623916625977
Validation loss: 2.1018533578483005

Epoch: 6| Step: 11
Training loss: 2.4604787826538086
Validation loss: 2.0961192679661576

Epoch: 6| Step: 12
Training loss: 1.9037911891937256
Validation loss: 2.108160090702836

Epoch: 6| Step: 13
Training loss: 1.4293596744537354
Validation loss: 2.0905009956770044

Epoch: 114| Step: 0
Training loss: 2.902836799621582
Validation loss: 2.079977789232808

Epoch: 6| Step: 1
Training loss: 2.256661891937256
Validation loss: 2.0834689870957406

Epoch: 6| Step: 2
Training loss: 1.8091847896575928
Validation loss: 2.0885374264050554

Epoch: 6| Step: 3
Training loss: 2.9608383178710938
Validation loss: 2.0953157332635697

Epoch: 6| Step: 4
Training loss: 2.1569478511810303
Validation loss: 2.0830859420120076

Epoch: 6| Step: 5
Training loss: 2.6752307415008545
Validation loss: 2.1061064761172057

Epoch: 6| Step: 6
Training loss: 2.0526130199432373
Validation loss: 2.0580611741670998

Epoch: 6| Step: 7
Training loss: 2.3559083938598633
Validation loss: 2.07919757340544

Epoch: 6| Step: 8
Training loss: 1.9296510219573975
Validation loss: 2.0852784315745034

Epoch: 6| Step: 9
Training loss: 1.2162725925445557
Validation loss: 2.06782458161795

Epoch: 6| Step: 10
Training loss: 2.134428024291992
Validation loss: 2.1040721221636702

Epoch: 6| Step: 11
Training loss: 2.2027010917663574
Validation loss: 2.0997853984114943

Epoch: 6| Step: 12
Training loss: 1.9716717004776
Validation loss: 2.0903919717316986

Epoch: 6| Step: 13
Training loss: 2.6022191047668457
Validation loss: 2.1076960486750447

Epoch: 115| Step: 0
Training loss: 2.997314691543579
Validation loss: 2.0885043592863184

Epoch: 6| Step: 1
Training loss: 2.5180795192718506
Validation loss: 2.09377961004934

Epoch: 6| Step: 2
Training loss: 2.213212490081787
Validation loss: 2.080525008581018

Epoch: 6| Step: 3
Training loss: 2.390928030014038
Validation loss: 2.112588015935754

Epoch: 6| Step: 4
Training loss: 2.408243179321289
Validation loss: 2.097753852926275

Epoch: 6| Step: 5
Training loss: 2.302619218826294
Validation loss: 2.067493338738718

Epoch: 6| Step: 6
Training loss: 2.139822483062744
Validation loss: 2.088149283521919

Epoch: 6| Step: 7
Training loss: 2.5481362342834473
Validation loss: 2.114620134394656

Epoch: 6| Step: 8
Training loss: 2.4212207794189453
Validation loss: 2.1065720435111754

Epoch: 6| Step: 9
Training loss: 1.678944706916809
Validation loss: 2.105035053786411

Epoch: 6| Step: 10
Training loss: 2.008967399597168
Validation loss: 2.102393445148263

Epoch: 6| Step: 11
Training loss: 2.155081272125244
Validation loss: 2.079722500616504

Epoch: 6| Step: 12
Training loss: 1.4557569026947021
Validation loss: 2.1031099596331195

Epoch: 6| Step: 13
Training loss: 2.153510808944702
Validation loss: 2.096285773861793

Epoch: 116| Step: 0
Training loss: 3.0372605323791504
Validation loss: 2.1009512678269417

Epoch: 6| Step: 1
Training loss: 1.7690372467041016
Validation loss: 2.1031232444188928

Epoch: 6| Step: 2
Training loss: 1.6100767850875854
Validation loss: 2.0833829846433414

Epoch: 6| Step: 3
Training loss: 1.8384480476379395
Validation loss: 2.10211617203169

Epoch: 6| Step: 4
Training loss: 2.0469796657562256
Validation loss: 2.1127367122198946

Epoch: 6| Step: 5
Training loss: 2.515021800994873
Validation loss: 2.1101474569689844

Epoch: 6| Step: 6
Training loss: 1.825506567955017
Validation loss: 2.1042667306879514

Epoch: 6| Step: 7
Training loss: 3.371199369430542
Validation loss: 2.099003111162493

Epoch: 6| Step: 8
Training loss: 1.912768006324768
Validation loss: 2.089590959651496

Epoch: 6| Step: 9
Training loss: 2.858400344848633
Validation loss: 2.1080397482841247

Epoch: 6| Step: 10
Training loss: 1.8211593627929688
Validation loss: 2.103746388548164

Epoch: 6| Step: 11
Training loss: 2.2671308517456055
Validation loss: 2.0719691181695588

Epoch: 6| Step: 12
Training loss: 2.0171172618865967
Validation loss: 2.100635549073578

Epoch: 6| Step: 13
Training loss: 2.567437171936035
Validation loss: 2.0837020207476873

Epoch: 117| Step: 0
Training loss: 1.7372031211853027
Validation loss: 2.0850850074521956

Epoch: 6| Step: 1
Training loss: 1.6688299179077148
Validation loss: 2.0729350492518437

Epoch: 6| Step: 2
Training loss: 3.114535093307495
Validation loss: 2.0551494411242905

Epoch: 6| Step: 3
Training loss: 2.1446950435638428
Validation loss: 2.086750284318001

Epoch: 6| Step: 4
Training loss: 1.0950791835784912
Validation loss: 2.0671382129833265

Epoch: 6| Step: 5
Training loss: 2.917996644973755
Validation loss: 2.0933436039955384

Epoch: 6| Step: 6
Training loss: 1.8350352048873901
Validation loss: 2.065338110411039

Epoch: 6| Step: 7
Training loss: 1.807027816772461
Validation loss: 2.0688904357212845

Epoch: 6| Step: 8
Training loss: 2.462996482849121
Validation loss: 2.1046246559389177

Epoch: 6| Step: 9
Training loss: 1.3387835025787354
Validation loss: 2.093092587686354

Epoch: 6| Step: 10
Training loss: 2.6188864707946777
Validation loss: 2.071714255117601

Epoch: 6| Step: 11
Training loss: 2.9574475288391113
Validation loss: 2.0764621509018766

Epoch: 6| Step: 12
Training loss: 2.9444336891174316
Validation loss: 2.090354645124046

Epoch: 6| Step: 13
Training loss: 2.7312068939208984
Validation loss: 2.0755056027443177

Epoch: 118| Step: 0
Training loss: 1.6506813764572144
Validation loss: 2.0766419197923396

Epoch: 6| Step: 1
Training loss: 1.8286033868789673
Validation loss: 2.0769135208540064

Epoch: 6| Step: 2
Training loss: 2.8661727905273438
Validation loss: 2.0809176262988838

Epoch: 6| Step: 3
Training loss: 2.195301055908203
Validation loss: 2.0771284513576056

Epoch: 6| Step: 4
Training loss: 1.932544469833374
Validation loss: 2.0652853083866898

Epoch: 6| Step: 5
Training loss: 2.360671043395996
Validation loss: 2.0854972254845405

Epoch: 6| Step: 6
Training loss: 2.481840133666992
Validation loss: 2.093619382509621

Epoch: 6| Step: 7
Training loss: 1.7602910995483398
Validation loss: 2.0756309686168546

Epoch: 6| Step: 8
Training loss: 2.556581974029541
Validation loss: 2.093158242523029

Epoch: 6| Step: 9
Training loss: 2.2191810607910156
Validation loss: 2.0905422959276425

Epoch: 6| Step: 10
Training loss: 2.6595563888549805
Validation loss: 2.064851112263177

Epoch: 6| Step: 11
Training loss: 2.0438289642333984
Validation loss: 2.0733130836999543

Epoch: 6| Step: 12
Training loss: 2.4526567459106445
Validation loss: 2.0678620646076817

Epoch: 6| Step: 13
Training loss: 1.8717882633209229
Validation loss: 2.0891568301826395

Epoch: 119| Step: 0
Training loss: 2.6186394691467285
Validation loss: 2.074587578414589

Epoch: 6| Step: 1
Training loss: 2.647897720336914
Validation loss: 2.0878619391431092

Epoch: 6| Step: 2
Training loss: 2.327472448348999
Validation loss: 2.1063907582272767

Epoch: 6| Step: 3
Training loss: 2.746393918991089
Validation loss: 2.102840050574272

Epoch: 6| Step: 4
Training loss: 1.4658701419830322
Validation loss: 2.118974911269321

Epoch: 6| Step: 5
Training loss: 2.5156941413879395
Validation loss: 2.1113356621034685

Epoch: 6| Step: 6
Training loss: 2.1926651000976562
Validation loss: 2.1242851595724783

Epoch: 6| Step: 7
Training loss: 2.2497012615203857
Validation loss: 2.129651028622863

Epoch: 6| Step: 8
Training loss: 2.899496555328369
Validation loss: 2.1215132436444684

Epoch: 6| Step: 9
Training loss: 1.5158355236053467
Validation loss: 2.1145788072257914

Epoch: 6| Step: 10
Training loss: 1.8418059349060059
Validation loss: 2.116364194500831

Epoch: 6| Step: 11
Training loss: 2.065408945083618
Validation loss: 2.1094486739045832

Epoch: 6| Step: 12
Training loss: 2.176003932952881
Validation loss: 2.1256595529535764

Epoch: 6| Step: 13
Training loss: 1.4405555725097656
Validation loss: 2.1439234133689635

Epoch: 120| Step: 0
Training loss: 2.27144455909729
Validation loss: 2.1161729212730163

Epoch: 6| Step: 1
Training loss: 1.5964717864990234
Validation loss: 2.118577580298147

Epoch: 6| Step: 2
Training loss: 2.70965313911438
Validation loss: 2.1192036777414303

Epoch: 6| Step: 3
Training loss: 2.409257173538208
Validation loss: 2.0947097091264624

Epoch: 6| Step: 4
Training loss: 2.275775909423828
Validation loss: 2.125737823465819

Epoch: 6| Step: 5
Training loss: 1.697794795036316
Validation loss: 2.104352230666786

Epoch: 6| Step: 6
Training loss: 2.139510154724121
Validation loss: 2.1193121735767653

Epoch: 6| Step: 7
Training loss: 2.0980145931243896
Validation loss: 2.1258796056111655

Epoch: 6| Step: 8
Training loss: 2.418862819671631
Validation loss: 2.129914568316552

Epoch: 6| Step: 9
Training loss: 2.333632469177246
Validation loss: 2.1170644349949335

Epoch: 6| Step: 10
Training loss: 1.915700912475586
Validation loss: 2.12082302570343

Epoch: 6| Step: 11
Training loss: 2.4143362045288086
Validation loss: 2.0992494795912053

Epoch: 6| Step: 12
Training loss: 2.689629077911377
Validation loss: 2.1030489860042447

Epoch: 6| Step: 13
Training loss: 1.846987247467041
Validation loss: 2.112436471446868

Epoch: 121| Step: 0
Training loss: 1.8711497783660889
Validation loss: 2.104819405463434

Epoch: 6| Step: 1
Training loss: 2.744441509246826
Validation loss: 2.0984606486494823

Epoch: 6| Step: 2
Training loss: 1.876323938369751
Validation loss: 2.0914941577501196

Epoch: 6| Step: 3
Training loss: 1.5117402076721191
Validation loss: 2.0935531777720295

Epoch: 6| Step: 4
Training loss: 2.0099873542785645
Validation loss: 2.089634797906363

Epoch: 6| Step: 5
Training loss: 2.6452853679656982
Validation loss: 2.1001834946293987

Epoch: 6| Step: 6
Training loss: 2.4687795639038086
Validation loss: 2.0774841641867035

Epoch: 6| Step: 7
Training loss: 1.6817455291748047
Validation loss: 2.103879690170288

Epoch: 6| Step: 8
Training loss: 2.845567226409912
Validation loss: 2.077674037666731

Epoch: 6| Step: 9
Training loss: 2.230567455291748
Validation loss: 2.1074293557033745

Epoch: 6| Step: 10
Training loss: 1.7443469762802124
Validation loss: 2.0900237329544558

Epoch: 6| Step: 11
Training loss: 2.6439642906188965
Validation loss: 2.091326346961401

Epoch: 6| Step: 12
Training loss: 1.9640684127807617
Validation loss: 2.0979532990404355

Epoch: 6| Step: 13
Training loss: 2.9460206031799316
Validation loss: 2.071814671639473

Epoch: 122| Step: 0
Training loss: 1.9464726448059082
Validation loss: 2.0701561102303128

Epoch: 6| Step: 1
Training loss: 2.0599188804626465
Validation loss: 2.0949148131955053

Epoch: 6| Step: 2
Training loss: 1.6722187995910645
Validation loss: 2.1086187067852227

Epoch: 6| Step: 3
Training loss: 2.394470691680908
Validation loss: 2.0803719412895942

Epoch: 6| Step: 4
Training loss: 2.5252950191497803
Validation loss: 2.1231964198491906

Epoch: 6| Step: 5
Training loss: 1.5965399742126465
Validation loss: 2.096641459772664

Epoch: 6| Step: 6
Training loss: 2.3521549701690674
Validation loss: 2.099496559430194

Epoch: 6| Step: 7
Training loss: 2.423788070678711
Validation loss: 2.103370161466701

Epoch: 6| Step: 8
Training loss: 1.8999912738800049
Validation loss: 2.0993983822484172

Epoch: 6| Step: 9
Training loss: 2.703033924102783
Validation loss: 2.1156131298311296

Epoch: 6| Step: 10
Training loss: 2.206887722015381
Validation loss: 2.0953604303380495

Epoch: 6| Step: 11
Training loss: 3.6185128688812256
Validation loss: 2.1207158462975615

Epoch: 6| Step: 12
Training loss: 1.6379936933517456
Validation loss: 2.0930631801646244

Epoch: 6| Step: 13
Training loss: 1.8460614681243896
Validation loss: 2.1077122611384236

Epoch: 123| Step: 0
Training loss: 2.6696267127990723
Validation loss: 2.1012943008894562

Epoch: 6| Step: 1
Training loss: 2.927031993865967
Validation loss: 2.126375275273477

Epoch: 6| Step: 2
Training loss: 1.4256322383880615
Validation loss: 2.1224645401841853

Epoch: 6| Step: 3
Training loss: 1.9095324277877808
Validation loss: 2.125842889149984

Epoch: 6| Step: 4
Training loss: 1.8723558187484741
Validation loss: 2.1048410259267336

Epoch: 6| Step: 5
Training loss: 2.183321475982666
Validation loss: 2.0880755275808354

Epoch: 6| Step: 6
Training loss: 1.8188509941101074
Validation loss: 2.124125229415073

Epoch: 6| Step: 7
Training loss: 2.007004737854004
Validation loss: 2.1062157436083724

Epoch: 6| Step: 8
Training loss: 1.913571834564209
Validation loss: 2.1264503425167454

Epoch: 6| Step: 9
Training loss: 2.5057454109191895
Validation loss: 2.1260099231555896

Epoch: 6| Step: 10
Training loss: 2.4553961753845215
Validation loss: 2.0989019255484305

Epoch: 6| Step: 11
Training loss: 3.299224376678467
Validation loss: 2.1186629751677155

Epoch: 6| Step: 12
Training loss: 1.9586045742034912
Validation loss: 2.1134105151699436

Epoch: 6| Step: 13
Training loss: 1.7851613759994507
Validation loss: 2.0963849175360894

Epoch: 124| Step: 0
Training loss: 1.3439679145812988
Validation loss: 2.1321505808061167

Epoch: 6| Step: 1
Training loss: 2.3966124057769775
Validation loss: 2.1024674484806676

Epoch: 6| Step: 2
Training loss: 2.292570114135742
Validation loss: 2.1270294240725938

Epoch: 6| Step: 3
Training loss: 2.3458027839660645
Validation loss: 2.1008104226922475

Epoch: 6| Step: 4
Training loss: 2.1318886280059814
Validation loss: 2.0918081370733117

Epoch: 6| Step: 5
Training loss: 2.4762625694274902
Validation loss: 2.0928675256749636

Epoch: 6| Step: 6
Training loss: 1.9749423265457153
Validation loss: 2.090392061459121

Epoch: 6| Step: 7
Training loss: 2.1940579414367676
Validation loss: 2.0814717815768335

Epoch: 6| Step: 8
Training loss: 2.6458380222320557
Validation loss: 2.074084622885591

Epoch: 6| Step: 9
Training loss: 1.8870339393615723
Validation loss: 2.0988461074008735

Epoch: 6| Step: 10
Training loss: 2.5406694412231445
Validation loss: 2.098003964270315

Epoch: 6| Step: 11
Training loss: 1.6567987203598022
Validation loss: 2.089510833063433

Epoch: 6| Step: 12
Training loss: 2.706068277359009
Validation loss: 2.0727099128948745

Epoch: 6| Step: 13
Training loss: 2.4216010570526123
Validation loss: 2.0939134090177474

Epoch: 125| Step: 0
Training loss: 2.1281914710998535
Validation loss: 2.10051792411394

Epoch: 6| Step: 1
Training loss: 1.8814616203308105
Validation loss: 2.086929664816908

Epoch: 6| Step: 2
Training loss: 2.5623202323913574
Validation loss: 2.0723832384232552

Epoch: 6| Step: 3
Training loss: 2.497044086456299
Validation loss: 2.1008700991189606

Epoch: 6| Step: 4
Training loss: 2.484755039215088
Validation loss: 2.0927913906753703

Epoch: 6| Step: 5
Training loss: 2.8504176139831543
Validation loss: 2.0916674162751887

Epoch: 6| Step: 6
Training loss: 2.027892589569092
Validation loss: 2.108989268220881

Epoch: 6| Step: 7
Training loss: 2.073054552078247
Validation loss: 2.137466853664767

Epoch: 6| Step: 8
Training loss: 2.4442505836486816
Validation loss: 2.1022905201040287

Epoch: 6| Step: 9
Training loss: 1.8813101053237915
Validation loss: 2.122785827165009

Epoch: 6| Step: 10
Training loss: 2.2281336784362793
Validation loss: 2.1038858454714537

Epoch: 6| Step: 11
Training loss: 2.2719874382019043
Validation loss: 2.087921166932711

Epoch: 6| Step: 12
Training loss: 1.7476590871810913
Validation loss: 2.1085383956150343

Epoch: 6| Step: 13
Training loss: 1.9208463430404663
Validation loss: 2.110651182871993

Testing loss: 2.013611936569214

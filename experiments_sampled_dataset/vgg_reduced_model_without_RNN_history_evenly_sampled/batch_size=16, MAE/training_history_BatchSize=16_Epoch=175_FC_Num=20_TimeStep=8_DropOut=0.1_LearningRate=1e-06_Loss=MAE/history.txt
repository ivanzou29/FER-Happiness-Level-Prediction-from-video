Epoch: 1| Step: 0
Training loss: 3.1811304092407227
Validation loss: 4.045130209256244

Epoch: 6| Step: 1
Training loss: 4.316900253295898
Validation loss: 4.041553799824048

Epoch: 6| Step: 2
Training loss: 3.083862781524658
Validation loss: 4.038294010264899

Epoch: 6| Step: 3
Training loss: 3.9941163063049316
Validation loss: 4.031859477361043

Epoch: 6| Step: 4
Training loss: 3.4066786766052246
Validation loss: 4.03255311648051

Epoch: 6| Step: 5
Training loss: 5.51002311706543
Validation loss: 4.028676258620395

Epoch: 6| Step: 6
Training loss: 4.059759140014648
Validation loss: 4.026180715971096

Epoch: 6| Step: 7
Training loss: 3.4461772441864014
Validation loss: 4.024205894880398

Epoch: 6| Step: 8
Training loss: 2.2307944297790527
Validation loss: 4.02033422839257

Epoch: 6| Step: 9
Training loss: 3.557769536972046
Validation loss: 4.017671192845991

Epoch: 6| Step: 10
Training loss: 4.206236362457275
Validation loss: 4.017397772881292

Epoch: 6| Step: 11
Training loss: 4.235228538513184
Validation loss: 4.014478370707522

Epoch: 6| Step: 12
Training loss: 4.092986106872559
Validation loss: 4.011808469731321

Epoch: 6| Step: 13
Training loss: 5.696508407592773
Validation loss: 4.011748421576716

Epoch: 2| Step: 0
Training loss: 4.651612758636475
Validation loss: 4.0085768725282405

Epoch: 6| Step: 1
Training loss: 3.80662202835083
Validation loss: 4.004699619867468

Epoch: 6| Step: 2
Training loss: 4.314940452575684
Validation loss: 4.002732384589411

Epoch: 6| Step: 3
Training loss: 4.300212860107422
Validation loss: 3.9983045362657115

Epoch: 6| Step: 4
Training loss: 5.14195442199707
Validation loss: 4.000504796222974

Epoch: 6| Step: 5
Training loss: 4.7102251052856445
Validation loss: 3.9962274079681723

Epoch: 6| Step: 6
Training loss: 4.062636375427246
Validation loss: 3.994045529314267

Epoch: 6| Step: 7
Training loss: 2.9106545448303223
Validation loss: 3.9909398530119207

Epoch: 6| Step: 8
Training loss: 4.000552654266357
Validation loss: 3.9892136255900064

Epoch: 6| Step: 9
Training loss: 2.9713354110717773
Validation loss: 3.9839242401943413

Epoch: 6| Step: 10
Training loss: 3.8819704055786133
Validation loss: 3.980132625948998

Epoch: 6| Step: 11
Training loss: 2.6780858039855957
Validation loss: 3.981353462383311

Epoch: 6| Step: 12
Training loss: 3.6424779891967773
Validation loss: 3.979810135338896

Epoch: 6| Step: 13
Training loss: 1.9197224378585815
Validation loss: 3.9799286088635846

Epoch: 3| Step: 0
Training loss: 3.7323362827301025
Validation loss: 3.9716004248588317

Epoch: 6| Step: 1
Training loss: 4.189619541168213
Validation loss: 3.9727194975781184

Epoch: 6| Step: 2
Training loss: 3.9689412117004395
Validation loss: 3.969270536976476

Epoch: 6| Step: 3
Training loss: 3.810609817504883
Validation loss: 3.966385390168877

Epoch: 6| Step: 4
Training loss: 3.4352035522460938
Validation loss: 3.9641909778759046

Epoch: 6| Step: 5
Training loss: 4.151356220245361
Validation loss: 3.9626785093738186

Epoch: 6| Step: 6
Training loss: 3.6804323196411133
Validation loss: 3.9604374465122016

Epoch: 6| Step: 7
Training loss: 4.770864486694336
Validation loss: 3.9583422368572605

Epoch: 6| Step: 8
Training loss: 3.4739267826080322
Validation loss: 3.954184801347794

Epoch: 6| Step: 9
Training loss: 3.9229400157928467
Validation loss: 3.95434392908568

Epoch: 6| Step: 10
Training loss: 4.040411472320557
Validation loss: 3.950959067190847

Epoch: 6| Step: 11
Training loss: 4.676356315612793
Validation loss: 3.9494934338395313

Epoch: 6| Step: 12
Training loss: 2.665029525756836
Validation loss: 3.9462476417582524

Epoch: 6| Step: 13
Training loss: 2.1968014240264893
Validation loss: 3.941584966515982

Epoch: 4| Step: 0
Training loss: 3.0946455001831055
Validation loss: 3.9411201733414845

Epoch: 6| Step: 1
Training loss: 5.181993007659912
Validation loss: 3.94017852762694

Epoch: 6| Step: 2
Training loss: 3.8307621479034424
Validation loss: 3.9362062177350445

Epoch: 6| Step: 3
Training loss: 3.7615411281585693
Validation loss: 3.9359320312417965

Epoch: 6| Step: 4
Training loss: 3.464686870574951
Validation loss: 3.931663474728984

Epoch: 6| Step: 5
Training loss: 3.141939163208008
Validation loss: 3.9318456906144337

Epoch: 6| Step: 6
Training loss: 2.990253210067749
Validation loss: 3.927379890154767

Epoch: 6| Step: 7
Training loss: 3.8043744564056396
Validation loss: 3.9275889294121855

Epoch: 6| Step: 8
Training loss: 4.490846157073975
Validation loss: 3.921765970927413

Epoch: 6| Step: 9
Training loss: 3.8454079627990723
Validation loss: 3.9213928612329627

Epoch: 6| Step: 10
Training loss: 3.6053311824798584
Validation loss: 3.9186204325768257

Epoch: 6| Step: 11
Training loss: 4.073781490325928
Validation loss: 3.917359208547941

Epoch: 6| Step: 12
Training loss: 3.7673442363739014
Validation loss: 3.91280460357666

Epoch: 6| Step: 13
Training loss: 4.135475158691406
Validation loss: 3.908043407624768

Epoch: 5| Step: 0
Training loss: 4.549023628234863
Validation loss: 3.9072072685405774

Epoch: 6| Step: 1
Training loss: 2.814610004425049
Validation loss: 3.9055825612878285

Epoch: 6| Step: 2
Training loss: 3.203666925430298
Validation loss: 3.902873598119264

Epoch: 6| Step: 3
Training loss: 4.10347318649292
Validation loss: 3.898218818890151

Epoch: 6| Step: 4
Training loss: 3.2966036796569824
Validation loss: 3.8965244421394925

Epoch: 6| Step: 5
Training loss: 3.682107448577881
Validation loss: 3.891721074299146

Epoch: 6| Step: 6
Training loss: 3.2141623497009277
Validation loss: 3.8910637235128753

Epoch: 6| Step: 7
Training loss: 4.388874530792236
Validation loss: 3.8899035017977477

Epoch: 6| Step: 8
Training loss: 4.061043739318848
Validation loss: 3.8845352870161816

Epoch: 6| Step: 9
Training loss: 4.179753303527832
Validation loss: 3.881788517839165

Epoch: 6| Step: 10
Training loss: 2.8498449325561523
Validation loss: 3.879864349160143

Epoch: 6| Step: 11
Training loss: 3.955596685409546
Validation loss: 3.8778065660948395

Epoch: 6| Step: 12
Training loss: 3.738176107406616
Validation loss: 3.872368868961129

Epoch: 6| Step: 13
Training loss: 5.203271865844727
Validation loss: 3.8722249384849303

Epoch: 6| Step: 0
Training loss: 2.8429877758026123
Validation loss: 3.870231023398779

Epoch: 6| Step: 1
Training loss: 2.066157817840576
Validation loss: 3.868689247356948

Epoch: 6| Step: 2
Training loss: 4.458192348480225
Validation loss: 3.8604046606248423

Epoch: 6| Step: 3
Training loss: 2.9646034240722656
Validation loss: 3.8584104250836115

Epoch: 6| Step: 4
Training loss: 3.5060503482818604
Validation loss: 3.8591631997016167

Epoch: 6| Step: 5
Training loss: 4.9381561279296875
Validation loss: 3.855588984745805

Epoch: 6| Step: 6
Training loss: 3.838468074798584
Validation loss: 3.8512741801559285

Epoch: 6| Step: 7
Training loss: 4.462224006652832
Validation loss: 3.845212085272676

Epoch: 6| Step: 8
Training loss: 3.521929979324341
Validation loss: 3.8486254548513763

Epoch: 6| Step: 9
Training loss: 4.218679428100586
Validation loss: 3.843474265067808

Epoch: 6| Step: 10
Training loss: 3.5509743690490723
Validation loss: 3.8390004352856706

Epoch: 6| Step: 11
Training loss: 4.325417518615723
Validation loss: 3.839752751012002

Epoch: 6| Step: 12
Training loss: 3.1916046142578125
Validation loss: 3.833453014332761

Epoch: 6| Step: 13
Training loss: 4.621952056884766
Validation loss: 3.8289652332182853

Epoch: 7| Step: 0
Training loss: 4.654969215393066
Validation loss: 3.8284615496153473

Epoch: 6| Step: 1
Training loss: 3.7796730995178223
Validation loss: 3.8236534262216217

Epoch: 6| Step: 2
Training loss: 3.4875826835632324
Validation loss: 3.8195589332170385

Epoch: 6| Step: 3
Training loss: 3.253601551055908
Validation loss: 3.816719501249252

Epoch: 6| Step: 4
Training loss: 3.7110280990600586
Validation loss: 3.8119250061691448

Epoch: 6| Step: 5
Training loss: 2.962301254272461
Validation loss: 3.80814782522058

Epoch: 6| Step: 6
Training loss: 4.606984615325928
Validation loss: 3.806004403739847

Epoch: 6| Step: 7
Training loss: 4.539258003234863
Validation loss: 3.805946360352219

Epoch: 6| Step: 8
Training loss: 3.365447998046875
Validation loss: 3.7955732704490743

Epoch: 6| Step: 9
Training loss: 2.584097146987915
Validation loss: 3.7940333453557824

Epoch: 6| Step: 10
Training loss: 4.293078899383545
Validation loss: 3.7943405746131815

Epoch: 6| Step: 11
Training loss: 3.7274787425994873
Validation loss: 3.7872216932235228

Epoch: 6| Step: 12
Training loss: 3.1330928802490234
Validation loss: 3.782238209119407

Epoch: 6| Step: 13
Training loss: 3.3045742511749268
Validation loss: 3.7786816320111676

Epoch: 8| Step: 0
Training loss: 3.178520441055298
Validation loss: 3.7816901001878964

Epoch: 6| Step: 1
Training loss: 4.084177017211914
Validation loss: 3.7731713684656287

Epoch: 6| Step: 2
Training loss: 3.687788486480713
Validation loss: 3.7707055102112474

Epoch: 6| Step: 3
Training loss: 2.854640245437622
Validation loss: 3.7669094557403238

Epoch: 6| Step: 4
Training loss: 4.114668369293213
Validation loss: 3.7600775995562152

Epoch: 6| Step: 5
Training loss: 2.957514762878418
Validation loss: 3.7599866877319994

Epoch: 6| Step: 6
Training loss: 3.4848668575286865
Validation loss: 3.757987924801406

Epoch: 6| Step: 7
Training loss: 4.331331253051758
Validation loss: 3.754524543721189

Epoch: 6| Step: 8
Training loss: 2.9748573303222656
Validation loss: 3.747524663966189

Epoch: 6| Step: 9
Training loss: 3.5776782035827637
Validation loss: 3.7455468485432286

Epoch: 6| Step: 10
Training loss: 4.183929443359375
Validation loss: 3.7407243482528196

Epoch: 6| Step: 11
Training loss: 4.772674083709717
Validation loss: 3.7382716517294607

Epoch: 6| Step: 12
Training loss: 3.128856658935547
Validation loss: 3.735769587178384

Epoch: 6| Step: 13
Training loss: 3.714529275894165
Validation loss: 3.730541529194001

Epoch: 9| Step: 0
Training loss: 4.205336093902588
Validation loss: 3.730045823640721

Epoch: 6| Step: 1
Training loss: 3.9869906902313232
Validation loss: 3.7237171511496268

Epoch: 6| Step: 2
Training loss: 3.4129514694213867
Validation loss: 3.7214153864050425

Epoch: 6| Step: 3
Training loss: 3.8093149662017822
Validation loss: 3.7111333057444584

Epoch: 6| Step: 4
Training loss: 3.6106300354003906
Validation loss: 3.7102041193234023

Epoch: 6| Step: 5
Training loss: 5.120928764343262
Validation loss: 3.7069707455173617

Epoch: 6| Step: 6
Training loss: 3.908839225769043
Validation loss: 3.7028438814224733

Epoch: 6| Step: 7
Training loss: 2.9428861141204834
Validation loss: 3.700358513862856

Epoch: 6| Step: 8
Training loss: 3.1894309520721436
Validation loss: 3.6952420434644146

Epoch: 6| Step: 9
Training loss: 2.828131675720215
Validation loss: 3.687893747001566

Epoch: 6| Step: 10
Training loss: 3.9146056175231934
Validation loss: 3.6864914099375405

Epoch: 6| Step: 11
Training loss: 3.8640191555023193
Validation loss: 3.6832048149519068

Epoch: 6| Step: 12
Training loss: 2.430037498474121
Validation loss: 3.6780457573552288

Epoch: 6| Step: 13
Training loss: 2.7393150329589844
Validation loss: 3.672321509289485

Epoch: 10| Step: 0
Training loss: 2.2015178203582764
Validation loss: 3.663099035140007

Epoch: 6| Step: 1
Training loss: 2.889634132385254
Validation loss: 3.66183106617261

Epoch: 6| Step: 2
Training loss: 4.748744010925293
Validation loss: 3.656600608620592

Epoch: 6| Step: 3
Training loss: 3.220651626586914
Validation loss: 3.6525635360389628

Epoch: 6| Step: 4
Training loss: 3.782486915588379
Validation loss: 3.650566498438517

Epoch: 6| Step: 5
Training loss: 3.911006450653076
Validation loss: 3.6401496779534126

Epoch: 6| Step: 6
Training loss: 3.8781187534332275
Validation loss: 3.6348713162124797

Epoch: 6| Step: 7
Training loss: 4.234152793884277
Validation loss: 3.630322471741707

Epoch: 6| Step: 8
Training loss: 3.7751078605651855
Validation loss: 3.627378620127196

Epoch: 6| Step: 9
Training loss: 4.350526809692383
Validation loss: 3.622785234964022

Epoch: 6| Step: 10
Training loss: 3.676870346069336
Validation loss: 3.6119562887376353

Epoch: 6| Step: 11
Training loss: 3.260606288909912
Validation loss: 3.606657628090151

Epoch: 6| Step: 12
Training loss: 2.9136416912078857
Validation loss: 3.602779988319643

Epoch: 6| Step: 13
Training loss: 2.1090667247772217
Validation loss: 3.5974446291564615

Epoch: 11| Step: 0
Training loss: 3.9169108867645264
Validation loss: 3.596070087084206

Epoch: 6| Step: 1
Training loss: 3.25966477394104
Validation loss: 3.5868242376594135

Epoch: 6| Step: 2
Training loss: 3.09257435798645
Validation loss: 3.584509644457089

Epoch: 6| Step: 3
Training loss: 2.412346124649048
Validation loss: 3.5768470712887344

Epoch: 6| Step: 4
Training loss: 4.218009948730469
Validation loss: 3.576724849721437

Epoch: 6| Step: 5
Training loss: 3.7560060024261475
Validation loss: 3.571353017642934

Epoch: 6| Step: 6
Training loss: 2.381197214126587
Validation loss: 3.564386716452978

Epoch: 6| Step: 7
Training loss: 3.0845556259155273
Validation loss: 3.5573604029993855

Epoch: 6| Step: 8
Training loss: 4.526484966278076
Validation loss: 3.554925528905725

Epoch: 6| Step: 9
Training loss: 3.7615983486175537
Validation loss: 3.5418874832891647

Epoch: 6| Step: 10
Training loss: 3.955585479736328
Validation loss: 3.5394355866216842

Epoch: 6| Step: 11
Training loss: 3.5855207443237305
Validation loss: 3.5354803275036555

Epoch: 6| Step: 12
Training loss: 3.3319389820098877
Validation loss: 3.521025344889651

Epoch: 6| Step: 13
Training loss: 3.265437602996826
Validation loss: 3.5207082225430395

Epoch: 12| Step: 0
Training loss: 3.303194999694824
Validation loss: 3.5144852233189408

Epoch: 6| Step: 1
Training loss: 3.423722743988037
Validation loss: 3.512204944446523

Epoch: 6| Step: 2
Training loss: 4.788625240325928
Validation loss: 3.493392700790077

Epoch: 6| Step: 3
Training loss: 2.5276715755462646
Validation loss: 3.491974405063096

Epoch: 6| Step: 4
Training loss: 3.876438617706299
Validation loss: 3.4825338086774273

Epoch: 6| Step: 5
Training loss: 3.474836826324463
Validation loss: 3.481026064965033

Epoch: 6| Step: 6
Training loss: 3.0447888374328613
Validation loss: 3.470052662716117

Epoch: 6| Step: 7
Training loss: 3.6422181129455566
Validation loss: 3.4624160284637124

Epoch: 6| Step: 8
Training loss: 2.1710402965545654
Validation loss: 3.458726444552022

Epoch: 6| Step: 9
Training loss: 3.1343307495117188
Validation loss: 3.45036659702178

Epoch: 6| Step: 10
Training loss: 4.055244445800781
Validation loss: 3.4416543591407036

Epoch: 6| Step: 11
Training loss: 3.23511004447937
Validation loss: 3.4338020458016345

Epoch: 6| Step: 12
Training loss: 3.092160224914551
Validation loss: 3.425833417523292

Epoch: 6| Step: 13
Training loss: 4.039471626281738
Validation loss: 3.419436767537107

Epoch: 13| Step: 0
Training loss: 2.3933324813842773
Validation loss: 3.4118253800176803

Epoch: 6| Step: 1
Training loss: 3.5128138065338135
Validation loss: 3.399180227710355

Epoch: 6| Step: 2
Training loss: 3.300365924835205
Validation loss: 3.3912745265550512

Epoch: 6| Step: 3
Training loss: 4.300841808319092
Validation loss: 3.3867621447450373

Epoch: 6| Step: 4
Training loss: 3.4744508266448975
Validation loss: 3.3715666032606557

Epoch: 6| Step: 5
Training loss: 4.201085567474365
Validation loss: 3.366569467770156

Epoch: 6| Step: 6
Training loss: 3.282743215560913
Validation loss: 3.3593068302318616

Epoch: 6| Step: 7
Training loss: 3.834596633911133
Validation loss: 3.3480650045538463

Epoch: 6| Step: 8
Training loss: 2.635887622833252
Validation loss: 3.337309891177762

Epoch: 6| Step: 9
Training loss: 2.8412764072418213
Validation loss: 3.329894704203452

Epoch: 6| Step: 10
Training loss: 2.8997421264648438
Validation loss: 3.3170482625243483

Epoch: 6| Step: 11
Training loss: 2.777531862258911
Validation loss: 3.312588399456393

Epoch: 6| Step: 12
Training loss: 3.8219001293182373
Validation loss: 3.29578895466302

Epoch: 6| Step: 13
Training loss: 2.5386545658111572
Validation loss: 3.289690702192245

Epoch: 14| Step: 0
Training loss: 2.8435778617858887
Validation loss: 3.280204032057075

Epoch: 6| Step: 1
Training loss: 2.528578281402588
Validation loss: 3.267844348825434

Epoch: 6| Step: 2
Training loss: 3.8049983978271484
Validation loss: 3.2586667845326085

Epoch: 6| Step: 3
Training loss: 3.8045060634613037
Validation loss: 3.2484170954714537

Epoch: 6| Step: 4
Training loss: 3.4067373275756836
Validation loss: 3.2403833789210164

Epoch: 6| Step: 5
Training loss: 4.008708953857422
Validation loss: 3.2258740394346175

Epoch: 6| Step: 6
Training loss: 3.508415460586548
Validation loss: 3.2181079464574016

Epoch: 6| Step: 7
Training loss: 2.9793550968170166
Validation loss: 3.2063347549848658

Epoch: 6| Step: 8
Training loss: 3.156365394592285
Validation loss: 3.1854773977751374

Epoch: 6| Step: 9
Training loss: 2.510348081588745
Validation loss: 3.1772522875057754

Epoch: 6| Step: 10
Training loss: 2.952838659286499
Validation loss: 3.1706725243599183

Epoch: 6| Step: 11
Training loss: 2.868746757507324
Validation loss: 3.151647316512241

Epoch: 6| Step: 12
Training loss: 3.0420022010803223
Validation loss: 3.1388526860103814

Epoch: 6| Step: 13
Training loss: 2.9267404079437256
Validation loss: 3.1369092105537333

Epoch: 15| Step: 0
Training loss: 3.01164174079895
Validation loss: 3.113175174241425

Epoch: 6| Step: 1
Training loss: 3.7929306030273438
Validation loss: 3.1065062220378588

Epoch: 6| Step: 2
Training loss: 2.892510414123535
Validation loss: 3.087193424983691

Epoch: 6| Step: 3
Training loss: 2.7546133995056152
Validation loss: 3.0772825338507213

Epoch: 6| Step: 4
Training loss: 3.0282235145568848
Validation loss: 3.066904631994104

Epoch: 6| Step: 5
Training loss: 3.018599033355713
Validation loss: 3.04877229403424

Epoch: 6| Step: 6
Training loss: 3.248859405517578
Validation loss: 3.046743054543772

Epoch: 6| Step: 7
Training loss: 2.9579696655273438
Validation loss: 3.020812039734215

Epoch: 6| Step: 8
Training loss: 2.452040195465088
Validation loss: 3.0249479457896244

Epoch: 6| Step: 9
Training loss: 3.5770013332366943
Validation loss: 3.004663085424772

Epoch: 6| Step: 10
Training loss: 2.7461347579956055
Validation loss: 2.9891176121209257

Epoch: 6| Step: 11
Training loss: 3.1761069297790527
Validation loss: 2.975529296423799

Epoch: 6| Step: 12
Training loss: 3.1035356521606445
Validation loss: 2.9545093223612797

Epoch: 6| Step: 13
Training loss: 2.9366273880004883
Validation loss: 2.954213391068161

Epoch: 16| Step: 0
Training loss: 3.253258228302002
Validation loss: 2.9449808700110323

Epoch: 6| Step: 1
Training loss: 3.915591239929199
Validation loss: 2.9245914079809703

Epoch: 6| Step: 2
Training loss: 3.8433237075805664
Validation loss: 2.905703075470463

Epoch: 6| Step: 3
Training loss: 2.716132640838623
Validation loss: 2.8962677601845033

Epoch: 6| Step: 4
Training loss: 3.5614113807678223
Validation loss: 2.8889402497199272

Epoch: 6| Step: 5
Training loss: 3.1417438983917236
Validation loss: 2.8549519687570553

Epoch: 6| Step: 6
Training loss: 2.8977580070495605
Validation loss: 2.8413167409999396

Epoch: 6| Step: 7
Training loss: 2.5597028732299805
Validation loss: 2.8318539588682112

Epoch: 6| Step: 8
Training loss: 2.6767287254333496
Validation loss: 2.82065398205993

Epoch: 6| Step: 9
Training loss: 2.745638847351074
Validation loss: 2.7991254278408584

Epoch: 6| Step: 10
Training loss: 2.3503079414367676
Validation loss: 2.7795120490494596

Epoch: 6| Step: 11
Training loss: 2.5862526893615723
Validation loss: 2.7700209771433184

Epoch: 6| Step: 12
Training loss: 2.51163387298584
Validation loss: 2.75575687039283

Epoch: 6| Step: 13
Training loss: 1.6107633113861084
Validation loss: 2.7380686370275353

Epoch: 17| Step: 0
Training loss: 3.0955936908721924
Validation loss: 2.724406575643888

Epoch: 6| Step: 1
Training loss: 2.3306326866149902
Validation loss: 2.715445545411879

Epoch: 6| Step: 2
Training loss: 3.67391037940979
Validation loss: 2.7141999942000195

Epoch: 6| Step: 3
Training loss: 2.448627233505249
Validation loss: 2.6831454512893513

Epoch: 6| Step: 4
Training loss: 2.875161647796631
Validation loss: 2.6795826522252892

Epoch: 6| Step: 5
Training loss: 3.249603748321533
Validation loss: 2.6683659809891895

Epoch: 6| Step: 6
Training loss: 2.233555316925049
Validation loss: 2.6234232379544165

Epoch: 6| Step: 7
Training loss: 2.479423999786377
Validation loss: 2.6194624234271306

Epoch: 6| Step: 8
Training loss: 2.9353694915771484
Validation loss: 2.5911885666590866

Epoch: 6| Step: 9
Training loss: 3.6441421508789062
Validation loss: 2.583820401981313

Epoch: 6| Step: 10
Training loss: 2.804715394973755
Validation loss: 2.5561508619657127

Epoch: 6| Step: 11
Training loss: 2.123572587966919
Validation loss: 2.554713297915715

Epoch: 6| Step: 12
Training loss: 2.5354833602905273
Validation loss: 2.5070078911319857

Epoch: 6| Step: 13
Training loss: 2.596181631088257
Validation loss: 2.5394441594359694

Epoch: 18| Step: 0
Training loss: 3.114104747772217
Validation loss: 2.518401310008059

Epoch: 6| Step: 1
Training loss: 2.983670234680176
Validation loss: 2.487792996950047

Epoch: 6| Step: 2
Training loss: 2.113220691680908
Validation loss: 2.4725376713660454

Epoch: 6| Step: 3
Training loss: 2.442221164703369
Validation loss: 2.4788105410914265

Epoch: 6| Step: 4
Training loss: 2.2521305084228516
Validation loss: 2.437092488811862

Epoch: 6| Step: 5
Training loss: 2.565202474594116
Validation loss: 2.4327321514006583

Epoch: 6| Step: 6
Training loss: 3.459909200668335
Validation loss: 2.4162631073305683

Epoch: 6| Step: 7
Training loss: 2.8148856163024902
Validation loss: 2.4279287527966242

Epoch: 6| Step: 8
Training loss: 2.1290040016174316
Validation loss: 2.410300088185136

Epoch: 6| Step: 9
Training loss: 2.1109986305236816
Validation loss: 2.3934998691722913

Epoch: 6| Step: 10
Training loss: 2.6970765590667725
Validation loss: 2.3665233145477953

Epoch: 6| Step: 11
Training loss: 3.1511669158935547
Validation loss: 2.352131143693001

Epoch: 6| Step: 12
Training loss: 2.558305501937866
Validation loss: 2.348583359872141

Epoch: 6| Step: 13
Training loss: 3.0990583896636963
Validation loss: 2.340646802738149

Epoch: 19| Step: 0
Training loss: 3.095353364944458
Validation loss: 2.354111402265487

Epoch: 6| Step: 1
Training loss: 2.7787554264068604
Validation loss: 2.31933807557629

Epoch: 6| Step: 2
Training loss: 2.3578009605407715
Validation loss: 2.3188043614869476

Epoch: 6| Step: 3
Training loss: 2.1594061851501465
Validation loss: 2.300803256291215

Epoch: 6| Step: 4
Training loss: 2.2059736251831055
Validation loss: 2.284259988415626

Epoch: 6| Step: 5
Training loss: 2.3458871841430664
Validation loss: 2.269792854145009

Epoch: 6| Step: 6
Training loss: 1.7545380592346191
Validation loss: 2.2730862773874754

Epoch: 6| Step: 7
Training loss: 2.3932995796203613
Validation loss: 2.27434951771972

Epoch: 6| Step: 8
Training loss: 2.7754321098327637
Validation loss: 2.2707332872575328

Epoch: 6| Step: 9
Training loss: 2.5396080017089844
Validation loss: 2.258741751793892

Epoch: 6| Step: 10
Training loss: 3.034865379333496
Validation loss: 2.2482482694810435

Epoch: 6| Step: 11
Training loss: 2.768538475036621
Validation loss: 2.2473177115122476

Epoch: 6| Step: 12
Training loss: 2.518244504928589
Validation loss: 2.2228137857170513

Epoch: 6| Step: 13
Training loss: 3.243548631668091
Validation loss: 2.223069944689351

Epoch: 20| Step: 0
Training loss: 2.5466487407684326
Validation loss: 2.215897024318736

Epoch: 6| Step: 1
Training loss: 2.79876708984375
Validation loss: 2.226900969782183

Epoch: 6| Step: 2
Training loss: 3.0361053943634033
Validation loss: 2.1903431313012236

Epoch: 6| Step: 3
Training loss: 2.512767791748047
Validation loss: 2.1970959068626486

Epoch: 6| Step: 4
Training loss: 2.6621346473693848
Validation loss: 2.182722530057353

Epoch: 6| Step: 5
Training loss: 2.774662733078003
Validation loss: 2.166351563187056

Epoch: 6| Step: 6
Training loss: 1.8476519584655762
Validation loss: 2.1671541198607414

Epoch: 6| Step: 7
Training loss: 3.0997841358184814
Validation loss: 2.1878904322142243

Epoch: 6| Step: 8
Training loss: 2.661290168762207
Validation loss: 2.175950989928297

Epoch: 6| Step: 9
Training loss: 2.574821949005127
Validation loss: 2.178062618419688

Epoch: 6| Step: 10
Training loss: 1.8752427101135254
Validation loss: 2.16058511118735

Epoch: 6| Step: 11
Training loss: 2.1121699810028076
Validation loss: 2.148154225400699

Epoch: 6| Step: 12
Training loss: 2.427807331085205
Validation loss: 2.1627137225161315

Epoch: 6| Step: 13
Training loss: 1.5204148292541504
Validation loss: 2.1538254907054286

Epoch: 21| Step: 0
Training loss: 2.179199695587158
Validation loss: 2.1558705837495866

Epoch: 6| Step: 1
Training loss: 2.3060762882232666
Validation loss: 2.1501121572268906

Epoch: 6| Step: 2
Training loss: 2.542109966278076
Validation loss: 2.1682643826289842

Epoch: 6| Step: 3
Training loss: 2.093231201171875
Validation loss: 2.1407361671488774

Epoch: 6| Step: 4
Training loss: 2.100088596343994
Validation loss: 2.1396796754611436

Epoch: 6| Step: 5
Training loss: 2.564521074295044
Validation loss: 2.1496900332871305

Epoch: 6| Step: 6
Training loss: 2.515652894973755
Validation loss: 2.1371194854859383

Epoch: 6| Step: 7
Training loss: 2.4108595848083496
Validation loss: 2.1404093183496946

Epoch: 6| Step: 8
Training loss: 2.929377555847168
Validation loss: 2.130165974299113

Epoch: 6| Step: 9
Training loss: 2.525197982788086
Validation loss: 2.1672676840136127

Epoch: 6| Step: 10
Training loss: 3.237945318222046
Validation loss: 2.1571976805246003

Epoch: 6| Step: 11
Training loss: 2.2441084384918213
Validation loss: 2.1648584104353383

Epoch: 6| Step: 12
Training loss: 1.8038840293884277
Validation loss: 2.124302820492816

Epoch: 6| Step: 13
Training loss: 3.261012315750122
Validation loss: 2.13589600721995

Epoch: 22| Step: 0
Training loss: 2.3363335132598877
Validation loss: 2.1468405903026624

Epoch: 6| Step: 1
Training loss: 3.2505574226379395
Validation loss: 2.156748419166893

Epoch: 6| Step: 2
Training loss: 2.3945207595825195
Validation loss: 2.145659605662028

Epoch: 6| Step: 3
Training loss: 2.9612197875976562
Validation loss: 2.1291465887459378

Epoch: 6| Step: 4
Training loss: 2.770212173461914
Validation loss: 2.1313856955497497

Epoch: 6| Step: 5
Training loss: 1.9682745933532715
Validation loss: 2.134007687209755

Epoch: 6| Step: 6
Training loss: 2.528571605682373
Validation loss: 2.1175636322267595

Epoch: 6| Step: 7
Training loss: 2.5719175338745117
Validation loss: 2.1113318499698432

Epoch: 6| Step: 8
Training loss: 2.770811080932617
Validation loss: 2.1323356166962655

Epoch: 6| Step: 9
Training loss: 2.28599214553833
Validation loss: 2.116314826473113

Epoch: 6| Step: 10
Training loss: 2.1291182041168213
Validation loss: 2.1395793973758654

Epoch: 6| Step: 11
Training loss: 1.830028772354126
Validation loss: 2.1307692117588495

Epoch: 6| Step: 12
Training loss: 2.158695697784424
Validation loss: 2.1147431481269097

Epoch: 6| Step: 13
Training loss: 2.305959463119507
Validation loss: 2.112578192064839

Epoch: 23| Step: 0
Training loss: 2.9646241664886475
Validation loss: 2.139402618972204

Epoch: 6| Step: 1
Training loss: 2.573972702026367
Validation loss: 2.115471168230939

Epoch: 6| Step: 2
Training loss: 2.283381700515747
Validation loss: 2.1121943304615636

Epoch: 6| Step: 3
Training loss: 2.6762032508850098
Validation loss: 2.1264622570365987

Epoch: 6| Step: 4
Training loss: 2.123133659362793
Validation loss: 2.120716779462753

Epoch: 6| Step: 5
Training loss: 3.0187463760375977
Validation loss: 2.103248634646016

Epoch: 6| Step: 6
Training loss: 2.1665849685668945
Validation loss: 2.124468339386807

Epoch: 6| Step: 7
Training loss: 2.374901056289673
Validation loss: 2.1134193533210346

Epoch: 6| Step: 8
Training loss: 2.916128158569336
Validation loss: 2.1190519871250277

Epoch: 6| Step: 9
Training loss: 2.3365845680236816
Validation loss: 2.1316736898114605

Epoch: 6| Step: 10
Training loss: 2.268996000289917
Validation loss: 2.1232886775847404

Epoch: 6| Step: 11
Training loss: 1.9032652378082275
Validation loss: 2.116222781519736

Epoch: 6| Step: 12
Training loss: 2.5786914825439453
Validation loss: 2.1096392088038947

Epoch: 6| Step: 13
Training loss: 1.712271809577942
Validation loss: 2.096493115989111

Epoch: 24| Step: 0
Training loss: 1.7032365798950195
Validation loss: 2.102962214459655

Epoch: 6| Step: 1
Training loss: 2.4410476684570312
Validation loss: 2.104614350103563

Epoch: 6| Step: 2
Training loss: 2.0218894481658936
Validation loss: 2.096321869921941

Epoch: 6| Step: 3
Training loss: 1.9367187023162842
Validation loss: 2.107171376546224

Epoch: 6| Step: 4
Training loss: 3.120208978652954
Validation loss: 2.1079372564951577

Epoch: 6| Step: 5
Training loss: 2.824634552001953
Validation loss: 2.107134980540122

Epoch: 6| Step: 6
Training loss: 2.6486053466796875
Validation loss: 2.1331672450547576

Epoch: 6| Step: 7
Training loss: 2.4574577808380127
Validation loss: 2.091600746236822

Epoch: 6| Step: 8
Training loss: 2.2954397201538086
Validation loss: 2.10585121698277

Epoch: 6| Step: 9
Training loss: 3.059553623199463
Validation loss: 2.0679280450267177

Epoch: 6| Step: 10
Training loss: 2.5582094192504883
Validation loss: 2.092818252501949

Epoch: 6| Step: 11
Training loss: 2.5164992809295654
Validation loss: 2.1033218394043627

Epoch: 6| Step: 12
Training loss: 2.5025320053100586
Validation loss: 2.0618801719398907

Epoch: 6| Step: 13
Training loss: 2.020958423614502
Validation loss: 2.094586797939834

Epoch: 25| Step: 0
Training loss: 2.8838248252868652
Validation loss: 2.105740552307457

Epoch: 6| Step: 1
Training loss: 1.8464831113815308
Validation loss: 2.1016394476736746

Epoch: 6| Step: 2
Training loss: 2.379166841506958
Validation loss: 2.1103758427404586

Epoch: 6| Step: 3
Training loss: 2.9001379013061523
Validation loss: 2.083240783342751

Epoch: 6| Step: 4
Training loss: 2.55570650100708
Validation loss: 2.0890562418968446

Epoch: 6| Step: 5
Training loss: 2.8809943199157715
Validation loss: 2.1088631896562475

Epoch: 6| Step: 6
Training loss: 1.876039981842041
Validation loss: 2.087929860238106

Epoch: 6| Step: 7
Training loss: 2.413599967956543
Validation loss: 2.0961041040317987

Epoch: 6| Step: 8
Training loss: 2.981030225753784
Validation loss: 2.0844227037122174

Epoch: 6| Step: 9
Training loss: 1.985961675643921
Validation loss: 2.11873794627446

Epoch: 6| Step: 10
Training loss: 2.606475591659546
Validation loss: 2.1026217591378

Epoch: 6| Step: 11
Training loss: 2.1142101287841797
Validation loss: 2.109968764807588

Epoch: 6| Step: 12
Training loss: 2.7578206062316895
Validation loss: 2.0779013351727555

Epoch: 6| Step: 13
Training loss: 1.861156940460205
Validation loss: 2.0893158323021344

Epoch: 26| Step: 0
Training loss: 2.6603572368621826
Validation loss: 2.074022800691666

Epoch: 6| Step: 1
Training loss: 1.7809367179870605
Validation loss: 2.0938046952729583

Epoch: 6| Step: 2
Training loss: 2.206787347793579
Validation loss: 2.0809025751647128

Epoch: 6| Step: 3
Training loss: 2.402738332748413
Validation loss: 2.0912797989383822

Epoch: 6| Step: 4
Training loss: 2.8315510749816895
Validation loss: 2.094914795250021

Epoch: 6| Step: 5
Training loss: 2.429579734802246
Validation loss: 2.1129690318979244

Epoch: 6| Step: 6
Training loss: 2.7787933349609375
Validation loss: 2.0962559330847954

Epoch: 6| Step: 7
Training loss: 2.3827707767486572
Validation loss: 2.0786988658289753

Epoch: 6| Step: 8
Training loss: 2.2807440757751465
Validation loss: 2.105880175867388

Epoch: 6| Step: 9
Training loss: 2.4402499198913574
Validation loss: 2.0677406954508957

Epoch: 6| Step: 10
Training loss: 3.1506717205047607
Validation loss: 2.1218679822901243

Epoch: 6| Step: 11
Training loss: 2.5772738456726074
Validation loss: 2.0699189145077943

Epoch: 6| Step: 12
Training loss: 1.7682218551635742
Validation loss: 2.1063611174142487

Epoch: 6| Step: 13
Training loss: 1.8850102424621582
Validation loss: 2.089462746856033

Epoch: 27| Step: 0
Training loss: 2.607147455215454
Validation loss: 2.1020548907659387

Epoch: 6| Step: 1
Training loss: 1.6433639526367188
Validation loss: 2.0741544154382523

Epoch: 6| Step: 2
Training loss: 2.308350086212158
Validation loss: 2.092923402786255

Epoch: 6| Step: 3
Training loss: 3.6995441913604736
Validation loss: 2.0963058189679216

Epoch: 6| Step: 4
Training loss: 2.984177350997925
Validation loss: 2.0737276436180196

Epoch: 6| Step: 5
Training loss: 2.251657247543335
Validation loss: 2.085110605403941

Epoch: 6| Step: 6
Training loss: 1.955109715461731
Validation loss: 2.0788810509507374

Epoch: 6| Step: 7
Training loss: 1.642913818359375
Validation loss: 2.0706555779262255

Epoch: 6| Step: 8
Training loss: 2.8780651092529297
Validation loss: 2.0604408389778546

Epoch: 6| Step: 9
Training loss: 2.5469632148742676
Validation loss: 2.087963750285487

Epoch: 6| Step: 10
Training loss: 2.6904525756835938
Validation loss: 2.109957964189591

Epoch: 6| Step: 11
Training loss: 2.3101186752319336
Validation loss: 2.0744751114999094

Epoch: 6| Step: 12
Training loss: 1.9273608922958374
Validation loss: 2.0967883230537496

Epoch: 6| Step: 13
Training loss: 2.812875986099243
Validation loss: 2.0970430053690428

Epoch: 28| Step: 0
Training loss: 2.391207695007324
Validation loss: 2.080482475219234

Epoch: 6| Step: 1
Training loss: 2.240290641784668
Validation loss: 2.067889960863257

Epoch: 6| Step: 2
Training loss: 2.5190141201019287
Validation loss: 2.089342068600398

Epoch: 6| Step: 3
Training loss: 2.402634859085083
Validation loss: 2.0762258601445023

Epoch: 6| Step: 4
Training loss: 3.0162856578826904
Validation loss: 2.0739364342022966

Epoch: 6| Step: 5
Training loss: 2.426872491836548
Validation loss: 2.0751271965683147

Epoch: 6| Step: 6
Training loss: 2.840223550796509
Validation loss: 2.0703101901597876

Epoch: 6| Step: 7
Training loss: 1.8223066329956055
Validation loss: 2.0616450181571384

Epoch: 6| Step: 8
Training loss: 2.632369041442871
Validation loss: 2.0413392666847474

Epoch: 6| Step: 9
Training loss: 2.3041155338287354
Validation loss: 2.0474837595416653

Epoch: 6| Step: 10
Training loss: 1.9025321006774902
Validation loss: 2.082170057040389

Epoch: 6| Step: 11
Training loss: 3.3030953407287598
Validation loss: 2.0623068924873107

Epoch: 6| Step: 12
Training loss: 2.231513261795044
Validation loss: 2.083227962575933

Epoch: 6| Step: 13
Training loss: 1.7431228160858154
Validation loss: 2.0903175825713785

Epoch: 29| Step: 0
Training loss: 2.519979953765869
Validation loss: 2.0808998846238658

Epoch: 6| Step: 1
Training loss: 2.0992088317871094
Validation loss: 2.0646006753367763

Epoch: 6| Step: 2
Training loss: 2.95097017288208
Validation loss: 2.063037847959867

Epoch: 6| Step: 3
Training loss: 2.552273988723755
Validation loss: 2.0674934874298754

Epoch: 6| Step: 4
Training loss: 2.2624762058258057
Validation loss: 2.0663645370032198

Epoch: 6| Step: 5
Training loss: 2.729217529296875
Validation loss: 2.1018197933832803

Epoch: 6| Step: 6
Training loss: 2.0819668769836426
Validation loss: 2.0731200479692027

Epoch: 6| Step: 7
Training loss: 2.4803783893585205
Validation loss: 2.095014933616884

Epoch: 6| Step: 8
Training loss: 2.0560302734375
Validation loss: 2.0867490127522457

Epoch: 6| Step: 9
Training loss: 2.890925407409668
Validation loss: 2.0763775533245457

Epoch: 6| Step: 10
Training loss: 2.623542070388794
Validation loss: 2.0758996971191896

Epoch: 6| Step: 11
Training loss: 1.740651249885559
Validation loss: 2.0763638942472395

Epoch: 6| Step: 12
Training loss: 2.468362808227539
Validation loss: 2.0713377024537776

Epoch: 6| Step: 13
Training loss: 2.257336139678955
Validation loss: 2.079550386756979

Epoch: 30| Step: 0
Training loss: 2.3844962120056152
Validation loss: 2.060472624276274

Epoch: 6| Step: 1
Training loss: 2.156748056411743
Validation loss: 2.1013517636124805

Epoch: 6| Step: 2
Training loss: 2.6503615379333496
Validation loss: 2.0707902549415507

Epoch: 6| Step: 3
Training loss: 2.411884069442749
Validation loss: 2.054405403393571

Epoch: 6| Step: 4
Training loss: 2.420358896255493
Validation loss: 2.0550901082254227

Epoch: 6| Step: 5
Training loss: 2.3888468742370605
Validation loss: 2.106055930096616

Epoch: 6| Step: 6
Training loss: 1.7514739036560059
Validation loss: 2.0737474426146476

Epoch: 6| Step: 7
Training loss: 2.962066411972046
Validation loss: 2.0502653993586057

Epoch: 6| Step: 8
Training loss: 2.8812270164489746
Validation loss: 2.083328866189526

Epoch: 6| Step: 9
Training loss: 2.181777000427246
Validation loss: 2.0556996727502472

Epoch: 6| Step: 10
Training loss: 2.3007779121398926
Validation loss: 2.088862237109933

Epoch: 6| Step: 11
Training loss: 2.339810848236084
Validation loss: 2.081354171999039

Epoch: 6| Step: 12
Training loss: 2.815398693084717
Validation loss: 2.0690136878721175

Epoch: 6| Step: 13
Training loss: 2.0751991271972656
Validation loss: 2.1028802369230535

Epoch: 31| Step: 0
Training loss: 2.1469597816467285
Validation loss: 2.0871977831727717

Epoch: 6| Step: 1
Training loss: 2.103177070617676
Validation loss: 2.054108096707252

Epoch: 6| Step: 2
Training loss: 2.088008165359497
Validation loss: 2.079505924255617

Epoch: 6| Step: 3
Training loss: 2.4453468322753906
Validation loss: 2.088680016097202

Epoch: 6| Step: 4
Training loss: 2.5293307304382324
Validation loss: 2.0691642222865934

Epoch: 6| Step: 5
Training loss: 2.4435205459594727
Validation loss: 2.0657041431755148

Epoch: 6| Step: 6
Training loss: 2.102008819580078
Validation loss: 2.0518449019360285

Epoch: 6| Step: 7
Training loss: 2.7769861221313477
Validation loss: 2.091103364062566

Epoch: 6| Step: 8
Training loss: 2.9463658332824707
Validation loss: 2.0969655629127257

Epoch: 6| Step: 9
Training loss: 2.479154586791992
Validation loss: 2.062409653458544

Epoch: 6| Step: 10
Training loss: 2.4636874198913574
Validation loss: 2.091842064293482

Epoch: 6| Step: 11
Training loss: 1.9657506942749023
Validation loss: 2.079722237843339

Epoch: 6| Step: 12
Training loss: 3.0360512733459473
Validation loss: 2.08715953493631

Epoch: 6| Step: 13
Training loss: 2.209758996963501
Validation loss: 2.064011222572737

Epoch: 32| Step: 0
Training loss: 2.228789806365967
Validation loss: 2.079448235932217

Epoch: 6| Step: 1
Training loss: 2.710296630859375
Validation loss: 2.085762973754637

Epoch: 6| Step: 2
Training loss: 2.2540247440338135
Validation loss: 2.071881362186965

Epoch: 6| Step: 3
Training loss: 2.281338691711426
Validation loss: 2.0928208469062723

Epoch: 6| Step: 4
Training loss: 2.8476076126098633
Validation loss: 2.079699836751466

Epoch: 6| Step: 5
Training loss: 2.651266574859619
Validation loss: 2.0640355553678287

Epoch: 6| Step: 6
Training loss: 2.4413881301879883
Validation loss: 2.051672858576621

Epoch: 6| Step: 7
Training loss: 2.7684452533721924
Validation loss: 2.059497476905905

Epoch: 6| Step: 8
Training loss: 2.517272472381592
Validation loss: 2.0555945327205043

Epoch: 6| Step: 9
Training loss: 2.5788512229919434
Validation loss: 2.0710801924428632

Epoch: 6| Step: 10
Training loss: 1.4789602756500244
Validation loss: 2.051899463899674

Epoch: 6| Step: 11
Training loss: 2.2508974075317383
Validation loss: 2.0965824921925864

Epoch: 6| Step: 12
Training loss: 2.7511889934539795
Validation loss: 2.051070222290613

Epoch: 6| Step: 13
Training loss: 1.713793158531189
Validation loss: 2.0664138793945312

Epoch: 33| Step: 0
Training loss: 2.1813149452209473
Validation loss: 2.088564786859738

Epoch: 6| Step: 1
Training loss: 2.794335126876831
Validation loss: 2.0833302646554928

Epoch: 6| Step: 2
Training loss: 2.380861520767212
Validation loss: 2.051715063792403

Epoch: 6| Step: 3
Training loss: 1.689347743988037
Validation loss: 2.0628948416761173

Epoch: 6| Step: 4
Training loss: 2.539121389389038
Validation loss: 2.065832035515898

Epoch: 6| Step: 5
Training loss: 2.4904885292053223
Validation loss: 2.0541214840386504

Epoch: 6| Step: 6
Training loss: 2.9538888931274414
Validation loss: 2.0677729114409416

Epoch: 6| Step: 7
Training loss: 2.6989784240722656
Validation loss: 2.060147311097832

Epoch: 6| Step: 8
Training loss: 2.2595906257629395
Validation loss: 2.08307328531819

Epoch: 6| Step: 9
Training loss: 2.1092958450317383
Validation loss: 2.0781242385987313

Epoch: 6| Step: 10
Training loss: 2.381807327270508
Validation loss: 2.080721820554426

Epoch: 6| Step: 11
Training loss: 2.49784255027771
Validation loss: 2.044478001133088

Epoch: 6| Step: 12
Training loss: 2.1153812408447266
Validation loss: 2.0659472134805497

Epoch: 6| Step: 13
Training loss: 2.0923593044281006
Validation loss: 2.054805435160155

Epoch: 34| Step: 0
Training loss: 1.8613312244415283
Validation loss: 2.0171549063856884

Epoch: 6| Step: 1
Training loss: 2.1653122901916504
Validation loss: 2.0594695639866654

Epoch: 6| Step: 2
Training loss: 2.098998785018921
Validation loss: 2.063338041305542

Epoch: 6| Step: 3
Training loss: 1.7290657758712769
Validation loss: 2.054375651062176

Epoch: 6| Step: 4
Training loss: 2.3492143154144287
Validation loss: 2.079977713605409

Epoch: 6| Step: 5
Training loss: 2.3050904273986816
Validation loss: 2.0697608699080763

Epoch: 6| Step: 6
Training loss: 1.7726154327392578
Validation loss: 2.0591877609170894

Epoch: 6| Step: 7
Training loss: 3.526547431945801
Validation loss: 2.0455688456053376

Epoch: 6| Step: 8
Training loss: 2.033095598220825
Validation loss: 2.0487933863875685

Epoch: 6| Step: 9
Training loss: 1.910125732421875
Validation loss: 2.0360638095486547

Epoch: 6| Step: 10
Training loss: 2.364076614379883
Validation loss: 2.0613716430561517

Epoch: 6| Step: 11
Training loss: 2.8266072273254395
Validation loss: 2.062194711418562

Epoch: 6| Step: 12
Training loss: 3.4129955768585205
Validation loss: 2.068031754544986

Epoch: 6| Step: 13
Training loss: 3.5490612983703613
Validation loss: 2.0812811043954667

Epoch: 35| Step: 0
Training loss: 3.4044430255889893
Validation loss: 2.0695321354814755

Epoch: 6| Step: 1
Training loss: 2.5991690158843994
Validation loss: 2.0695118083748767

Epoch: 6| Step: 2
Training loss: 2.091430902481079
Validation loss: 2.1048490296127977

Epoch: 6| Step: 3
Training loss: 2.3864870071411133
Validation loss: 2.061504324277242

Epoch: 6| Step: 4
Training loss: 2.0991621017456055
Validation loss: 2.058151511735814

Epoch: 6| Step: 5
Training loss: 2.8330605030059814
Validation loss: 2.0807229626563286

Epoch: 6| Step: 6
Training loss: 2.360032796859741
Validation loss: 2.072174725993987

Epoch: 6| Step: 7
Training loss: 1.9950147867202759
Validation loss: 2.0585924861251668

Epoch: 6| Step: 8
Training loss: 2.377976417541504
Validation loss: 2.076888709939936

Epoch: 6| Step: 9
Training loss: 2.423058032989502
Validation loss: 2.0520081878990255

Epoch: 6| Step: 10
Training loss: 2.470889091491699
Validation loss: 2.0755906181950725

Epoch: 6| Step: 11
Training loss: 2.1458261013031006
Validation loss: 2.070472559621257

Epoch: 6| Step: 12
Training loss: 2.2834854125976562
Validation loss: 2.0795283830294045

Epoch: 6| Step: 13
Training loss: 1.5631964206695557
Validation loss: 2.059486817288142

Epoch: 36| Step: 0
Training loss: 2.354753017425537
Validation loss: 2.0531813739448466

Epoch: 6| Step: 1
Training loss: 2.818943500518799
Validation loss: 2.072529254421111

Epoch: 6| Step: 2
Training loss: 2.509550094604492
Validation loss: 2.070552772091281

Epoch: 6| Step: 3
Training loss: 3.341588020324707
Validation loss: 2.0398978007737028

Epoch: 6| Step: 4
Training loss: 2.179861307144165
Validation loss: 2.069317968942786

Epoch: 6| Step: 5
Training loss: 2.4823355674743652
Validation loss: 2.0459193080984135

Epoch: 6| Step: 6
Training loss: 2.238110065460205
Validation loss: 2.062674178872057

Epoch: 6| Step: 7
Training loss: 2.5085620880126953
Validation loss: 2.0161888009758404

Epoch: 6| Step: 8
Training loss: 2.189241886138916
Validation loss: 2.1049356819480978

Epoch: 6| Step: 9
Training loss: 2.8108344078063965
Validation loss: 2.0583482609000257

Epoch: 6| Step: 10
Training loss: 1.9734385013580322
Validation loss: 2.0490701903579054

Epoch: 6| Step: 11
Training loss: 2.2177438735961914
Validation loss: 2.057700696811881

Epoch: 6| Step: 12
Training loss: 1.767456293106079
Validation loss: 2.072884587831395

Epoch: 6| Step: 13
Training loss: 1.711281657218933
Validation loss: 2.052418562673753

Epoch: 37| Step: 0
Training loss: 2.641017198562622
Validation loss: 2.0674575118608374

Epoch: 6| Step: 1
Training loss: 1.9063183069229126
Validation loss: 2.0395281763486963

Epoch: 6| Step: 2
Training loss: 2.2429091930389404
Validation loss: 2.0723769626309796

Epoch: 6| Step: 3
Training loss: 2.5434470176696777
Validation loss: 2.060291733793033

Epoch: 6| Step: 4
Training loss: 2.4368185997009277
Validation loss: 2.068444016159222

Epoch: 6| Step: 5
Training loss: 1.9897962808609009
Validation loss: 2.0526708608032553

Epoch: 6| Step: 6
Training loss: 2.7718262672424316
Validation loss: 2.0494598432253768

Epoch: 6| Step: 7
Training loss: 2.5172057151794434
Validation loss: 2.060839968342935

Epoch: 6| Step: 8
Training loss: 2.3249073028564453
Validation loss: 2.0743373029975483

Epoch: 6| Step: 9
Training loss: 2.3003733158111572
Validation loss: 2.066211031329247

Epoch: 6| Step: 10
Training loss: 3.362922191619873
Validation loss: 2.0579979547890286

Epoch: 6| Step: 11
Training loss: 2.373239517211914
Validation loss: 2.0707774854475454

Epoch: 6| Step: 12
Training loss: 1.7398062944412231
Validation loss: 2.076969744056784

Epoch: 6| Step: 13
Training loss: 2.068150758743286
Validation loss: 2.0481221368235927

Epoch: 38| Step: 0
Training loss: 2.9228155612945557
Validation loss: 2.0692533600714897

Epoch: 6| Step: 1
Training loss: 2.1009883880615234
Validation loss: 2.056083712526547

Epoch: 6| Step: 2
Training loss: 1.6644867658615112
Validation loss: 2.0441724523421256

Epoch: 6| Step: 3
Training loss: 2.456726551055908
Validation loss: 2.0233716708357616

Epoch: 6| Step: 4
Training loss: 2.0269293785095215
Validation loss: 2.048243584171418

Epoch: 6| Step: 5
Training loss: 2.5952022075653076
Validation loss: 2.0498753106722267

Epoch: 6| Step: 6
Training loss: 2.71331787109375
Validation loss: 2.0768588742902203

Epoch: 6| Step: 7
Training loss: 2.4886646270751953
Validation loss: 2.0657343441440212

Epoch: 6| Step: 8
Training loss: 2.6706268787384033
Validation loss: 2.0580626841514342

Epoch: 6| Step: 9
Training loss: 2.470426559448242
Validation loss: 2.0599755805025817

Epoch: 6| Step: 10
Training loss: 2.3090360164642334
Validation loss: 2.06530898873524

Epoch: 6| Step: 11
Training loss: 2.4169111251831055
Validation loss: 2.064469655354818

Epoch: 6| Step: 12
Training loss: 2.5336525440216064
Validation loss: 2.0622947933853313

Epoch: 6| Step: 13
Training loss: 1.6798362731933594
Validation loss: 2.049353375229784

Epoch: 39| Step: 0
Training loss: 2.519531488418579
Validation loss: 2.0826557938770582

Epoch: 6| Step: 1
Training loss: 2.4153037071228027
Validation loss: 2.0549418695511354

Epoch: 6| Step: 2
Training loss: 2.2746152877807617
Validation loss: 2.0853743527525213

Epoch: 6| Step: 3
Training loss: 2.9359583854675293
Validation loss: 2.061004400253296

Epoch: 6| Step: 4
Training loss: 1.7753106355667114
Validation loss: 2.0725572198949833

Epoch: 6| Step: 5
Training loss: 2.238097667694092
Validation loss: 2.1009929872328237

Epoch: 6| Step: 6
Training loss: 2.8288562297821045
Validation loss: 2.0481089007469917

Epoch: 6| Step: 7
Training loss: 2.7205801010131836
Validation loss: 2.0486488419194377

Epoch: 6| Step: 8
Training loss: 1.7769229412078857
Validation loss: 2.0680928255922053

Epoch: 6| Step: 9
Training loss: 2.15879487991333
Validation loss: 2.05030333611273

Epoch: 6| Step: 10
Training loss: 2.829498767852783
Validation loss: 2.041414504410118

Epoch: 6| Step: 11
Training loss: 2.908102512359619
Validation loss: 2.081967612748505

Epoch: 6| Step: 12
Training loss: 1.434988021850586
Validation loss: 2.0924297917273735

Epoch: 6| Step: 13
Training loss: 2.335118532180786
Validation loss: 2.091536178383776

Epoch: 40| Step: 0
Training loss: 2.518200159072876
Validation loss: 2.073614493493111

Epoch: 6| Step: 1
Training loss: 2.314987897872925
Validation loss: 2.0580815217828237

Epoch: 6| Step: 2
Training loss: 2.7377681732177734
Validation loss: 2.0641865012466267

Epoch: 6| Step: 3
Training loss: 2.2268588542938232
Validation loss: 2.0777194141059794

Epoch: 6| Step: 4
Training loss: 2.646315336227417
Validation loss: 2.059628794270177

Epoch: 6| Step: 5
Training loss: 2.0582046508789062
Validation loss: 2.060157040114044

Epoch: 6| Step: 6
Training loss: 2.213641405105591
Validation loss: 2.0406110209803425

Epoch: 6| Step: 7
Training loss: 2.384392261505127
Validation loss: 2.047195952425721

Epoch: 6| Step: 8
Training loss: 2.92587947845459
Validation loss: 2.0348201182580765

Epoch: 6| Step: 9
Training loss: 1.8929396867752075
Validation loss: 2.0646687720411565

Epoch: 6| Step: 10
Training loss: 2.154542922973633
Validation loss: 2.069363104399814

Epoch: 6| Step: 11
Training loss: 2.595804452896118
Validation loss: 2.074301108237236

Epoch: 6| Step: 12
Training loss: 1.9806523323059082
Validation loss: 2.051587394488755

Epoch: 6| Step: 13
Training loss: 2.536882162094116
Validation loss: 2.050423235021612

Epoch: 41| Step: 0
Training loss: 1.9694573879241943
Validation loss: 2.0457930616153184

Epoch: 6| Step: 1
Training loss: 1.7916351556777954
Validation loss: 2.042703861831337

Epoch: 6| Step: 2
Training loss: 1.961893916130066
Validation loss: 2.0231744525253132

Epoch: 6| Step: 3
Training loss: 2.183030128479004
Validation loss: 2.0503703881335515

Epoch: 6| Step: 4
Training loss: 2.180999994277954
Validation loss: 2.037571135387626

Epoch: 6| Step: 5
Training loss: 2.7798964977264404
Validation loss: 2.0529012218598397

Epoch: 6| Step: 6
Training loss: 1.9623534679412842
Validation loss: 2.065676919875606

Epoch: 6| Step: 7
Training loss: 1.5533249378204346
Validation loss: 2.0266316654861614

Epoch: 6| Step: 8
Training loss: 2.498994827270508
Validation loss: 2.075256734765986

Epoch: 6| Step: 9
Training loss: 3.0072388648986816
Validation loss: 2.031325319761871

Epoch: 6| Step: 10
Training loss: 3.2001147270202637
Validation loss: 2.0564946513022146

Epoch: 6| Step: 11
Training loss: 2.652297019958496
Validation loss: 2.062835967668923

Epoch: 6| Step: 12
Training loss: 2.3840131759643555
Validation loss: 2.0559690716446086

Epoch: 6| Step: 13
Training loss: 3.4866018295288086
Validation loss: 2.0639688148293445

Epoch: 42| Step: 0
Training loss: 2.220552921295166
Validation loss: 2.009300531879548

Epoch: 6| Step: 1
Training loss: 2.473376750946045
Validation loss: 2.0624505037902505

Epoch: 6| Step: 2
Training loss: 2.4644131660461426
Validation loss: 2.046953316657774

Epoch: 6| Step: 3
Training loss: 3.0037283897399902
Validation loss: 2.0441190529895086

Epoch: 6| Step: 4
Training loss: 2.050384044647217
Validation loss: 2.065012311422697

Epoch: 6| Step: 5
Training loss: 2.234039783477783
Validation loss: 2.057185066643582

Epoch: 6| Step: 6
Training loss: 2.3616843223571777
Validation loss: 2.0392513531510548

Epoch: 6| Step: 7
Training loss: 2.5572962760925293
Validation loss: 2.036531058690881

Epoch: 6| Step: 8
Training loss: 2.2601399421691895
Validation loss: 2.0341662642776326

Epoch: 6| Step: 9
Training loss: 1.7092230319976807
Validation loss: 2.082030250180152

Epoch: 6| Step: 10
Training loss: 2.364353656768799
Validation loss: 2.0323524039278746

Epoch: 6| Step: 11
Training loss: 2.586543560028076
Validation loss: 2.0689182025130077

Epoch: 6| Step: 12
Training loss: 2.1912035942077637
Validation loss: 2.036358775631074

Epoch: 6| Step: 13
Training loss: 2.6168251037597656
Validation loss: 2.0366583331938712

Epoch: 43| Step: 0
Training loss: 2.674931049346924
Validation loss: 2.0300908998776506

Epoch: 6| Step: 1
Training loss: 1.928839921951294
Validation loss: 2.023172354185453

Epoch: 6| Step: 2
Training loss: 2.529322624206543
Validation loss: 2.0299118103519564

Epoch: 6| Step: 3
Training loss: 2.8430328369140625
Validation loss: 2.057010125088435

Epoch: 6| Step: 4
Training loss: 2.163525104522705
Validation loss: 2.051801053426599

Epoch: 6| Step: 5
Training loss: 2.131559371948242
Validation loss: 2.036342388840132

Epoch: 6| Step: 6
Training loss: 1.8348037004470825
Validation loss: 2.040104478918096

Epoch: 6| Step: 7
Training loss: 2.1413869857788086
Validation loss: 2.026212769169961

Epoch: 6| Step: 8
Training loss: 3.0670340061187744
Validation loss: 2.0461814993171283

Epoch: 6| Step: 9
Training loss: 2.2616043090820312
Validation loss: 2.023986408787389

Epoch: 6| Step: 10
Training loss: 1.922440767288208
Validation loss: 2.0645456455087148

Epoch: 6| Step: 11
Training loss: 2.546205520629883
Validation loss: 2.0273822058913527

Epoch: 6| Step: 12
Training loss: 2.418206214904785
Validation loss: 2.0287243166277484

Epoch: 6| Step: 13
Training loss: 2.3964130878448486
Validation loss: 2.01159228304381

Epoch: 44| Step: 0
Training loss: 2.41171932220459
Validation loss: 2.045063885309363

Epoch: 6| Step: 1
Training loss: 2.4874892234802246
Validation loss: 2.053211851786542

Epoch: 6| Step: 2
Training loss: 2.045339584350586
Validation loss: 2.041328501957719

Epoch: 6| Step: 3
Training loss: 2.21305251121521
Validation loss: 2.0386248929526216

Epoch: 6| Step: 4
Training loss: 3.2426464557647705
Validation loss: 2.0332090970008605

Epoch: 6| Step: 5
Training loss: 2.236982822418213
Validation loss: 2.0348766208976827

Epoch: 6| Step: 6
Training loss: 2.0150158405303955
Validation loss: 2.0574145470896075

Epoch: 6| Step: 7
Training loss: 2.433316230773926
Validation loss: 2.018714302329607

Epoch: 6| Step: 8
Training loss: 2.292041778564453
Validation loss: 2.0553614516412058

Epoch: 6| Step: 9
Training loss: 2.640315532684326
Validation loss: 2.0303513580752957

Epoch: 6| Step: 10
Training loss: 2.447387218475342
Validation loss: 2.0375021926818357

Epoch: 6| Step: 11
Training loss: 1.6708403825759888
Validation loss: 2.0244452350883075

Epoch: 6| Step: 12
Training loss: 2.3235909938812256
Validation loss: 2.015034552543394

Epoch: 6| Step: 13
Training loss: 2.546375274658203
Validation loss: 2.0505429365301646

Epoch: 45| Step: 0
Training loss: 2.449172019958496
Validation loss: 2.007121511684951

Epoch: 6| Step: 1
Training loss: 2.5131030082702637
Validation loss: 2.0152603759560535

Epoch: 6| Step: 2
Training loss: 1.8820234537124634
Validation loss: 2.0549355796588364

Epoch: 6| Step: 3
Training loss: 2.990766763687134
Validation loss: 2.0713940820386334

Epoch: 6| Step: 4
Training loss: 1.923629879951477
Validation loss: 2.0338991175415697

Epoch: 6| Step: 5
Training loss: 1.9345009326934814
Validation loss: 2.0448741297568045

Epoch: 6| Step: 6
Training loss: 2.232816696166992
Validation loss: 2.0399819753503285

Epoch: 6| Step: 7
Training loss: 2.0613694190979004
Validation loss: 2.0447849522354784

Epoch: 6| Step: 8
Training loss: 3.1879987716674805
Validation loss: 2.036339530380823

Epoch: 6| Step: 9
Training loss: 2.038424015045166
Validation loss: 2.0127649012432305

Epoch: 6| Step: 10
Training loss: 2.7704617977142334
Validation loss: 2.0339649518330893

Epoch: 6| Step: 11
Training loss: 2.0906081199645996
Validation loss: 2.0322609383572816

Epoch: 6| Step: 12
Training loss: 2.3784356117248535
Validation loss: 2.024842454541114

Epoch: 6| Step: 13
Training loss: 2.514659881591797
Validation loss: 2.037798975103645

Epoch: 46| Step: 0
Training loss: 2.3431906700134277
Validation loss: 2.0558211136889715

Epoch: 6| Step: 1
Training loss: 1.806979775428772
Validation loss: 2.0262933584951583

Epoch: 6| Step: 2
Training loss: 3.1509652137756348
Validation loss: 2.0499131525716474

Epoch: 6| Step: 3
Training loss: 2.226017951965332
Validation loss: 2.0202453636354014

Epoch: 6| Step: 4
Training loss: 1.9430639743804932
Validation loss: 2.015718303700929

Epoch: 6| Step: 5
Training loss: 2.2896597385406494
Validation loss: 2.0233366540683213

Epoch: 6| Step: 6
Training loss: 2.32912015914917
Validation loss: 2.004860157607704

Epoch: 6| Step: 7
Training loss: 2.4879629611968994
Validation loss: 2.0178271865332

Epoch: 6| Step: 8
Training loss: 1.8600378036499023
Validation loss: 1.9905745444759246

Epoch: 6| Step: 9
Training loss: 2.639012336730957
Validation loss: 2.0422821839650473

Epoch: 6| Step: 10
Training loss: 2.591538667678833
Validation loss: 2.0314676428353913

Epoch: 6| Step: 11
Training loss: 1.8747003078460693
Validation loss: 2.041339917849469

Epoch: 6| Step: 12
Training loss: 2.505397319793701
Validation loss: 2.0370583136876426

Epoch: 6| Step: 13
Training loss: 3.16225266456604
Validation loss: 2.017627303318311

Epoch: 47| Step: 0
Training loss: 2.636523962020874
Validation loss: 2.0388394145555395

Epoch: 6| Step: 1
Training loss: 2.1347813606262207
Validation loss: 2.019935155427584

Epoch: 6| Step: 2
Training loss: 2.3551883697509766
Validation loss: 2.0122627686428767

Epoch: 6| Step: 3
Training loss: 2.925609588623047
Validation loss: 2.016977184562273

Epoch: 6| Step: 4
Training loss: 2.1101605892181396
Validation loss: 2.027351020484842

Epoch: 6| Step: 5
Training loss: 2.799337863922119
Validation loss: 2.0349214999906478

Epoch: 6| Step: 6
Training loss: 2.3267688751220703
Validation loss: 2.0296672108352825

Epoch: 6| Step: 7
Training loss: 2.5257515907287598
Validation loss: 2.0422388225473385

Epoch: 6| Step: 8
Training loss: 1.8008785247802734
Validation loss: 2.0466644225582

Epoch: 6| Step: 9
Training loss: 2.725841522216797
Validation loss: 2.047947181168423

Epoch: 6| Step: 10
Training loss: 1.7085882425308228
Validation loss: 2.013081363452378

Epoch: 6| Step: 11
Training loss: 2.188951253890991
Validation loss: 2.039320812430433

Epoch: 6| Step: 12
Training loss: 1.8256309032440186
Validation loss: 2.0122061775576685

Epoch: 6| Step: 13
Training loss: 2.9642698764801025
Validation loss: 2.03509719141068

Epoch: 48| Step: 0
Training loss: 2.366973400115967
Validation loss: 2.052414619794456

Epoch: 6| Step: 1
Training loss: 2.5865864753723145
Validation loss: 2.011505560208392

Epoch: 6| Step: 2
Training loss: 1.9916279315948486
Validation loss: 2.0370887966566187

Epoch: 6| Step: 3
Training loss: 1.940474033355713
Validation loss: 2.01778144221152

Epoch: 6| Step: 4
Training loss: 2.2853713035583496
Validation loss: 2.008377032895242

Epoch: 6| Step: 5
Training loss: 2.466780424118042
Validation loss: 2.0056175083242436

Epoch: 6| Step: 6
Training loss: 2.079829216003418
Validation loss: 2.0499686118095153

Epoch: 6| Step: 7
Training loss: 2.6704249382019043
Validation loss: 2.0204089277534076

Epoch: 6| Step: 8
Training loss: 2.716200828552246
Validation loss: 2.019091716376684

Epoch: 6| Step: 9
Training loss: 1.9440765380859375
Validation loss: 2.04102760617451

Epoch: 6| Step: 10
Training loss: 1.7290904521942139
Validation loss: 2.0322168437383508

Epoch: 6| Step: 11
Training loss: 2.586313247680664
Validation loss: 2.0091419476334766

Epoch: 6| Step: 12
Training loss: 2.500338554382324
Validation loss: 1.9963087317764119

Epoch: 6| Step: 13
Training loss: 2.8202247619628906
Validation loss: 1.9950804492478729

Epoch: 49| Step: 0
Training loss: 2.0430450439453125
Validation loss: 2.026384981729651

Epoch: 6| Step: 1
Training loss: 1.6702107191085815
Validation loss: 2.0230681255299556

Epoch: 6| Step: 2
Training loss: 2.640110969543457
Validation loss: 2.0192015529960714

Epoch: 6| Step: 3
Training loss: 2.6955928802490234
Validation loss: 2.0562858248269684

Epoch: 6| Step: 4
Training loss: 1.8016209602355957
Validation loss: 2.0254473788763887

Epoch: 6| Step: 5
Training loss: 2.2200822830200195
Validation loss: 2.012564879591747

Epoch: 6| Step: 6
Training loss: 3.3790111541748047
Validation loss: 2.031798408877465

Epoch: 6| Step: 7
Training loss: 2.309422492980957
Validation loss: 2.0219595227190243

Epoch: 6| Step: 8
Training loss: 1.662013053894043
Validation loss: 2.0347448728417836

Epoch: 6| Step: 9
Training loss: 2.5585572719573975
Validation loss: 1.9921275851547078

Epoch: 6| Step: 10
Training loss: 2.8821754455566406
Validation loss: 2.0021504599560975

Epoch: 6| Step: 11
Training loss: 2.547182083129883
Validation loss: 2.0433600897430093

Epoch: 6| Step: 12
Training loss: 2.1181702613830566
Validation loss: 2.002584344597273

Epoch: 6| Step: 13
Training loss: 1.7178101539611816
Validation loss: 2.0182553875830864

Epoch: 50| Step: 0
Training loss: 2.704336166381836
Validation loss: 2.04873836681407

Epoch: 6| Step: 1
Training loss: 2.3186206817626953
Validation loss: 2.049466292063395

Epoch: 6| Step: 2
Training loss: 1.9037208557128906
Validation loss: 2.0428735210049536

Epoch: 6| Step: 3
Training loss: 2.589249610900879
Validation loss: 2.049511391629455

Epoch: 6| Step: 4
Training loss: 2.8764777183532715
Validation loss: 2.0308800871654222

Epoch: 6| Step: 5
Training loss: 2.367607355117798
Validation loss: 2.0059689244916363

Epoch: 6| Step: 6
Training loss: 1.3868122100830078
Validation loss: 2.018108867829846

Epoch: 6| Step: 7
Training loss: 2.253176212310791
Validation loss: 2.0326497042050926

Epoch: 6| Step: 8
Training loss: 2.2574844360351562
Validation loss: 2.0302732785542807

Epoch: 6| Step: 9
Training loss: 2.3607609272003174
Validation loss: 2.0322861261265253

Epoch: 6| Step: 10
Training loss: 2.1670618057250977
Validation loss: 1.9970586812624367

Epoch: 6| Step: 11
Training loss: 2.5414772033691406
Validation loss: 2.0413391833664267

Epoch: 6| Step: 12
Training loss: 2.2091612815856934
Validation loss: 2.016818304215708

Epoch: 6| Step: 13
Training loss: 1.943267822265625
Validation loss: 2.016146044577322

Epoch: 51| Step: 0
Training loss: 2.3009207248687744
Validation loss: 2.040967736192929

Epoch: 6| Step: 1
Training loss: 2.450645685195923
Validation loss: 1.9965112388774913

Epoch: 6| Step: 2
Training loss: 2.144956111907959
Validation loss: 2.0368333234581897

Epoch: 6| Step: 3
Training loss: 2.2839550971984863
Validation loss: 2.035697703720421

Epoch: 6| Step: 4
Training loss: 2.1242713928222656
Validation loss: 2.0249163079005417

Epoch: 6| Step: 5
Training loss: 2.679741382598877
Validation loss: 2.0106159692169516

Epoch: 6| Step: 6
Training loss: 2.281332492828369
Validation loss: 2.0475700516854562

Epoch: 6| Step: 7
Training loss: 2.0206871032714844
Validation loss: 2.0031537984007146

Epoch: 6| Step: 8
Training loss: 2.532688617706299
Validation loss: 2.021137732331471

Epoch: 6| Step: 9
Training loss: 1.7945858240127563
Validation loss: 2.000137044537452

Epoch: 6| Step: 10
Training loss: 2.6889290809631348
Validation loss: 2.035847871534286

Epoch: 6| Step: 11
Training loss: 1.57804536819458
Validation loss: 2.037014297259751

Epoch: 6| Step: 12
Training loss: 2.294529914855957
Validation loss: 2.0327529548316874

Epoch: 6| Step: 13
Training loss: 3.6527812480926514
Validation loss: 2.02803736348306

Epoch: 52| Step: 0
Training loss: 2.1388072967529297
Validation loss: 2.0255485221903813

Epoch: 6| Step: 1
Training loss: 1.369747519493103
Validation loss: 2.03978923443825

Epoch: 6| Step: 2
Training loss: 2.162569522857666
Validation loss: 2.0051866949245496

Epoch: 6| Step: 3
Training loss: 1.7066341638565063
Validation loss: 2.033927336815865

Epoch: 6| Step: 4
Training loss: 2.431706666946411
Validation loss: 2.031671543275156

Epoch: 6| Step: 5
Training loss: 1.9871560335159302
Validation loss: 1.9985832424574002

Epoch: 6| Step: 6
Training loss: 2.332388401031494
Validation loss: 1.9901604883132442

Epoch: 6| Step: 7
Training loss: 1.987460732460022
Validation loss: 2.045816917573252

Epoch: 6| Step: 8
Training loss: 3.473644256591797
Validation loss: 2.020346599240457

Epoch: 6| Step: 9
Training loss: 2.5048227310180664
Validation loss: 2.004667830723588

Epoch: 6| Step: 10
Training loss: 1.9271490573883057
Validation loss: 2.0163525842851207

Epoch: 6| Step: 11
Training loss: 2.6913623809814453
Validation loss: 2.0198188392064904

Epoch: 6| Step: 12
Training loss: 3.143709421157837
Validation loss: 2.02488068098663

Epoch: 6| Step: 13
Training loss: 2.50881290435791
Validation loss: 2.024637504290509

Epoch: 53| Step: 0
Training loss: 2.6958489418029785
Validation loss: 1.9987903641116234

Epoch: 6| Step: 1
Training loss: 2.6637165546417236
Validation loss: 1.9852348219963811

Epoch: 6| Step: 2
Training loss: 1.9760326147079468
Validation loss: 2.000178237115183

Epoch: 6| Step: 3
Training loss: 2.1635279655456543
Validation loss: 2.020598009068479

Epoch: 6| Step: 4
Training loss: 2.0188074111938477
Validation loss: 2.0205470208198792

Epoch: 6| Step: 5
Training loss: 3.041372776031494
Validation loss: 2.0307269993648736

Epoch: 6| Step: 6
Training loss: 2.0529816150665283
Validation loss: 2.023169240643901

Epoch: 6| Step: 7
Training loss: 2.45174503326416
Validation loss: 2.0228222980294177

Epoch: 6| Step: 8
Training loss: 2.3398780822753906
Validation loss: 2.009332950397204

Epoch: 6| Step: 9
Training loss: 2.3047122955322266
Validation loss: 2.015299256129931

Epoch: 6| Step: 10
Training loss: 2.275146961212158
Validation loss: 2.0236723294822117

Epoch: 6| Step: 11
Training loss: 2.575580596923828
Validation loss: 2.0287599909690117

Epoch: 6| Step: 12
Training loss: 1.9827320575714111
Validation loss: 2.0079100772898686

Epoch: 6| Step: 13
Training loss: 1.47641921043396
Validation loss: 2.0248633200122463

Epoch: 54| Step: 0
Training loss: 2.696707248687744
Validation loss: 2.030791001935159

Epoch: 6| Step: 1
Training loss: 2.661017417907715
Validation loss: 2.0275398890177407

Epoch: 6| Step: 2
Training loss: 2.1194119453430176
Validation loss: 2.0255431257268435

Epoch: 6| Step: 3
Training loss: 1.5369627475738525
Validation loss: 2.0094762476541663

Epoch: 6| Step: 4
Training loss: 1.952020525932312
Validation loss: 2.0499434778767247

Epoch: 6| Step: 5
Training loss: 2.0185608863830566
Validation loss: 2.0474227359217982

Epoch: 6| Step: 6
Training loss: 2.843050718307495
Validation loss: 2.0262190308622134

Epoch: 6| Step: 7
Training loss: 2.169771671295166
Validation loss: 2.027309730488767

Epoch: 6| Step: 8
Training loss: 2.3956713676452637
Validation loss: 2.0017893852726107

Epoch: 6| Step: 9
Training loss: 2.4264345169067383
Validation loss: 1.979508005162721

Epoch: 6| Step: 10
Training loss: 2.594038963317871
Validation loss: 2.014086061908353

Epoch: 6| Step: 11
Training loss: 1.833226203918457
Validation loss: 1.994208046185073

Epoch: 6| Step: 12
Training loss: 2.5754313468933105
Validation loss: 2.029540499051412

Epoch: 6| Step: 13
Training loss: 2.735039710998535
Validation loss: 2.0040982218198877

Epoch: 55| Step: 0
Training loss: 2.75404691696167
Validation loss: 2.0120594040040047

Epoch: 6| Step: 1
Training loss: 1.8085991144180298
Validation loss: 2.0165562629699707

Epoch: 6| Step: 2
Training loss: 2.5553293228149414
Validation loss: 2.0114753900035733

Epoch: 6| Step: 3
Training loss: 2.900905132293701
Validation loss: 2.0173402960582445

Epoch: 6| Step: 4
Training loss: 1.0468864440917969
Validation loss: 2.0341568005982267

Epoch: 6| Step: 5
Training loss: 2.5694878101348877
Validation loss: 2.002064376749018

Epoch: 6| Step: 6
Training loss: 2.451918125152588
Validation loss: 2.0367624887856106

Epoch: 6| Step: 7
Training loss: 2.6572306156158447
Validation loss: 2.0226225878602717

Epoch: 6| Step: 8
Training loss: 1.9367488622665405
Validation loss: 2.0129425371846845

Epoch: 6| Step: 9
Training loss: 2.062993049621582
Validation loss: 2.035108703438954

Epoch: 6| Step: 10
Training loss: 2.486069917678833
Validation loss: 1.995968549482284

Epoch: 6| Step: 11
Training loss: 1.735897421836853
Validation loss: 2.009363500020837

Epoch: 6| Step: 12
Training loss: 2.3104562759399414
Validation loss: 2.0335114438046693

Epoch: 6| Step: 13
Training loss: 3.6472630500793457
Validation loss: 2.0542908201935473

Epoch: 56| Step: 0
Training loss: 2.5014986991882324
Validation loss: 2.0258144358152985

Epoch: 6| Step: 1
Training loss: 2.2104618549346924
Validation loss: 2.0139901920031478

Epoch: 6| Step: 2
Training loss: 2.824310302734375
Validation loss: 2.022485440777194

Epoch: 6| Step: 3
Training loss: 1.970362901687622
Validation loss: 1.9904377851434933

Epoch: 6| Step: 4
Training loss: 2.9335713386535645
Validation loss: 2.005081339548993

Epoch: 6| Step: 5
Training loss: 2.116476058959961
Validation loss: 2.0243183848678425

Epoch: 6| Step: 6
Training loss: 2.3057241439819336
Validation loss: 1.9912734134222871

Epoch: 6| Step: 7
Training loss: 2.49468994140625
Validation loss: 2.024323663403911

Epoch: 6| Step: 8
Training loss: 2.3045549392700195
Validation loss: 2.031007941051196

Epoch: 6| Step: 9
Training loss: 2.2019896507263184
Validation loss: 1.9990893794644264

Epoch: 6| Step: 10
Training loss: 2.442349910736084
Validation loss: 1.9924810099345382

Epoch: 6| Step: 11
Training loss: 1.7455767393112183
Validation loss: 1.9997950689766997

Epoch: 6| Step: 12
Training loss: 2.4405133724212646
Validation loss: 2.0080851072906167

Epoch: 6| Step: 13
Training loss: 1.5179662704467773
Validation loss: 1.996028066963278

Epoch: 57| Step: 0
Training loss: 1.8468443155288696
Validation loss: 1.987975820418327

Epoch: 6| Step: 1
Training loss: 2.3188796043395996
Validation loss: 2.0071662446503997

Epoch: 6| Step: 2
Training loss: 2.565251111984253
Validation loss: 2.0042661018269037

Epoch: 6| Step: 3
Training loss: 2.1582746505737305
Validation loss: 2.011001471550234

Epoch: 6| Step: 4
Training loss: 2.6364974975585938
Validation loss: 2.0053089229009484

Epoch: 6| Step: 5
Training loss: 2.2173821926116943
Validation loss: 1.9950778894526984

Epoch: 6| Step: 6
Training loss: 2.4415671825408936
Validation loss: 2.0328543109278523

Epoch: 6| Step: 7
Training loss: 2.266209125518799
Validation loss: 2.0078695717678277

Epoch: 6| Step: 8
Training loss: 3.131486415863037
Validation loss: 1.9914070534449753

Epoch: 6| Step: 9
Training loss: 1.6849379539489746
Validation loss: 2.0040845294152536

Epoch: 6| Step: 10
Training loss: 2.011869430541992
Validation loss: 2.0124630133310952

Epoch: 6| Step: 11
Training loss: 2.4204773902893066
Validation loss: 2.028617433322373

Epoch: 6| Step: 12
Training loss: 2.3157176971435547
Validation loss: 2.006541852028139

Epoch: 6| Step: 13
Training loss: 2.052945852279663
Validation loss: 2.008883160929526

Epoch: 58| Step: 0
Training loss: 2.245861530303955
Validation loss: 2.01205765047381

Epoch: 6| Step: 1
Training loss: 1.9181692600250244
Validation loss: 2.013698708626532

Epoch: 6| Step: 2
Training loss: 2.1534130573272705
Validation loss: 2.001582271309309

Epoch: 6| Step: 3
Training loss: 2.861574649810791
Validation loss: 1.9930913217606083

Epoch: 6| Step: 4
Training loss: 2.3412280082702637
Validation loss: 2.010304795798435

Epoch: 6| Step: 5
Training loss: 1.8895456790924072
Validation loss: 2.01095897664306

Epoch: 6| Step: 6
Training loss: 2.4262170791625977
Validation loss: 2.0108199632295998

Epoch: 6| Step: 7
Training loss: 2.107090473175049
Validation loss: 2.035366904351019

Epoch: 6| Step: 8
Training loss: 2.899610996246338
Validation loss: 2.0437837108489005

Epoch: 6| Step: 9
Training loss: 2.1620850563049316
Validation loss: 2.0218143616953204

Epoch: 6| Step: 10
Training loss: 2.464998483657837
Validation loss: 2.021201205509965

Epoch: 6| Step: 11
Training loss: 2.021354913711548
Validation loss: 2.045341690381368

Epoch: 6| Step: 12
Training loss: 1.9756242036819458
Validation loss: 2.0307590474364576

Epoch: 6| Step: 13
Training loss: 2.456623077392578
Validation loss: 2.035414336830057

Epoch: 59| Step: 0
Training loss: 2.014676094055176
Validation loss: 2.040605911644556

Epoch: 6| Step: 1
Training loss: 1.4061717987060547
Validation loss: 2.0383483107371996

Epoch: 6| Step: 2
Training loss: 2.9276041984558105
Validation loss: 2.0058566447227233

Epoch: 6| Step: 3
Training loss: 2.3268115520477295
Validation loss: 2.0003901386773713

Epoch: 6| Step: 4
Training loss: 2.085979461669922
Validation loss: 2.0300841139208887

Epoch: 6| Step: 5
Training loss: 2.089974880218506
Validation loss: 2.0159573119173766

Epoch: 6| Step: 6
Training loss: 2.3708243370056152
Validation loss: 2.0306242281390774

Epoch: 6| Step: 7
Training loss: 2.249406337738037
Validation loss: 1.991054678475985

Epoch: 6| Step: 8
Training loss: 2.1183419227600098
Validation loss: 2.007078775795557

Epoch: 6| Step: 9
Training loss: 2.265160322189331
Validation loss: 1.981020035282258

Epoch: 6| Step: 10
Training loss: 1.8878016471862793
Validation loss: 2.0030045534974787

Epoch: 6| Step: 11
Training loss: 2.9401519298553467
Validation loss: 1.9921673395300423

Epoch: 6| Step: 12
Training loss: 2.542912721633911
Validation loss: 1.9933371441338652

Epoch: 6| Step: 13
Training loss: 3.041600465774536
Validation loss: 1.9945400837929017

Epoch: 60| Step: 0
Training loss: 2.0791115760803223
Validation loss: 2.0116913190452

Epoch: 6| Step: 1
Training loss: 2.107926607131958
Validation loss: 2.0139414636037682

Epoch: 6| Step: 2
Training loss: 2.480518341064453
Validation loss: 1.9782325849738172

Epoch: 6| Step: 3
Training loss: 2.3424973487854004
Validation loss: 1.983648398871063

Epoch: 6| Step: 4
Training loss: 2.8588929176330566
Validation loss: 1.9966514559202297

Epoch: 6| Step: 5
Training loss: 2.0394644737243652
Validation loss: 1.9961078474598546

Epoch: 6| Step: 6
Training loss: 2.3551316261291504
Validation loss: 1.991024382652775

Epoch: 6| Step: 7
Training loss: 2.3512256145477295
Validation loss: 1.98277618551767

Epoch: 6| Step: 8
Training loss: 1.8430333137512207
Validation loss: 2.0146483067543275

Epoch: 6| Step: 9
Training loss: 1.9303613901138306
Validation loss: 2.027953892625788

Epoch: 6| Step: 10
Training loss: 2.2667458057403564
Validation loss: 2.014485652728747

Epoch: 6| Step: 11
Training loss: 2.324073553085327
Validation loss: 2.0131802635808147

Epoch: 6| Step: 12
Training loss: 2.5281553268432617
Validation loss: 2.0341025142259497

Epoch: 6| Step: 13
Training loss: 2.7699403762817383
Validation loss: 2.0055819070467384

Epoch: 61| Step: 0
Training loss: 2.1178650856018066
Validation loss: 2.033607639292235

Epoch: 6| Step: 1
Training loss: 2.502041816711426
Validation loss: 2.0189864738013155

Epoch: 6| Step: 2
Training loss: 1.9450430870056152
Validation loss: 1.9938801155295423

Epoch: 6| Step: 3
Training loss: 2.225951910018921
Validation loss: 2.00948642787113

Epoch: 6| Step: 4
Training loss: 2.143606185913086
Validation loss: 2.011169106729569

Epoch: 6| Step: 5
Training loss: 2.966599941253662
Validation loss: 2.016033418716923

Epoch: 6| Step: 6
Training loss: 2.9653615951538086
Validation loss: 2.017746945863129

Epoch: 6| Step: 7
Training loss: 1.8758068084716797
Validation loss: 1.9709687130425566

Epoch: 6| Step: 8
Training loss: 1.9680811166763306
Validation loss: 2.030195105460382

Epoch: 6| Step: 9
Training loss: 2.4062681198120117
Validation loss: 2.0264968192705544

Epoch: 6| Step: 10
Training loss: 1.586425542831421
Validation loss: 2.006903922686013

Epoch: 6| Step: 11
Training loss: 2.434574842453003
Validation loss: 2.0343045393625894

Epoch: 6| Step: 12
Training loss: 2.5383992195129395
Validation loss: 2.0067629942329983

Epoch: 6| Step: 13
Training loss: 2.3374199867248535
Validation loss: 2.0036337093640397

Epoch: 62| Step: 0
Training loss: 2.039638042449951
Validation loss: 2.0157361312579085

Epoch: 6| Step: 1
Training loss: 1.9668086767196655
Validation loss: 2.0256692901734383

Epoch: 6| Step: 2
Training loss: 2.913315534591675
Validation loss: 2.015889959950601

Epoch: 6| Step: 3
Training loss: 2.079529285430908
Validation loss: 1.9833148423061575

Epoch: 6| Step: 4
Training loss: 2.1548104286193848
Validation loss: 2.0176735578044767

Epoch: 6| Step: 5
Training loss: 1.9448328018188477
Validation loss: 2.0203492000538814

Epoch: 6| Step: 6
Training loss: 2.0177838802337646
Validation loss: 2.031478097361903

Epoch: 6| Step: 7
Training loss: 1.8176144361495972
Validation loss: 2.0000478541979225

Epoch: 6| Step: 8
Training loss: 2.3460052013397217
Validation loss: 2.019753251024472

Epoch: 6| Step: 9
Training loss: 3.593742847442627
Validation loss: 1.9957727796287947

Epoch: 6| Step: 10
Training loss: 2.1087682247161865
Validation loss: 1.991760815343549

Epoch: 6| Step: 11
Training loss: 2.7383298873901367
Validation loss: 2.0227171092905025

Epoch: 6| Step: 12
Training loss: 2.000575542449951
Validation loss: 2.005093856524396

Epoch: 6| Step: 13
Training loss: 2.1913676261901855
Validation loss: 2.0186066922321113

Epoch: 63| Step: 0
Training loss: 1.3573856353759766
Validation loss: 1.9851442024271975

Epoch: 6| Step: 1
Training loss: 2.5222322940826416
Validation loss: 2.0199965623117264

Epoch: 6| Step: 2
Training loss: 2.163118839263916
Validation loss: 1.9438672399008146

Epoch: 6| Step: 3
Training loss: 2.47548770904541
Validation loss: 1.9924314893702024

Epoch: 6| Step: 4
Training loss: 2.5800464153289795
Validation loss: 2.0352632204691568

Epoch: 6| Step: 5
Training loss: 2.678581476211548
Validation loss: 2.013700951812088

Epoch: 6| Step: 6
Training loss: 2.5931506156921387
Validation loss: 2.0171153083924325

Epoch: 6| Step: 7
Training loss: 1.978905200958252
Validation loss: 2.0256657946494316

Epoch: 6| Step: 8
Training loss: 1.8104772567749023
Validation loss: 2.0290933860245572

Epoch: 6| Step: 9
Training loss: 1.7892932891845703
Validation loss: 1.9751286827107912

Epoch: 6| Step: 10
Training loss: 2.2019340991973877
Validation loss: 1.9979125427943405

Epoch: 6| Step: 11
Training loss: 2.7920875549316406
Validation loss: 2.0220497256966046

Epoch: 6| Step: 12
Training loss: 2.2973082065582275
Validation loss: 2.02882448832194

Epoch: 6| Step: 13
Training loss: 3.1196517944335938
Validation loss: 1.9872050028975292

Epoch: 64| Step: 0
Training loss: 1.8865797519683838
Validation loss: 2.027070974790922

Epoch: 6| Step: 1
Training loss: 2.0451438426971436
Validation loss: 2.0220911118292038

Epoch: 6| Step: 2
Training loss: 2.415086269378662
Validation loss: 2.0524391769081034

Epoch: 6| Step: 3
Training loss: 2.3881750106811523
Validation loss: 2.0332254350826306

Epoch: 6| Step: 4
Training loss: 1.8889356851577759
Validation loss: 2.0120468062739216

Epoch: 6| Step: 5
Training loss: 1.9387147426605225
Validation loss: 2.035739039862028

Epoch: 6| Step: 6
Training loss: 3.3824400901794434
Validation loss: 2.0228632906431794

Epoch: 6| Step: 7
Training loss: 2.475874423980713
Validation loss: 2.0492821149928595

Epoch: 6| Step: 8
Training loss: 1.7506338357925415
Validation loss: 2.0499690399375012

Epoch: 6| Step: 9
Training loss: 2.1239500045776367
Validation loss: 2.0343520000416744

Epoch: 6| Step: 10
Training loss: 2.2412524223327637
Validation loss: 2.051775675947948

Epoch: 6| Step: 11
Training loss: 2.7534217834472656
Validation loss: 2.056380482130153

Epoch: 6| Step: 12
Training loss: 2.0679187774658203
Validation loss: 2.033967307818833

Epoch: 6| Step: 13
Training loss: 2.4385411739349365
Validation loss: 2.0540273215181086

Epoch: 65| Step: 0
Training loss: 2.468769073486328
Validation loss: 2.07186452034981

Epoch: 6| Step: 1
Training loss: 2.8236780166625977
Validation loss: 2.025637762520903

Epoch: 6| Step: 2
Training loss: 1.7549989223480225
Validation loss: 2.0269944001269597

Epoch: 6| Step: 3
Training loss: 2.192162036895752
Validation loss: 2.0335602042495564

Epoch: 6| Step: 4
Training loss: 1.7620129585266113
Validation loss: 2.0310936999577347

Epoch: 6| Step: 5
Training loss: 2.3357176780700684
Validation loss: 2.0412298684479087

Epoch: 6| Step: 6
Training loss: 1.7539829015731812
Validation loss: 2.0417100088570708

Epoch: 6| Step: 7
Training loss: 2.4670395851135254
Validation loss: 2.0489193136974047

Epoch: 6| Step: 8
Training loss: 2.4598822593688965
Validation loss: 2.0104701096011746

Epoch: 6| Step: 9
Training loss: 3.0008459091186523
Validation loss: 2.0000545876000517

Epoch: 6| Step: 10
Training loss: 2.0001296997070312
Validation loss: 2.0218296192025624

Epoch: 6| Step: 11
Training loss: 1.852180004119873
Validation loss: 2.014143837395535

Epoch: 6| Step: 12
Training loss: 2.893747329711914
Validation loss: 2.0092393172684537

Epoch: 6| Step: 13
Training loss: 2.4829001426696777
Validation loss: 1.9899598552334694

Epoch: 66| Step: 0
Training loss: 2.07767391204834
Validation loss: 1.9735482174863097

Epoch: 6| Step: 1
Training loss: 2.131237745285034
Validation loss: 2.044402135315762

Epoch: 6| Step: 2
Training loss: 2.0849063396453857
Validation loss: 2.003504801821965

Epoch: 6| Step: 3
Training loss: 1.8059594631195068
Validation loss: 1.9963486027973953

Epoch: 6| Step: 4
Training loss: 2.7688851356506348
Validation loss: 2.027759285383327

Epoch: 6| Step: 5
Training loss: 2.5162241458892822
Validation loss: 1.9870246302696966

Epoch: 6| Step: 6
Training loss: 1.6982284784317017
Validation loss: 2.044125859455396

Epoch: 6| Step: 7
Training loss: 2.1913528442382812
Validation loss: 1.9994416967515023

Epoch: 6| Step: 8
Training loss: 2.937561511993408
Validation loss: 2.03494042863128

Epoch: 6| Step: 9
Training loss: 2.284304618835449
Validation loss: 1.9870467698702248

Epoch: 6| Step: 10
Training loss: 2.411259174346924
Validation loss: 2.0094974271712767

Epoch: 6| Step: 11
Training loss: 2.86042857170105
Validation loss: 1.9836007677098757

Epoch: 6| Step: 12
Training loss: 1.8027613162994385
Validation loss: 2.0149110324921145

Epoch: 6| Step: 13
Training loss: 1.9811121225357056
Validation loss: 2.008938668876566

Epoch: 67| Step: 0
Training loss: 2.3261985778808594
Validation loss: 2.0452746601514917

Epoch: 6| Step: 1
Training loss: 2.878692865371704
Validation loss: 2.001107733736756

Epoch: 6| Step: 2
Training loss: 2.328495979309082
Validation loss: 2.025917919733191

Epoch: 6| Step: 3
Training loss: 2.283714771270752
Validation loss: 2.0296540132132908

Epoch: 6| Step: 4
Training loss: 2.1283178329467773
Validation loss: 1.9925254660267984

Epoch: 6| Step: 5
Training loss: 2.745993137359619
Validation loss: 1.9859537027215446

Epoch: 6| Step: 6
Training loss: 1.6735109090805054
Validation loss: 2.015445158045779

Epoch: 6| Step: 7
Training loss: 1.883678913116455
Validation loss: 2.0321400293739895

Epoch: 6| Step: 8
Training loss: 2.343931198120117
Validation loss: 2.009511637431319

Epoch: 6| Step: 9
Training loss: 2.6328978538513184
Validation loss: 2.0016372203826904

Epoch: 6| Step: 10
Training loss: 2.758401393890381
Validation loss: 2.0509065658815446

Epoch: 6| Step: 11
Training loss: 1.7197262048721313
Validation loss: 2.0291491823811687

Epoch: 6| Step: 12
Training loss: 2.327857494354248
Validation loss: 2.0385780513927503

Epoch: 6| Step: 13
Training loss: 1.5189274549484253
Validation loss: 2.037107966279471

Epoch: 68| Step: 0
Training loss: 1.9445526599884033
Validation loss: 2.0357343201996176

Epoch: 6| Step: 1
Training loss: 2.1208808422088623
Validation loss: 2.0294556258827128

Epoch: 6| Step: 2
Training loss: 2.3381338119506836
Validation loss: 2.0195180049506565

Epoch: 6| Step: 3
Training loss: 2.6555628776550293
Validation loss: 2.0438337646504885

Epoch: 6| Step: 4
Training loss: 2.113147735595703
Validation loss: 2.062508770214614

Epoch: 6| Step: 5
Training loss: 2.935507297515869
Validation loss: 2.0421217795341247

Epoch: 6| Step: 6
Training loss: 2.6115574836730957
Validation loss: 2.0684939481878795

Epoch: 6| Step: 7
Training loss: 2.095198154449463
Validation loss: 2.053129784522518

Epoch: 6| Step: 8
Training loss: 2.1438639163970947
Validation loss: 2.045382768877091

Epoch: 6| Step: 9
Training loss: 1.9112563133239746
Validation loss: 2.045565720527403

Epoch: 6| Step: 10
Training loss: 2.196746826171875
Validation loss: 2.0350250287722518

Epoch: 6| Step: 11
Training loss: 2.090045928955078
Validation loss: 2.041764577229818

Epoch: 6| Step: 12
Training loss: 2.281243324279785
Validation loss: 2.0285670411202217

Epoch: 6| Step: 13
Training loss: 2.5198869705200195
Validation loss: 2.057449339538492

Epoch: 69| Step: 0
Training loss: 2.4112629890441895
Validation loss: 2.0184823338703444

Epoch: 6| Step: 1
Training loss: 1.7215781211853027
Validation loss: 2.033449661347174

Epoch: 6| Step: 2
Training loss: 2.4786624908447266
Validation loss: 2.04931701126919

Epoch: 6| Step: 3
Training loss: 2.4528605937957764
Validation loss: 2.0097639022334928

Epoch: 6| Step: 4
Training loss: 1.4537901878356934
Validation loss: 2.0300207714880667

Epoch: 6| Step: 5
Training loss: 2.4440925121307373
Validation loss: 2.0342637390218754

Epoch: 6| Step: 6
Training loss: 2.2681827545166016
Validation loss: 2.050225119436941

Epoch: 6| Step: 7
Training loss: 2.6536989212036133
Validation loss: 2.030829992345584

Epoch: 6| Step: 8
Training loss: 2.9339065551757812
Validation loss: 2.0593235313251452

Epoch: 6| Step: 9
Training loss: 2.5132884979248047
Validation loss: 2.0142630774487733

Epoch: 6| Step: 10
Training loss: 1.3630499839782715
Validation loss: 2.0294283384917886

Epoch: 6| Step: 11
Training loss: 2.557331085205078
Validation loss: 2.0257249109206663

Epoch: 6| Step: 12
Training loss: 2.3029980659484863
Validation loss: 2.008717388235113

Epoch: 6| Step: 13
Training loss: 2.3660614490509033
Validation loss: 2.0086875551490375

Epoch: 70| Step: 0
Training loss: 2.2274057865142822
Validation loss: 2.016875687465873

Epoch: 6| Step: 1
Training loss: 1.9064502716064453
Validation loss: 2.0374229313224874

Epoch: 6| Step: 2
Training loss: 2.2732365131378174
Validation loss: 2.0047761368495163

Epoch: 6| Step: 3
Training loss: 2.232510805130005
Validation loss: 2.009800826349566

Epoch: 6| Step: 4
Training loss: 2.325897216796875
Validation loss: 2.05577278009025

Epoch: 6| Step: 5
Training loss: 3.238525390625
Validation loss: 2.053620469185614

Epoch: 6| Step: 6
Training loss: 2.5645503997802734
Validation loss: 2.0575439263415594

Epoch: 6| Step: 7
Training loss: 2.3528757095336914
Validation loss: 2.0400083206033193

Epoch: 6| Step: 8
Training loss: 1.6930787563323975
Validation loss: 2.04994660423648

Epoch: 6| Step: 9
Training loss: 1.803879737854004
Validation loss: 2.0475501552704842

Epoch: 6| Step: 10
Training loss: 2.4592649936676025
Validation loss: 2.051114530973537

Epoch: 6| Step: 11
Training loss: 2.492231607437134
Validation loss: 2.0367988437734623

Epoch: 6| Step: 12
Training loss: 2.1206600666046143
Validation loss: 2.0210676039418867

Epoch: 6| Step: 13
Training loss: 1.7406573295593262
Validation loss: 2.030041499804425

Epoch: 71| Step: 0
Training loss: 1.4223191738128662
Validation loss: 2.0042925162981917

Epoch: 6| Step: 1
Training loss: 2.087331533432007
Validation loss: 2.017459031074278

Epoch: 6| Step: 2
Training loss: 2.044792413711548
Validation loss: 2.006488142475005

Epoch: 6| Step: 3
Training loss: 2.288674831390381
Validation loss: 2.035800057072793

Epoch: 6| Step: 4
Training loss: 1.8271172046661377
Validation loss: 2.0261143792060112

Epoch: 6| Step: 5
Training loss: 2.44000244140625
Validation loss: 2.010440109878458

Epoch: 6| Step: 6
Training loss: 2.27603816986084
Validation loss: 2.0102611254620295

Epoch: 6| Step: 7
Training loss: 2.5853519439697266
Validation loss: 2.0133198781680037

Epoch: 6| Step: 8
Training loss: 2.235238552093506
Validation loss: 2.025984582080636

Epoch: 6| Step: 9
Training loss: 2.5642194747924805
Validation loss: 2.029073743410008

Epoch: 6| Step: 10
Training loss: 2.000603675842285
Validation loss: 2.006764437562676

Epoch: 6| Step: 11
Training loss: 2.973592519760132
Validation loss: 2.0278351794007006

Epoch: 6| Step: 12
Training loss: 2.9018454551696777
Validation loss: 2.022242849872958

Epoch: 6| Step: 13
Training loss: 1.989735722541809
Validation loss: 2.017248644623705

Epoch: 72| Step: 0
Training loss: 1.8266571760177612
Validation loss: 2.025686352483688

Epoch: 6| Step: 1
Training loss: 1.3910961151123047
Validation loss: 2.052005242275935

Epoch: 6| Step: 2
Training loss: 2.1216483116149902
Validation loss: 1.9979487670365201

Epoch: 6| Step: 3
Training loss: 1.8755565881729126
Validation loss: 2.0366666137531237

Epoch: 6| Step: 4
Training loss: 3.3622469902038574
Validation loss: 2.0406003728989632

Epoch: 6| Step: 5
Training loss: 2.339763641357422
Validation loss: 2.0100093618516

Epoch: 6| Step: 6
Training loss: 2.7881581783294678
Validation loss: 2.017174852791653

Epoch: 6| Step: 7
Training loss: 2.593629837036133
Validation loss: 2.048753628166773

Epoch: 6| Step: 8
Training loss: 2.081719160079956
Validation loss: 2.0136034181041103

Epoch: 6| Step: 9
Training loss: 1.688520908355713
Validation loss: 2.0293289871626

Epoch: 6| Step: 10
Training loss: 2.4907805919647217
Validation loss: 2.030839452179529

Epoch: 6| Step: 11
Training loss: 3.109081745147705
Validation loss: 2.0168021571251655

Epoch: 6| Step: 12
Training loss: 2.0492918491363525
Validation loss: 2.01475199063619

Epoch: 6| Step: 13
Training loss: 1.2714810371398926
Validation loss: 2.0155742373517764

Epoch: 73| Step: 0
Training loss: 2.759382486343384
Validation loss: 1.995170126679123

Epoch: 6| Step: 1
Training loss: 1.6242034435272217
Validation loss: 2.019804172618415

Epoch: 6| Step: 2
Training loss: 2.1177210807800293
Validation loss: 2.005197755752071

Epoch: 6| Step: 3
Training loss: 2.848281145095825
Validation loss: 2.017369783052834

Epoch: 6| Step: 4
Training loss: 1.9986522197723389
Validation loss: 2.0278167993791643

Epoch: 6| Step: 5
Training loss: 2.3837356567382812
Validation loss: 2.0187673004724647

Epoch: 6| Step: 6
Training loss: 2.290072441101074
Validation loss: 2.002162802603937

Epoch: 6| Step: 7
Training loss: 2.0478355884552
Validation loss: 1.9826585631216727

Epoch: 6| Step: 8
Training loss: 1.9498718976974487
Validation loss: 1.992304580186003

Epoch: 6| Step: 9
Training loss: 2.4817285537719727
Validation loss: 1.9988337178384104

Epoch: 6| Step: 10
Training loss: 2.0855865478515625
Validation loss: 1.9757303909588886

Epoch: 6| Step: 11
Training loss: 2.9669532775878906
Validation loss: 1.9911007547891268

Epoch: 6| Step: 12
Training loss: 1.800674319267273
Validation loss: 2.0001830798323437

Epoch: 6| Step: 13
Training loss: 2.040469169616699
Validation loss: 2.0031265481825797

Epoch: 74| Step: 0
Training loss: 1.8694407939910889
Validation loss: 1.9985336257565407

Epoch: 6| Step: 1
Training loss: 2.9932072162628174
Validation loss: 1.9946882314579462

Epoch: 6| Step: 2
Training loss: 1.1985259056091309
Validation loss: 2.0178440270885343

Epoch: 6| Step: 3
Training loss: 3.001730442047119
Validation loss: 2.0130541042615007

Epoch: 6| Step: 4
Training loss: 2.6587114334106445
Validation loss: 1.995060561805643

Epoch: 6| Step: 5
Training loss: 2.337509870529175
Validation loss: 2.0045714378356934

Epoch: 6| Step: 6
Training loss: 2.0326318740844727
Validation loss: 2.0008883732621388

Epoch: 6| Step: 7
Training loss: 2.392338275909424
Validation loss: 1.99871354205634

Epoch: 6| Step: 8
Training loss: 2.1139845848083496
Validation loss: 1.9905381946153538

Epoch: 6| Step: 9
Training loss: 2.2689433097839355
Validation loss: 2.0144629132363105

Epoch: 6| Step: 10
Training loss: 2.140052318572998
Validation loss: 2.0092768207673104

Epoch: 6| Step: 11
Training loss: 1.7784172296524048
Validation loss: 2.0179614418296405

Epoch: 6| Step: 12
Training loss: 2.343689441680908
Validation loss: 2.0118094849330124

Epoch: 6| Step: 13
Training loss: 2.1929397583007812
Validation loss: 2.0045258255415064

Epoch: 75| Step: 0
Training loss: 1.8840551376342773
Validation loss: 1.9968467886729906

Epoch: 6| Step: 1
Training loss: 2.355257272720337
Validation loss: 2.017483297214713

Epoch: 6| Step: 2
Training loss: 2.344207763671875
Validation loss: 2.0266528321850683

Epoch: 6| Step: 3
Training loss: 2.4519872665405273
Validation loss: 2.007509869913901

Epoch: 6| Step: 4
Training loss: 2.1881041526794434
Validation loss: 2.009111376218898

Epoch: 6| Step: 5
Training loss: 2.202453136444092
Validation loss: 2.0236947562104914

Epoch: 6| Step: 6
Training loss: 2.5504226684570312
Validation loss: 2.03061556559737

Epoch: 6| Step: 7
Training loss: 2.825801372528076
Validation loss: 2.0118075519479732

Epoch: 6| Step: 8
Training loss: 2.307114601135254
Validation loss: 2.0038201014200845

Epoch: 6| Step: 9
Training loss: 2.056910514831543
Validation loss: 2.0035735407183246

Epoch: 6| Step: 10
Training loss: 2.030313491821289
Validation loss: 2.004700829905848

Epoch: 6| Step: 11
Training loss: 2.1325302124023438
Validation loss: 1.995389582008444

Epoch: 6| Step: 12
Training loss: 1.9100275039672852
Validation loss: 1.9957983621986963

Epoch: 6| Step: 13
Training loss: 2.1051368713378906
Validation loss: 2.0134300467788533

Epoch: 76| Step: 0
Training loss: 1.968586802482605
Validation loss: 2.0028444464488695

Epoch: 6| Step: 1
Training loss: 2.168206214904785
Validation loss: 2.0235015576885593

Epoch: 6| Step: 2
Training loss: 2.499809980392456
Validation loss: 2.0211286314072145

Epoch: 6| Step: 3
Training loss: 2.322753667831421
Validation loss: 2.032110036060374

Epoch: 6| Step: 4
Training loss: 2.52925181388855
Validation loss: 2.0708395870782996

Epoch: 6| Step: 5
Training loss: 1.892728328704834
Validation loss: 2.0424145780583864

Epoch: 6| Step: 6
Training loss: 2.3343660831451416
Validation loss: 2.039800663148203

Epoch: 6| Step: 7
Training loss: 2.8453385829925537
Validation loss: 2.04512720723306

Epoch: 6| Step: 8
Training loss: 2.0481488704681396
Validation loss: 2.031304713218443

Epoch: 6| Step: 9
Training loss: 2.1473636627197266
Validation loss: 2.0480709563019457

Epoch: 6| Step: 10
Training loss: 2.8968958854675293
Validation loss: 2.0292854565446095

Epoch: 6| Step: 11
Training loss: 2.186574935913086
Validation loss: 2.060617099526108

Epoch: 6| Step: 12
Training loss: 2.3620433807373047
Validation loss: 2.031131693111953

Epoch: 6| Step: 13
Training loss: 0.659575879573822
Validation loss: 2.0495445741120206

Epoch: 77| Step: 0
Training loss: 1.9971256256103516
Validation loss: 2.0283659799124605

Epoch: 6| Step: 1
Training loss: 2.2658543586730957
Validation loss: 2.0431730490858837

Epoch: 6| Step: 2
Training loss: 2.7981977462768555
Validation loss: 2.05220377573403

Epoch: 6| Step: 3
Training loss: 1.5437772274017334
Validation loss: 2.0507104589093115

Epoch: 6| Step: 4
Training loss: 2.36567759513855
Validation loss: 2.018901473732405

Epoch: 6| Step: 5
Training loss: 1.9803589582443237
Validation loss: 2.0428680783958844

Epoch: 6| Step: 6
Training loss: 2.7107672691345215
Validation loss: 2.0355578571237545

Epoch: 6| Step: 7
Training loss: 2.2831850051879883
Validation loss: 2.0339576070026686

Epoch: 6| Step: 8
Training loss: 2.4967920780181885
Validation loss: 2.044852092701902

Epoch: 6| Step: 9
Training loss: 2.484572410583496
Validation loss: 2.030615629688386

Epoch: 6| Step: 10
Training loss: 2.7571959495544434
Validation loss: 2.009214865264072

Epoch: 6| Step: 11
Training loss: 2.0393803119659424
Validation loss: 2.014906624312042

Epoch: 6| Step: 12
Training loss: 1.8837908506393433
Validation loss: 2.017252891294418

Epoch: 6| Step: 13
Training loss: 1.4362059831619263
Validation loss: 2.0230862004782564

Epoch: 78| Step: 0
Training loss: 2.148805618286133
Validation loss: 2.002223789051015

Epoch: 6| Step: 1
Training loss: 2.596705913543701
Validation loss: 2.0004222867309407

Epoch: 6| Step: 2
Training loss: 1.855105996131897
Validation loss: 2.0139027641665552

Epoch: 6| Step: 3
Training loss: 2.065232992172241
Validation loss: 2.0016980696749944

Epoch: 6| Step: 4
Training loss: 2.0004496574401855
Validation loss: 1.9975841429925734

Epoch: 6| Step: 5
Training loss: 3.1501574516296387
Validation loss: 2.017524705138258

Epoch: 6| Step: 6
Training loss: 2.2490200996398926
Validation loss: 2.0525113254465084

Epoch: 6| Step: 7
Training loss: 2.4734151363372803
Validation loss: 2.0457867037865425

Epoch: 6| Step: 8
Training loss: 1.977250099182129
Validation loss: 2.020370941008291

Epoch: 6| Step: 9
Training loss: 2.7568113803863525
Validation loss: 2.083820059735288

Epoch: 6| Step: 10
Training loss: 1.932943344116211
Validation loss: 2.0595856763983287

Epoch: 6| Step: 11
Training loss: 1.3308601379394531
Validation loss: 2.0405786998810305

Epoch: 6| Step: 12
Training loss: 2.1434788703918457
Validation loss: 2.0450400460150933

Epoch: 6| Step: 13
Training loss: 2.90877366065979
Validation loss: 2.054268503701815

Epoch: 79| Step: 0
Training loss: 1.9891084432601929
Validation loss: 2.039217791249675

Epoch: 6| Step: 1
Training loss: 1.8464992046356201
Validation loss: 2.024492625267275

Epoch: 6| Step: 2
Training loss: 2.289088487625122
Validation loss: 2.0359575953534854

Epoch: 6| Step: 3
Training loss: 2.6302883625030518
Validation loss: 2.030414758190032

Epoch: 6| Step: 4
Training loss: 1.9183087348937988
Validation loss: 2.0174395666327527

Epoch: 6| Step: 5
Training loss: 1.9494909048080444
Validation loss: 2.0005405487552768

Epoch: 6| Step: 6
Training loss: 2.9245922565460205
Validation loss: 2.0252060364651423

Epoch: 6| Step: 7
Training loss: 2.8944272994995117
Validation loss: 2.0036829799734135

Epoch: 6| Step: 8
Training loss: 2.1805901527404785
Validation loss: 2.030826735240157

Epoch: 6| Step: 9
Training loss: 2.374359130859375
Validation loss: 2.014670430973012

Epoch: 6| Step: 10
Training loss: 1.8481141328811646
Validation loss: 2.016333131379979

Epoch: 6| Step: 11
Training loss: 1.4536311626434326
Validation loss: 2.00585009718454

Epoch: 6| Step: 12
Training loss: 2.224341869354248
Validation loss: 1.9924589164795414

Epoch: 6| Step: 13
Training loss: 3.0513370037078857
Validation loss: 1.9892323760576145

Epoch: 80| Step: 0
Training loss: 2.360267162322998
Validation loss: 2.0106409877859135

Epoch: 6| Step: 1
Training loss: 1.9083667993545532
Validation loss: 2.0044479241935154

Epoch: 6| Step: 2
Training loss: 3.03338623046875
Validation loss: 2.011100379369592

Epoch: 6| Step: 3
Training loss: 2.377077579498291
Validation loss: 2.0087419709851666

Epoch: 6| Step: 4
Training loss: 2.7940549850463867
Validation loss: 2.01991863917279

Epoch: 6| Step: 5
Training loss: 2.0542821884155273
Validation loss: 1.9849970392001572

Epoch: 6| Step: 6
Training loss: 2.348127841949463
Validation loss: 2.0019171750673683

Epoch: 6| Step: 7
Training loss: 2.4672436714172363
Validation loss: 2.003583941408383

Epoch: 6| Step: 8
Training loss: 1.985960841178894
Validation loss: 1.9963965480045607

Epoch: 6| Step: 9
Training loss: 1.8961598873138428
Validation loss: 2.0009862863889305

Epoch: 6| Step: 10
Training loss: 2.40251088142395
Validation loss: 2.0060885901092202

Epoch: 6| Step: 11
Training loss: 1.9520914554595947
Validation loss: 2.0175762586696173

Epoch: 6| Step: 12
Training loss: 1.6268419027328491
Validation loss: 2.0326876935138496

Epoch: 6| Step: 13
Training loss: 2.124943494796753
Validation loss: 1.9646905801629508

Epoch: 81| Step: 0
Training loss: 2.6794233322143555
Validation loss: 2.0148832105821177

Epoch: 6| Step: 1
Training loss: 2.81718373298645
Validation loss: 2.003753446763562

Epoch: 6| Step: 2
Training loss: 2.316493511199951
Validation loss: 2.017957741214383

Epoch: 6| Step: 3
Training loss: 2.657508134841919
Validation loss: 1.9988868915906517

Epoch: 6| Step: 4
Training loss: 2.3020479679107666
Validation loss: 1.9852015690136982

Epoch: 6| Step: 5
Training loss: 2.013298749923706
Validation loss: 2.006611167743642

Epoch: 6| Step: 6
Training loss: 2.039473533630371
Validation loss: 2.005981622203704

Epoch: 6| Step: 7
Training loss: 1.4325144290924072
Validation loss: 2.034840269755292

Epoch: 6| Step: 8
Training loss: 2.026928663253784
Validation loss: 2.0132989421967538

Epoch: 6| Step: 9
Training loss: 1.9877362251281738
Validation loss: 2.0209899961307483

Epoch: 6| Step: 10
Training loss: 2.445826768875122
Validation loss: 2.028531183478653

Epoch: 6| Step: 11
Training loss: 1.9798377752304077
Validation loss: 1.9992133468709967

Epoch: 6| Step: 12
Training loss: 2.060527801513672
Validation loss: 2.041244101780717

Epoch: 6| Step: 13
Training loss: 2.2913832664489746
Validation loss: 1.9983136974355227

Epoch: 82| Step: 0
Training loss: 2.5096631050109863
Validation loss: 1.9805320552600327

Epoch: 6| Step: 1
Training loss: 1.594399333000183
Validation loss: 2.001668653180522

Epoch: 6| Step: 2
Training loss: 2.2423176765441895
Validation loss: 1.9982508536308043

Epoch: 6| Step: 3
Training loss: 2.3894104957580566
Validation loss: 2.022662124326152

Epoch: 6| Step: 4
Training loss: 1.6684670448303223
Validation loss: 2.0145526829586236

Epoch: 6| Step: 5
Training loss: 2.066640615463257
Validation loss: 2.004851552747911

Epoch: 6| Step: 6
Training loss: 2.6762313842773438
Validation loss: 2.0208010417158886

Epoch: 6| Step: 7
Training loss: 2.5148074626922607
Validation loss: 2.049453877633618

Epoch: 6| Step: 8
Training loss: 1.3872179985046387
Validation loss: 2.03922511172551

Epoch: 6| Step: 9
Training loss: 2.5869789123535156
Validation loss: 2.0091607301465926

Epoch: 6| Step: 10
Training loss: 2.04451847076416
Validation loss: 2.046469452560589

Epoch: 6| Step: 11
Training loss: 2.156198740005493
Validation loss: 2.026142998408246

Epoch: 6| Step: 12
Training loss: 2.4183661937713623
Validation loss: 1.9972790928297146

Epoch: 6| Step: 13
Training loss: 3.135859489440918
Validation loss: 2.010631824052462

Epoch: 83| Step: 0
Training loss: 2.1013784408569336
Validation loss: 2.042825457870319

Epoch: 6| Step: 1
Training loss: 2.4909205436706543
Validation loss: 2.0317130986080376

Epoch: 6| Step: 2
Training loss: 3.135228157043457
Validation loss: 2.015816104027533

Epoch: 6| Step: 3
Training loss: 1.416074275970459
Validation loss: 2.0375514850821546

Epoch: 6| Step: 4
Training loss: 1.8493399620056152
Validation loss: 2.0025245912613405

Epoch: 6| Step: 5
Training loss: 2.41616153717041
Validation loss: 2.023154144646019

Epoch: 6| Step: 6
Training loss: 1.831740379333496
Validation loss: 2.015402550338417

Epoch: 6| Step: 7
Training loss: 2.679187297821045
Validation loss: 2.019632226677351

Epoch: 6| Step: 8
Training loss: 2.6581168174743652
Validation loss: 2.025293460456274

Epoch: 6| Step: 9
Training loss: 2.7118639945983887
Validation loss: 1.994809648042084

Epoch: 6| Step: 10
Training loss: 1.8317174911499023
Validation loss: 1.994596299304757

Epoch: 6| Step: 11
Training loss: 2.124699115753174
Validation loss: 1.9994058967918478

Epoch: 6| Step: 12
Training loss: 1.8479121923446655
Validation loss: 2.0192726812055035

Epoch: 6| Step: 13
Training loss: 1.8353303670883179
Validation loss: 2.0177829739868

Epoch: 84| Step: 0
Training loss: 1.892615795135498
Validation loss: 1.9755507912687076

Epoch: 6| Step: 1
Training loss: 2.2112183570861816
Validation loss: 2.019535090333672

Epoch: 6| Step: 2
Training loss: 2.629885196685791
Validation loss: 1.9879210033724386

Epoch: 6| Step: 3
Training loss: 2.269273519515991
Validation loss: 2.0062239618711573

Epoch: 6| Step: 4
Training loss: 1.7558672428131104
Validation loss: 2.02029933211624

Epoch: 6| Step: 5
Training loss: 1.4133596420288086
Validation loss: 2.022556353640813

Epoch: 6| Step: 6
Training loss: 2.7438204288482666
Validation loss: 2.0355096465797833

Epoch: 6| Step: 7
Training loss: 2.603405475616455
Validation loss: 1.994782381160285

Epoch: 6| Step: 8
Training loss: 1.9502851963043213
Validation loss: 2.0234462163781606

Epoch: 6| Step: 9
Training loss: 2.2271249294281006
Validation loss: 2.0293843874367337

Epoch: 6| Step: 10
Training loss: 2.1683096885681152
Validation loss: 2.0181772196164696

Epoch: 6| Step: 11
Training loss: 2.269857406616211
Validation loss: 2.0179198172784623

Epoch: 6| Step: 12
Training loss: 2.574272632598877
Validation loss: 1.9995650642661638

Epoch: 6| Step: 13
Training loss: 2.4934563636779785
Validation loss: 2.045619385216826

Epoch: 85| Step: 0
Training loss: 2.4017512798309326
Validation loss: 2.0369079138643

Epoch: 6| Step: 1
Training loss: 2.0988478660583496
Validation loss: 2.013664386605704

Epoch: 6| Step: 2
Training loss: 2.21523380279541
Validation loss: 2.023004689524251

Epoch: 6| Step: 3
Training loss: 1.9828577041625977
Validation loss: 2.0139479201327086

Epoch: 6| Step: 4
Training loss: 2.114474296569824
Validation loss: 2.034362926278063

Epoch: 6| Step: 5
Training loss: 2.5822696685791016
Validation loss: 2.035282278573641

Epoch: 6| Step: 6
Training loss: 2.90777850151062
Validation loss: 2.015132850216281

Epoch: 6| Step: 7
Training loss: 1.7774969339370728
Validation loss: 2.016389173846091

Epoch: 6| Step: 8
Training loss: 2.4968888759613037
Validation loss: 2.032077858524938

Epoch: 6| Step: 9
Training loss: 2.111090660095215
Validation loss: 2.0098787264157365

Epoch: 6| Step: 10
Training loss: 2.0101213455200195
Validation loss: 2.049222154002036

Epoch: 6| Step: 11
Training loss: 2.52683687210083
Validation loss: 2.0281693884121474

Epoch: 6| Step: 12
Training loss: 2.3434066772460938
Validation loss: 2.0108607430611887

Epoch: 6| Step: 13
Training loss: 1.3289307355880737
Validation loss: 2.031559269915345

Epoch: 86| Step: 0
Training loss: 2.0886809825897217
Validation loss: 2.0218163741532194

Epoch: 6| Step: 1
Training loss: 2.1463561058044434
Validation loss: 2.026965977043234

Epoch: 6| Step: 2
Training loss: 3.0446600914001465
Validation loss: 2.062052252472088

Epoch: 6| Step: 3
Training loss: 2.373189926147461
Validation loss: 2.0218926873258365

Epoch: 6| Step: 4
Training loss: 1.4309145212173462
Validation loss: 2.0269328086606917

Epoch: 6| Step: 5
Training loss: 1.962179183959961
Validation loss: 2.0212490673988097

Epoch: 6| Step: 6
Training loss: 1.7583656311035156
Validation loss: 2.0479981258351314

Epoch: 6| Step: 7
Training loss: 2.346215009689331
Validation loss: 2.062913538307272

Epoch: 6| Step: 8
Training loss: 2.567059278488159
Validation loss: 2.031621984256211

Epoch: 6| Step: 9
Training loss: 2.150822639465332
Validation loss: 2.0292172765219085

Epoch: 6| Step: 10
Training loss: 1.9272174835205078
Validation loss: 2.0424667686544438

Epoch: 6| Step: 11
Training loss: 2.6163110733032227
Validation loss: 2.0528699095531175

Epoch: 6| Step: 12
Training loss: 2.4382758140563965
Validation loss: 2.037601097937553

Epoch: 6| Step: 13
Training loss: 2.622495174407959
Validation loss: 2.006523289988118

Epoch: 87| Step: 0
Training loss: 2.4999985694885254
Validation loss: 2.028367773179085

Epoch: 6| Step: 1
Training loss: 2.7545318603515625
Validation loss: 2.03882275601869

Epoch: 6| Step: 2
Training loss: 2.103318452835083
Validation loss: 2.0415803617046726

Epoch: 6| Step: 3
Training loss: 2.226527690887451
Validation loss: 2.043967852028467

Epoch: 6| Step: 4
Training loss: 1.8994293212890625
Validation loss: 2.0439731254372546

Epoch: 6| Step: 5
Training loss: 1.7775278091430664
Validation loss: 2.0632926853754188

Epoch: 6| Step: 6
Training loss: 2.4124655723571777
Validation loss: 2.0224412525853803

Epoch: 6| Step: 7
Training loss: 2.987989664077759
Validation loss: 2.0302520669916624

Epoch: 6| Step: 8
Training loss: 2.2911839485168457
Validation loss: 2.013160567129812

Epoch: 6| Step: 9
Training loss: 1.7362533807754517
Validation loss: 2.0367323172989713

Epoch: 6| Step: 10
Training loss: 2.2876453399658203
Validation loss: 2.0126190211183284

Epoch: 6| Step: 11
Training loss: 2.0639140605926514
Validation loss: 2.0155885719483897

Epoch: 6| Step: 12
Training loss: 1.6883249282836914
Validation loss: 2.0629874557577152

Epoch: 6| Step: 13
Training loss: 2.7886946201324463
Validation loss: 2.0412954976481776

Epoch: 88| Step: 0
Training loss: 2.425783157348633
Validation loss: 2.033180079152507

Epoch: 6| Step: 1
Training loss: 1.9755162000656128
Validation loss: 2.075903833553355

Epoch: 6| Step: 2
Training loss: 1.9934484958648682
Validation loss: 2.108688026346186

Epoch: 6| Step: 3
Training loss: 2.203065872192383
Validation loss: 2.044691224252024

Epoch: 6| Step: 4
Training loss: 2.132910966873169
Validation loss: 2.045846187940208

Epoch: 6| Step: 5
Training loss: 2.3859667778015137
Validation loss: 2.057943533825618

Epoch: 6| Step: 6
Training loss: 3.3188791275024414
Validation loss: 2.081089510712572

Epoch: 6| Step: 7
Training loss: 2.795301675796509
Validation loss: 2.0464937917647825

Epoch: 6| Step: 8
Training loss: 2.0608999729156494
Validation loss: 2.0732083884618615

Epoch: 6| Step: 9
Training loss: 2.0434956550598145
Validation loss: 2.0692663628567933

Epoch: 6| Step: 10
Training loss: 1.989350438117981
Validation loss: 2.058721664131329

Epoch: 6| Step: 11
Training loss: 2.27164363861084
Validation loss: 2.0388219151445615

Epoch: 6| Step: 12
Training loss: 1.5068042278289795
Validation loss: 2.0254154769323205

Epoch: 6| Step: 13
Training loss: 2.1986923217773438
Validation loss: 2.0181179379904144

Epoch: 89| Step: 0
Training loss: 2.081134796142578
Validation loss: 2.0609755221233574

Epoch: 6| Step: 1
Training loss: 2.066298007965088
Validation loss: 2.0637134275128766

Epoch: 6| Step: 2
Training loss: 2.216047525405884
Validation loss: 2.0358245295862996

Epoch: 6| Step: 3
Training loss: 2.1442251205444336
Validation loss: 2.0419602471013225

Epoch: 6| Step: 4
Training loss: 2.1698813438415527
Validation loss: 2.0771224678203626

Epoch: 6| Step: 5
Training loss: 2.1975295543670654
Validation loss: 2.0217434385771393

Epoch: 6| Step: 6
Training loss: 1.8562644720077515
Validation loss: 2.0573499818002023

Epoch: 6| Step: 7
Training loss: 1.610874891281128
Validation loss: 2.057041920641417

Epoch: 6| Step: 8
Training loss: 2.4361772537231445
Validation loss: 2.038315212854775

Epoch: 6| Step: 9
Training loss: 2.6631221771240234
Validation loss: 2.0135035489195134

Epoch: 6| Step: 10
Training loss: 2.6714346408843994
Validation loss: 2.0237869883096344

Epoch: 6| Step: 11
Training loss: 2.248159646987915
Validation loss: 2.0417175241695937

Epoch: 6| Step: 12
Training loss: 2.268334150314331
Validation loss: 2.036835930680716

Epoch: 6| Step: 13
Training loss: 2.134777069091797
Validation loss: 2.0878945601883756

Epoch: 90| Step: 0
Training loss: 2.415060043334961
Validation loss: 2.017218739755692

Epoch: 6| Step: 1
Training loss: 1.7617909908294678
Validation loss: 2.0315147933139595

Epoch: 6| Step: 2
Training loss: 2.3377645015716553
Validation loss: 2.035870671272278

Epoch: 6| Step: 3
Training loss: 2.6492748260498047
Validation loss: 2.00904098890161

Epoch: 6| Step: 4
Training loss: 2.233199119567871
Validation loss: 2.043290594572662

Epoch: 6| Step: 5
Training loss: 2.5930185317993164
Validation loss: 2.0144596817672893

Epoch: 6| Step: 6
Training loss: 1.5352177619934082
Validation loss: 2.0158677024226033

Epoch: 6| Step: 7
Training loss: 2.1108169555664062
Validation loss: 2.0289827982584634

Epoch: 6| Step: 8
Training loss: 2.414778470993042
Validation loss: 2.0221064193274385

Epoch: 6| Step: 9
Training loss: 1.9595496654510498
Validation loss: 2.0301784853781424

Epoch: 6| Step: 10
Training loss: 1.6747429370880127
Validation loss: 2.016084632565898

Epoch: 6| Step: 11
Training loss: 2.781290054321289
Validation loss: 2.0282498713462584

Epoch: 6| Step: 12
Training loss: 2.786470413208008
Validation loss: 2.035595969487262

Epoch: 6| Step: 13
Training loss: 1.2111271619796753
Validation loss: 2.010361061301283

Epoch: 91| Step: 0
Training loss: 2.5820960998535156
Validation loss: 2.013955316235942

Epoch: 6| Step: 1
Training loss: 3.1657917499542236
Validation loss: 2.013529331453385

Epoch: 6| Step: 2
Training loss: 1.5016634464263916
Validation loss: 2.01854464315599

Epoch: 6| Step: 3
Training loss: 2.125852346420288
Validation loss: 2.027606938474922

Epoch: 6| Step: 4
Training loss: 2.50264573097229
Validation loss: 2.0285451002018426

Epoch: 6| Step: 5
Training loss: 2.1615755558013916
Validation loss: 2.028866167991392

Epoch: 6| Step: 6
Training loss: 1.7899131774902344
Validation loss: 2.0213508016319683

Epoch: 6| Step: 7
Training loss: 1.8671786785125732
Validation loss: 2.040492602573928

Epoch: 6| Step: 8
Training loss: 2.2177367210388184
Validation loss: 2.00156888141427

Epoch: 6| Step: 9
Training loss: 1.5626405477523804
Validation loss: 1.997261760055378

Epoch: 6| Step: 10
Training loss: 2.4697110652923584
Validation loss: 2.062384766917075

Epoch: 6| Step: 11
Training loss: 2.5541462898254395
Validation loss: 2.0248229144721903

Epoch: 6| Step: 12
Training loss: 2.0185208320617676
Validation loss: 2.050002923575781

Epoch: 6| Step: 13
Training loss: 2.5824034214019775
Validation loss: 2.0345209542141167

Epoch: 92| Step: 0
Training loss: 2.3573966026306152
Validation loss: 2.04209610467316

Epoch: 6| Step: 1
Training loss: 1.9741519689559937
Validation loss: 2.03908960793608

Epoch: 6| Step: 2
Training loss: 2.5991079807281494
Validation loss: 2.020218422335963

Epoch: 6| Step: 3
Training loss: 2.313857078552246
Validation loss: 1.995080622293616

Epoch: 6| Step: 4
Training loss: 2.959925651550293
Validation loss: 2.0065483188116424

Epoch: 6| Step: 5
Training loss: 2.043851852416992
Validation loss: 2.021656500395908

Epoch: 6| Step: 6
Training loss: 2.2229371070861816
Validation loss: 2.009732823218069

Epoch: 6| Step: 7
Training loss: 2.0419726371765137
Validation loss: 2.0210391706035984

Epoch: 6| Step: 8
Training loss: 2.000868797302246
Validation loss: 2.0498885082942184

Epoch: 6| Step: 9
Training loss: 1.9917995929718018
Validation loss: 2.031330080442531

Epoch: 6| Step: 10
Training loss: 2.2511134147644043
Validation loss: 2.0431690856974614

Epoch: 6| Step: 11
Training loss: 1.8974857330322266
Validation loss: 2.015904588084067

Epoch: 6| Step: 12
Training loss: 2.2111282348632812
Validation loss: 2.0202282590250813

Epoch: 6| Step: 13
Training loss: 2.317776679992676
Validation loss: 2.0435831546783447

Epoch: 93| Step: 0
Training loss: 1.8653310537338257
Validation loss: 2.0204439470844884

Epoch: 6| Step: 1
Training loss: 2.154385566711426
Validation loss: 2.018555996238544

Epoch: 6| Step: 2
Training loss: 2.52986741065979
Validation loss: 2.0414113152411675

Epoch: 6| Step: 3
Training loss: 2.4323458671569824
Validation loss: 2.0223767039596394

Epoch: 6| Step: 4
Training loss: 2.160947322845459
Validation loss: 2.023934946265272

Epoch: 6| Step: 5
Training loss: 2.3790690898895264
Validation loss: 1.9977596113758702

Epoch: 6| Step: 6
Training loss: 2.535797595977783
Validation loss: 2.01669507129218

Epoch: 6| Step: 7
Training loss: 2.332688808441162
Validation loss: 2.0145998001098633

Epoch: 6| Step: 8
Training loss: 2.503239631652832
Validation loss: 2.0298899578791794

Epoch: 6| Step: 9
Training loss: 1.754831314086914
Validation loss: 2.0022894977241434

Epoch: 6| Step: 10
Training loss: 2.2573189735412598
Validation loss: 1.985006522106868

Epoch: 6| Step: 11
Training loss: 2.456969738006592
Validation loss: 1.9959898225722774

Epoch: 6| Step: 12
Training loss: 1.7173497676849365
Validation loss: 2.0095556166864212

Epoch: 6| Step: 13
Training loss: 1.7866859436035156
Validation loss: 1.9883328765951178

Epoch: 94| Step: 0
Training loss: 2.199988842010498
Validation loss: 2.0129214179131294

Epoch: 6| Step: 1
Training loss: 1.8246971368789673
Validation loss: 2.0310203606082546

Epoch: 6| Step: 2
Training loss: 2.429234266281128
Validation loss: 2.0158967548801052

Epoch: 6| Step: 3
Training loss: 1.7776062488555908
Validation loss: 2.0318070842373754

Epoch: 6| Step: 4
Training loss: 1.975306749343872
Validation loss: 2.0226443864965953

Epoch: 6| Step: 5
Training loss: 3.0369720458984375
Validation loss: 2.0215650540526195

Epoch: 6| Step: 6
Training loss: 2.345102071762085
Validation loss: 2.047528161797472

Epoch: 6| Step: 7
Training loss: 1.726696491241455
Validation loss: 2.0089840786431425

Epoch: 6| Step: 8
Training loss: 2.880730628967285
Validation loss: 2.0366225755342873

Epoch: 6| Step: 9
Training loss: 1.756778597831726
Validation loss: 2.075400424259965

Epoch: 6| Step: 10
Training loss: 2.124565601348877
Validation loss: 2.0689476946348786

Epoch: 6| Step: 11
Training loss: 2.0003252029418945
Validation loss: 2.0177039638642342

Epoch: 6| Step: 12
Training loss: 2.025312900543213
Validation loss: 2.035600818613524

Epoch: 6| Step: 13
Training loss: 2.9953041076660156
Validation loss: 2.076175688415445

Epoch: 95| Step: 0
Training loss: 2.039184093475342
Validation loss: 2.0504837523224535

Epoch: 6| Step: 1
Training loss: 2.4082603454589844
Validation loss: 2.0566076450450446

Epoch: 6| Step: 2
Training loss: 2.454963207244873
Validation loss: 2.059213807505946

Epoch: 6| Step: 3
Training loss: 1.7766011953353882
Validation loss: 2.034857283356369

Epoch: 6| Step: 4
Training loss: 2.6892518997192383
Validation loss: 2.04163852301977

Epoch: 6| Step: 5
Training loss: 1.8620381355285645
Validation loss: 2.083041839702155

Epoch: 6| Step: 6
Training loss: 2.283642530441284
Validation loss: 2.091297112485414

Epoch: 6| Step: 7
Training loss: 2.537536382675171
Validation loss: 2.075262790085167

Epoch: 6| Step: 8
Training loss: 1.964796781539917
Validation loss: 2.0409831013730777

Epoch: 6| Step: 9
Training loss: 2.5444226264953613
Validation loss: 2.0667830846642934

Epoch: 6| Step: 10
Training loss: 2.3108716011047363
Validation loss: 2.0967207570229807

Epoch: 6| Step: 11
Training loss: 2.622086524963379
Validation loss: 2.077536359910042

Epoch: 6| Step: 12
Training loss: 2.0555810928344727
Validation loss: 2.091356900430495

Epoch: 6| Step: 13
Training loss: 0.9707732796669006
Validation loss: 2.0551087138473347

Epoch: 96| Step: 0
Training loss: 1.9458634853363037
Validation loss: 2.0719188105675483

Epoch: 6| Step: 1
Training loss: 1.8883147239685059
Validation loss: 2.0258245493776057

Epoch: 6| Step: 2
Training loss: 2.8312854766845703
Validation loss: 2.0397747537141204

Epoch: 6| Step: 3
Training loss: 2.488016128540039
Validation loss: 2.0717417937453075

Epoch: 6| Step: 4
Training loss: 3.031468152999878
Validation loss: 2.086042277274593

Epoch: 6| Step: 5
Training loss: 2.2944583892822266
Validation loss: 2.0455613956656507

Epoch: 6| Step: 6
Training loss: 2.0767416954040527
Validation loss: 2.0225618475226947

Epoch: 6| Step: 7
Training loss: 2.272055149078369
Validation loss: 2.0613678040043

Epoch: 6| Step: 8
Training loss: 2.358922004699707
Validation loss: 2.0414213608669978

Epoch: 6| Step: 9
Training loss: 1.6379151344299316
Validation loss: 2.0388085021767566

Epoch: 6| Step: 10
Training loss: 2.3684730529785156
Validation loss: 2.041770406948623

Epoch: 6| Step: 11
Training loss: 1.4838593006134033
Validation loss: 2.0547609906042776

Epoch: 6| Step: 12
Training loss: 2.464371919631958
Validation loss: 2.042665054721217

Epoch: 6| Step: 13
Training loss: 1.5085599422454834
Validation loss: 2.040113365778359

Epoch: 97| Step: 0
Training loss: 2.697537422180176
Validation loss: 2.023790649188462

Epoch: 6| Step: 1
Training loss: 1.9152004718780518
Validation loss: 2.0400061415087793

Epoch: 6| Step: 2
Training loss: 1.6084296703338623
Validation loss: 2.033937051732053

Epoch: 6| Step: 3
Training loss: 2.174419641494751
Validation loss: 2.0241690643372072

Epoch: 6| Step: 4
Training loss: 2.899470806121826
Validation loss: 2.0158161553003455

Epoch: 6| Step: 5
Training loss: 1.6333601474761963
Validation loss: 2.024661805040093

Epoch: 6| Step: 6
Training loss: 1.7790638208389282
Validation loss: 2.0019309905267533

Epoch: 6| Step: 7
Training loss: 2.0630431175231934
Validation loss: 2.0107798678900606

Epoch: 6| Step: 8
Training loss: 2.700265407562256
Validation loss: 2.0177011746232227

Epoch: 6| Step: 9
Training loss: 2.184684991836548
Validation loss: 2.013786974773612

Epoch: 6| Step: 10
Training loss: 2.326620578765869
Validation loss: 2.0193538063315937

Epoch: 6| Step: 11
Training loss: 2.347153902053833
Validation loss: 2.027221025959138

Epoch: 6| Step: 12
Training loss: 2.3345985412597656
Validation loss: 2.0160216464791247

Epoch: 6| Step: 13
Training loss: 2.1078245639801025
Validation loss: 2.0430527476854223

Epoch: 98| Step: 0
Training loss: 1.967771291732788
Validation loss: 2.0185907733055855

Epoch: 6| Step: 1
Training loss: 2.5590720176696777
Validation loss: 2.0101312539910756

Epoch: 6| Step: 2
Training loss: 2.2600555419921875
Validation loss: 2.025069390573809

Epoch: 6| Step: 3
Training loss: 2.0056099891662598
Validation loss: 1.9906811483444706

Epoch: 6| Step: 4
Training loss: 1.7703349590301514
Validation loss: 2.039110904098839

Epoch: 6| Step: 5
Training loss: 2.5388846397399902
Validation loss: 2.0312196259857505

Epoch: 6| Step: 6
Training loss: 2.4448652267456055
Validation loss: 2.009812815214998

Epoch: 6| Step: 7
Training loss: 2.420670986175537
Validation loss: 2.054307499239522

Epoch: 6| Step: 8
Training loss: 2.450843334197998
Validation loss: 2.006891183955695

Epoch: 6| Step: 9
Training loss: 1.6757919788360596
Validation loss: 2.0236728473376204

Epoch: 6| Step: 10
Training loss: 2.084324598312378
Validation loss: 2.009234272023683

Epoch: 6| Step: 11
Training loss: 2.317986249923706
Validation loss: 2.0226440378414687

Epoch: 6| Step: 12
Training loss: 1.6503692865371704
Validation loss: 2.000441987027404

Epoch: 6| Step: 13
Training loss: 3.367025136947632
Validation loss: 1.973647202214887

Epoch: 99| Step: 0
Training loss: 2.766352653503418
Validation loss: 2.01006882421432

Epoch: 6| Step: 1
Training loss: 2.484539031982422
Validation loss: 2.0254949190283336

Epoch: 6| Step: 2
Training loss: 2.4246082305908203
Validation loss: 2.0211192715552544

Epoch: 6| Step: 3
Training loss: 2.09002423286438
Validation loss: 2.027426194119197

Epoch: 6| Step: 4
Training loss: 2.013064384460449
Validation loss: 2.0403370870056974

Epoch: 6| Step: 5
Training loss: 1.9816489219665527
Validation loss: 1.9999811136594383

Epoch: 6| Step: 6
Training loss: 2.3081109523773193
Validation loss: 2.030369863715223

Epoch: 6| Step: 7
Training loss: 1.1170907020568848
Validation loss: 1.9969626344660276

Epoch: 6| Step: 8
Training loss: 2.740873336791992
Validation loss: 1.9936854557324482

Epoch: 6| Step: 9
Training loss: 1.9597543478012085
Validation loss: 2.0116020594873736

Epoch: 6| Step: 10
Training loss: 1.9149140119552612
Validation loss: 2.0061255552435435

Epoch: 6| Step: 11
Training loss: 2.2397079467773438
Validation loss: 2.0186128975242696

Epoch: 6| Step: 12
Training loss: 2.2920520305633545
Validation loss: 2.0216337903853385

Epoch: 6| Step: 13
Training loss: 2.4432289600372314
Validation loss: 1.9901930721857215

Epoch: 100| Step: 0
Training loss: 1.7078557014465332
Validation loss: 2.035987925785844

Epoch: 6| Step: 1
Training loss: 1.9999295473098755
Validation loss: 2.0370371418614543

Epoch: 6| Step: 2
Training loss: 2.5868420600891113
Validation loss: 2.0178876153884397

Epoch: 6| Step: 3
Training loss: 2.404721260070801
Validation loss: 1.9980329723768337

Epoch: 6| Step: 4
Training loss: 2.6817970275878906
Validation loss: 2.015088337723927

Epoch: 6| Step: 5
Training loss: 1.5054081678390503
Validation loss: 2.006761033047912

Epoch: 6| Step: 6
Training loss: 1.9193605184555054
Validation loss: 2.0116601208204865

Epoch: 6| Step: 7
Training loss: 2.3498826026916504
Validation loss: 2.003932386316279

Epoch: 6| Step: 8
Training loss: 3.172563076019287
Validation loss: 2.0261664313654744

Epoch: 6| Step: 9
Training loss: 2.705691337585449
Validation loss: 2.0230214134339364

Epoch: 6| Step: 10
Training loss: 1.3910095691680908
Validation loss: 2.025861827276086

Epoch: 6| Step: 11
Training loss: 2.3853161334991455
Validation loss: 2.035154806670322

Epoch: 6| Step: 12
Training loss: 1.6689791679382324
Validation loss: 2.0662074704324045

Epoch: 6| Step: 13
Training loss: 2.9149911403656006
Validation loss: 2.052743440033287

Epoch: 101| Step: 0
Training loss: 2.6079750061035156
Validation loss: 2.0357532642220937

Epoch: 6| Step: 1
Training loss: 1.6240153312683105
Validation loss: 2.028396564145242

Epoch: 6| Step: 2
Training loss: 1.9397945404052734
Validation loss: 2.021080906673144

Epoch: 6| Step: 3
Training loss: 2.5110206604003906
Validation loss: 2.036930878957113

Epoch: 6| Step: 4
Training loss: 2.3062729835510254
Validation loss: 2.0354487780601747

Epoch: 6| Step: 5
Training loss: 2.6993398666381836
Validation loss: 2.0601859887441

Epoch: 6| Step: 6
Training loss: 2.784804344177246
Validation loss: 2.0509738460663827

Epoch: 6| Step: 7
Training loss: 1.3767952919006348
Validation loss: 2.084552400855608

Epoch: 6| Step: 8
Training loss: 2.0196080207824707
Validation loss: 2.0503084864667667

Epoch: 6| Step: 9
Training loss: 2.1458005905151367
Validation loss: 2.0047565608896236

Epoch: 6| Step: 10
Training loss: 2.5280425548553467
Validation loss: 2.022896215479861

Epoch: 6| Step: 11
Training loss: 1.7700722217559814
Validation loss: 1.9961900634150351

Epoch: 6| Step: 12
Training loss: 2.745836019515991
Validation loss: 2.036408514104864

Epoch: 6| Step: 13
Training loss: 1.2471567392349243
Validation loss: 2.0345493170522873

Epoch: 102| Step: 0
Training loss: 2.6065239906311035
Validation loss: 2.0099315361310075

Epoch: 6| Step: 1
Training loss: 1.9263371229171753
Validation loss: 2.028512079228637

Epoch: 6| Step: 2
Training loss: 2.3788018226623535
Validation loss: 2.0450664451045375

Epoch: 6| Step: 3
Training loss: 2.590956449508667
Validation loss: 2.010699690029185

Epoch: 6| Step: 4
Training loss: 1.9724667072296143
Validation loss: 2.0279631332684587

Epoch: 6| Step: 5
Training loss: 1.9167360067367554
Validation loss: 2.015159365951374

Epoch: 6| Step: 6
Training loss: 2.357874870300293
Validation loss: 2.024039977340288

Epoch: 6| Step: 7
Training loss: 2.021038293838501
Validation loss: 2.0395952988696355

Epoch: 6| Step: 8
Training loss: 2.4759302139282227
Validation loss: 2.0368519277982813

Epoch: 6| Step: 9
Training loss: 1.9771136045455933
Validation loss: 2.0198388817489787

Epoch: 6| Step: 10
Training loss: 1.764562964439392
Validation loss: 2.014303443252399

Epoch: 6| Step: 11
Training loss: 2.2473063468933105
Validation loss: 1.9749610180495887

Epoch: 6| Step: 12
Training loss: 2.254322052001953
Validation loss: 2.030886955158685

Epoch: 6| Step: 13
Training loss: 2.2926857471466064
Validation loss: 2.014171051722701

Epoch: 103| Step: 0
Training loss: 2.09372878074646
Validation loss: 2.0368667597411783

Epoch: 6| Step: 1
Training loss: 1.8454937934875488
Validation loss: 2.0163752378955966

Epoch: 6| Step: 2
Training loss: 2.521282196044922
Validation loss: 2.0194087618140766

Epoch: 6| Step: 3
Training loss: 2.0433387756347656
Validation loss: 2.0476601072537

Epoch: 6| Step: 4
Training loss: 2.7494044303894043
Validation loss: 2.008788399798896

Epoch: 6| Step: 5
Training loss: 1.9539247751235962
Validation loss: 2.010651929404146

Epoch: 6| Step: 6
Training loss: 2.5097527503967285
Validation loss: 2.0207506072136665

Epoch: 6| Step: 7
Training loss: 2.0028326511383057
Validation loss: 2.0565947101962183

Epoch: 6| Step: 8
Training loss: 2.0229780673980713
Validation loss: 1.9960151744145218

Epoch: 6| Step: 9
Training loss: 2.699690341949463
Validation loss: 2.037562088299823

Epoch: 6| Step: 10
Training loss: 2.1597676277160645
Validation loss: 2.0274740393443773

Epoch: 6| Step: 11
Training loss: 1.7187061309814453
Validation loss: 2.027613662904309

Epoch: 6| Step: 12
Training loss: 2.4242401123046875
Validation loss: 2.033227352685826

Epoch: 6| Step: 13
Training loss: 1.8377922773361206
Validation loss: 2.000225028684062

Epoch: 104| Step: 0
Training loss: 2.3599586486816406
Validation loss: 2.0258972349987237

Epoch: 6| Step: 1
Training loss: 2.1527512073516846
Validation loss: 2.0133715957723637

Epoch: 6| Step: 2
Training loss: 1.5952045917510986
Validation loss: 2.0365574962349346

Epoch: 6| Step: 3
Training loss: 1.6481966972351074
Validation loss: 2.0156136956266177

Epoch: 6| Step: 4
Training loss: 2.1604156494140625
Validation loss: 2.063244714531847

Epoch: 6| Step: 5
Training loss: 2.30469012260437
Validation loss: 2.0562604024846065

Epoch: 6| Step: 6
Training loss: 2.4254493713378906
Validation loss: 2.0621016166543447

Epoch: 6| Step: 7
Training loss: 2.520864248275757
Validation loss: 2.0374473781995874

Epoch: 6| Step: 8
Training loss: 1.785179615020752
Validation loss: 2.0166528571036553

Epoch: 6| Step: 9
Training loss: 2.3980188369750977
Validation loss: 2.0704676976767917

Epoch: 6| Step: 10
Training loss: 2.518761157989502
Validation loss: 2.07570206990806

Epoch: 6| Step: 11
Training loss: 2.254739761352539
Validation loss: 2.0582206582510345

Epoch: 6| Step: 12
Training loss: 2.815229892730713
Validation loss: 2.0655528165960826

Epoch: 6| Step: 13
Training loss: 1.6433295011520386
Validation loss: 2.0455852272689983

Epoch: 105| Step: 0
Training loss: 1.744831919670105
Validation loss: 2.0228162247647523

Epoch: 6| Step: 1
Training loss: 2.115410566329956
Validation loss: 2.0412590401147

Epoch: 6| Step: 2
Training loss: 2.2572765350341797
Validation loss: 2.022376327104466

Epoch: 6| Step: 3
Training loss: 2.720595359802246
Validation loss: 2.0336253309762604

Epoch: 6| Step: 4
Training loss: 1.594290852546692
Validation loss: 2.0307268096554663

Epoch: 6| Step: 5
Training loss: 1.4832086563110352
Validation loss: 2.0385633899319555

Epoch: 6| Step: 6
Training loss: 2.5840773582458496
Validation loss: 2.044381445454013

Epoch: 6| Step: 7
Training loss: 2.2564539909362793
Validation loss: 2.007481736521567

Epoch: 6| Step: 8
Training loss: 2.148423671722412
Validation loss: 2.03661693808853

Epoch: 6| Step: 9
Training loss: 2.1531152725219727
Validation loss: 2.0398779684497463

Epoch: 6| Step: 10
Training loss: 2.0256307125091553
Validation loss: 2.026436751888644

Epoch: 6| Step: 11
Training loss: 2.6178605556488037
Validation loss: 2.0350234277786745

Epoch: 6| Step: 12
Training loss: 2.826943874359131
Validation loss: 2.0386923590014057

Epoch: 6| Step: 13
Training loss: 1.6973716020584106
Validation loss: 1.9884792527844828

Epoch: 106| Step: 0
Training loss: 1.5185933113098145
Validation loss: 2.005081631804025

Epoch: 6| Step: 1
Training loss: 2.1327836513519287
Validation loss: 1.9795913388652187

Epoch: 6| Step: 2
Training loss: 1.9489141702651978
Validation loss: 2.0352499754198137

Epoch: 6| Step: 3
Training loss: 2.32446551322937
Validation loss: 2.0069907185851887

Epoch: 6| Step: 4
Training loss: 2.352693796157837
Validation loss: 2.002317807366771

Epoch: 6| Step: 5
Training loss: 2.332900047302246
Validation loss: 2.0230082337574293

Epoch: 6| Step: 6
Training loss: 2.720949172973633
Validation loss: 2.01386772688999

Epoch: 6| Step: 7
Training loss: 2.0572519302368164
Validation loss: 2.047796687772197

Epoch: 6| Step: 8
Training loss: 1.6323277950286865
Validation loss: 2.002588451549571

Epoch: 6| Step: 9
Training loss: 2.2934341430664062
Validation loss: 2.0297963285958893

Epoch: 6| Step: 10
Training loss: 1.884605884552002
Validation loss: 2.026593990223382

Epoch: 6| Step: 11
Training loss: 2.3911805152893066
Validation loss: 2.0366296281096754

Epoch: 6| Step: 12
Training loss: 2.5916402339935303
Validation loss: 2.0281015685809556

Epoch: 6| Step: 13
Training loss: 2.546576499938965
Validation loss: 2.0106976365530365

Epoch: 107| Step: 0
Training loss: 2.1469125747680664
Validation loss: 2.0230677717475483

Epoch: 6| Step: 1
Training loss: 2.626098394393921
Validation loss: 2.0152648174634544

Epoch: 6| Step: 2
Training loss: 2.470827102661133
Validation loss: 2.0316121706398587

Epoch: 6| Step: 3
Training loss: 2.093003511428833
Validation loss: 2.028903248489544

Epoch: 6| Step: 4
Training loss: 2.5387940406799316
Validation loss: 2.018301237014032

Epoch: 6| Step: 5
Training loss: 2.792778491973877
Validation loss: 2.0420879792141657

Epoch: 6| Step: 6
Training loss: 2.051083564758301
Validation loss: 2.0207016788503176

Epoch: 6| Step: 7
Training loss: 1.6373707056045532
Validation loss: 2.0092715524858042

Epoch: 6| Step: 8
Training loss: 1.911089539527893
Validation loss: 2.0139627482301448

Epoch: 6| Step: 9
Training loss: 1.7841062545776367
Validation loss: 2.0469763176415556

Epoch: 6| Step: 10
Training loss: 2.372950553894043
Validation loss: 2.0152161839187785

Epoch: 6| Step: 11
Training loss: 2.6397628784179688
Validation loss: 2.060505892640801

Epoch: 6| Step: 12
Training loss: 1.7619061470031738
Validation loss: 2.0353783920247066

Epoch: 6| Step: 13
Training loss: 1.57102632522583
Validation loss: 2.0668251155525126

Epoch: 108| Step: 0
Training loss: 1.858386754989624
Validation loss: 2.0325817087645173

Epoch: 6| Step: 1
Training loss: 2.0295639038085938
Validation loss: 2.027463866818336

Epoch: 6| Step: 2
Training loss: 2.797579765319824
Validation loss: 2.0371217343115036

Epoch: 6| Step: 3
Training loss: 2.121474266052246
Validation loss: 2.051913492141231

Epoch: 6| Step: 4
Training loss: 1.8442555665969849
Validation loss: 2.0329168958048665

Epoch: 6| Step: 5
Training loss: 2.103440761566162
Validation loss: 2.0158029371692288

Epoch: 6| Step: 6
Training loss: 2.68650484085083
Validation loss: 2.0414773392420944

Epoch: 6| Step: 7
Training loss: 2.1795613765716553
Validation loss: 2.0297205217422976

Epoch: 6| Step: 8
Training loss: 2.813852548599243
Validation loss: 2.0071848412995696

Epoch: 6| Step: 9
Training loss: 2.305664539337158
Validation loss: 2.02004474337383

Epoch: 6| Step: 10
Training loss: 1.701308250427246
Validation loss: 2.0171232479874805

Epoch: 6| Step: 11
Training loss: 1.8516738414764404
Validation loss: 2.0156151017835064

Epoch: 6| Step: 12
Training loss: 2.179525852203369
Validation loss: 2.0235478134565454

Epoch: 6| Step: 13
Training loss: 2.0300872325897217
Validation loss: 2.050617310308641

Epoch: 109| Step: 0
Training loss: 2.3382630348205566
Validation loss: 2.026110836254653

Epoch: 6| Step: 1
Training loss: 2.407931089401245
Validation loss: 2.0364336788013415

Epoch: 6| Step: 2
Training loss: 2.291995048522949
Validation loss: 2.0321739437759563

Epoch: 6| Step: 3
Training loss: 1.7172269821166992
Validation loss: 2.027820384630593

Epoch: 6| Step: 4
Training loss: 2.8000588417053223
Validation loss: 2.013696752568727

Epoch: 6| Step: 5
Training loss: 2.5692474842071533
Validation loss: 2.0182307586875012

Epoch: 6| Step: 6
Training loss: 1.9834367036819458
Validation loss: 2.034535925875428

Epoch: 6| Step: 7
Training loss: 2.0255024433135986
Validation loss: 2.0557987946335987

Epoch: 6| Step: 8
Training loss: 1.2266925573349
Validation loss: 2.0381553826793546

Epoch: 6| Step: 9
Training loss: 2.0266783237457275
Validation loss: 2.0661300125942437

Epoch: 6| Step: 10
Training loss: 2.4229230880737305
Validation loss: 2.0707733041496685

Epoch: 6| Step: 11
Training loss: 2.3853042125701904
Validation loss: 2.0664053193984495

Epoch: 6| Step: 12
Training loss: 2.3681459426879883
Validation loss: 2.05112854126961

Epoch: 6| Step: 13
Training loss: 1.880493402481079
Validation loss: 2.045265902755081

Epoch: 110| Step: 0
Training loss: 1.396710991859436
Validation loss: 2.0200236843478296

Epoch: 6| Step: 1
Training loss: 2.710883140563965
Validation loss: 2.031513526875486

Epoch: 6| Step: 2
Training loss: 2.161482572555542
Validation loss: 2.0337478601804344

Epoch: 6| Step: 3
Training loss: 2.0434341430664062
Validation loss: 2.0529025331620248

Epoch: 6| Step: 4
Training loss: 2.028898000717163
Validation loss: 2.0705890040243826

Epoch: 6| Step: 5
Training loss: 1.6419429779052734
Validation loss: 2.0376086952865764

Epoch: 6| Step: 6
Training loss: 1.9920382499694824
Validation loss: 2.026058507222001

Epoch: 6| Step: 7
Training loss: 2.343291997909546
Validation loss: 2.031586685488301

Epoch: 6| Step: 8
Training loss: 2.391343116760254
Validation loss: 2.0420192492905485

Epoch: 6| Step: 9
Training loss: 2.175398349761963
Validation loss: 2.0544999466147473

Epoch: 6| Step: 10
Training loss: 2.0982308387756348
Validation loss: 2.0615909778943626

Epoch: 6| Step: 11
Training loss: 2.8954434394836426
Validation loss: 2.0471181497778943

Epoch: 6| Step: 12
Training loss: 2.1888346672058105
Validation loss: 2.053737496816984

Epoch: 6| Step: 13
Training loss: 2.3603246212005615
Validation loss: 2.0646685131134523

Epoch: 111| Step: 0
Training loss: 2.9950942993164062
Validation loss: 2.03870399280261

Epoch: 6| Step: 1
Training loss: 3.0090067386627197
Validation loss: 2.0351147420944704

Epoch: 6| Step: 2
Training loss: 2.118971586227417
Validation loss: 2.0284434621052077

Epoch: 6| Step: 3
Training loss: 2.111560821533203
Validation loss: 2.063114330332766

Epoch: 6| Step: 4
Training loss: 1.5117801427841187
Validation loss: 2.0314298483633224

Epoch: 6| Step: 5
Training loss: 1.702117681503296
Validation loss: 2.0448208188497894

Epoch: 6| Step: 6
Training loss: 2.021427631378174
Validation loss: 2.038814636968797

Epoch: 6| Step: 7
Training loss: 1.9816513061523438
Validation loss: 2.021718935299945

Epoch: 6| Step: 8
Training loss: 2.541581630706787
Validation loss: 2.0276525905055385

Epoch: 6| Step: 9
Training loss: 1.8469483852386475
Validation loss: 2.038311231520868

Epoch: 6| Step: 10
Training loss: 1.9957612752914429
Validation loss: 2.035986281210376

Epoch: 6| Step: 11
Training loss: 2.4925124645233154
Validation loss: 2.018777976753891

Epoch: 6| Step: 12
Training loss: 1.7700213193893433
Validation loss: 2.0147109800769436

Epoch: 6| Step: 13
Training loss: 2.8247148990631104
Validation loss: 2.0667010725185437

Epoch: 112| Step: 0
Training loss: 2.383906126022339
Validation loss: 2.0033974493703535

Epoch: 6| Step: 1
Training loss: 2.1034092903137207
Validation loss: 2.0474530009813208

Epoch: 6| Step: 2
Training loss: 1.7398439645767212
Validation loss: 2.0314386711325696

Epoch: 6| Step: 3
Training loss: 3.011002540588379
Validation loss: 2.0131122630129576

Epoch: 6| Step: 4
Training loss: 1.6114370822906494
Validation loss: 2.017544892526442

Epoch: 6| Step: 5
Training loss: 2.2554006576538086
Validation loss: 2.0171743016089163

Epoch: 6| Step: 6
Training loss: 2.6864945888519287
Validation loss: 2.050439232139177

Epoch: 6| Step: 7
Training loss: 1.96034836769104
Validation loss: 2.0565604061208744

Epoch: 6| Step: 8
Training loss: 1.974794626235962
Validation loss: 2.0348675071552234

Epoch: 6| Step: 9
Training loss: 1.66134512424469
Validation loss: 2.034267161482124

Epoch: 6| Step: 10
Training loss: 1.917435884475708
Validation loss: 2.0281108079418058

Epoch: 6| Step: 11
Training loss: 2.625070095062256
Validation loss: 2.0659648679917857

Epoch: 6| Step: 12
Training loss: 2.0932986736297607
Validation loss: 2.030904103350896

Epoch: 6| Step: 13
Training loss: 2.5205917358398438
Validation loss: 2.0431331306375484

Epoch: 113| Step: 0
Training loss: 2.005906105041504
Validation loss: 2.0684808505478727

Epoch: 6| Step: 1
Training loss: 2.6163525581359863
Validation loss: 2.0095895080156225

Epoch: 6| Step: 2
Training loss: 2.3757405281066895
Validation loss: 2.050180568489977

Epoch: 6| Step: 3
Training loss: 2.1398427486419678
Validation loss: 2.0816060766097038

Epoch: 6| Step: 4
Training loss: 2.2908239364624023
Validation loss: 2.0318067355822493

Epoch: 6| Step: 5
Training loss: 2.2421951293945312
Validation loss: 2.035945407805904

Epoch: 6| Step: 6
Training loss: 2.394897699356079
Validation loss: 2.0604196902244323

Epoch: 6| Step: 7
Training loss: 1.6176307201385498
Validation loss: 2.0505262882478776

Epoch: 6| Step: 8
Training loss: 2.6074559688568115
Validation loss: 2.045878948703889

Epoch: 6| Step: 9
Training loss: 1.9033167362213135
Validation loss: 2.007703859318969

Epoch: 6| Step: 10
Training loss: 2.2629499435424805
Validation loss: 2.041535310847785

Epoch: 6| Step: 11
Training loss: 2.2299675941467285
Validation loss: 2.0302278944241103

Epoch: 6| Step: 12
Training loss: 1.399468183517456
Validation loss: 2.0329095932745163

Epoch: 6| Step: 13
Training loss: 2.1921579837799072
Validation loss: 2.0144132426989976

Epoch: 114| Step: 0
Training loss: 1.7145435810089111
Validation loss: 2.0302808566759993

Epoch: 6| Step: 1
Training loss: 2.703573226928711
Validation loss: 2.0281203152031027

Epoch: 6| Step: 2
Training loss: 2.5789473056793213
Validation loss: 2.0471470253441924

Epoch: 6| Step: 3
Training loss: 1.4742945432662964
Validation loss: 2.0541529476001696

Epoch: 6| Step: 4
Training loss: 1.7214665412902832
Validation loss: 2.0367833337476178

Epoch: 6| Step: 5
Training loss: 2.1534080505371094
Validation loss: 2.0382369410607124

Epoch: 6| Step: 6
Training loss: 2.0412240028381348
Validation loss: 2.0664378750708794

Epoch: 6| Step: 7
Training loss: 2.237010955810547
Validation loss: 2.0551711590059343

Epoch: 6| Step: 8
Training loss: 2.424365997314453
Validation loss: 2.0411313887565368

Epoch: 6| Step: 9
Training loss: 2.692030429840088
Validation loss: 2.055835275239842

Epoch: 6| Step: 10
Training loss: 1.776752233505249
Validation loss: 2.0072776822633642

Epoch: 6| Step: 11
Training loss: 2.5655319690704346
Validation loss: 2.0230070929373465

Epoch: 6| Step: 12
Training loss: 2.5637359619140625
Validation loss: 2.0388813326435704

Epoch: 6| Step: 13
Training loss: 1.6174967288970947
Validation loss: 2.0344299129260484

Epoch: 115| Step: 0
Training loss: 2.3968095779418945
Validation loss: 2.042326663130073

Epoch: 6| Step: 1
Training loss: 2.2736337184906006
Validation loss: 2.033343440742903

Epoch: 6| Step: 2
Training loss: 1.7594941854476929
Validation loss: 2.0481190630184707

Epoch: 6| Step: 3
Training loss: 1.6715519428253174
Validation loss: 2.063175309088922

Epoch: 6| Step: 4
Training loss: 1.918639898300171
Validation loss: 2.0629364508454517

Epoch: 6| Step: 5
Training loss: 2.0093021392822266
Validation loss: 2.028253560425133

Epoch: 6| Step: 6
Training loss: 2.4122018814086914
Validation loss: 2.0493735292906403

Epoch: 6| Step: 7
Training loss: 2.453702926635742
Validation loss: 2.0163835646003805

Epoch: 6| Step: 8
Training loss: 2.981985092163086
Validation loss: 2.0577684422974944

Epoch: 6| Step: 9
Training loss: 2.181962013244629
Validation loss: 2.0566105509317048

Epoch: 6| Step: 10
Training loss: 1.416699767112732
Validation loss: 2.0408923907946517

Epoch: 6| Step: 11
Training loss: 2.6938509941101074
Validation loss: 2.0432331318496377

Epoch: 6| Step: 12
Training loss: 2.17348051071167
Validation loss: 2.071769191372779

Epoch: 6| Step: 13
Training loss: 2.3845746517181396
Validation loss: 2.067095248929916

Epoch: 116| Step: 0
Training loss: 2.1251635551452637
Validation loss: 2.0487250128099994

Epoch: 6| Step: 1
Training loss: 2.35547137260437
Validation loss: 2.0579264112698135

Epoch: 6| Step: 2
Training loss: 1.5453882217407227
Validation loss: 2.0356183359699864

Epoch: 6| Step: 3
Training loss: 1.63628351688385
Validation loss: 2.052706583853691

Epoch: 6| Step: 4
Training loss: 2.502257823944092
Validation loss: 2.0624461481648106

Epoch: 6| Step: 5
Training loss: 1.6756713390350342
Validation loss: 2.071434244032829

Epoch: 6| Step: 6
Training loss: 1.9147650003433228
Validation loss: 2.0639910364663727

Epoch: 6| Step: 7
Training loss: 1.819812297821045
Validation loss: 2.0528642823619228

Epoch: 6| Step: 8
Training loss: 2.078531503677368
Validation loss: 2.049617859625047

Epoch: 6| Step: 9
Training loss: 2.6233975887298584
Validation loss: 2.063819646835327

Epoch: 6| Step: 10
Training loss: 2.9038784503936768
Validation loss: 2.0728755035708026

Epoch: 6| Step: 11
Training loss: 2.1096351146698
Validation loss: 2.0450989559132564

Epoch: 6| Step: 12
Training loss: 2.5487594604492188
Validation loss: 2.020758416063042

Epoch: 6| Step: 13
Training loss: 2.9447808265686035
Validation loss: 2.030032196352559

Epoch: 117| Step: 0
Training loss: 1.9664394855499268
Validation loss: 2.046375718168033

Epoch: 6| Step: 1
Training loss: 2.7207274436950684
Validation loss: 2.0220381726500807

Epoch: 6| Step: 2
Training loss: 2.5097341537475586
Validation loss: 2.0374663055583997

Epoch: 6| Step: 3
Training loss: 2.0348217487335205
Validation loss: 2.023158880972093

Epoch: 6| Step: 4
Training loss: 2.1886916160583496
Validation loss: 2.007923690221643

Epoch: 6| Step: 5
Training loss: 2.392975091934204
Validation loss: 2.017599518581103

Epoch: 6| Step: 6
Training loss: 1.5185208320617676
Validation loss: 2.045884124694332

Epoch: 6| Step: 7
Training loss: 2.5627899169921875
Validation loss: 2.0080700356473207

Epoch: 6| Step: 8
Training loss: 1.7975468635559082
Validation loss: 2.013110381300731

Epoch: 6| Step: 9
Training loss: 2.1636483669281006
Validation loss: 2.0036203681781726

Epoch: 6| Step: 10
Training loss: 2.3270487785339355
Validation loss: 2.0221992128638813

Epoch: 6| Step: 11
Training loss: 2.04544734954834
Validation loss: 2.0124656936173797

Epoch: 6| Step: 12
Training loss: 2.067446708679199
Validation loss: 2.034793133376747

Epoch: 6| Step: 13
Training loss: 1.8678019046783447
Validation loss: 2.0299450402618735

Epoch: 118| Step: 0
Training loss: 2.0148262977600098
Validation loss: 2.012086006902879

Epoch: 6| Step: 1
Training loss: 2.3707621097564697
Validation loss: 2.0297567998209307

Epoch: 6| Step: 2
Training loss: 2.3542656898498535
Validation loss: 2.0276554630648707

Epoch: 6| Step: 3
Training loss: 2.6816821098327637
Validation loss: 2.0253762468214958

Epoch: 6| Step: 4
Training loss: 1.708503007888794
Validation loss: 2.0287217709325973

Epoch: 6| Step: 5
Training loss: 2.0323493480682373
Validation loss: 2.0441581023636686

Epoch: 6| Step: 6
Training loss: 2.767810583114624
Validation loss: 2.0382938103009294

Epoch: 6| Step: 7
Training loss: 2.1660189628601074
Validation loss: 2.0486392808216873

Epoch: 6| Step: 8
Training loss: 1.862367868423462
Validation loss: 2.02845444474169

Epoch: 6| Step: 9
Training loss: 1.9503014087677002
Validation loss: 2.0738100749190136

Epoch: 6| Step: 10
Training loss: 2.5237083435058594
Validation loss: 2.048022844458139

Epoch: 6| Step: 11
Training loss: 2.263394832611084
Validation loss: 2.0446256283790833

Epoch: 6| Step: 12
Training loss: 1.6234930753707886
Validation loss: 2.0726520246075046

Epoch: 6| Step: 13
Training loss: 1.8561736345291138
Validation loss: 2.065893809000651

Epoch: 119| Step: 0
Training loss: 2.356781005859375
Validation loss: 2.0446674208487234

Epoch: 6| Step: 1
Training loss: 2.195192337036133
Validation loss: 2.0585272183982273

Epoch: 6| Step: 2
Training loss: 2.2703256607055664
Validation loss: 2.0445270461420857

Epoch: 6| Step: 3
Training loss: 1.7931818962097168
Validation loss: 2.070727273982058

Epoch: 6| Step: 4
Training loss: 2.121269941329956
Validation loss: 2.0542170770706667

Epoch: 6| Step: 5
Training loss: 2.603059768676758
Validation loss: 2.057022653600221

Epoch: 6| Step: 6
Training loss: 1.645883321762085
Validation loss: 2.0683066588576122

Epoch: 6| Step: 7
Training loss: 1.9270436763763428
Validation loss: 2.0535126527150473

Epoch: 6| Step: 8
Training loss: 1.6059492826461792
Validation loss: 2.0603358527665496

Epoch: 6| Step: 9
Training loss: 2.0104053020477295
Validation loss: 2.0909263856949343

Epoch: 6| Step: 10
Training loss: 2.7632346153259277
Validation loss: 2.0523057099311584

Epoch: 6| Step: 11
Training loss: 2.716442823410034
Validation loss: 2.031811602653996

Epoch: 6| Step: 12
Training loss: 2.2091832160949707
Validation loss: 2.0503796095489175

Epoch: 6| Step: 13
Training loss: 2.207909107208252
Validation loss: 2.0800674602549565

Epoch: 120| Step: 0
Training loss: 2.1275670528411865
Validation loss: 2.0390903334463797

Epoch: 6| Step: 1
Training loss: 1.9095693826675415
Validation loss: 2.0638822368396226

Epoch: 6| Step: 2
Training loss: 2.337376117706299
Validation loss: 2.052793454098445

Epoch: 6| Step: 3
Training loss: 1.9654704332351685
Validation loss: 2.0191101335710093

Epoch: 6| Step: 4
Training loss: 2.3238372802734375
Validation loss: 2.056346598491874

Epoch: 6| Step: 5
Training loss: 1.8585176467895508
Validation loss: 2.0332726047885035

Epoch: 6| Step: 6
Training loss: 1.9501738548278809
Validation loss: 2.035522271228093

Epoch: 6| Step: 7
Training loss: 1.6632119417190552
Validation loss: 2.0591260899779615

Epoch: 6| Step: 8
Training loss: 2.4024510383605957
Validation loss: 2.0214531524207002

Epoch: 6| Step: 9
Training loss: 2.596442699432373
Validation loss: 2.051255319708137

Epoch: 6| Step: 10
Training loss: 2.1807589530944824
Validation loss: 2.0244527350189867

Epoch: 6| Step: 11
Training loss: 2.644944429397583
Validation loss: 2.011099115494759

Epoch: 6| Step: 12
Training loss: 2.1994118690490723
Validation loss: 2.0342393216266426

Epoch: 6| Step: 13
Training loss: 2.1014621257781982
Validation loss: 2.0414623701444237

Epoch: 121| Step: 0
Training loss: 2.623563766479492
Validation loss: 2.0309077052659887

Epoch: 6| Step: 1
Training loss: 2.2253851890563965
Validation loss: 2.0179470687784176

Epoch: 6| Step: 2
Training loss: 2.4351394176483154
Validation loss: 2.0190887681899534

Epoch: 6| Step: 3
Training loss: 2.323154926300049
Validation loss: 2.0517739429268786

Epoch: 6| Step: 4
Training loss: 2.4983468055725098
Validation loss: 2.0469632379470335

Epoch: 6| Step: 5
Training loss: 2.0249767303466797
Validation loss: 2.0409465451394357

Epoch: 6| Step: 6
Training loss: 1.8014237880706787
Validation loss: 2.0173852982059604

Epoch: 6| Step: 7
Training loss: 2.5170774459838867
Validation loss: 2.0181979056327575

Epoch: 6| Step: 8
Training loss: 1.6336140632629395
Validation loss: 2.051740905290009

Epoch: 6| Step: 9
Training loss: 1.8446893692016602
Validation loss: 2.024277403790464

Epoch: 6| Step: 10
Training loss: 2.138779401779175
Validation loss: 2.0334259348530925

Epoch: 6| Step: 11
Training loss: 1.8314321041107178
Validation loss: 2.016735174322641

Epoch: 6| Step: 12
Training loss: 1.6966450214385986
Validation loss: 2.037536121183826

Epoch: 6| Step: 13
Training loss: 2.7439498901367188
Validation loss: 2.0312563398832917

Epoch: 122| Step: 0
Training loss: 1.9089386463165283
Validation loss: 1.9913673541879142

Epoch: 6| Step: 1
Training loss: 2.410137176513672
Validation loss: 2.0193329703423286

Epoch: 6| Step: 2
Training loss: 1.9260252714157104
Validation loss: 2.0403300280212076

Epoch: 6| Step: 3
Training loss: 1.9636259078979492
Validation loss: 2.0355639175702165

Epoch: 6| Step: 4
Training loss: 2.373051643371582
Validation loss: 2.034071704392792

Epoch: 6| Step: 5
Training loss: 2.440016984939575
Validation loss: 2.0438897404619443

Epoch: 6| Step: 6
Training loss: 1.7910046577453613
Validation loss: 2.0423275527133735

Epoch: 6| Step: 7
Training loss: 2.060619831085205
Validation loss: 2.0499278729961765

Epoch: 6| Step: 8
Training loss: 2.485196590423584
Validation loss: 2.027325922443021

Epoch: 6| Step: 9
Training loss: 1.8731907606124878
Validation loss: 2.0389427215822282

Epoch: 6| Step: 10
Training loss: 1.7760587930679321
Validation loss: 2.0353146086456957

Epoch: 6| Step: 11
Training loss: 1.988867998123169
Validation loss: 2.0701775525205877

Epoch: 6| Step: 12
Training loss: 2.221270799636841
Validation loss: 2.027606859002062

Epoch: 6| Step: 13
Training loss: 3.045523166656494
Validation loss: 2.0746829458462295

Epoch: 123| Step: 0
Training loss: 1.9775334596633911
Validation loss: 2.0422605429926226

Epoch: 6| Step: 1
Training loss: 2.024963140487671
Validation loss: 2.0251192956842403

Epoch: 6| Step: 2
Training loss: 1.9524950981140137
Validation loss: 2.0182235305027296

Epoch: 6| Step: 3
Training loss: 1.5507190227508545
Validation loss: 2.029470818017119

Epoch: 6| Step: 4
Training loss: 2.4928534030914307
Validation loss: 2.0336138458662134

Epoch: 6| Step: 5
Training loss: 2.484145164489746
Validation loss: 2.020250017924975

Epoch: 6| Step: 6
Training loss: 2.1774818897247314
Validation loss: 2.0562048727466213

Epoch: 6| Step: 7
Training loss: 1.8278264999389648
Validation loss: 2.0435698455379856

Epoch: 6| Step: 8
Training loss: 2.3004982471466064
Validation loss: 2.0422803176346647

Epoch: 6| Step: 9
Training loss: 2.6968274116516113
Validation loss: 2.041818767465571

Epoch: 6| Step: 10
Training loss: 2.0124993324279785
Validation loss: 1.9972944323734572

Epoch: 6| Step: 11
Training loss: 1.7882137298583984
Validation loss: 2.0309576552401305

Epoch: 6| Step: 12
Training loss: 2.3999435901641846
Validation loss: 2.038674441717004

Epoch: 6| Step: 13
Training loss: 2.8902409076690674
Validation loss: 2.0237123427852506

Epoch: 124| Step: 0
Training loss: 2.9134697914123535
Validation loss: 2.03569612708143

Epoch: 6| Step: 1
Training loss: 1.3959894180297852
Validation loss: 2.048542566196893

Epoch: 6| Step: 2
Training loss: 2.014902114868164
Validation loss: 2.048614050752373

Epoch: 6| Step: 3
Training loss: 2.7851028442382812
Validation loss: 1.9836683504043087

Epoch: 6| Step: 4
Training loss: 1.8804043531417847
Validation loss: 2.042761896246223

Epoch: 6| Step: 5
Training loss: 2.622802972793579
Validation loss: 2.0445371814953384

Epoch: 6| Step: 6
Training loss: 2.026369571685791
Validation loss: 2.0342603216889086

Epoch: 6| Step: 7
Training loss: 2.3449134826660156
Validation loss: 2.044939838429933

Epoch: 6| Step: 8
Training loss: 1.6223002672195435
Validation loss: 2.0353547834580943

Epoch: 6| Step: 9
Training loss: 1.8222591876983643
Validation loss: 2.0184249326746952

Epoch: 6| Step: 10
Training loss: 2.198700428009033
Validation loss: 2.009205146502423

Epoch: 6| Step: 11
Training loss: 2.077404022216797
Validation loss: 2.0432690907550115

Epoch: 6| Step: 12
Training loss: 1.372603178024292
Validation loss: 2.0171856546914704

Epoch: 6| Step: 13
Training loss: 3.5372657775878906
Validation loss: 2.066052641919864

Epoch: 125| Step: 0
Training loss: 2.2364659309387207
Validation loss: 2.0250130032980316

Epoch: 6| Step: 1
Training loss: 2.647294044494629
Validation loss: 2.0397061096724642

Epoch: 6| Step: 2
Training loss: 1.712564468383789
Validation loss: 2.0546852670690066

Epoch: 6| Step: 3
Training loss: 2.690783977508545
Validation loss: 2.0287626046006397

Epoch: 6| Step: 4
Training loss: 1.8092752695083618
Validation loss: 2.046810083491828

Epoch: 6| Step: 5
Training loss: 1.7411830425262451
Validation loss: 2.0321198304494223

Epoch: 6| Step: 6
Training loss: 1.5780470371246338
Validation loss: 2.034142945402412

Epoch: 6| Step: 7
Training loss: 2.284806251525879
Validation loss: 2.086424460975073

Epoch: 6| Step: 8
Training loss: 2.3491904735565186
Validation loss: 2.057425670726325

Epoch: 6| Step: 9
Training loss: 1.935096263885498
Validation loss: 2.0262058396493234

Epoch: 6| Step: 10
Training loss: 2.0036368370056152
Validation loss: 2.0355178720207623

Epoch: 6| Step: 11
Training loss: 3.0337419509887695
Validation loss: 2.033250306242256

Epoch: 6| Step: 12
Training loss: 2.3767099380493164
Validation loss: 2.0549994643016527

Epoch: 6| Step: 13
Training loss: 1.8139195442199707
Validation loss: 2.05505467743002

Epoch: 126| Step: 0
Training loss: 1.9953713417053223
Validation loss: 2.0728261381067257

Epoch: 6| Step: 1
Training loss: 2.5572409629821777
Validation loss: 2.0370955928679435

Epoch: 6| Step: 2
Training loss: 2.3184549808502197
Validation loss: 2.0592995638488443

Epoch: 6| Step: 3
Training loss: 2.4967763423919678
Validation loss: 2.0732327866297897

Epoch: 6| Step: 4
Training loss: 1.2780942916870117
Validation loss: 2.065449933851919

Epoch: 6| Step: 5
Training loss: 2.0437815189361572
Validation loss: 2.046524291397423

Epoch: 6| Step: 6
Training loss: 2.282104730606079
Validation loss: 2.059224843978882

Epoch: 6| Step: 7
Training loss: 2.4058687686920166
Validation loss: 2.075744080287154

Epoch: 6| Step: 8
Training loss: 2.539729118347168
Validation loss: 2.0334844768688245

Epoch: 6| Step: 9
Training loss: 2.1004085540771484
Validation loss: 2.0637553174008607

Epoch: 6| Step: 10
Training loss: 2.1405527591705322
Validation loss: 2.047991660333449

Epoch: 6| Step: 11
Training loss: 1.5926979780197144
Validation loss: 2.041319554851901

Epoch: 6| Step: 12
Training loss: 2.131331443786621
Validation loss: 2.037586905622995

Epoch: 6| Step: 13
Training loss: 2.405622959136963
Validation loss: 2.023798499056088

Epoch: 127| Step: 0
Training loss: 2.2254984378814697
Validation loss: 2.0397617624652002

Epoch: 6| Step: 1
Training loss: 2.0386545658111572
Validation loss: 2.0502622114714755

Epoch: 6| Step: 2
Training loss: 2.054959297180176
Validation loss: 2.0263479358406475

Epoch: 6| Step: 3
Training loss: 2.9545748233795166
Validation loss: 2.061720105909532

Epoch: 6| Step: 4
Training loss: 1.4710805416107178
Validation loss: 2.042343912586089

Epoch: 6| Step: 5
Training loss: 2.0931475162506104
Validation loss: 2.0361685829777874

Epoch: 6| Step: 6
Training loss: 1.7718112468719482
Validation loss: 2.030755783921929

Epoch: 6| Step: 7
Training loss: 2.2075867652893066
Validation loss: 2.0176179806391397

Epoch: 6| Step: 8
Training loss: 1.4947357177734375
Validation loss: 2.0129231304250736

Epoch: 6| Step: 9
Training loss: 2.6069321632385254
Validation loss: 2.020370137306952

Epoch: 6| Step: 10
Training loss: 1.805238962173462
Validation loss: 2.0240631539334535

Epoch: 6| Step: 11
Training loss: 2.609189510345459
Validation loss: 2.0220706834588

Epoch: 6| Step: 12
Training loss: 2.4711685180664062
Validation loss: 2.0340189497957946

Epoch: 6| Step: 13
Training loss: 2.158966302871704
Validation loss: 2.0167854242427374

Epoch: 128| Step: 0
Training loss: 2.283761501312256
Validation loss: 2.037442389354911

Epoch: 6| Step: 1
Training loss: 2.2567734718322754
Validation loss: 2.0582508822923065

Epoch: 6| Step: 2
Training loss: 2.2200284004211426
Validation loss: 2.0254022895648913

Epoch: 6| Step: 3
Training loss: 1.4961438179016113
Validation loss: 2.035036184454477

Epoch: 6| Step: 4
Training loss: 2.4456610679626465
Validation loss: 2.040752277579359

Epoch: 6| Step: 5
Training loss: 1.9999120235443115
Validation loss: 2.071452271553778

Epoch: 6| Step: 6
Training loss: 2.1812705993652344
Validation loss: 2.070968608702383

Epoch: 6| Step: 7
Training loss: 1.9638577699661255
Validation loss: 2.056122341463643

Epoch: 6| Step: 8
Training loss: 2.3151473999023438
Validation loss: 2.0224114476993518

Epoch: 6| Step: 9
Training loss: 1.7498329877853394
Validation loss: 2.0569331722874797

Epoch: 6| Step: 10
Training loss: 2.2150015830993652
Validation loss: 2.0442320992869716

Epoch: 6| Step: 11
Training loss: 1.497630000114441
Validation loss: 2.052459038713927

Epoch: 6| Step: 12
Training loss: 2.606421947479248
Validation loss: 2.032178927493352

Epoch: 6| Step: 13
Training loss: 3.189950704574585
Validation loss: 2.025137719287667

Epoch: 129| Step: 0
Training loss: 2.0503125190734863
Validation loss: 2.0630834128267024

Epoch: 6| Step: 1
Training loss: 2.065511703491211
Validation loss: 2.0609605645620697

Epoch: 6| Step: 2
Training loss: 2.8395133018493652
Validation loss: 2.075552958314137

Epoch: 6| Step: 3
Training loss: 1.868851900100708
Validation loss: 2.1167909201755317

Epoch: 6| Step: 4
Training loss: 2.17079496383667
Validation loss: 2.072684955853288

Epoch: 6| Step: 5
Training loss: 2.3993704319000244
Validation loss: 2.0436845171836113

Epoch: 6| Step: 6
Training loss: 2.3533496856689453
Validation loss: 2.017788061531641

Epoch: 6| Step: 7
Training loss: 2.5636510848999023
Validation loss: 2.0726952680977444

Epoch: 6| Step: 8
Training loss: 1.6931967735290527
Validation loss: 2.0296800341657413

Epoch: 6| Step: 9
Training loss: 2.3691086769104004
Validation loss: 2.067893848624281

Epoch: 6| Step: 10
Training loss: 1.6917531490325928
Validation loss: 2.0394635995229087

Epoch: 6| Step: 11
Training loss: 1.7927510738372803
Validation loss: 2.0447517582165298

Epoch: 6| Step: 12
Training loss: 1.7877888679504395
Validation loss: 2.0538227058226064

Epoch: 6| Step: 13
Training loss: 2.7559187412261963
Validation loss: 2.068323658358666

Epoch: 130| Step: 0
Training loss: 2.5363521575927734
Validation loss: 2.0525608575472267

Epoch: 6| Step: 1
Training loss: 1.7899186611175537
Validation loss: 2.0463448519347818

Epoch: 6| Step: 2
Training loss: 2.610940933227539
Validation loss: 2.0234057108561196

Epoch: 6| Step: 3
Training loss: 2.6848487854003906
Validation loss: 2.0651658376057944

Epoch: 6| Step: 4
Training loss: 2.4779868125915527
Validation loss: 2.0170689000878284

Epoch: 6| Step: 5
Training loss: 1.4336292743682861
Validation loss: 2.0233701685423493

Epoch: 6| Step: 6
Training loss: 2.5742850303649902
Validation loss: 2.0299480243395736

Epoch: 6| Step: 7
Training loss: 1.9020142555236816
Validation loss: 2.05517408668354

Epoch: 6| Step: 8
Training loss: 2.1621246337890625
Validation loss: 2.032844643439016

Epoch: 6| Step: 9
Training loss: 1.6372058391571045
Validation loss: 2.0288140850682415

Epoch: 6| Step: 10
Training loss: 1.6936606168746948
Validation loss: 2.0311600751774286

Epoch: 6| Step: 11
Training loss: 1.6144535541534424
Validation loss: 2.0491174574821227

Epoch: 6| Step: 12
Training loss: 2.941880941390991
Validation loss: 2.039297785810245

Epoch: 6| Step: 13
Training loss: 1.7923107147216797
Validation loss: 2.0333805930229927

Epoch: 131| Step: 0
Training loss: 2.104008197784424
Validation loss: 2.0303039499508437

Epoch: 6| Step: 1
Training loss: 1.9924402236938477
Validation loss: 2.048404183439029

Epoch: 6| Step: 2
Training loss: 1.817183017730713
Validation loss: 2.0141591410483084

Epoch: 6| Step: 3
Training loss: 2.368933916091919
Validation loss: 2.05586427257907

Epoch: 6| Step: 4
Training loss: 2.6885766983032227
Validation loss: 1.996222469114488

Epoch: 6| Step: 5
Training loss: 2.5006637573242188
Validation loss: 2.008377786605589

Epoch: 6| Step: 6
Training loss: 2.437657117843628
Validation loss: 2.020728926504812

Epoch: 6| Step: 7
Training loss: 1.9488011598587036
Validation loss: 1.999887994540635

Epoch: 6| Step: 8
Training loss: 2.080315113067627
Validation loss: 2.0356576032536005

Epoch: 6| Step: 9
Training loss: 2.5921216011047363
Validation loss: 2.02143899215165

Epoch: 6| Step: 10
Training loss: 1.374192237854004
Validation loss: 2.039926944240447

Epoch: 6| Step: 11
Training loss: 2.225355625152588
Validation loss: 2.000088974993716

Epoch: 6| Step: 12
Training loss: 1.922795057296753
Validation loss: 2.0425225816747195

Epoch: 6| Step: 13
Training loss: 1.7712267637252808
Validation loss: 2.0116554562763502

Epoch: 132| Step: 0
Training loss: 2.0206615924835205
Validation loss: 2.0288121674650457

Epoch: 6| Step: 1
Training loss: 2.196096181869507
Validation loss: 2.036325354729929

Epoch: 6| Step: 2
Training loss: 2.2801055908203125
Validation loss: 1.995143323816279

Epoch: 6| Step: 3
Training loss: 1.4771517515182495
Validation loss: 1.9890865561782674

Epoch: 6| Step: 4
Training loss: 2.408432960510254
Validation loss: 2.048088309585407

Epoch: 6| Step: 5
Training loss: 2.6887049674987793
Validation loss: 2.01997140402435

Epoch: 6| Step: 6
Training loss: 2.048969030380249
Validation loss: 2.0496133783812165

Epoch: 6| Step: 7
Training loss: 2.456387996673584
Validation loss: 2.0371799866358438

Epoch: 6| Step: 8
Training loss: 2.3074002265930176
Validation loss: 2.048604085881223

Epoch: 6| Step: 9
Training loss: 2.428180694580078
Validation loss: 2.032264963273079

Epoch: 6| Step: 10
Training loss: 2.190392017364502
Validation loss: 2.0572688528286514

Epoch: 6| Step: 11
Training loss: 1.2551534175872803
Validation loss: 2.0644743775808685

Epoch: 6| Step: 12
Training loss: 2.572219133377075
Validation loss: 2.0720593660108504

Epoch: 6| Step: 13
Training loss: 1.8907686471939087
Validation loss: 2.071940137493995

Epoch: 133| Step: 0
Training loss: 2.143425941467285
Validation loss: 2.0687759614759877

Epoch: 6| Step: 1
Training loss: 2.0835094451904297
Validation loss: 2.0816351200944636

Epoch: 6| Step: 2
Training loss: 1.7961379289627075
Validation loss: 2.0269526640574136

Epoch: 6| Step: 3
Training loss: 2.9782495498657227
Validation loss: 2.046172894457335

Epoch: 6| Step: 4
Training loss: 2.268388509750366
Validation loss: 2.065537824425646

Epoch: 6| Step: 5
Training loss: 2.518113613128662
Validation loss: 2.0667223584267402

Epoch: 6| Step: 6
Training loss: 2.493744134902954
Validation loss: 2.053243206393334

Epoch: 6| Step: 7
Training loss: 1.495060920715332
Validation loss: 2.0368350500701577

Epoch: 6| Step: 8
Training loss: 2.127668857574463
Validation loss: 2.074072518656331

Epoch: 6| Step: 9
Training loss: 1.0508335828781128
Validation loss: 2.0301864019004245

Epoch: 6| Step: 10
Training loss: 2.502737283706665
Validation loss: 2.0618965010489188

Epoch: 6| Step: 11
Training loss: 1.857012391090393
Validation loss: 2.048546511639831

Epoch: 6| Step: 12
Training loss: 2.34985613822937
Validation loss: 2.0374056844301123

Epoch: 6| Step: 13
Training loss: 2.4543638229370117
Validation loss: 2.0082225466287262

Epoch: 134| Step: 0
Training loss: 2.7679357528686523
Validation loss: 2.0264728607669955

Epoch: 6| Step: 1
Training loss: 3.282968521118164
Validation loss: 2.0345534829683203

Epoch: 6| Step: 2
Training loss: 1.3294705152511597
Validation loss: 2.0235387407323366

Epoch: 6| Step: 3
Training loss: 1.9073748588562012
Validation loss: 2.011891370178551

Epoch: 6| Step: 4
Training loss: 1.954655408859253
Validation loss: 2.020237745777253

Epoch: 6| Step: 5
Training loss: 2.1987013816833496
Validation loss: 2.054026424243886

Epoch: 6| Step: 6
Training loss: 1.8603789806365967
Validation loss: 2.029628469097999

Epoch: 6| Step: 7
Training loss: 1.7748538255691528
Validation loss: 2.0302448580341954

Epoch: 6| Step: 8
Training loss: 1.2523150444030762
Validation loss: 2.0255525291606946

Epoch: 6| Step: 9
Training loss: 3.0315096378326416
Validation loss: 2.047818778663553

Epoch: 6| Step: 10
Training loss: 1.9457910060882568
Validation loss: 2.0437012295569144

Epoch: 6| Step: 11
Training loss: 1.7648528814315796
Validation loss: 2.045153410203995

Epoch: 6| Step: 12
Training loss: 2.761025905609131
Validation loss: 2.0180565644336004

Epoch: 6| Step: 13
Training loss: 1.9904539585113525
Validation loss: 2.0438949625979186

Epoch: 135| Step: 0
Training loss: 2.4502053260803223
Validation loss: 2.0316885261125464

Epoch: 6| Step: 1
Training loss: 2.6588659286499023
Validation loss: 2.03096668950973

Epoch: 6| Step: 2
Training loss: 2.4796481132507324
Validation loss: 2.0353980192574124

Epoch: 6| Step: 3
Training loss: 2.448118209838867
Validation loss: 2.0319110014105357

Epoch: 6| Step: 4
Training loss: 2.274061441421509
Validation loss: 2.0290499502612698

Epoch: 6| Step: 5
Training loss: 1.6559228897094727
Validation loss: 2.055277430883018

Epoch: 6| Step: 6
Training loss: 2.115874767303467
Validation loss: 2.0497589085691716

Epoch: 6| Step: 7
Training loss: 2.434887409210205
Validation loss: 2.042438542971047

Epoch: 6| Step: 8
Training loss: 2.44415283203125
Validation loss: 2.061136272645766

Epoch: 6| Step: 9
Training loss: 2.361389636993408
Validation loss: 2.0670786967841526

Epoch: 6| Step: 10
Training loss: 1.347306489944458
Validation loss: 2.0490904931099183

Epoch: 6| Step: 11
Training loss: 1.7870838642120361
Validation loss: 2.059265613555908

Epoch: 6| Step: 12
Training loss: 1.8484936952590942
Validation loss: 2.025729181945965

Epoch: 6| Step: 13
Training loss: 1.7924749851226807
Validation loss: 2.0726976574108167

Epoch: 136| Step: 0
Training loss: 1.9410852193832397
Validation loss: 2.0460279859522337

Epoch: 6| Step: 1
Training loss: 1.563666582107544
Validation loss: 2.052290816460886

Epoch: 6| Step: 2
Training loss: 2.6688318252563477
Validation loss: 2.0495077307506273

Epoch: 6| Step: 3
Training loss: 1.5230178833007812
Validation loss: 2.0543152286160375

Epoch: 6| Step: 4
Training loss: 2.020468235015869
Validation loss: 2.0453167038579143

Epoch: 6| Step: 5
Training loss: 2.280135154724121
Validation loss: 2.0564568965665755

Epoch: 6| Step: 6
Training loss: 2.0356690883636475
Validation loss: 2.0104850158896497

Epoch: 6| Step: 7
Training loss: 1.6708109378814697
Validation loss: 2.069244030983217

Epoch: 6| Step: 8
Training loss: 2.1846652030944824
Validation loss: 2.0501058229836087

Epoch: 6| Step: 9
Training loss: 2.1693942546844482
Validation loss: 2.0382311677420013

Epoch: 6| Step: 10
Training loss: 2.5176503658294678
Validation loss: 2.0180698235829673

Epoch: 6| Step: 11
Training loss: 2.4536890983581543
Validation loss: 2.036190745651081

Epoch: 6| Step: 12
Training loss: 2.302337169647217
Validation loss: 2.0412033642491987

Epoch: 6| Step: 13
Training loss: 2.564131021499634
Validation loss: 2.0432296722166

Epoch: 137| Step: 0
Training loss: 2.059392213821411
Validation loss: 2.0199778772169545

Epoch: 6| Step: 1
Training loss: 2.066826343536377
Validation loss: 2.028905266074724

Epoch: 6| Step: 2
Training loss: 2.0612564086914062
Validation loss: 2.0199496207698697

Epoch: 6| Step: 3
Training loss: 2.221754550933838
Validation loss: 2.0573460414845455

Epoch: 6| Step: 4
Training loss: 2.0299925804138184
Validation loss: 2.0117777470619447

Epoch: 6| Step: 5
Training loss: 2.815829277038574
Validation loss: 2.053352138047577

Epoch: 6| Step: 6
Training loss: 1.9357736110687256
Validation loss: 2.0480623783603793

Epoch: 6| Step: 7
Training loss: 1.7613447904586792
Validation loss: 2.0334630935422835

Epoch: 6| Step: 8
Training loss: 2.4553236961364746
Validation loss: 2.07281921755883

Epoch: 6| Step: 9
Training loss: 2.0974719524383545
Validation loss: 2.0299888862076627

Epoch: 6| Step: 10
Training loss: 2.6791434288024902
Validation loss: 2.0451719273803053

Epoch: 6| Step: 11
Training loss: 1.893659234046936
Validation loss: 2.035080054754852

Epoch: 6| Step: 12
Training loss: 1.727663278579712
Validation loss: 2.0594761115248486

Epoch: 6| Step: 13
Training loss: 1.8218048810958862
Validation loss: 2.0335246209175355

Epoch: 138| Step: 0
Training loss: 1.819087266921997
Validation loss: 2.0315476181686565

Epoch: 6| Step: 1
Training loss: 2.4895219802856445
Validation loss: 2.0404224562388595

Epoch: 6| Step: 2
Training loss: 1.9726568460464478
Validation loss: 2.0439930372340704

Epoch: 6| Step: 3
Training loss: 2.7500879764556885
Validation loss: 2.0613126011304956

Epoch: 6| Step: 4
Training loss: 1.8409796953201294
Validation loss: 2.0597861941142748

Epoch: 6| Step: 5
Training loss: 2.44514536857605
Validation loss: 2.0714346131970807

Epoch: 6| Step: 6
Training loss: 2.8957014083862305
Validation loss: 2.0426295803439234

Epoch: 6| Step: 7
Training loss: 2.000901222229004
Validation loss: 2.080745097129576

Epoch: 6| Step: 8
Training loss: 1.6336750984191895
Validation loss: 2.077365436861592

Epoch: 6| Step: 9
Training loss: 1.6766971349716187
Validation loss: 2.039405680471851

Epoch: 6| Step: 10
Training loss: 1.579329013824463
Validation loss: 2.0534429421988865

Epoch: 6| Step: 11
Training loss: 2.4464120864868164
Validation loss: 2.0201988348396878

Epoch: 6| Step: 12
Training loss: 2.2302980422973633
Validation loss: 2.0325653527372625

Epoch: 6| Step: 13
Training loss: 1.9136136770248413
Validation loss: 2.067919131248228

Epoch: 139| Step: 0
Training loss: 1.7281819581985474
Validation loss: 2.0659530316629717

Epoch: 6| Step: 1
Training loss: 2.3489725589752197
Validation loss: 2.038439543016495

Epoch: 6| Step: 2
Training loss: 2.631340503692627
Validation loss: 2.0395688395346365

Epoch: 6| Step: 3
Training loss: 1.750542402267456
Validation loss: 2.052238700210407

Epoch: 6| Step: 4
Training loss: 1.7852414846420288
Validation loss: 2.0605379253305416

Epoch: 6| Step: 5
Training loss: 2.660074234008789
Validation loss: 2.044886908223552

Epoch: 6| Step: 6
Training loss: 1.773331880569458
Validation loss: 2.068110563421762

Epoch: 6| Step: 7
Training loss: 2.1434192657470703
Validation loss: 2.0521409447475145

Epoch: 6| Step: 8
Training loss: 2.647056818008423
Validation loss: 2.0898718346831617

Epoch: 6| Step: 9
Training loss: 2.1106343269348145
Validation loss: 2.068716642677143

Epoch: 6| Step: 10
Training loss: 1.9974037408828735
Validation loss: 2.075321724337916

Epoch: 6| Step: 11
Training loss: 1.9105498790740967
Validation loss: 2.0662190324516705

Epoch: 6| Step: 12
Training loss: 1.6845555305480957
Validation loss: 2.0893309039454304

Epoch: 6| Step: 13
Training loss: 2.7536802291870117
Validation loss: 2.084655786073336

Epoch: 140| Step: 0
Training loss: 2.2319071292877197
Validation loss: 2.0966192906902683

Epoch: 6| Step: 1
Training loss: 1.8592860698699951
Validation loss: 2.0535949250703216

Epoch: 6| Step: 2
Training loss: 2.339977741241455
Validation loss: 2.0379342468835975

Epoch: 6| Step: 3
Training loss: 2.2529361248016357
Validation loss: 2.0505627893632457

Epoch: 6| Step: 4
Training loss: 1.6469042301177979
Validation loss: 2.071286993642007

Epoch: 6| Step: 5
Training loss: 1.8543087244033813
Validation loss: 2.098538035987526

Epoch: 6| Step: 6
Training loss: 2.163158893585205
Validation loss: 2.0805110905760076

Epoch: 6| Step: 7
Training loss: 2.327404737472534
Validation loss: 2.0463273845693117

Epoch: 6| Step: 8
Training loss: 2.8065707683563232
Validation loss: 2.0549183673756097

Epoch: 6| Step: 9
Training loss: 2.0355515480041504
Validation loss: 2.0454699711133073

Epoch: 6| Step: 10
Training loss: 2.1641407012939453
Validation loss: 2.0638776056228147

Epoch: 6| Step: 11
Training loss: 2.1718015670776367
Validation loss: 2.0503209560148177

Epoch: 6| Step: 12
Training loss: 1.754716157913208
Validation loss: 2.056907776863344

Epoch: 6| Step: 13
Training loss: 2.481707811355591
Validation loss: 2.033087773989606

Epoch: 141| Step: 0
Training loss: 1.8145675659179688
Validation loss: 2.0324437387527956

Epoch: 6| Step: 1
Training loss: 2.918084144592285
Validation loss: 2.0488681242030156

Epoch: 6| Step: 2
Training loss: 1.9191865921020508
Validation loss: 2.0178025153375443

Epoch: 6| Step: 3
Training loss: 2.159327745437622
Validation loss: 2.0109606891550045

Epoch: 6| Step: 4
Training loss: 1.8510630130767822
Validation loss: 2.042938147821734

Epoch: 6| Step: 5
Training loss: 2.4437661170959473
Validation loss: 2.0283892090602587

Epoch: 6| Step: 6
Training loss: 2.542041778564453
Validation loss: 2.0713609828743884

Epoch: 6| Step: 7
Training loss: 2.3188743591308594
Validation loss: 2.0723203971821773

Epoch: 6| Step: 8
Training loss: 2.0022261142730713
Validation loss: 2.0362705415295017

Epoch: 6| Step: 9
Training loss: 1.8902803659439087
Validation loss: 2.070748549635692

Epoch: 6| Step: 10
Training loss: 1.9180564880371094
Validation loss: 2.0560995199347056

Epoch: 6| Step: 11
Training loss: 2.177827835083008
Validation loss: 2.0836866927403275

Epoch: 6| Step: 12
Training loss: 2.252816677093506
Validation loss: 2.0574067869494037

Epoch: 6| Step: 13
Training loss: 1.2366706132888794
Validation loss: 2.033553018364855

Epoch: 142| Step: 0
Training loss: 2.1535091400146484
Validation loss: 2.0514728202614734

Epoch: 6| Step: 1
Training loss: 2.414409637451172
Validation loss: 2.060051138683032

Epoch: 6| Step: 2
Training loss: 2.3246212005615234
Validation loss: 2.0328328160829443

Epoch: 6| Step: 3
Training loss: 1.887271761894226
Validation loss: 2.063816806321503

Epoch: 6| Step: 4
Training loss: 1.8209038972854614
Validation loss: 2.019946059873027

Epoch: 6| Step: 5
Training loss: 2.0671894550323486
Validation loss: 2.0296951019635765

Epoch: 6| Step: 6
Training loss: 1.913461446762085
Validation loss: 2.045946882617089

Epoch: 6| Step: 7
Training loss: 2.4678750038146973
Validation loss: 2.0356285905325286

Epoch: 6| Step: 8
Training loss: 2.1400766372680664
Validation loss: 2.0254157153508996

Epoch: 6| Step: 9
Training loss: 2.2862467765808105
Validation loss: 2.029521293537591

Epoch: 6| Step: 10
Training loss: 2.1413986682891846
Validation loss: 2.0485375350521458

Epoch: 6| Step: 11
Training loss: 2.0828099250793457
Validation loss: 2.038161257261871

Epoch: 6| Step: 12
Training loss: 1.9486432075500488
Validation loss: 2.018500884373983

Epoch: 6| Step: 13
Training loss: 1.9548087120056152
Validation loss: 1.9981838836464831

Epoch: 143| Step: 0
Training loss: 1.9034264087677002
Validation loss: 2.030398202198808

Epoch: 6| Step: 1
Training loss: 2.010735034942627
Validation loss: 2.0379376565256426

Epoch: 6| Step: 2
Training loss: 1.6632182598114014
Validation loss: 2.053900190578994

Epoch: 6| Step: 3
Training loss: 1.7832449674606323
Validation loss: 2.0345659973800823

Epoch: 6| Step: 4
Training loss: 2.644228458404541
Validation loss: 2.0286466972802275

Epoch: 6| Step: 5
Training loss: 1.642082691192627
Validation loss: 2.0357148339671474

Epoch: 6| Step: 6
Training loss: 2.8584823608398438
Validation loss: 2.04880202457469

Epoch: 6| Step: 7
Training loss: 2.281351089477539
Validation loss: 2.011825253886561

Epoch: 6| Step: 8
Training loss: 1.8712477684020996
Validation loss: 2.0088469366873465

Epoch: 6| Step: 9
Training loss: 2.259903907775879
Validation loss: 2.045124110355172

Epoch: 6| Step: 10
Training loss: 2.10094952583313
Validation loss: 2.0413789287690194

Epoch: 6| Step: 11
Training loss: 2.723377227783203
Validation loss: 2.0373924778353785

Epoch: 6| Step: 12
Training loss: 1.6503125429153442
Validation loss: 2.0569009639883555

Epoch: 6| Step: 13
Training loss: 2.5693368911743164
Validation loss: 1.9970953182507587

Epoch: 144| Step: 0
Training loss: 2.81729793548584
Validation loss: 2.060678541019399

Epoch: 6| Step: 1
Training loss: 2.0606842041015625
Validation loss: 2.036719858005483

Epoch: 6| Step: 2
Training loss: 1.1481462717056274
Validation loss: 2.018425162120532

Epoch: 6| Step: 3
Training loss: 2.0542051792144775
Validation loss: 2.0524093258765435

Epoch: 6| Step: 4
Training loss: 2.3260746002197266
Validation loss: 2.015974183236399

Epoch: 6| Step: 5
Training loss: 1.2418913841247559
Validation loss: 2.029909039056429

Epoch: 6| Step: 6
Training loss: 2.1556668281555176
Validation loss: 2.0519967643163537

Epoch: 6| Step: 7
Training loss: 2.192852020263672
Validation loss: 2.0426463798810075

Epoch: 6| Step: 8
Training loss: 2.289752721786499
Validation loss: 2.059962567462716

Epoch: 6| Step: 9
Training loss: 2.122973918914795
Validation loss: 2.0219791730244956

Epoch: 6| Step: 10
Training loss: 1.9632072448730469
Validation loss: 2.0637234039204095

Epoch: 6| Step: 11
Training loss: 2.7234809398651123
Validation loss: 2.0424239353467057

Epoch: 6| Step: 12
Training loss: 2.5868887901306152
Validation loss: 2.045757665429064

Epoch: 6| Step: 13
Training loss: 2.3094770908355713
Validation loss: 2.068068908106896

Epoch: 145| Step: 0
Training loss: 2.2914772033691406
Validation loss: 2.073165060371481

Epoch: 6| Step: 1
Training loss: 2.1663196086883545
Validation loss: 2.0082572557592906

Epoch: 6| Step: 2
Training loss: 2.268003463745117
Validation loss: 2.0742587658666793

Epoch: 6| Step: 3
Training loss: 2.5230443477630615
Validation loss: 2.0798252731241207

Epoch: 6| Step: 4
Training loss: 2.2460341453552246
Validation loss: 2.0548338659348024

Epoch: 6| Step: 5
Training loss: 1.914045810699463
Validation loss: 2.0520873531218498

Epoch: 6| Step: 6
Training loss: 1.717306137084961
Validation loss: 2.0629376365292456

Epoch: 6| Step: 7
Training loss: 1.8526413440704346
Validation loss: 2.076867563750154

Epoch: 6| Step: 8
Training loss: 1.324767827987671
Validation loss: 2.0733364410297845

Epoch: 6| Step: 9
Training loss: 1.7676516771316528
Validation loss: 2.0675699390390867

Epoch: 6| Step: 10
Training loss: 2.4968369007110596
Validation loss: 2.078004688344976

Epoch: 6| Step: 11
Training loss: 2.4617395401000977
Validation loss: 2.075581201943018

Epoch: 6| Step: 12
Training loss: 1.8726532459259033
Validation loss: 2.0524971228773876

Epoch: 6| Step: 13
Training loss: 3.297207832336426
Validation loss: 2.0685068484275573

Epoch: 146| Step: 0
Training loss: 2.4052624702453613
Validation loss: 2.0383288065592446

Epoch: 6| Step: 1
Training loss: 1.9592089653015137
Validation loss: 2.072173556973857

Epoch: 6| Step: 2
Training loss: 1.4315605163574219
Validation loss: 2.0776730199013986

Epoch: 6| Step: 3
Training loss: 2.7094953060150146
Validation loss: 2.0589981617466098

Epoch: 6| Step: 4
Training loss: 2.3306427001953125
Validation loss: 2.0657077963634203

Epoch: 6| Step: 5
Training loss: 2.947230339050293
Validation loss: 2.0626157791383806

Epoch: 6| Step: 6
Training loss: 2.168060779571533
Validation loss: 2.0726249756351596

Epoch: 6| Step: 7
Training loss: 1.8807495832443237
Validation loss: 2.0210748898085726

Epoch: 6| Step: 8
Training loss: 2.041497230529785
Validation loss: 2.073795953104573

Epoch: 6| Step: 9
Training loss: 1.8818359375
Validation loss: 2.063123551748132

Epoch: 6| Step: 10
Training loss: 1.673649787902832
Validation loss: 2.075732320867559

Epoch: 6| Step: 11
Training loss: 1.9016444683074951
Validation loss: 2.0630707202419156

Epoch: 6| Step: 12
Training loss: 2.195145606994629
Validation loss: 2.0754118619426603

Epoch: 6| Step: 13
Training loss: 2.351997137069702
Validation loss: 2.0752210001791678

Epoch: 147| Step: 0
Training loss: 2.139641046524048
Validation loss: 2.0701528492794243

Epoch: 6| Step: 1
Training loss: 2.29850435256958
Validation loss: 2.034511708444165

Epoch: 6| Step: 2
Training loss: 2.1652889251708984
Validation loss: 2.016425060969527

Epoch: 6| Step: 3
Training loss: 1.9536120891571045
Validation loss: 1.997028172657054

Epoch: 6| Step: 4
Training loss: 2.3429694175720215
Validation loss: 2.0085726937940045

Epoch: 6| Step: 5
Training loss: 1.950479507446289
Validation loss: 2.0336684667935936

Epoch: 6| Step: 6
Training loss: 2.424689769744873
Validation loss: 2.0376864799889187

Epoch: 6| Step: 7
Training loss: 1.9689619541168213
Validation loss: 2.0333203897681287

Epoch: 6| Step: 8
Training loss: 2.192927122116089
Validation loss: 2.0392398911137737

Epoch: 6| Step: 9
Training loss: 1.940495491027832
Validation loss: 2.031731759348223

Epoch: 6| Step: 10
Training loss: 2.2817890644073486
Validation loss: 2.018539039037561

Epoch: 6| Step: 11
Training loss: 2.1965858936309814
Validation loss: 2.060674480212632

Epoch: 6| Step: 12
Training loss: 2.0535717010498047
Validation loss: 2.009224186661423

Epoch: 6| Step: 13
Training loss: 1.4148133993148804
Validation loss: 2.0758586224689277

Epoch: 148| Step: 0
Training loss: 2.214308261871338
Validation loss: 2.045153457631347

Epoch: 6| Step: 1
Training loss: 2.2822670936584473
Validation loss: 2.0535257939369447

Epoch: 6| Step: 2
Training loss: 2.0003488063812256
Validation loss: 2.0386312289904525

Epoch: 6| Step: 3
Training loss: 2.116101026535034
Validation loss: 2.070257741917846

Epoch: 6| Step: 4
Training loss: 2.297605514526367
Validation loss: 2.05099457053728

Epoch: 6| Step: 5
Training loss: 1.9595624208450317
Validation loss: 2.0275598033781974

Epoch: 6| Step: 6
Training loss: 2.1985809803009033
Validation loss: 2.0368640499730266

Epoch: 6| Step: 7
Training loss: 1.7687686681747437
Validation loss: 2.0790360704545052

Epoch: 6| Step: 8
Training loss: 2.3282175064086914
Validation loss: 2.0630783855274157

Epoch: 6| Step: 9
Training loss: 1.8178911209106445
Validation loss: 2.0375598810052358

Epoch: 6| Step: 10
Training loss: 2.349821090698242
Validation loss: 2.065041445916699

Epoch: 6| Step: 11
Training loss: 1.9249318838119507
Validation loss: 2.047188546067925

Epoch: 6| Step: 12
Training loss: 2.2856879234313965
Validation loss: 2.064684378203525

Epoch: 6| Step: 13
Training loss: 2.1484429836273193
Validation loss: 2.0429065791509484

Epoch: 149| Step: 0
Training loss: 2.2695159912109375
Validation loss: 2.0714375049837175

Epoch: 6| Step: 1
Training loss: 2.1866650581359863
Validation loss: 2.0781264484569593

Epoch: 6| Step: 2
Training loss: 1.5121817588806152
Validation loss: 2.0520251886819

Epoch: 6| Step: 3
Training loss: 2.1844122409820557
Validation loss: 2.0708038012186685

Epoch: 6| Step: 4
Training loss: 2.235502004623413
Validation loss: 2.064987463335837

Epoch: 6| Step: 5
Training loss: 2.3009989261627197
Validation loss: 2.075371612784683

Epoch: 6| Step: 6
Training loss: 2.6717967987060547
Validation loss: 2.076745653665194

Epoch: 6| Step: 7
Training loss: 1.9982306957244873
Validation loss: 2.0385666662646877

Epoch: 6| Step: 8
Training loss: 1.6513731479644775
Validation loss: 2.0655201071052143

Epoch: 6| Step: 9
Training loss: 2.0617103576660156
Validation loss: 2.0492316574178715

Epoch: 6| Step: 10
Training loss: 2.493197441101074
Validation loss: 2.0494754340059016

Epoch: 6| Step: 11
Training loss: 2.1779274940490723
Validation loss: 2.048969581562986

Epoch: 6| Step: 12
Training loss: 1.6569538116455078
Validation loss: 2.038423074189053

Epoch: 6| Step: 13
Training loss: 1.939136266708374
Validation loss: 2.0426317466202604

Epoch: 150| Step: 0
Training loss: 1.8093161582946777
Validation loss: 2.0348278194345455

Epoch: 6| Step: 1
Training loss: 1.7873984575271606
Validation loss: 2.0580057777384275

Epoch: 6| Step: 2
Training loss: 2.2607460021972656
Validation loss: 2.028257580213649

Epoch: 6| Step: 3
Training loss: 2.584782123565674
Validation loss: 2.052391626501596

Epoch: 6| Step: 4
Training loss: 2.052487850189209
Validation loss: 2.063314027683709

Epoch: 6| Step: 5
Training loss: 1.9957337379455566
Validation loss: 2.011488678634808

Epoch: 6| Step: 6
Training loss: 1.8957561254501343
Validation loss: 2.0570503537372877

Epoch: 6| Step: 7
Training loss: 2.238313674926758
Validation loss: 2.061810524232926

Epoch: 6| Step: 8
Training loss: 2.0540056228637695
Validation loss: 2.0286122624592116

Epoch: 6| Step: 9
Training loss: 2.1815309524536133
Validation loss: 2.018890097577085

Epoch: 6| Step: 10
Training loss: 2.9398927688598633
Validation loss: 2.049801638049464

Epoch: 6| Step: 11
Training loss: 2.5277600288391113
Validation loss: 2.0313530634808283

Epoch: 6| Step: 12
Training loss: 1.345693826675415
Validation loss: 2.0738221291572816

Epoch: 6| Step: 13
Training loss: 2.061674118041992
Validation loss: 2.05536913359037

Epoch: 151| Step: 0
Training loss: 2.4490909576416016
Validation loss: 2.039468057693974

Epoch: 6| Step: 1
Training loss: 1.4670679569244385
Validation loss: 2.037621223798362

Epoch: 6| Step: 2
Training loss: 2.3802285194396973
Validation loss: 2.0281051589596655

Epoch: 6| Step: 3
Training loss: 2.254368782043457
Validation loss: 2.0735939087406283

Epoch: 6| Step: 4
Training loss: 2.292020559310913
Validation loss: 2.0239441548624346

Epoch: 6| Step: 5
Training loss: 2.319399833679199
Validation loss: 2.0499432074126376

Epoch: 6| Step: 6
Training loss: 1.9652800559997559
Validation loss: 2.025472007771974

Epoch: 6| Step: 7
Training loss: 2.425607204437256
Validation loss: 2.0349856307429652

Epoch: 6| Step: 8
Training loss: 2.1228251457214355
Validation loss: 2.036332896960679

Epoch: 6| Step: 9
Training loss: 1.9429761171340942
Validation loss: 2.0582720977003857

Epoch: 6| Step: 10
Training loss: 2.2513256072998047
Validation loss: 2.040799339612325

Epoch: 6| Step: 11
Training loss: 2.39432692527771
Validation loss: 2.031426404112129

Epoch: 6| Step: 12
Training loss: 1.281765103340149
Validation loss: 2.0499235532617055

Epoch: 6| Step: 13
Training loss: 2.2398841381073
Validation loss: 2.0493961175282798

Epoch: 152| Step: 0
Training loss: 2.404222011566162
Validation loss: 2.042586238153519

Epoch: 6| Step: 1
Training loss: 1.7880301475524902
Validation loss: 1.9948716625090568

Epoch: 6| Step: 2
Training loss: 2.205960988998413
Validation loss: 2.0412077198746386

Epoch: 6| Step: 3
Training loss: 2.3875412940979004
Validation loss: 2.0606236380915486

Epoch: 6| Step: 4
Training loss: 2.2966785430908203
Validation loss: 2.033573149352945

Epoch: 6| Step: 5
Training loss: 1.4380342960357666
Validation loss: 2.045424463928387

Epoch: 6| Step: 6
Training loss: 1.7205653190612793
Validation loss: 2.074174065743723

Epoch: 6| Step: 7
Training loss: 1.9619282484054565
Validation loss: 2.0454252330205773

Epoch: 6| Step: 8
Training loss: 2.240096092224121
Validation loss: 2.0902123553778535

Epoch: 6| Step: 9
Training loss: 2.265778064727783
Validation loss: 2.063951205181819

Epoch: 6| Step: 10
Training loss: 1.8530797958374023
Validation loss: 2.0382409044491347

Epoch: 6| Step: 11
Training loss: 2.297121047973633
Validation loss: 2.03851560367051

Epoch: 6| Step: 12
Training loss: 2.503722667694092
Validation loss: 2.0461362254235054

Epoch: 6| Step: 13
Training loss: 1.9447100162506104
Validation loss: 2.046511785958403

Epoch: 153| Step: 0
Training loss: 2.173219919204712
Validation loss: 2.0816799671419206

Epoch: 6| Step: 1
Training loss: 2.01308012008667
Validation loss: 2.0786561581396286

Epoch: 6| Step: 2
Training loss: 1.6265785694122314
Validation loss: 2.0508627994086153

Epoch: 6| Step: 3
Training loss: 2.786154270172119
Validation loss: 2.0737072421658422

Epoch: 6| Step: 4
Training loss: 2.6126739978790283
Validation loss: 2.0460352743825605

Epoch: 6| Step: 5
Training loss: 2.0718250274658203
Validation loss: 2.0646752875338317

Epoch: 6| Step: 6
Training loss: 1.4328279495239258
Validation loss: 2.033773583750571

Epoch: 6| Step: 7
Training loss: 1.9467411041259766
Validation loss: 2.0857822818140828

Epoch: 6| Step: 8
Training loss: 2.619029998779297
Validation loss: 2.0799005877587105

Epoch: 6| Step: 9
Training loss: 2.209439516067505
Validation loss: 2.075557825385883

Epoch: 6| Step: 10
Training loss: 1.6811988353729248
Validation loss: 2.064180438236524

Epoch: 6| Step: 11
Training loss: 1.7389929294586182
Validation loss: 2.062985656081989

Epoch: 6| Step: 12
Training loss: 2.7281203269958496
Validation loss: 2.0372484909590853

Epoch: 6| Step: 13
Training loss: 1.4293015003204346
Validation loss: 2.0767135184298278

Epoch: 154| Step: 0
Training loss: 1.6682602167129517
Validation loss: 2.0640226205190024

Epoch: 6| Step: 1
Training loss: 2.7447214126586914
Validation loss: 2.0581600948046614

Epoch: 6| Step: 2
Training loss: 2.265995502471924
Validation loss: 2.059590216605894

Epoch: 6| Step: 3
Training loss: 2.0523910522460938
Validation loss: 2.0719929100364767

Epoch: 6| Step: 4
Training loss: 1.9893920421600342
Validation loss: 2.0613747604431643

Epoch: 6| Step: 5
Training loss: 2.8194260597229004
Validation loss: 2.102333435448267

Epoch: 6| Step: 6
Training loss: 2.013733386993408
Validation loss: 2.043382329325522

Epoch: 6| Step: 7
Training loss: 2.5385208129882812
Validation loss: 2.0561522770953435

Epoch: 6| Step: 8
Training loss: 1.9433436393737793
Validation loss: 2.0348474620490946

Epoch: 6| Step: 9
Training loss: 1.8179607391357422
Validation loss: 2.0324171691812496

Epoch: 6| Step: 10
Training loss: 1.7852579355239868
Validation loss: 2.0398528639988234

Epoch: 6| Step: 11
Training loss: 2.3295388221740723
Validation loss: 2.0561188241486907

Epoch: 6| Step: 12
Training loss: 1.5889317989349365
Validation loss: 2.020610726007851

Epoch: 6| Step: 13
Training loss: 1.909757137298584
Validation loss: 2.0195995325683267

Epoch: 155| Step: 0
Training loss: 2.5781338214874268
Validation loss: 2.0355962553331928

Epoch: 6| Step: 1
Training loss: 2.240743637084961
Validation loss: 2.0396262497030277

Epoch: 6| Step: 2
Training loss: 2.146286964416504
Validation loss: 2.0286240167515253

Epoch: 6| Step: 3
Training loss: 2.1586170196533203
Validation loss: 2.0710322574902604

Epoch: 6| Step: 4
Training loss: 2.0235886573791504
Validation loss: 2.0877349889406593

Epoch: 6| Step: 5
Training loss: 2.2699997425079346
Validation loss: 2.06517118536016

Epoch: 6| Step: 6
Training loss: 1.9218392372131348
Validation loss: 2.0903451365809285

Epoch: 6| Step: 7
Training loss: 2.210142135620117
Validation loss: 2.0583994080943446

Epoch: 6| Step: 8
Training loss: 1.3436694145202637
Validation loss: 2.0536975834959295

Epoch: 6| Step: 9
Training loss: 1.7220096588134766
Validation loss: 2.0535508483968754

Epoch: 6| Step: 10
Training loss: 2.2368741035461426
Validation loss: 2.067803572582942

Epoch: 6| Step: 11
Training loss: 2.4487266540527344
Validation loss: 2.073156637530173

Epoch: 6| Step: 12
Training loss: 1.857114553451538
Validation loss: 2.0588606916448122

Epoch: 6| Step: 13
Training loss: 2.2917864322662354
Validation loss: 2.068163897401543

Epoch: 156| Step: 0
Training loss: 1.9761717319488525
Validation loss: 2.0817471447811333

Epoch: 6| Step: 1
Training loss: 2.2576539516448975
Validation loss: 2.0572495345146424

Epoch: 6| Step: 2
Training loss: 2.5545356273651123
Validation loss: 2.0990196274172876

Epoch: 6| Step: 3
Training loss: 1.8212366104125977
Validation loss: 2.043626368686717

Epoch: 6| Step: 4
Training loss: 2.6461291313171387
Validation loss: 2.0579174026366203

Epoch: 6| Step: 5
Training loss: 1.6609513759613037
Validation loss: 2.0161873653370845

Epoch: 6| Step: 6
Training loss: 1.9239171743392944
Validation loss: 2.0827385943423034

Epoch: 6| Step: 7
Training loss: 1.8743877410888672
Validation loss: 2.0618801129761564

Epoch: 6| Step: 8
Training loss: 1.8471062183380127
Validation loss: 2.0817798696538454

Epoch: 6| Step: 9
Training loss: 1.8388737440109253
Validation loss: 2.0481585225751324

Epoch: 6| Step: 10
Training loss: 2.2138559818267822
Validation loss: 2.068403151727492

Epoch: 6| Step: 11
Training loss: 2.677427291870117
Validation loss: 2.032243444073585

Epoch: 6| Step: 12
Training loss: 2.383502960205078
Validation loss: 2.0333506432912682

Epoch: 6| Step: 13
Training loss: 1.5654090642929077
Validation loss: 2.0270751842888455

Epoch: 157| Step: 0
Training loss: 1.7900123596191406
Validation loss: 2.0277395274049494

Epoch: 6| Step: 1
Training loss: 1.7337013483047485
Validation loss: 2.02680524446631

Epoch: 6| Step: 2
Training loss: 2.2237539291381836
Validation loss: 2.042268301851006

Epoch: 6| Step: 3
Training loss: 3.0048604011535645
Validation loss: 2.056393955343513

Epoch: 6| Step: 4
Training loss: 1.8847218751907349
Validation loss: 2.0235698928115187

Epoch: 6| Step: 5
Training loss: 3.0515024662017822
Validation loss: 2.0739820349601006

Epoch: 6| Step: 6
Training loss: 1.4916646480560303
Validation loss: 2.0308573681821107

Epoch: 6| Step: 7
Training loss: 1.928165078163147
Validation loss: 2.0578835664256925

Epoch: 6| Step: 8
Training loss: 2.2191262245178223
Validation loss: 2.0520572380353044

Epoch: 6| Step: 9
Training loss: 2.109997272491455
Validation loss: 2.021986824209972

Epoch: 6| Step: 10
Training loss: 1.9195581674575806
Validation loss: 2.039918622662944

Epoch: 6| Step: 11
Training loss: 2.5521464347839355
Validation loss: 2.0477059989847164

Epoch: 6| Step: 12
Training loss: 1.640612006187439
Validation loss: 2.042501966158549

Epoch: 6| Step: 13
Training loss: 2.026392698287964
Validation loss: 2.031786039311399

Epoch: 158| Step: 0
Training loss: 1.8106695413589478
Validation loss: 2.0084142326026835

Epoch: 6| Step: 1
Training loss: 1.8445345163345337
Validation loss: 2.0421267171059885

Epoch: 6| Step: 2
Training loss: 1.4389511346817017
Validation loss: 2.033367536401236

Epoch: 6| Step: 3
Training loss: 2.0239272117614746
Validation loss: 2.009484657677271

Epoch: 6| Step: 4
Training loss: 2.4018542766571045
Validation loss: 2.0795425227893296

Epoch: 6| Step: 5
Training loss: 2.5484066009521484
Validation loss: 2.0629925420207362

Epoch: 6| Step: 6
Training loss: 2.071967124938965
Validation loss: 2.066611155386894

Epoch: 6| Step: 7
Training loss: 1.8776708841323853
Validation loss: 2.0674917928634153

Epoch: 6| Step: 8
Training loss: 3.069864511489868
Validation loss: 2.0409214599158174

Epoch: 6| Step: 9
Training loss: 2.3178746700286865
Validation loss: 2.0492671484588296

Epoch: 6| Step: 10
Training loss: 1.380955457687378
Validation loss: 2.0442738110019314

Epoch: 6| Step: 11
Training loss: 2.8487701416015625
Validation loss: 2.0478727996990247

Epoch: 6| Step: 12
Training loss: 1.7013269662857056
Validation loss: 2.0678352604630175

Epoch: 6| Step: 13
Training loss: 2.0472612380981445
Validation loss: 2.0300889015197754

Epoch: 159| Step: 0
Training loss: 1.876617193222046
Validation loss: 2.073246132942938

Epoch: 6| Step: 1
Training loss: 2.4671502113342285
Validation loss: 2.0835806951727918

Epoch: 6| Step: 2
Training loss: 2.7365617752075195
Validation loss: 2.0791891838914607

Epoch: 6| Step: 3
Training loss: 1.8340647220611572
Validation loss: 2.0419828712299304

Epoch: 6| Step: 4
Training loss: 2.1625897884368896
Validation loss: 2.0605718076870008

Epoch: 6| Step: 5
Training loss: 2.082242727279663
Validation loss: 2.047686605043309

Epoch: 6| Step: 6
Training loss: 2.4036436080932617
Validation loss: 2.067316403952978

Epoch: 6| Step: 7
Training loss: 2.1311960220336914
Validation loss: 2.0604469827426377

Epoch: 6| Step: 8
Training loss: 1.7971502542495728
Validation loss: 2.0667790456484725

Epoch: 6| Step: 9
Training loss: 2.116009473800659
Validation loss: 2.062362786262266

Epoch: 6| Step: 10
Training loss: 1.8028459548950195
Validation loss: 2.0334408744688957

Epoch: 6| Step: 11
Training loss: 2.236933469772339
Validation loss: 2.084395953404006

Epoch: 6| Step: 12
Training loss: 2.531968116760254
Validation loss: 2.0425213229271675

Epoch: 6| Step: 13
Training loss: 1.2384390830993652
Validation loss: 2.0688948028831073

Epoch: 160| Step: 0
Training loss: 1.9019110202789307
Validation loss: 2.066453203078239

Epoch: 6| Step: 1
Training loss: 2.1714558601379395
Validation loss: 2.05363485505504

Epoch: 6| Step: 2
Training loss: 2.5255699157714844
Validation loss: 2.03168850047614

Epoch: 6| Step: 3
Training loss: 1.4915297031402588
Validation loss: 2.0531823994011007

Epoch: 6| Step: 4
Training loss: 1.8044722080230713
Validation loss: 2.0449478498069187

Epoch: 6| Step: 5
Training loss: 2.315336227416992
Validation loss: 2.0696379702578307

Epoch: 6| Step: 6
Training loss: 2.033806800842285
Validation loss: 2.0202375893951743

Epoch: 6| Step: 7
Training loss: 1.8833163976669312
Validation loss: 2.0655057430267334

Epoch: 6| Step: 8
Training loss: 2.6282639503479004
Validation loss: 2.0411275676501694

Epoch: 6| Step: 9
Training loss: 2.1888325214385986
Validation loss: 2.0152009751207087

Epoch: 6| Step: 10
Training loss: 2.6225593090057373
Validation loss: 2.0842470635649977

Epoch: 6| Step: 11
Training loss: 1.722064733505249
Validation loss: 2.0618741307207333

Epoch: 6| Step: 12
Training loss: 1.966845989227295
Validation loss: 1.9943361692531134

Epoch: 6| Step: 13
Training loss: 2.1715011596679688
Validation loss: 2.055082339112477

Epoch: 161| Step: 0
Training loss: 1.5662943124771118
Validation loss: 2.023668860876432

Epoch: 6| Step: 1
Training loss: 1.8565115928649902
Validation loss: 2.0266018298364457

Epoch: 6| Step: 2
Training loss: 2.9356441497802734
Validation loss: 2.040630136766741

Epoch: 6| Step: 3
Training loss: 2.026597023010254
Validation loss: 2.028836001632034

Epoch: 6| Step: 4
Training loss: 2.324009656906128
Validation loss: 2.0481196193284887

Epoch: 6| Step: 5
Training loss: 1.804734468460083
Validation loss: 2.029569434863265

Epoch: 6| Step: 6
Training loss: 1.984438180923462
Validation loss: 2.043284308525824

Epoch: 6| Step: 7
Training loss: 2.2895984649658203
Validation loss: 2.016886877757247

Epoch: 6| Step: 8
Training loss: 2.530946731567383
Validation loss: 2.0444664262956187

Epoch: 6| Step: 9
Training loss: 2.201143264770508
Validation loss: 2.039774471713651

Epoch: 6| Step: 10
Training loss: 1.4640250205993652
Validation loss: 2.0522373491717922

Epoch: 6| Step: 11
Training loss: 2.1226634979248047
Validation loss: 1.9980631720635198

Epoch: 6| Step: 12
Training loss: 1.8362571001052856
Validation loss: 2.01516144762757

Epoch: 6| Step: 13
Training loss: 3.083078145980835
Validation loss: 2.0083067186417116

Epoch: 162| Step: 0
Training loss: 2.0223922729492188
Validation loss: 2.037324574685866

Epoch: 6| Step: 1
Training loss: 2.9980721473693848
Validation loss: 2.023311215062295

Epoch: 6| Step: 2
Training loss: 1.966749906539917
Validation loss: 2.024899895473193

Epoch: 6| Step: 3
Training loss: 2.011141538619995
Validation loss: 2.049729229301535

Epoch: 6| Step: 4
Training loss: 2.044588804244995
Validation loss: 2.010435258188555

Epoch: 6| Step: 5
Training loss: 2.0247247219085693
Validation loss: 2.0036640321054766

Epoch: 6| Step: 6
Training loss: 1.8841333389282227
Validation loss: 2.0174008595046176

Epoch: 6| Step: 7
Training loss: 1.832869052886963
Validation loss: 1.9698326831222863

Epoch: 6| Step: 8
Training loss: 2.6444714069366455
Validation loss: 2.0244606207775813

Epoch: 6| Step: 9
Training loss: 2.178999662399292
Validation loss: 2.0213875129658687

Epoch: 6| Step: 10
Training loss: 2.165865898132324
Validation loss: 2.028016985103648

Epoch: 6| Step: 11
Training loss: 1.6659404039382935
Validation loss: 2.0555615745564944

Epoch: 6| Step: 12
Training loss: 2.0405020713806152
Validation loss: 2.0204951942607923

Epoch: 6| Step: 13
Training loss: 1.622266173362732
Validation loss: 2.0137922020368677

Epoch: 163| Step: 0
Training loss: 2.5212390422821045
Validation loss: 2.042338917332311

Epoch: 6| Step: 1
Training loss: 2.1396188735961914
Validation loss: 2.061358105751776

Epoch: 6| Step: 2
Training loss: 1.678422212600708
Validation loss: 2.031019422315782

Epoch: 6| Step: 3
Training loss: 1.895278811454773
Validation loss: 2.0538826886043755

Epoch: 6| Step: 4
Training loss: 1.6290358304977417
Validation loss: 2.0574140830706527

Epoch: 6| Step: 5
Training loss: 1.3174493312835693
Validation loss: 2.0791850705300607

Epoch: 6| Step: 6
Training loss: 2.267169713973999
Validation loss: 2.070451969741493

Epoch: 6| Step: 7
Training loss: 2.025641441345215
Validation loss: 2.074200380233026

Epoch: 6| Step: 8
Training loss: 1.8210207223892212
Validation loss: 2.0500890901011806

Epoch: 6| Step: 9
Training loss: 2.2488515377044678
Validation loss: 2.0381904455923263

Epoch: 6| Step: 10
Training loss: 2.3223624229431152
Validation loss: 2.104531068955698

Epoch: 6| Step: 11
Training loss: 2.562102794647217
Validation loss: 2.069632609685262

Epoch: 6| Step: 12
Training loss: 2.354407787322998
Validation loss: 2.039414941623647

Epoch: 6| Step: 13
Training loss: 3.0624711513519287
Validation loss: 2.0565110457840787

Epoch: 164| Step: 0
Training loss: 2.1059632301330566
Validation loss: 2.0322400985225553

Epoch: 6| Step: 1
Training loss: 1.6822302341461182
Validation loss: 2.0555919677980485

Epoch: 6| Step: 2
Training loss: 1.8149529695510864
Validation loss: 2.096627909650085

Epoch: 6| Step: 3
Training loss: 1.9911346435546875
Validation loss: 2.06043259046411

Epoch: 6| Step: 4
Training loss: 3.132539749145508
Validation loss: 2.0678514665172947

Epoch: 6| Step: 5
Training loss: 1.7332490682601929
Validation loss: 2.053834189650833

Epoch: 6| Step: 6
Training loss: 2.124270439147949
Validation loss: 2.0567825071273313

Epoch: 6| Step: 7
Training loss: 2.213843822479248
Validation loss: 2.0518112772254535

Epoch: 6| Step: 8
Training loss: 2.2549970149993896
Validation loss: 2.052591053388452

Epoch: 6| Step: 9
Training loss: 2.36979341506958
Validation loss: 2.0745722606617916

Epoch: 6| Step: 10
Training loss: 2.080249071121216
Validation loss: 2.054723487105421

Epoch: 6| Step: 11
Training loss: 1.4547005891799927
Validation loss: 2.063593687549714

Epoch: 6| Step: 12
Training loss: 2.1094348430633545
Validation loss: 2.071644941965739

Epoch: 6| Step: 13
Training loss: 2.1173887252807617
Validation loss: 2.058775478793729

Epoch: 165| Step: 0
Training loss: 2.113424777984619
Validation loss: 2.070708123586511

Epoch: 6| Step: 1
Training loss: 1.7996963262557983
Validation loss: 2.0229533372386808

Epoch: 6| Step: 2
Training loss: 2.1013264656066895
Validation loss: 2.0259791394715667

Epoch: 6| Step: 3
Training loss: 2.0326669216156006
Validation loss: 2.0318715290356706

Epoch: 6| Step: 4
Training loss: 1.4496535062789917
Validation loss: 2.03337069865196

Epoch: 6| Step: 5
Training loss: 1.882519245147705
Validation loss: 2.032374617873981

Epoch: 6| Step: 6
Training loss: 2.185469388961792
Validation loss: 2.058448786376625

Epoch: 6| Step: 7
Training loss: 2.0496115684509277
Validation loss: 2.0298531363087315

Epoch: 6| Step: 8
Training loss: 2.3235745429992676
Validation loss: 2.0031444372669345

Epoch: 6| Step: 9
Training loss: 2.6873397827148438
Validation loss: 2.038619408043482

Epoch: 6| Step: 10
Training loss: 2.5013442039489746
Validation loss: 2.0201875522572506

Epoch: 6| Step: 11
Training loss: 2.1255979537963867
Validation loss: 2.045411717507147

Epoch: 6| Step: 12
Training loss: 1.466212511062622
Validation loss: 2.056741242767662

Epoch: 6| Step: 13
Training loss: 3.175445318222046
Validation loss: 2.0476171842185398

Epoch: 166| Step: 0
Training loss: 2.407036066055298
Validation loss: 2.0453669255779636

Epoch: 6| Step: 1
Training loss: 1.8704103231430054
Validation loss: 2.0230089131221978

Epoch: 6| Step: 2
Training loss: 1.9937716722488403
Validation loss: 2.0284056586603962

Epoch: 6| Step: 3
Training loss: 2.282356023788452
Validation loss: 2.0332747095374653

Epoch: 6| Step: 4
Training loss: 2.5212786197662354
Validation loss: 2.085802388447587

Epoch: 6| Step: 5
Training loss: 1.9848459959030151
Validation loss: 2.044694439057381

Epoch: 6| Step: 6
Training loss: 1.6965347528457642
Validation loss: 2.0213102512462164

Epoch: 6| Step: 7
Training loss: 1.7821167707443237
Validation loss: 2.031420769230012

Epoch: 6| Step: 8
Training loss: 1.9675023555755615
Validation loss: 2.0181994117716306

Epoch: 6| Step: 9
Training loss: 1.9033066034317017
Validation loss: 2.0631110027272213

Epoch: 6| Step: 10
Training loss: 1.9147858619689941
Validation loss: 2.0749418350958053

Epoch: 6| Step: 11
Training loss: 2.814850091934204
Validation loss: 2.0859737447513047

Epoch: 6| Step: 12
Training loss: 2.3127245903015137
Validation loss: 2.047164652937202

Epoch: 6| Step: 13
Training loss: 1.678481936454773
Validation loss: 2.072902306433647

Epoch: 167| Step: 0
Training loss: 1.7631316184997559
Validation loss: 2.0784396227969917

Epoch: 6| Step: 1
Training loss: 2.7473998069763184
Validation loss: 2.0541066802958006

Epoch: 6| Step: 2
Training loss: 1.8667371273040771
Validation loss: 2.065864525815492

Epoch: 6| Step: 3
Training loss: 1.6826255321502686
Validation loss: 2.06155179777453

Epoch: 6| Step: 4
Training loss: 1.3440957069396973
Validation loss: 2.0471299130429506

Epoch: 6| Step: 5
Training loss: 2.2746214866638184
Validation loss: 2.04221438336116

Epoch: 6| Step: 6
Training loss: 3.1528491973876953
Validation loss: 2.027444433140498

Epoch: 6| Step: 7
Training loss: 2.2667267322540283
Validation loss: 2.0435628532081522

Epoch: 6| Step: 8
Training loss: 2.0563149452209473
Validation loss: 2.027284468373945

Epoch: 6| Step: 9
Training loss: 2.3503293991088867
Validation loss: 2.067835589890839

Epoch: 6| Step: 10
Training loss: 1.6801645755767822
Validation loss: 2.0661304150858233

Epoch: 6| Step: 11
Training loss: 2.4039676189422607
Validation loss: 2.0677078475234327

Epoch: 6| Step: 12
Training loss: 1.7243717908859253
Validation loss: 2.0374640264818744

Epoch: 6| Step: 13
Training loss: 1.9183557033538818
Validation loss: 2.052998237712409

Epoch: 168| Step: 0
Training loss: 1.788496494293213
Validation loss: 2.0686193486695648

Epoch: 6| Step: 1
Training loss: 2.226926326751709
Validation loss: 2.062600040948519

Epoch: 6| Step: 2
Training loss: 1.944921612739563
Validation loss: 2.0197174485011766

Epoch: 6| Step: 3
Training loss: 1.8875120878219604
Validation loss: 2.041183743425595

Epoch: 6| Step: 4
Training loss: 2.7087414264678955
Validation loss: 2.067569717284172

Epoch: 6| Step: 5
Training loss: 2.317528247833252
Validation loss: 2.07113459802443

Epoch: 6| Step: 6
Training loss: 2.330379009246826
Validation loss: 2.04025008345163

Epoch: 6| Step: 7
Training loss: 1.8539326190948486
Validation loss: 2.072441613802346

Epoch: 6| Step: 8
Training loss: 2.1519198417663574
Validation loss: 2.019734431338567

Epoch: 6| Step: 9
Training loss: 1.7027511596679688
Validation loss: 2.0827169110698085

Epoch: 6| Step: 10
Training loss: 1.7794245481491089
Validation loss: 2.0778917215203725

Epoch: 6| Step: 11
Training loss: 2.4499106407165527
Validation loss: 2.0376729375572613

Epoch: 6| Step: 12
Training loss: 1.5120477676391602
Validation loss: 2.0382634478230632

Epoch: 6| Step: 13
Training loss: 2.689666271209717
Validation loss: 2.0390809415489115

Epoch: 169| Step: 0
Training loss: 2.153541088104248
Validation loss: 2.036476537745486

Epoch: 6| Step: 1
Training loss: 2.4115707874298096
Validation loss: 2.049023461598222

Epoch: 6| Step: 2
Training loss: 2.3077445030212402
Validation loss: 2.054669455815387

Epoch: 6| Step: 3
Training loss: 1.7745100259780884
Validation loss: 2.029183610793083

Epoch: 6| Step: 4
Training loss: 2.2653846740722656
Validation loss: 2.0277995870959376

Epoch: 6| Step: 5
Training loss: 2.256114959716797
Validation loss: 2.038007297823506

Epoch: 6| Step: 6
Training loss: 2.4150874614715576
Validation loss: 2.0658843722394717

Epoch: 6| Step: 7
Training loss: 1.610844612121582
Validation loss: 2.0415642646051224

Epoch: 6| Step: 8
Training loss: 1.6025575399398804
Validation loss: 2.0646870995080597

Epoch: 6| Step: 9
Training loss: 2.1665849685668945
Validation loss: 2.0503998161644064

Epoch: 6| Step: 10
Training loss: 2.039647340774536
Validation loss: 2.0714698671012797

Epoch: 6| Step: 11
Training loss: 2.6991000175476074
Validation loss: 2.0498140255610147

Epoch: 6| Step: 12
Training loss: 1.6846156120300293
Validation loss: 2.063619222692264

Epoch: 6| Step: 13
Training loss: 1.6141031980514526
Validation loss: 2.0544755651104833

Epoch: 170| Step: 0
Training loss: 1.8426148891448975
Validation loss: 2.051353734026673

Epoch: 6| Step: 1
Training loss: 3.036614179611206
Validation loss: 2.069049401949811

Epoch: 6| Step: 2
Training loss: 1.8848683834075928
Validation loss: 2.0362757200835855

Epoch: 6| Step: 3
Training loss: 1.8794432878494263
Validation loss: 2.0888394437810427

Epoch: 6| Step: 4
Training loss: 1.6903568506240845
Validation loss: 2.0621299666743123

Epoch: 6| Step: 5
Training loss: 2.3930985927581787
Validation loss: 2.042964735338765

Epoch: 6| Step: 6
Training loss: 2.3698348999023438
Validation loss: 2.025979300980927

Epoch: 6| Step: 7
Training loss: 1.2551777362823486
Validation loss: 2.0662509523412234

Epoch: 6| Step: 8
Training loss: 2.014538049697876
Validation loss: 2.0572177466525825

Epoch: 6| Step: 9
Training loss: 2.079169511795044
Validation loss: 2.009315070285592

Epoch: 6| Step: 10
Training loss: 2.437323570251465
Validation loss: 2.025422448753029

Epoch: 6| Step: 11
Training loss: 2.0122783184051514
Validation loss: 2.0610130243403937

Epoch: 6| Step: 12
Training loss: 2.124452590942383
Validation loss: 2.055437200812883

Epoch: 6| Step: 13
Training loss: 1.4864555597305298
Validation loss: 2.021002365696815

Epoch: 171| Step: 0
Training loss: 1.697442650794983
Validation loss: 2.0820647132012153

Epoch: 6| Step: 1
Training loss: 1.9366230964660645
Validation loss: 2.027934971676078

Epoch: 6| Step: 2
Training loss: 2.8010151386260986
Validation loss: 2.0691233091456915

Epoch: 6| Step: 3
Training loss: 2.4162087440490723
Validation loss: 2.067873171580735

Epoch: 6| Step: 4
Training loss: 1.7002543210983276
Validation loss: 2.079776492170108

Epoch: 6| Step: 5
Training loss: 2.158193588256836
Validation loss: 2.0509309794313166

Epoch: 6| Step: 6
Training loss: 2.017500400543213
Validation loss: 2.0638670741870837

Epoch: 6| Step: 7
Training loss: 2.873668670654297
Validation loss: 2.0542862107676845

Epoch: 6| Step: 8
Training loss: 2.270170211791992
Validation loss: 2.0244518595357097

Epoch: 6| Step: 9
Training loss: 1.7870800495147705
Validation loss: 2.0177953038164365

Epoch: 6| Step: 10
Training loss: 1.6466219425201416
Validation loss: 2.04558712949035

Epoch: 6| Step: 11
Training loss: 1.7567678689956665
Validation loss: 2.0517591994295836

Epoch: 6| Step: 12
Training loss: 1.6279865503311157
Validation loss: 2.013111132447438

Epoch: 6| Step: 13
Training loss: 2.610321044921875
Validation loss: 2.068845510482788

Epoch: 172| Step: 0
Training loss: 2.663975238800049
Validation loss: 2.05541213609839

Epoch: 6| Step: 1
Training loss: 2.056936502456665
Validation loss: 2.049844657221148

Epoch: 6| Step: 2
Training loss: 2.105592727661133
Validation loss: 2.050152127460767

Epoch: 6| Step: 3
Training loss: 2.292642593383789
Validation loss: 2.0306178216011292

Epoch: 6| Step: 4
Training loss: 1.9230928421020508
Validation loss: 2.0465827911130843

Epoch: 6| Step: 5
Training loss: 2.2028136253356934
Validation loss: 2.0029940451345136

Epoch: 6| Step: 6
Training loss: 2.3525967597961426
Validation loss: 2.0331531006802797

Epoch: 6| Step: 7
Training loss: 1.8658885955810547
Validation loss: 2.045284234067445

Epoch: 6| Step: 8
Training loss: 1.6347535848617554
Validation loss: 2.0415326805524927

Epoch: 6| Step: 9
Training loss: 2.4258687496185303
Validation loss: 2.0662639628174486

Epoch: 6| Step: 10
Training loss: 2.4483327865600586
Validation loss: 2.0568469493619856

Epoch: 6| Step: 11
Training loss: 2.1928701400756836
Validation loss: 2.070555320350073

Epoch: 6| Step: 12
Training loss: 1.6176711320877075
Validation loss: 2.0788654396610875

Epoch: 6| Step: 13
Training loss: 1.1644740104675293
Validation loss: 2.1151116048136065

Epoch: 173| Step: 0
Training loss: 1.3955650329589844
Validation loss: 2.087870849076138

Epoch: 6| Step: 1
Training loss: 2.571969747543335
Validation loss: 2.0504116396750174

Epoch: 6| Step: 2
Training loss: 2.0121912956237793
Validation loss: 2.073838791539592

Epoch: 6| Step: 3
Training loss: 1.549124836921692
Validation loss: 2.0570773822005077

Epoch: 6| Step: 4
Training loss: 2.249776840209961
Validation loss: 2.093422069344469

Epoch: 6| Step: 5
Training loss: 1.9996870756149292
Validation loss: 2.0569696067481913

Epoch: 6| Step: 6
Training loss: 2.1737799644470215
Validation loss: 2.0644374021919827

Epoch: 6| Step: 7
Training loss: 2.4672787189483643
Validation loss: 2.0442382212608092

Epoch: 6| Step: 8
Training loss: 2.030439853668213
Validation loss: 2.0648249221104447

Epoch: 6| Step: 9
Training loss: 2.153399705886841
Validation loss: 2.0887421536189255

Epoch: 6| Step: 10
Training loss: 2.123316526412964
Validation loss: 2.063587778358049

Epoch: 6| Step: 11
Training loss: 2.640681505203247
Validation loss: 2.048738697523712

Epoch: 6| Step: 12
Training loss: 1.7994036674499512
Validation loss: 2.049678348725842

Epoch: 6| Step: 13
Training loss: 1.7764248847961426
Validation loss: 2.0456010910772506

Epoch: 174| Step: 0
Training loss: 1.6809300184249878
Validation loss: 2.039370157385385

Epoch: 6| Step: 1
Training loss: 2.2705414295196533
Validation loss: 2.0598419789345033

Epoch: 6| Step: 2
Training loss: 2.3297505378723145
Validation loss: 2.029835279269885

Epoch: 6| Step: 3
Training loss: 2.4392616748809814
Validation loss: 2.0555554564281175

Epoch: 6| Step: 4
Training loss: 1.7823569774627686
Validation loss: 2.035409060857629

Epoch: 6| Step: 5
Training loss: 2.0860304832458496
Validation loss: 2.024359840218739

Epoch: 6| Step: 6
Training loss: 2.469799280166626
Validation loss: 2.043961440363238

Epoch: 6| Step: 7
Training loss: 2.1807971000671387
Validation loss: 2.0812786086913078

Epoch: 6| Step: 8
Training loss: 2.319436550140381
Validation loss: 2.0344031164723058

Epoch: 6| Step: 9
Training loss: 2.027480363845825
Validation loss: 2.067894540807252

Epoch: 6| Step: 10
Training loss: 1.849584698677063
Validation loss: 2.0507376809273996

Epoch: 6| Step: 11
Training loss: 1.850516676902771
Validation loss: 2.0616687420875794

Epoch: 6| Step: 12
Training loss: 1.863851547241211
Validation loss: 2.0716509280666227

Epoch: 6| Step: 13
Training loss: 2.212266445159912
Validation loss: 2.037958695042518

Epoch: 175| Step: 0
Training loss: 2.3464853763580322
Validation loss: 2.0570672160835675

Epoch: 6| Step: 1
Training loss: 2.262424945831299
Validation loss: 2.0335351408168836

Epoch: 6| Step: 2
Training loss: 2.0573792457580566
Validation loss: 2.060239052259794

Epoch: 6| Step: 3
Training loss: 2.331317901611328
Validation loss: 2.045421108122795

Epoch: 6| Step: 4
Training loss: 1.2555294036865234
Validation loss: 2.0390145778656006

Epoch: 6| Step: 5
Training loss: 2.0392191410064697
Validation loss: 2.0515965774495113

Epoch: 6| Step: 6
Training loss: 2.515638828277588
Validation loss: 2.0354753655772053

Epoch: 6| Step: 7
Training loss: 1.734195351600647
Validation loss: 2.067900619199199

Epoch: 6| Step: 8
Training loss: 1.6398134231567383
Validation loss: 2.037684444458254

Epoch: 6| Step: 9
Training loss: 2.151209831237793
Validation loss: 2.0763727285528697

Epoch: 6| Step: 10
Training loss: 1.9674619436264038
Validation loss: 2.0534904426144016

Epoch: 6| Step: 11
Training loss: 1.9186279773712158
Validation loss: 2.037226782050184

Epoch: 6| Step: 12
Training loss: 1.813675880432129
Validation loss: 2.0878547981221187

Epoch: 6| Step: 13
Training loss: 2.8976590633392334
Validation loss: 2.034549597770937

Testing loss: 2.116396443049113

Epoch: 1| Step: 0
Training loss: 4.555134092130553
Validation loss: 5.521687310594126

Epoch: 5| Step: 1
Training loss: 6.255337686548497
Validation loss: 5.518366016873621

Epoch: 5| Step: 2
Training loss: 6.042595031268773
Validation loss: 5.513682624880489

Epoch: 5| Step: 3
Training loss: 5.670271381429861
Validation loss: 5.5114022249066315

Epoch: 5| Step: 4
Training loss: 4.885490086159014
Validation loss: 5.5087307053715024

Epoch: 5| Step: 5
Training loss: 6.409224847590592
Validation loss: 5.50433242738409

Epoch: 5| Step: 6
Training loss: 5.8067989488170175
Validation loss: 5.498743624114276

Epoch: 5| Step: 7
Training loss: 5.302487980578522
Validation loss: 5.495431910332755

Epoch: 5| Step: 8
Training loss: 4.546590160348923
Validation loss: 5.494449548102252

Epoch: 5| Step: 9
Training loss: 5.4140686651637875
Validation loss: 5.488405093005652

Epoch: 5| Step: 10
Training loss: 5.873262188957205
Validation loss: 5.48777886513995

Epoch: 2| Step: 0
Training loss: 5.223507477259914
Validation loss: 5.4825684298023445

Epoch: 5| Step: 1
Training loss: 5.912151960327302
Validation loss: 5.478117348670688

Epoch: 5| Step: 2
Training loss: 3.826736011364174
Validation loss: 5.475171905649253

Epoch: 5| Step: 3
Training loss: 5.1845867755259905
Validation loss: 5.470129432446428

Epoch: 5| Step: 4
Training loss: 5.793984733690475
Validation loss: 5.468500719614982

Epoch: 5| Step: 5
Training loss: 6.319284249349151
Validation loss: 5.4638934972974305

Epoch: 5| Step: 6
Training loss: 5.655787833706058
Validation loss: 5.460013287540966

Epoch: 5| Step: 7
Training loss: 5.509983538483609
Validation loss: 5.455795198953775

Epoch: 5| Step: 8
Training loss: 5.972964731037901
Validation loss: 5.451506975720886

Epoch: 5| Step: 9
Training loss: 4.8012432872404505
Validation loss: 5.448069305750548

Epoch: 5| Step: 10
Training loss: 6.087054364383559
Validation loss: 5.443544895014522

Epoch: 3| Step: 0
Training loss: 5.058864842518828
Validation loss: 5.438357928501348

Epoch: 5| Step: 1
Training loss: 5.651924460465791
Validation loss: 5.436397943872433

Epoch: 5| Step: 2
Training loss: 4.581728197565542
Validation loss: 5.43210944599216

Epoch: 5| Step: 3
Training loss: 6.165429412167962
Validation loss: 5.428122579917978

Epoch: 5| Step: 4
Training loss: 6.922716490011289
Validation loss: 5.42226903323521

Epoch: 5| Step: 5
Training loss: 4.709758554089305
Validation loss: 5.418288083666024

Epoch: 5| Step: 6
Training loss: 5.266541534257151
Validation loss: 5.413725691453351

Epoch: 5| Step: 7
Training loss: 4.928220116393443
Validation loss: 5.411539405948886

Epoch: 5| Step: 8
Training loss: 4.598374718183385
Validation loss: 5.408108834286491

Epoch: 5| Step: 9
Training loss: 6.667519641986031
Validation loss: 5.401549763401835

Epoch: 5| Step: 10
Training loss: 4.888376712461364
Validation loss: 5.39863121568797

Epoch: 4| Step: 0
Training loss: 4.028494669522123
Validation loss: 5.39302363022138

Epoch: 5| Step: 1
Training loss: 5.850207346314577
Validation loss: 5.389370636288885

Epoch: 5| Step: 2
Training loss: 6.0250072684976255
Validation loss: 5.383454816285596

Epoch: 5| Step: 3
Training loss: 5.136749940494658
Validation loss: 5.379475578204199

Epoch: 5| Step: 4
Training loss: 5.44694085313883
Validation loss: 5.375814680901351

Epoch: 5| Step: 5
Training loss: 5.726070200743933
Validation loss: 5.368849468879572

Epoch: 5| Step: 6
Training loss: 5.841878826009098
Validation loss: 5.363563938537671

Epoch: 5| Step: 7
Training loss: 5.4021324397732595
Validation loss: 5.35975925188955

Epoch: 5| Step: 8
Training loss: 5.298346841048335
Validation loss: 5.357985294668089

Epoch: 5| Step: 9
Training loss: 5.13113222532984
Validation loss: 5.349993446582993

Epoch: 5| Step: 10
Training loss: 5.479795277245088
Validation loss: 5.344866876919611

Epoch: 5| Step: 0
Training loss: 5.291890998150256
Validation loss: 5.341199881314183

Epoch: 5| Step: 1
Training loss: 6.392232305704922
Validation loss: 5.339782714975028

Epoch: 5| Step: 2
Training loss: 4.697564472042309
Validation loss: 5.331235281958305

Epoch: 5| Step: 3
Training loss: 5.654728806095249
Validation loss: 5.328842144173881

Epoch: 5| Step: 4
Training loss: 5.35672456470685
Validation loss: 5.322518413524199

Epoch: 5| Step: 5
Training loss: 5.230610236603911
Validation loss: 5.320281063423823

Epoch: 5| Step: 6
Training loss: 5.5935663800497135
Validation loss: 5.310016074065412

Epoch: 5| Step: 7
Training loss: 6.242630542983576
Validation loss: 5.311005754624787

Epoch: 5| Step: 8
Training loss: 5.124707283406593
Validation loss: 5.303003297547115

Epoch: 5| Step: 9
Training loss: 3.997254144433455
Validation loss: 5.2994976969664025

Epoch: 5| Step: 10
Training loss: 5.028930318862804
Validation loss: 5.291388325649592

Epoch: 6| Step: 0
Training loss: 5.762993767823175
Validation loss: 5.287043447669232

Epoch: 5| Step: 1
Training loss: 5.1145180298572
Validation loss: 5.282208501274382

Epoch: 5| Step: 2
Training loss: 5.553527625090345
Validation loss: 5.274965717496065

Epoch: 5| Step: 3
Training loss: 6.1332074663819744
Validation loss: 5.267812609482446

Epoch: 5| Step: 4
Training loss: 5.029309579777642
Validation loss: 5.263506869645741

Epoch: 5| Step: 5
Training loss: 5.368607023775573
Validation loss: 5.259472992023099

Epoch: 5| Step: 6
Training loss: 4.612764205188783
Validation loss: 5.254779210767257

Epoch: 5| Step: 7
Training loss: 4.940209912616354
Validation loss: 5.251150197625038

Epoch: 5| Step: 8
Training loss: 6.13416274447325
Validation loss: 5.2424930043169224

Epoch: 5| Step: 9
Training loss: 5.201465238863326
Validation loss: 5.236625707391524

Epoch: 5| Step: 10
Training loss: 4.119393439965697
Validation loss: 5.2335404180235

Epoch: 7| Step: 0
Training loss: 5.260754423975234
Validation loss: 5.225079909534593

Epoch: 5| Step: 1
Training loss: 5.149576943807648
Validation loss: 5.2216992385089425

Epoch: 5| Step: 2
Training loss: 5.699621676555329
Validation loss: 5.213033839766965

Epoch: 5| Step: 3
Training loss: 5.174327626062727
Validation loss: 5.209291212372633

Epoch: 5| Step: 4
Training loss: 5.468119906502568
Validation loss: 5.201548241272766

Epoch: 5| Step: 5
Training loss: 3.8769046655504185
Validation loss: 5.197899450481211

Epoch: 5| Step: 6
Training loss: 4.589638667293262
Validation loss: 5.190453141582128

Epoch: 5| Step: 7
Training loss: 5.5777450987798725
Validation loss: 5.184606601896506

Epoch: 5| Step: 8
Training loss: 5.922104964408573
Validation loss: 5.176539406393832

Epoch: 5| Step: 9
Training loss: 5.13980903062924
Validation loss: 5.17236774673494

Epoch: 5| Step: 10
Training loss: 5.67188038050709
Validation loss: 5.166193412808945

Epoch: 8| Step: 0
Training loss: 5.288088372142791
Validation loss: 5.1596428620262555

Epoch: 5| Step: 1
Training loss: 5.581335175175066
Validation loss: 5.154321568828948

Epoch: 5| Step: 2
Training loss: 4.914712010869027
Validation loss: 5.14931663911921

Epoch: 5| Step: 3
Training loss: 5.758884697595975
Validation loss: 5.142780122377173

Epoch: 5| Step: 4
Training loss: 6.048010115821896
Validation loss: 5.133956841801348

Epoch: 5| Step: 5
Training loss: 4.5334149680547675
Validation loss: 5.126883402063671

Epoch: 5| Step: 6
Training loss: 5.5719294829387245
Validation loss: 5.119970088856628

Epoch: 5| Step: 7
Training loss: 5.106935624558887
Validation loss: 5.11174234107025

Epoch: 5| Step: 8
Training loss: 5.092325397507661
Validation loss: 5.105140746111656

Epoch: 5| Step: 9
Training loss: 4.509702606884301
Validation loss: 5.099651773649872

Epoch: 5| Step: 10
Training loss: 4.1928835751259
Validation loss: 5.0944084558738005

Epoch: 9| Step: 0
Training loss: 5.461326399723013
Validation loss: 5.0881666743344205

Epoch: 5| Step: 1
Training loss: 4.692131094477612
Validation loss: 5.076989883763314

Epoch: 5| Step: 2
Training loss: 4.75965001494548
Validation loss: 5.07201930001993

Epoch: 5| Step: 3
Training loss: 4.542351534007039
Validation loss: 5.065178368622753

Epoch: 5| Step: 4
Training loss: 5.454300630739325
Validation loss: 5.0540480039192985

Epoch: 5| Step: 5
Training loss: 5.0107006011052
Validation loss: 5.049337483454286

Epoch: 5| Step: 6
Training loss: 5.594389980084297
Validation loss: 5.046180284371279

Epoch: 5| Step: 7
Training loss: 5.089807400460188
Validation loss: 5.036695444627179

Epoch: 5| Step: 8
Training loss: 4.556256346312238
Validation loss: 5.02751351003828

Epoch: 5| Step: 9
Training loss: 5.625498092638838
Validation loss: 5.017735710267679

Epoch: 5| Step: 10
Training loss: 5.270784742209272
Validation loss: 5.0148842144766475

Epoch: 10| Step: 0
Training loss: 4.506315038796315
Validation loss: 5.001823179033115

Epoch: 5| Step: 1
Training loss: 4.9894823078713815
Validation loss: 4.997126292855382

Epoch: 5| Step: 2
Training loss: 5.588735709209425
Validation loss: 4.988875509880994

Epoch: 5| Step: 3
Training loss: 4.835014883844925
Validation loss: 4.9816181238854975

Epoch: 5| Step: 4
Training loss: 5.06019313669829
Validation loss: 4.972265639353971

Epoch: 5| Step: 5
Training loss: 5.089237015952491
Validation loss: 4.9621456805088595

Epoch: 5| Step: 6
Training loss: 4.470531615320932
Validation loss: 4.954837576371879

Epoch: 5| Step: 7
Training loss: 5.168203104923435
Validation loss: 4.94904869752134

Epoch: 5| Step: 8
Training loss: 5.559753940269549
Validation loss: 4.938021014362214

Epoch: 5| Step: 9
Training loss: 5.40753317362027
Validation loss: 4.929734865125549

Epoch: 5| Step: 10
Training loss: 4.274017685135564
Validation loss: 4.919555413326086

Epoch: 11| Step: 0
Training loss: 4.775361867244698
Validation loss: 4.908314548693752

Epoch: 5| Step: 1
Training loss: 5.002730958898264
Validation loss: 4.900033748193227

Epoch: 5| Step: 2
Training loss: 4.623788210919007
Validation loss: 4.889201045830183

Epoch: 5| Step: 3
Training loss: 4.373605342368685
Validation loss: 4.884426043883027

Epoch: 5| Step: 4
Training loss: 4.9653020447654495
Validation loss: 4.874314867144883

Epoch: 5| Step: 5
Training loss: 5.235498290735555
Validation loss: 4.864258448105785

Epoch: 5| Step: 6
Training loss: 6.1443626928764
Validation loss: 4.857818194812722

Epoch: 5| Step: 7
Training loss: 4.607069777440331
Validation loss: 4.8471778662077245

Epoch: 5| Step: 8
Training loss: 4.900510699069469
Validation loss: 4.835012088503512

Epoch: 5| Step: 9
Training loss: 4.498999378428996
Validation loss: 4.826098781949881

Epoch: 5| Step: 10
Training loss: 4.839987788854911
Validation loss: 4.818849131759649

Epoch: 12| Step: 0
Training loss: 4.084641435217646
Validation loss: 4.805675409229866

Epoch: 5| Step: 1
Training loss: 5.259972320019664
Validation loss: 4.798376453043507

Epoch: 5| Step: 2
Training loss: 4.4244963623335405
Validation loss: 4.775389890597113

Epoch: 5| Step: 3
Training loss: 5.466208079727117
Validation loss: 4.775779668714993

Epoch: 5| Step: 4
Training loss: 4.447384852376336
Validation loss: 4.757607150529875

Epoch: 5| Step: 5
Training loss: 4.253869034645521
Validation loss: 4.748293023205243

Epoch: 5| Step: 6
Training loss: 5.6651083355584015
Validation loss: 4.742005667949421

Epoch: 5| Step: 7
Training loss: 4.621978391325292
Validation loss: 4.725315560787618

Epoch: 5| Step: 8
Training loss: 4.918793794662385
Validation loss: 4.712689630540241

Epoch: 5| Step: 9
Training loss: 4.589703912430973
Validation loss: 4.703356035274749

Epoch: 5| Step: 10
Training loss: 5.092521472524207
Validation loss: 4.693200763230054

Epoch: 13| Step: 0
Training loss: 4.746426693890219
Validation loss: 4.684545067695615

Epoch: 5| Step: 1
Training loss: 4.7196056272332045
Validation loss: 4.668575271592154

Epoch: 5| Step: 2
Training loss: 4.934054560823146
Validation loss: 4.661573637279139

Epoch: 5| Step: 3
Training loss: 4.885952115624126
Validation loss: 4.653492206318131

Epoch: 5| Step: 4
Training loss: 4.939056923611807
Validation loss: 4.637219832554406

Epoch: 5| Step: 5
Training loss: 4.246690022171289
Validation loss: 4.621526900174101

Epoch: 5| Step: 6
Training loss: 5.026223271964614
Validation loss: 4.6043134844194045

Epoch: 5| Step: 7
Training loss: 3.8201513587963722
Validation loss: 4.601009603726383

Epoch: 5| Step: 8
Training loss: 5.62000595499721
Validation loss: 4.585785447926972

Epoch: 5| Step: 9
Training loss: 4.4556239561779885
Validation loss: 4.571822599426231

Epoch: 5| Step: 10
Training loss: 3.891952472024116
Validation loss: 4.556662120270564

Epoch: 14| Step: 0
Training loss: 4.994658668929861
Validation loss: 4.546143353720327

Epoch: 5| Step: 1
Training loss: 4.798817790994164
Validation loss: 4.531590892400196

Epoch: 5| Step: 2
Training loss: 5.321584317057721
Validation loss: 4.520063916704667

Epoch: 5| Step: 3
Training loss: 4.5510209834923305
Validation loss: 4.510587931376853

Epoch: 5| Step: 4
Training loss: 4.073615024855385
Validation loss: 4.497012490023741

Epoch: 5| Step: 5
Training loss: 3.895278537570055
Validation loss: 4.477230527925871

Epoch: 5| Step: 6
Training loss: 5.912573603036847
Validation loss: 4.459943473516935

Epoch: 5| Step: 7
Training loss: 3.269597316060786
Validation loss: 4.454618108746176

Epoch: 5| Step: 8
Training loss: 4.5102172995445935
Validation loss: 4.437238009279912

Epoch: 5| Step: 9
Training loss: 3.50872451039111
Validation loss: 4.42818591792929

Epoch: 5| Step: 10
Training loss: 4.744826058836433
Validation loss: 4.409411018330115

Epoch: 15| Step: 0
Training loss: 4.810676526495844
Validation loss: 4.4002829556735055

Epoch: 5| Step: 1
Training loss: 4.107361506926799
Validation loss: 4.382472029157405

Epoch: 5| Step: 2
Training loss: 4.259571293137181
Validation loss: 4.360392453783098

Epoch: 5| Step: 3
Training loss: 3.9214041985839554
Validation loss: 4.347884850602024

Epoch: 5| Step: 4
Training loss: 4.190775031832613
Validation loss: 4.337583033288335

Epoch: 5| Step: 5
Training loss: 5.06124994362275
Validation loss: 4.325516656927664

Epoch: 5| Step: 6
Training loss: 3.99833382236933
Validation loss: 4.3051012227959395

Epoch: 5| Step: 7
Training loss: 4.739554716030821
Validation loss: 4.289399292485497

Epoch: 5| Step: 8
Training loss: 5.335273489122685
Validation loss: 4.273312142789462

Epoch: 5| Step: 9
Training loss: 3.8105910478042584
Validation loss: 4.258466374326758

Epoch: 5| Step: 10
Training loss: 3.8024574966613387
Validation loss: 4.24180541932219

Epoch: 16| Step: 0
Training loss: 4.148883917688126
Validation loss: 4.22644393148336

Epoch: 5| Step: 1
Training loss: 4.5183253678369235
Validation loss: 4.203551948717828

Epoch: 5| Step: 2
Training loss: 3.7966197991653132
Validation loss: 4.2020789135932

Epoch: 5| Step: 3
Training loss: 4.332980850751422
Validation loss: 4.178543010490178

Epoch: 5| Step: 4
Training loss: 4.472002246467149
Validation loss: 4.168117274783516

Epoch: 5| Step: 5
Training loss: 3.1892293091153867
Validation loss: 4.140861632321937

Epoch: 5| Step: 6
Training loss: 4.667003324263496
Validation loss: 4.124446547126193

Epoch: 5| Step: 7
Training loss: 5.132656983410306
Validation loss: 4.107378156919468

Epoch: 5| Step: 8
Training loss: 3.982671755088141
Validation loss: 4.084000176195919

Epoch: 5| Step: 9
Training loss: 3.7342683784873847
Validation loss: 4.0713837827997255

Epoch: 5| Step: 10
Training loss: 4.370885603707029
Validation loss: 4.05283799380102

Epoch: 17| Step: 0
Training loss: 3.629938641914954
Validation loss: 4.035037866655067

Epoch: 5| Step: 1
Training loss: 4.3859730033130635
Validation loss: 4.003264209204199

Epoch: 5| Step: 2
Training loss: 3.799865424132296
Validation loss: 3.999074498134231

Epoch: 5| Step: 3
Training loss: 3.864076661709858
Validation loss: 3.98041742879384

Epoch: 5| Step: 4
Training loss: 4.6671935646878495
Validation loss: 3.9619479857114692

Epoch: 5| Step: 5
Training loss: 4.2533139043222725
Validation loss: 3.9460266006350087

Epoch: 5| Step: 6
Training loss: 3.724780282318716
Validation loss: 3.928185653443414

Epoch: 5| Step: 7
Training loss: 5.070978670484986
Validation loss: 3.9043527950118606

Epoch: 5| Step: 8
Training loss: 3.4358473533056273
Validation loss: 3.8848642529863615

Epoch: 5| Step: 9
Training loss: 3.5104110370594617
Validation loss: 3.8633342559989456

Epoch: 5| Step: 10
Training loss: 3.9568750740404353
Validation loss: 3.843838254254061

Epoch: 18| Step: 0
Training loss: 3.7606474716633476
Validation loss: 3.830723867250079

Epoch: 5| Step: 1
Training loss: 3.710989283150217
Validation loss: 3.815720428393896

Epoch: 5| Step: 2
Training loss: 4.078479970632394
Validation loss: 3.7926450537830463

Epoch: 5| Step: 3
Training loss: 3.382029524265412
Validation loss: 3.7810373879571615

Epoch: 5| Step: 4
Training loss: 3.2932652965524296
Validation loss: 3.750478134300579

Epoch: 5| Step: 5
Training loss: 4.197135657015231
Validation loss: 3.735296688192847

Epoch: 5| Step: 6
Training loss: 3.9578045675903777
Validation loss: 3.7099146239678524

Epoch: 5| Step: 7
Training loss: 3.7673768839625574
Validation loss: 3.691557074627309

Epoch: 5| Step: 8
Training loss: 2.723315822664864
Validation loss: 3.668564681825113

Epoch: 5| Step: 9
Training loss: 5.102926215127086
Validation loss: 3.6675109021884387

Epoch: 5| Step: 10
Training loss: 4.001155209620667
Validation loss: 3.6341540504648906

Epoch: 19| Step: 0
Training loss: 4.192627685276697
Validation loss: 3.6147108054048145

Epoch: 5| Step: 1
Training loss: 3.783159514704117
Validation loss: 3.5957561484398366

Epoch: 5| Step: 2
Training loss: 3.462246959267638
Validation loss: 3.564671422971813

Epoch: 5| Step: 3
Training loss: 3.783291352385017
Validation loss: 3.560001359874562

Epoch: 5| Step: 4
Training loss: 3.6641257469721236
Validation loss: 3.5278218034352657

Epoch: 5| Step: 5
Training loss: 3.3191440096947114
Validation loss: 3.5072426753252515

Epoch: 5| Step: 6
Training loss: 3.622702692757144
Validation loss: 3.494219575779175

Epoch: 5| Step: 7
Training loss: 3.3736750509806623
Validation loss: 3.477603886811243

Epoch: 5| Step: 8
Training loss: 3.1178797667825977
Validation loss: 3.4654006418908923

Epoch: 5| Step: 9
Training loss: 3.6524018471732904
Validation loss: 3.4334041771736517

Epoch: 5| Step: 10
Training loss: 4.3027824714317084
Validation loss: 3.4282365695000534

Epoch: 20| Step: 0
Training loss: 3.2597521672505305
Validation loss: 3.400651458083

Epoch: 5| Step: 1
Training loss: 3.7820267313028157
Validation loss: 3.360399698627422

Epoch: 5| Step: 2
Training loss: 4.438352690962605
Validation loss: 3.358685870028444

Epoch: 5| Step: 3
Training loss: 2.884703804030373
Validation loss: 3.347210120908813

Epoch: 5| Step: 4
Training loss: 3.2100493230625196
Validation loss: 3.317746881764687

Epoch: 5| Step: 5
Training loss: 4.053949130822667
Validation loss: 3.298464134466571

Epoch: 5| Step: 6
Training loss: 3.1249885558872483
Validation loss: 3.2639409373460717

Epoch: 5| Step: 7
Training loss: 2.7243844763251217
Validation loss: 3.261445640923144

Epoch: 5| Step: 8
Training loss: 3.2811670202026186
Validation loss: 3.232148778576704

Epoch: 5| Step: 9
Training loss: 3.5935193858581154
Validation loss: 3.21737577048269

Epoch: 5| Step: 10
Training loss: 3.4995941199246845
Validation loss: 3.1937942546447133

Epoch: 21| Step: 0
Training loss: 2.986245413034588
Validation loss: 3.1814987016099137

Epoch: 5| Step: 1
Training loss: 3.0375630815651062
Validation loss: 3.1560260827703224

Epoch: 5| Step: 2
Training loss: 2.505432330379205
Validation loss: 3.143669613770445

Epoch: 5| Step: 3
Training loss: 3.0835769316526562
Validation loss: 3.12604906255049

Epoch: 5| Step: 4
Training loss: 3.602982313601334
Validation loss: 3.114796953444334

Epoch: 5| Step: 5
Training loss: 3.120597637598236
Validation loss: 3.098339344035531

Epoch: 5| Step: 6
Training loss: 3.107464534099863
Validation loss: 3.084073532995629

Epoch: 5| Step: 7
Training loss: 3.6038548917189224
Validation loss: 3.0587583835006384

Epoch: 5| Step: 8
Training loss: 4.228237081255924
Validation loss: 3.057726495999365

Epoch: 5| Step: 9
Training loss: 3.053885352140541
Validation loss: 3.0371121115886996

Epoch: 5| Step: 10
Training loss: 3.5888687818874008
Validation loss: 3.0142721535411683

Epoch: 22| Step: 0
Training loss: 3.0208707259045284
Validation loss: 2.9839763395488754

Epoch: 5| Step: 1
Training loss: 2.8310558665162255
Validation loss: 2.9825693793194055

Epoch: 5| Step: 2
Training loss: 4.0458111045742715
Validation loss: 2.962893042507063

Epoch: 5| Step: 3
Training loss: 2.922649097408382
Validation loss: 2.9453595472307805

Epoch: 5| Step: 4
Training loss: 3.2190045006143726
Validation loss: 2.9292459677224687

Epoch: 5| Step: 5
Training loss: 2.997344590231549
Validation loss: 2.9155755948268665

Epoch: 5| Step: 6
Training loss: 3.0832662489183043
Validation loss: 2.9051126474662072

Epoch: 5| Step: 7
Training loss: 2.748963334017695
Validation loss: 2.896648520352137

Epoch: 5| Step: 8
Training loss: 3.285718509126401
Validation loss: 2.886451071693585

Epoch: 5| Step: 9
Training loss: 3.2787755446142466
Validation loss: 2.8506747718372614

Epoch: 5| Step: 10
Training loss: 3.1142743487919393
Validation loss: 2.8399108895934235

Epoch: 23| Step: 0
Training loss: 3.1587837548937507
Validation loss: 2.8402067980014913

Epoch: 5| Step: 1
Training loss: 3.8273294673231923
Validation loss: 2.800676603795507

Epoch: 5| Step: 2
Training loss: 3.3088312525440915
Validation loss: 2.807636125169886

Epoch: 5| Step: 3
Training loss: 3.1416382934206495
Validation loss: 2.8000667918936246

Epoch: 5| Step: 4
Training loss: 2.9496901785879484
Validation loss: 2.794826987599618

Epoch: 5| Step: 5
Training loss: 3.0822223173803223
Validation loss: 2.7712669733317634

Epoch: 5| Step: 6
Training loss: 2.957792761042083
Validation loss: 2.7578390694726074

Epoch: 5| Step: 7
Training loss: 2.522576719692025
Validation loss: 2.7584066517003727

Epoch: 5| Step: 8
Training loss: 3.0562729099405734
Validation loss: 2.749917725481748

Epoch: 5| Step: 9
Training loss: 2.488003366425129
Validation loss: 2.7486985008317744

Epoch: 5| Step: 10
Training loss: 2.973546379850181
Validation loss: 2.7320766520423687

Epoch: 24| Step: 0
Training loss: 3.080439902510834
Validation loss: 2.7206304899848623

Epoch: 5| Step: 1
Training loss: 2.64416167749158
Validation loss: 2.724747865558971

Epoch: 5| Step: 2
Training loss: 2.8369061625858163
Validation loss: 2.7009778096490935

Epoch: 5| Step: 3
Training loss: 2.7322836589417143
Validation loss: 2.681518531832154

Epoch: 5| Step: 4
Training loss: 3.2614576406527895
Validation loss: 2.7062271084608645

Epoch: 5| Step: 5
Training loss: 2.6276507844497776
Validation loss: 2.69260427825687

Epoch: 5| Step: 6
Training loss: 3.614051464573578
Validation loss: 2.691928419179597

Epoch: 5| Step: 7
Training loss: 3.0731013010882813
Validation loss: 2.6722757091411964

Epoch: 5| Step: 8
Training loss: 2.952593235801551
Validation loss: 2.6852104944970896

Epoch: 5| Step: 9
Training loss: 2.9177814714625154
Validation loss: 2.6483227102987295

Epoch: 5| Step: 10
Training loss: 3.084913020955825
Validation loss: 2.661642178326307

Epoch: 25| Step: 0
Training loss: 2.5689301179355155
Validation loss: 2.6697176225866657

Epoch: 5| Step: 1
Training loss: 3.462892001991407
Validation loss: 2.654585621980478

Epoch: 5| Step: 2
Training loss: 2.915089117032565
Validation loss: 2.6403055470489636

Epoch: 5| Step: 3
Training loss: 2.394575656896044
Validation loss: 2.6332241946442303

Epoch: 5| Step: 4
Training loss: 2.957628802012376
Validation loss: 2.620931115600524

Epoch: 5| Step: 5
Training loss: 2.8420118112983066
Validation loss: 2.6327261708693914

Epoch: 5| Step: 6
Training loss: 3.048399558483773
Validation loss: 2.623023006484637

Epoch: 5| Step: 7
Training loss: 3.294445860463768
Validation loss: 2.6128993602133126

Epoch: 5| Step: 8
Training loss: 3.7054070631856697
Validation loss: 2.6207130182139866

Epoch: 5| Step: 9
Training loss: 2.362920761988286
Validation loss: 2.607758936585103

Epoch: 5| Step: 10
Training loss: 2.867386730454632
Validation loss: 2.598140944027714

Epoch: 26| Step: 0
Training loss: 3.363769637567588
Validation loss: 2.6083072911749667

Epoch: 5| Step: 1
Training loss: 2.6645487679950475
Validation loss: 2.606670840968953

Epoch: 5| Step: 2
Training loss: 2.5382417731533597
Validation loss: 2.6080772823318314

Epoch: 5| Step: 3
Training loss: 3.0920174110978462
Validation loss: 2.6142948980060363

Epoch: 5| Step: 4
Training loss: 3.25615900831436
Validation loss: 2.618254569012766

Epoch: 5| Step: 5
Training loss: 3.0781919210682247
Validation loss: 2.596310925418555

Epoch: 5| Step: 6
Training loss: 3.3550040264077055
Validation loss: 2.6068720508052228

Epoch: 5| Step: 7
Training loss: 3.2301251045304684
Validation loss: 2.6190226747786958

Epoch: 5| Step: 8
Training loss: 2.508969809382833
Validation loss: 2.5913498793449263

Epoch: 5| Step: 9
Training loss: 2.5914062573602843
Validation loss: 2.5917523921651404

Epoch: 5| Step: 10
Training loss: 2.7260053442484917
Validation loss: 2.597880184943284

Epoch: 27| Step: 0
Training loss: 3.093800245705457
Validation loss: 2.5957789288292945

Epoch: 5| Step: 1
Training loss: 2.691161478148598
Validation loss: 2.6123319920487282

Epoch: 5| Step: 2
Training loss: 2.805058361813082
Validation loss: 2.5870937350872114

Epoch: 5| Step: 3
Training loss: 2.6497834423037627
Validation loss: 2.599855249507341

Epoch: 5| Step: 4
Training loss: 3.6148314083986017
Validation loss: 2.5865030280316486

Epoch: 5| Step: 5
Training loss: 2.706716279701075
Validation loss: 2.6022777914829005

Epoch: 5| Step: 6
Training loss: 2.6263255450982177
Validation loss: 2.6126954157640636

Epoch: 5| Step: 7
Training loss: 3.380409285690379
Validation loss: 2.581404456760631

Epoch: 5| Step: 8
Training loss: 1.98514638010309
Validation loss: 2.6054587058595553

Epoch: 5| Step: 9
Training loss: 3.304104800966005
Validation loss: 2.58716782620458

Epoch: 5| Step: 10
Training loss: 3.264360685045063
Validation loss: 2.590081595930157

Epoch: 28| Step: 0
Training loss: 2.369753312579537
Validation loss: 2.6045989992324996

Epoch: 5| Step: 1
Training loss: 2.539446522942214
Validation loss: 2.609070037279126

Epoch: 5| Step: 2
Training loss: 3.0528129425970065
Validation loss: 2.5733889576035245

Epoch: 5| Step: 3
Training loss: 3.5270390341977906
Validation loss: 2.5821374911635915

Epoch: 5| Step: 4
Training loss: 3.2168398624366623
Validation loss: 2.607573915972244

Epoch: 5| Step: 5
Training loss: 3.08200804262888
Validation loss: 2.5810789763976465

Epoch: 5| Step: 6
Training loss: 3.1330551091572905
Validation loss: 2.5810393784441747

Epoch: 5| Step: 7
Training loss: 3.3734268478076954
Validation loss: 2.584034740827829

Epoch: 5| Step: 8
Training loss: 2.579529997871359
Validation loss: 2.6018383743698785

Epoch: 5| Step: 9
Training loss: 2.6204615824080895
Validation loss: 2.5862439721872468

Epoch: 5| Step: 10
Training loss: 2.590460104117682
Validation loss: 2.5903583901779847

Epoch: 29| Step: 0
Training loss: 2.3290066681729975
Validation loss: 2.605712997250158

Epoch: 5| Step: 1
Training loss: 2.819191242200734
Validation loss: 2.584181733269149

Epoch: 5| Step: 2
Training loss: 2.4746897262137395
Validation loss: 2.5888359744503897

Epoch: 5| Step: 3
Training loss: 3.6773083035357437
Validation loss: 2.607245524604247

Epoch: 5| Step: 4
Training loss: 3.0279683705420255
Validation loss: 2.5822892303634664

Epoch: 5| Step: 5
Training loss: 3.251467300187257
Validation loss: 2.5868683669295676

Epoch: 5| Step: 6
Training loss: 3.4836548202099573
Validation loss: 2.5715728740598953

Epoch: 5| Step: 7
Training loss: 2.572816689797257
Validation loss: 2.5771796806895035

Epoch: 5| Step: 8
Training loss: 2.5050524202447
Validation loss: 2.57627386245017

Epoch: 5| Step: 9
Training loss: 2.901364064864947
Validation loss: 2.571218496058414

Epoch: 5| Step: 10
Training loss: 3.0996112210533253
Validation loss: 2.5758393329191627

Epoch: 30| Step: 0
Training loss: 3.095732390930276
Validation loss: 2.5656624240190946

Epoch: 5| Step: 1
Training loss: 3.1310735047447
Validation loss: 2.5845464192938628

Epoch: 5| Step: 2
Training loss: 2.513026346520156
Validation loss: 2.5695224663276672

Epoch: 5| Step: 3
Training loss: 2.534156731591595
Validation loss: 2.5832695270032517

Epoch: 5| Step: 4
Training loss: 3.273267937322265
Validation loss: 2.5691282449899635

Epoch: 5| Step: 5
Training loss: 2.7818278023241207
Validation loss: 2.5795868200595384

Epoch: 5| Step: 6
Training loss: 3.5400074082636497
Validation loss: 2.5877656837992484

Epoch: 5| Step: 7
Training loss: 2.5321447415856952
Validation loss: 2.5726572455387724

Epoch: 5| Step: 8
Training loss: 2.793033369690617
Validation loss: 2.5967445497636694

Epoch: 5| Step: 9
Training loss: 3.3058776876597538
Validation loss: 2.569550525796503

Epoch: 5| Step: 10
Training loss: 2.4866658331261977
Validation loss: 2.5564726508442615

Epoch: 31| Step: 0
Training loss: 2.7394579047630927
Validation loss: 2.574608126780303

Epoch: 5| Step: 1
Training loss: 2.7058602314481375
Validation loss: 2.5898160361153684

Epoch: 5| Step: 2
Training loss: 2.726495080677256
Validation loss: 2.571746572729104

Epoch: 5| Step: 3
Training loss: 3.8119762873917784
Validation loss: 2.5650299552916676

Epoch: 5| Step: 4
Training loss: 2.4004027426082466
Validation loss: 2.5767775518651597

Epoch: 5| Step: 5
Training loss: 2.764474920596837
Validation loss: 2.589373969608678

Epoch: 5| Step: 6
Training loss: 3.144125131480493
Validation loss: 2.576809639238366

Epoch: 5| Step: 7
Training loss: 2.46163543008121
Validation loss: 2.572797853150179

Epoch: 5| Step: 8
Training loss: 3.0935651935905
Validation loss: 2.5653461194564096

Epoch: 5| Step: 9
Training loss: 2.7931531298405896
Validation loss: 2.5846515666894874

Epoch: 5| Step: 10
Training loss: 3.4675538346141455
Validation loss: 2.5703311623633462

Epoch: 32| Step: 0
Training loss: 3.2208575413539755
Validation loss: 2.5881974919185113

Epoch: 5| Step: 1
Training loss: 3.675295790621426
Validation loss: 2.5735917962788104

Epoch: 5| Step: 2
Training loss: 2.7585906316911895
Validation loss: 2.581990150503218

Epoch: 5| Step: 3
Training loss: 3.004184982801271
Validation loss: 2.5804683751008657

Epoch: 5| Step: 4
Training loss: 3.3373377271199867
Validation loss: 2.569847821211458

Epoch: 5| Step: 5
Training loss: 2.4955482901790784
Validation loss: 2.584687320411968

Epoch: 5| Step: 6
Training loss: 2.2813762473093098
Validation loss: 2.5618240401307326

Epoch: 5| Step: 7
Training loss: 2.8045891449665508
Validation loss: 2.582935036259495

Epoch: 5| Step: 8
Training loss: 2.9196393440862063
Validation loss: 2.579024774447069

Epoch: 5| Step: 9
Training loss: 3.0165963461532272
Validation loss: 2.5798253174801724

Epoch: 5| Step: 10
Training loss: 2.364364905218587
Validation loss: 2.572883967108399

Epoch: 33| Step: 0
Training loss: 3.051750156240396
Validation loss: 2.588578782222293

Epoch: 5| Step: 1
Training loss: 2.784721180024326
Validation loss: 2.5720780412349833

Epoch: 5| Step: 2
Training loss: 2.9884795877296626
Validation loss: 2.5763011279450123

Epoch: 5| Step: 3
Training loss: 2.9839501203300425
Validation loss: 2.581804880894479

Epoch: 5| Step: 4
Training loss: 2.8771642748592523
Validation loss: 2.5838609990793873

Epoch: 5| Step: 5
Training loss: 2.5692884266084266
Validation loss: 2.5679596555776985

Epoch: 5| Step: 6
Training loss: 3.1650137519236345
Validation loss: 2.5945542264026353

Epoch: 5| Step: 7
Training loss: 3.4217752241803456
Validation loss: 2.5690946132598724

Epoch: 5| Step: 8
Training loss: 2.7226174569342234
Validation loss: 2.5969947201440458

Epoch: 5| Step: 9
Training loss: 2.150228967229125
Validation loss: 2.574413175387841

Epoch: 5| Step: 10
Training loss: 3.473774750227744
Validation loss: 2.595183647280893

Epoch: 34| Step: 0
Training loss: 3.1297741851761947
Validation loss: 2.5768642013809155

Epoch: 5| Step: 1
Training loss: 2.5329907871685715
Validation loss: 2.5682861327663336

Epoch: 5| Step: 2
Training loss: 3.0136785204277023
Validation loss: 2.568272210955833

Epoch: 5| Step: 3
Training loss: 2.9078338573067097
Validation loss: 2.5713984617813384

Epoch: 5| Step: 4
Training loss: 2.9036647003427585
Validation loss: 2.563913788198297

Epoch: 5| Step: 5
Training loss: 1.9872093084068174
Validation loss: 2.5960353020658467

Epoch: 5| Step: 6
Training loss: 2.736358533495607
Validation loss: 2.575361982876064

Epoch: 5| Step: 7
Training loss: 3.3861879066922307
Validation loss: 2.578201455890258

Epoch: 5| Step: 8
Training loss: 3.047573381249272
Validation loss: 2.572356782692746

Epoch: 5| Step: 9
Training loss: 3.130707525900804
Validation loss: 2.5869704539429317

Epoch: 5| Step: 10
Training loss: 3.285480642008294
Validation loss: 2.5853036920973156

Epoch: 35| Step: 0
Training loss: 2.881761147263161
Validation loss: 2.5558509291108003

Epoch: 5| Step: 1
Training loss: 3.3560911229781025
Validation loss: 2.5559591250846725

Epoch: 5| Step: 2
Training loss: 2.4032825155436517
Validation loss: 2.5729269687034

Epoch: 5| Step: 3
Training loss: 3.247157394274025
Validation loss: 2.5773017312135265

Epoch: 5| Step: 4
Training loss: 3.0929797205835214
Validation loss: 2.5814185739021323

Epoch: 5| Step: 5
Training loss: 3.624105803597818
Validation loss: 2.5596383546283437

Epoch: 5| Step: 6
Training loss: 3.182979295093382
Validation loss: 2.5767520071372667

Epoch: 5| Step: 7
Training loss: 2.962308460594617
Validation loss: 2.570022988106259

Epoch: 5| Step: 8
Training loss: 2.344968961986112
Validation loss: 2.5776619444075255

Epoch: 5| Step: 9
Training loss: 2.4875385607313687
Validation loss: 2.574923562909439

Epoch: 5| Step: 10
Training loss: 2.3673843956570586
Validation loss: 2.5843788552563307

Epoch: 36| Step: 0
Training loss: 2.994277423438447
Validation loss: 2.5773496305208274

Epoch: 5| Step: 1
Training loss: 2.789932756331427
Validation loss: 2.5587279065958

Epoch: 5| Step: 2
Training loss: 3.3285439097788947
Validation loss: 2.5916501794472713

Epoch: 5| Step: 3
Training loss: 3.5458295852754884
Validation loss: 2.578739417960076

Epoch: 5| Step: 4
Training loss: 2.751429533033736
Validation loss: 2.5702101034416795

Epoch: 5| Step: 5
Training loss: 3.254537789009796
Validation loss: 2.5518914996725073

Epoch: 5| Step: 6
Training loss: 2.198624254440293
Validation loss: 2.587085954257214

Epoch: 5| Step: 7
Training loss: 2.7945993212012255
Validation loss: 2.5453078209842257

Epoch: 5| Step: 8
Training loss: 2.586154525270418
Validation loss: 2.5672573465739337

Epoch: 5| Step: 9
Training loss: 2.863571006612567
Validation loss: 2.5744592692847132

Epoch: 5| Step: 10
Training loss: 2.8740856541112287
Validation loss: 2.5565563785812366

Epoch: 37| Step: 0
Training loss: 3.1148930186213546
Validation loss: 2.5596273674519403

Epoch: 5| Step: 1
Training loss: 2.939303879101508
Validation loss: 2.5782559816182764

Epoch: 5| Step: 2
Training loss: 2.1748831794322303
Validation loss: 2.574545265934127

Epoch: 5| Step: 3
Training loss: 2.7863428014649223
Validation loss: 2.5772820241525176

Epoch: 5| Step: 4
Training loss: 3.1257695585650507
Validation loss: 2.573851646461645

Epoch: 5| Step: 5
Training loss: 2.9977659808354775
Validation loss: 2.5827898587795435

Epoch: 5| Step: 6
Training loss: 2.9744567631567067
Validation loss: 2.570615130542882

Epoch: 5| Step: 7
Training loss: 3.751028937004176
Validation loss: 2.5715401864911067

Epoch: 5| Step: 8
Training loss: 2.9883106105583774
Validation loss: 2.5700474002144724

Epoch: 5| Step: 9
Training loss: 2.4917057731732086
Validation loss: 2.561167268614358

Epoch: 5| Step: 10
Training loss: 2.5942562482467655
Validation loss: 2.5756321951846544

Epoch: 38| Step: 0
Training loss: 2.999277981657399
Validation loss: 2.576835440571291

Epoch: 5| Step: 1
Training loss: 2.862956321359139
Validation loss: 2.5732641481571306

Epoch: 5| Step: 2
Training loss: 2.8884026309719975
Validation loss: 2.5572877202234223

Epoch: 5| Step: 3
Training loss: 2.9910798973500428
Validation loss: 2.5506493550134186

Epoch: 5| Step: 4
Training loss: 2.9649572941243933
Validation loss: 2.575865780933846

Epoch: 5| Step: 5
Training loss: 3.0224964343129623
Validation loss: 2.5825444262995894

Epoch: 5| Step: 6
Training loss: 2.1700059271876517
Validation loss: 2.5600298589393047

Epoch: 5| Step: 7
Training loss: 3.326803455432722
Validation loss: 2.5808135928478726

Epoch: 5| Step: 8
Training loss: 2.513322617721313
Validation loss: 2.5719583982100302

Epoch: 5| Step: 9
Training loss: 3.167657764001482
Validation loss: 2.5821101309237338

Epoch: 5| Step: 10
Training loss: 3.133746609588968
Validation loss: 2.5675474663652

Epoch: 39| Step: 0
Training loss: 3.4933284116181316
Validation loss: 2.556219080316199

Epoch: 5| Step: 1
Training loss: 2.608127501372494
Validation loss: 2.578104164633487

Epoch: 5| Step: 2
Training loss: 2.5136052903113004
Validation loss: 2.560862685663158

Epoch: 5| Step: 3
Training loss: 2.682347703860694
Validation loss: 2.56035972231581

Epoch: 5| Step: 4
Training loss: 3.0514385770528576
Validation loss: 2.540447344746147

Epoch: 5| Step: 5
Training loss: 2.855835271396282
Validation loss: 2.5524264625852497

Epoch: 5| Step: 6
Training loss: 2.9932085093049867
Validation loss: 2.564273347672955

Epoch: 5| Step: 7
Training loss: 2.7837562841199732
Validation loss: 2.572873836614374

Epoch: 5| Step: 8
Training loss: 3.7111893417833595
Validation loss: 2.557645744437415

Epoch: 5| Step: 9
Training loss: 2.657977092474857
Validation loss: 2.5445506233760047

Epoch: 5| Step: 10
Training loss: 2.4720059906291283
Validation loss: 2.5772639751296404

Epoch: 40| Step: 0
Training loss: 2.5873645175717996
Validation loss: 2.554101773525197

Epoch: 5| Step: 1
Training loss: 3.3799873518848216
Validation loss: 2.561745154018338

Epoch: 5| Step: 2
Training loss: 3.190240616767189
Validation loss: 2.560546421452626

Epoch: 5| Step: 3
Training loss: 2.916113573947295
Validation loss: 2.544338199774355

Epoch: 5| Step: 4
Training loss: 2.770612454513967
Validation loss: 2.558956496453482

Epoch: 5| Step: 5
Training loss: 2.82210494379315
Validation loss: 2.5617915208813473

Epoch: 5| Step: 6
Training loss: 2.762186272529496
Validation loss: 2.5632520062936606

Epoch: 5| Step: 7
Training loss: 2.9072208218682825
Validation loss: 2.5603105639322528

Epoch: 5| Step: 8
Training loss: 3.2870925476767745
Validation loss: 2.574255416444413

Epoch: 5| Step: 9
Training loss: 2.7057990810747157
Validation loss: 2.548009587883898

Epoch: 5| Step: 10
Training loss: 2.53625906305776
Validation loss: 2.560736486808469

Epoch: 41| Step: 0
Training loss: 2.8608951384211885
Validation loss: 2.5506752762385

Epoch: 5| Step: 1
Training loss: 3.0176966375392706
Validation loss: 2.5587305656900683

Epoch: 5| Step: 2
Training loss: 2.3691210010844825
Validation loss: 2.560669538093314

Epoch: 5| Step: 3
Training loss: 2.522956731484968
Validation loss: 2.563848895291245

Epoch: 5| Step: 4
Training loss: 3.1646896682630876
Validation loss: 2.5804657026414985

Epoch: 5| Step: 5
Training loss: 3.1285397127986254
Validation loss: 2.5696291234027555

Epoch: 5| Step: 6
Training loss: 2.8099789658706085
Validation loss: 2.5623505761625074

Epoch: 5| Step: 7
Training loss: 2.3779253761832067
Validation loss: 2.5822688415956385

Epoch: 5| Step: 8
Training loss: 3.8613983904282723
Validation loss: 2.560755382143888

Epoch: 5| Step: 9
Training loss: 2.614850723353041
Validation loss: 2.5506438722234503

Epoch: 5| Step: 10
Training loss: 3.1525909062225423
Validation loss: 2.583947810893281

Epoch: 42| Step: 0
Training loss: 2.5025799790138743
Validation loss: 2.5560587110427178

Epoch: 5| Step: 1
Training loss: 3.453669854586211
Validation loss: 2.578079954607123

Epoch: 5| Step: 2
Training loss: 2.620512259687756
Validation loss: 2.57108583176796

Epoch: 5| Step: 3
Training loss: 3.403918947306046
Validation loss: 2.582255100406544

Epoch: 5| Step: 4
Training loss: 3.130988677002844
Validation loss: 2.5623509633573196

Epoch: 5| Step: 5
Training loss: 2.749672610128454
Validation loss: 2.550578366294351

Epoch: 5| Step: 6
Training loss: 2.866815444131423
Validation loss: 2.5807683673988717

Epoch: 5| Step: 7
Training loss: 3.1529139649436857
Validation loss: 2.546767032644163

Epoch: 5| Step: 8
Training loss: 2.6189704856826297
Validation loss: 2.5802392071168847

Epoch: 5| Step: 9
Training loss: 2.7852594856721797
Validation loss: 2.575092079705752

Epoch: 5| Step: 10
Training loss: 2.4935825951395634
Validation loss: 2.5643682624907314

Epoch: 43| Step: 0
Training loss: 2.842959954483251
Validation loss: 2.5608283812615418

Epoch: 5| Step: 1
Training loss: 2.953804704867448
Validation loss: 2.570814558234415

Epoch: 5| Step: 2
Training loss: 3.069727562859718
Validation loss: 2.560699197294887

Epoch: 5| Step: 3
Training loss: 2.4141776541987783
Validation loss: 2.563745608601175

Epoch: 5| Step: 4
Training loss: 3.2064635207556194
Validation loss: 2.565548403904301

Epoch: 5| Step: 5
Training loss: 3.029012267013903
Validation loss: 2.5518565663354127

Epoch: 5| Step: 6
Training loss: 3.1595684773322543
Validation loss: 2.552902478636369

Epoch: 5| Step: 7
Training loss: 2.4362961168068855
Validation loss: 2.5681363238535106

Epoch: 5| Step: 8
Training loss: 2.6081644322449153
Validation loss: 2.569748855919845

Epoch: 5| Step: 9
Training loss: 3.151412883151812
Validation loss: 2.55794110410061

Epoch: 5| Step: 10
Training loss: 3.0379198765287567
Validation loss: 2.563446944433353

Epoch: 44| Step: 0
Training loss: 2.707647872648565
Validation loss: 2.5731908313288443

Epoch: 5| Step: 1
Training loss: 3.1281658158511934
Validation loss: 2.580733141501749

Epoch: 5| Step: 2
Training loss: 2.3132205562236905
Validation loss: 2.5618245084625495

Epoch: 5| Step: 3
Training loss: 3.207335742244108
Validation loss: 2.576578568298427

Epoch: 5| Step: 4
Training loss: 2.131323214081642
Validation loss: 2.5742321785558158

Epoch: 5| Step: 5
Training loss: 2.9960501417743943
Validation loss: 2.5660635268156207

Epoch: 5| Step: 6
Training loss: 3.252278189693609
Validation loss: 2.560468010614683

Epoch: 5| Step: 7
Training loss: 2.9873180956506875
Validation loss: 2.582934198563999

Epoch: 5| Step: 8
Training loss: 3.1702506881411097
Validation loss: 2.5565559834894263

Epoch: 5| Step: 9
Training loss: 2.410081562387385
Validation loss: 2.5775116957589734

Epoch: 5| Step: 10
Training loss: 3.348968426742096
Validation loss: 2.540832367136361

Epoch: 45| Step: 0
Training loss: 3.17599355215576
Validation loss: 2.539364149224761

Epoch: 5| Step: 1
Training loss: 3.517295881149574
Validation loss: 2.5480630201144416

Epoch: 5| Step: 2
Training loss: 2.6426918924431977
Validation loss: 2.5645804214957493

Epoch: 5| Step: 3
Training loss: 2.465739962335674
Validation loss: 2.5653168347710373

Epoch: 5| Step: 4
Training loss: 2.878263528577848
Validation loss: 2.5480064120180193

Epoch: 5| Step: 5
Training loss: 2.969996461705627
Validation loss: 2.5534053074912255

Epoch: 5| Step: 6
Training loss: 2.9116403776429367
Validation loss: 2.5600029859490183

Epoch: 5| Step: 7
Training loss: 2.583964168270623
Validation loss: 2.554594737361956

Epoch: 5| Step: 8
Training loss: 3.0329473403600287
Validation loss: 2.5418388228376245

Epoch: 5| Step: 9
Training loss: 3.005483067128623
Validation loss: 2.567064962764974

Epoch: 5| Step: 10
Training loss: 2.394873639717474
Validation loss: 2.5549664118863418

Epoch: 46| Step: 0
Training loss: 2.4954849479770127
Validation loss: 2.567657633609979

Epoch: 5| Step: 1
Training loss: 3.150570130599722
Validation loss: 2.5475082601669556

Epoch: 5| Step: 2
Training loss: 3.0075126993952868
Validation loss: 2.5726561812816624

Epoch: 5| Step: 3
Training loss: 3.2025982679526876
Validation loss: 2.5602278411438633

Epoch: 5| Step: 4
Training loss: 3.029836738141189
Validation loss: 2.552039987892045

Epoch: 5| Step: 5
Training loss: 2.7905609636125464
Validation loss: 2.555979917802171

Epoch: 5| Step: 6
Training loss: 2.841299489333347
Validation loss: 2.564408278820759

Epoch: 5| Step: 7
Training loss: 2.974760215777001
Validation loss: 2.5553968421373305

Epoch: 5| Step: 8
Training loss: 3.149789676002472
Validation loss: 2.555933064365728

Epoch: 5| Step: 9
Training loss: 2.7782306354980277
Validation loss: 2.5694030485298667

Epoch: 5| Step: 10
Training loss: 2.2634325666214528
Validation loss: 2.5660069956198446

Epoch: 47| Step: 0
Training loss: 3.225563762980092
Validation loss: 2.565152815765606

Epoch: 5| Step: 1
Training loss: 2.256433296683417
Validation loss: 2.560426702160738

Epoch: 5| Step: 2
Training loss: 2.8552693559614957
Validation loss: 2.5410397477783246

Epoch: 5| Step: 3
Training loss: 2.8615337623418777
Validation loss: 2.558907229963153

Epoch: 5| Step: 4
Training loss: 3.0653775767611977
Validation loss: 2.5557599650998526

Epoch: 5| Step: 5
Training loss: 2.391469768788495
Validation loss: 2.579261280290978

Epoch: 5| Step: 6
Training loss: 3.1169672149608223
Validation loss: 2.556068659445651

Epoch: 5| Step: 7
Training loss: 3.107537114603804
Validation loss: 2.5599340862586994

Epoch: 5| Step: 8
Training loss: 3.437908633826113
Validation loss: 2.5605641868331914

Epoch: 5| Step: 9
Training loss: 2.5694950740282265
Validation loss: 2.563612829599541

Epoch: 5| Step: 10
Training loss: 2.8527077243337833
Validation loss: 2.5626546671501296

Epoch: 48| Step: 0
Training loss: 3.3382981678225803
Validation loss: 2.531663656015506

Epoch: 5| Step: 1
Training loss: 2.6777359776732075
Validation loss: 2.5484014582485663

Epoch: 5| Step: 2
Training loss: 3.2346878269045396
Validation loss: 2.558946429039105

Epoch: 5| Step: 3
Training loss: 3.632832844995114
Validation loss: 2.576820711338882

Epoch: 5| Step: 4
Training loss: 2.446089063680735
Validation loss: 2.5668369032465193

Epoch: 5| Step: 5
Training loss: 2.9644451853522282
Validation loss: 2.5684798823570336

Epoch: 5| Step: 6
Training loss: 3.0819619367998063
Validation loss: 2.5611526845023143

Epoch: 5| Step: 7
Training loss: 2.8450326227821754
Validation loss: 2.5571236925016105

Epoch: 5| Step: 8
Training loss: 2.28877147087389
Validation loss: 2.5529305902314734

Epoch: 5| Step: 9
Training loss: 2.772347679391246
Validation loss: 2.5526877411859097

Epoch: 5| Step: 10
Training loss: 2.1751382586503287
Validation loss: 2.5546184178098996

Epoch: 49| Step: 0
Training loss: 2.9329568346751
Validation loss: 2.553383745313615

Epoch: 5| Step: 1
Training loss: 3.54506203450535
Validation loss: 2.59134887618741

Epoch: 5| Step: 2
Training loss: 2.631181432415037
Validation loss: 2.5599943386644655

Epoch: 5| Step: 3
Training loss: 2.723246922174576
Validation loss: 2.567213700776646

Epoch: 5| Step: 4
Training loss: 3.025367613594701
Validation loss: 2.555093628173425

Epoch: 5| Step: 5
Training loss: 2.6779207238506446
Validation loss: 2.559101824870128

Epoch: 5| Step: 6
Training loss: 3.0637901858212118
Validation loss: 2.5514231444234845

Epoch: 5| Step: 7
Training loss: 2.399943323260885
Validation loss: 2.544317287232122

Epoch: 5| Step: 8
Training loss: 2.7984430616569522
Validation loss: 2.5484988332302394

Epoch: 5| Step: 9
Training loss: 2.643289158834022
Validation loss: 2.5654647742743797

Epoch: 5| Step: 10
Training loss: 3.3354034353673256
Validation loss: 2.5460079269057005

Epoch: 50| Step: 0
Training loss: 2.478341218961045
Validation loss: 2.5360654983363586

Epoch: 5| Step: 1
Training loss: 2.4733650433066225
Validation loss: 2.572021045185725

Epoch: 5| Step: 2
Training loss: 2.6149261270157815
Validation loss: 2.573712646332522

Epoch: 5| Step: 3
Training loss: 2.6818758645341876
Validation loss: 2.5464486684377676

Epoch: 5| Step: 4
Training loss: 2.579071980732472
Validation loss: 2.5647662646328335

Epoch: 5| Step: 5
Training loss: 3.505588293619602
Validation loss: 2.561847797855369

Epoch: 5| Step: 6
Training loss: 2.877465559208936
Validation loss: 2.542353198609249

Epoch: 5| Step: 7
Training loss: 3.213327376991967
Validation loss: 2.5640534265170465

Epoch: 5| Step: 8
Training loss: 3.1927082669546962
Validation loss: 2.5710908282525513

Epoch: 5| Step: 9
Training loss: 3.0550563423717603
Validation loss: 2.5597474536290927

Epoch: 5| Step: 10
Training loss: 2.765207722537187
Validation loss: 2.5738942225271284

Epoch: 51| Step: 0
Training loss: 3.4932749034518347
Validation loss: 2.5429884971548926

Epoch: 5| Step: 1
Training loss: 2.639055948379181
Validation loss: 2.5535227597284362

Epoch: 5| Step: 2
Training loss: 3.076527447207785
Validation loss: 2.5545721826777252

Epoch: 5| Step: 3
Training loss: 2.8112327263735204
Validation loss: 2.5504385219448222

Epoch: 5| Step: 4
Training loss: 2.991138084986554
Validation loss: 2.5540073494506847

Epoch: 5| Step: 5
Training loss: 3.039487519735752
Validation loss: 2.5591269081562493

Epoch: 5| Step: 6
Training loss: 3.362494435358938
Validation loss: 2.549277600631616

Epoch: 5| Step: 7
Training loss: 2.786934005684127
Validation loss: 2.5719244949852693

Epoch: 5| Step: 8
Training loss: 2.2987495091299306
Validation loss: 2.56089359697216

Epoch: 5| Step: 9
Training loss: 2.7451948319795707
Validation loss: 2.5537078164092417

Epoch: 5| Step: 10
Training loss: 2.314355054620543
Validation loss: 2.525626519741851

Epoch: 52| Step: 0
Training loss: 3.236913891617762
Validation loss: 2.5721419780101877

Epoch: 5| Step: 1
Training loss: 2.4281625163004237
Validation loss: 2.567449676026558

Epoch: 5| Step: 2
Training loss: 2.7247829805802453
Validation loss: 2.545080339149399

Epoch: 5| Step: 3
Training loss: 3.1588945546074987
Validation loss: 2.5419425629695285

Epoch: 5| Step: 4
Training loss: 2.306284412654524
Validation loss: 2.555131457013161

Epoch: 5| Step: 5
Training loss: 2.8501160179750524
Validation loss: 2.55236114252253

Epoch: 5| Step: 6
Training loss: 3.1651880593496817
Validation loss: 2.54794613521458

Epoch: 5| Step: 7
Training loss: 2.7708892721012597
Validation loss: 2.5653051323957565

Epoch: 5| Step: 8
Training loss: 3.1106555544636607
Validation loss: 2.5485221749889684

Epoch: 5| Step: 9
Training loss: 2.884675041951216
Validation loss: 2.5484138337832056

Epoch: 5| Step: 10
Training loss: 3.2055445023171103
Validation loss: 2.553485210759591

Epoch: 53| Step: 0
Training loss: 3.043644214334946
Validation loss: 2.559848100497222

Epoch: 5| Step: 1
Training loss: 2.5410483707836717
Validation loss: 2.5458277115510453

Epoch: 5| Step: 2
Training loss: 3.5829891808421275
Validation loss: 2.5538130168495994

Epoch: 5| Step: 3
Training loss: 2.907473398941383
Validation loss: 2.555396570263061

Epoch: 5| Step: 4
Training loss: 3.1087282361172575
Validation loss: 2.5395538485709706

Epoch: 5| Step: 5
Training loss: 2.71541741985806
Validation loss: 2.555096924159906

Epoch: 5| Step: 6
Training loss: 2.6728389852588172
Validation loss: 2.565037021452548

Epoch: 5| Step: 7
Training loss: 2.538866240912586
Validation loss: 2.553048803361464

Epoch: 5| Step: 8
Training loss: 2.918043919505344
Validation loss: 2.5515842280356864

Epoch: 5| Step: 9
Training loss: 2.7481096012463997
Validation loss: 2.5693575548843683

Epoch: 5| Step: 10
Training loss: 2.8032481290892157
Validation loss: 2.536741151792921

Epoch: 54| Step: 0
Training loss: 2.7809390633610076
Validation loss: 2.5457425640070137

Epoch: 5| Step: 1
Training loss: 2.7585532946822053
Validation loss: 2.5570858339832307

Epoch: 5| Step: 2
Training loss: 2.8109476679840157
Validation loss: 2.5551707260533227

Epoch: 5| Step: 3
Training loss: 3.03966071117727
Validation loss: 2.5546973925291074

Epoch: 5| Step: 4
Training loss: 2.513737699155146
Validation loss: 2.5484267937263234

Epoch: 5| Step: 5
Training loss: 3.4032048603982656
Validation loss: 2.5267014536923833

Epoch: 5| Step: 6
Training loss: 3.352628525013381
Validation loss: 2.532168180439205

Epoch: 5| Step: 7
Training loss: 2.690945036282506
Validation loss: 2.552611204046249

Epoch: 5| Step: 8
Training loss: 2.5493423586264
Validation loss: 2.5417499918587385

Epoch: 5| Step: 9
Training loss: 3.250076586481181
Validation loss: 2.5580001456830304

Epoch: 5| Step: 10
Training loss: 2.3035465843238123
Validation loss: 2.5505877340076357

Epoch: 55| Step: 0
Training loss: 2.481413029232148
Validation loss: 2.5604212743503645

Epoch: 5| Step: 1
Training loss: 3.370389968766959
Validation loss: 2.568858721251951

Epoch: 5| Step: 2
Training loss: 2.4986364460762682
Validation loss: 2.5486040444876066

Epoch: 5| Step: 3
Training loss: 3.366649611275658
Validation loss: 2.5648452117535836

Epoch: 5| Step: 4
Training loss: 3.1262341923182477
Validation loss: 2.561563286099507

Epoch: 5| Step: 5
Training loss: 2.5708422825027415
Validation loss: 2.5584033020024437

Epoch: 5| Step: 6
Training loss: 2.605387948082947
Validation loss: 2.5578112421000037

Epoch: 5| Step: 7
Training loss: 2.433018697576648
Validation loss: 2.5686892206534337

Epoch: 5| Step: 8
Training loss: 2.7816393290418797
Validation loss: 2.546632861030217

Epoch: 5| Step: 9
Training loss: 2.5293367478457047
Validation loss: 2.529505144619908

Epoch: 5| Step: 10
Training loss: 3.708640710900206
Validation loss: 2.5478918583593835

Epoch: 56| Step: 0
Training loss: 2.2525528095248926
Validation loss: 2.5474731982912218

Epoch: 5| Step: 1
Training loss: 3.1067690256416753
Validation loss: 2.541828191403322

Epoch: 5| Step: 2
Training loss: 3.202675690190639
Validation loss: 2.555563520700289

Epoch: 5| Step: 3
Training loss: 3.230744023801121
Validation loss: 2.5313218204179515

Epoch: 5| Step: 4
Training loss: 3.1680042721112662
Validation loss: 2.5683592192866076

Epoch: 5| Step: 5
Training loss: 2.5700871794939126
Validation loss: 2.5480813655221897

Epoch: 5| Step: 6
Training loss: 2.659733720516077
Validation loss: 2.553799859377166

Epoch: 5| Step: 7
Training loss: 3.056604744697557
Validation loss: 2.5376231203024555

Epoch: 5| Step: 8
Training loss: 3.1003761617028305
Validation loss: 2.5507759996585753

Epoch: 5| Step: 9
Training loss: 3.0224568356191877
Validation loss: 2.5402747010229096

Epoch: 5| Step: 10
Training loss: 2.051114068045826
Validation loss: 2.5431462511333547

Epoch: 57| Step: 0
Training loss: 3.083307541060611
Validation loss: 2.535577687919462

Epoch: 5| Step: 1
Training loss: 2.611165055152865
Validation loss: 2.5518378055305684

Epoch: 5| Step: 2
Training loss: 2.90980001227751
Validation loss: 2.5429547127725542

Epoch: 5| Step: 3
Training loss: 3.086034845675853
Validation loss: 2.5546421070700367

Epoch: 5| Step: 4
Training loss: 2.865907804455708
Validation loss: 2.5563850764890352

Epoch: 5| Step: 5
Training loss: 3.013512222105754
Validation loss: 2.5340160811413903

Epoch: 5| Step: 6
Training loss: 3.1527111496422586
Validation loss: 2.5494301909921266

Epoch: 5| Step: 7
Training loss: 3.13998573591552
Validation loss: 2.5491633399291307

Epoch: 5| Step: 8
Training loss: 2.3425124906778905
Validation loss: 2.5592664400287832

Epoch: 5| Step: 9
Training loss: 2.6479491737305225
Validation loss: 2.5539459208871613

Epoch: 5| Step: 10
Training loss: 2.565109784786572
Validation loss: 2.545921889698453

Epoch: 58| Step: 0
Training loss: 2.4647600791501425
Validation loss: 2.5527664874117573

Epoch: 5| Step: 1
Training loss: 2.9604198637445536
Validation loss: 2.54937803234238

Epoch: 5| Step: 2
Training loss: 2.956026619663576
Validation loss: 2.554983750493872

Epoch: 5| Step: 3
Training loss: 3.0950698349181103
Validation loss: 2.559683243314254

Epoch: 5| Step: 4
Training loss: 3.669986190354146
Validation loss: 2.539384493797693

Epoch: 5| Step: 5
Training loss: 2.3373837283994727
Validation loss: 2.5525972951690044

Epoch: 5| Step: 6
Training loss: 2.870038104452326
Validation loss: 2.5734134423824773

Epoch: 5| Step: 7
Training loss: 2.751634025618887
Validation loss: 2.5504889923581295

Epoch: 5| Step: 8
Training loss: 2.7445678812737415
Validation loss: 2.553705729321978

Epoch: 5| Step: 9
Training loss: 3.2455562141896928
Validation loss: 2.5490173372708558

Epoch: 5| Step: 10
Training loss: 2.0484528029602758
Validation loss: 2.543464794861343

Epoch: 59| Step: 0
Training loss: 2.692677589304941
Validation loss: 2.559839520790432

Epoch: 5| Step: 1
Training loss: 2.7065698800219247
Validation loss: 2.5337475662126265

Epoch: 5| Step: 2
Training loss: 3.0636166463628545
Validation loss: 2.5524238571870925

Epoch: 5| Step: 3
Training loss: 2.964928184784612
Validation loss: 2.562894286108468

Epoch: 5| Step: 4
Training loss: 2.7734864996221678
Validation loss: 2.549674866570462

Epoch: 5| Step: 5
Training loss: 2.635537658648153
Validation loss: 2.537304736361952

Epoch: 5| Step: 6
Training loss: 2.857607435875047
Validation loss: 2.5477283748462685

Epoch: 5| Step: 7
Training loss: 3.3838265465095567
Validation loss: 2.5575679312960102

Epoch: 5| Step: 8
Training loss: 2.761943198313986
Validation loss: 2.5521174266380395

Epoch: 5| Step: 9
Training loss: 3.345468525141129
Validation loss: 2.5395141210011984

Epoch: 5| Step: 10
Training loss: 2.264722025910636
Validation loss: 2.5512134886985036

Epoch: 60| Step: 0
Training loss: 3.171184295466826
Validation loss: 2.5649376276644396

Epoch: 5| Step: 1
Training loss: 2.8399826078822614
Validation loss: 2.5516525202940588

Epoch: 5| Step: 2
Training loss: 2.565553659987526
Validation loss: 2.539908458800036

Epoch: 5| Step: 3
Training loss: 3.0789501033158198
Validation loss: 2.5292167939449928

Epoch: 5| Step: 4
Training loss: 2.4421880584149935
Validation loss: 2.5527418447670605

Epoch: 5| Step: 5
Training loss: 3.594145645255841
Validation loss: 2.54901322480961

Epoch: 5| Step: 6
Training loss: 2.2296507716746023
Validation loss: 2.52879637520824

Epoch: 5| Step: 7
Training loss: 2.8318433396710794
Validation loss: 2.555911562630713

Epoch: 5| Step: 8
Training loss: 2.2704463827375414
Validation loss: 2.541948565768832

Epoch: 5| Step: 9
Training loss: 2.814002419888907
Validation loss: 2.5330230183084037

Epoch: 5| Step: 10
Training loss: 3.451782932383275
Validation loss: 2.551904269665435

Epoch: 61| Step: 0
Training loss: 3.7472480849069583
Validation loss: 2.5263992484420226

Epoch: 5| Step: 1
Training loss: 2.5455511666203696
Validation loss: 2.539519471347921

Epoch: 5| Step: 2
Training loss: 2.531355773222852
Validation loss: 2.534896880042674

Epoch: 5| Step: 3
Training loss: 3.407641485331711
Validation loss: 2.545816655204478

Epoch: 5| Step: 4
Training loss: 3.100756485220929
Validation loss: 2.542489857728989

Epoch: 5| Step: 5
Training loss: 2.8622345513946383
Validation loss: 2.547163168360615

Epoch: 5| Step: 6
Training loss: 2.681604083674242
Validation loss: 2.5542677467099666

Epoch: 5| Step: 7
Training loss: 2.7661875352042626
Validation loss: 2.5585123905415075

Epoch: 5| Step: 8
Training loss: 2.5669777058315733
Validation loss: 2.5554840029350268

Epoch: 5| Step: 9
Training loss: 2.359771998270083
Validation loss: 2.5400486487355693

Epoch: 5| Step: 10
Training loss: 2.6210200701617907
Validation loss: 2.5477213663355642

Epoch: 62| Step: 0
Training loss: 2.855957991283071
Validation loss: 2.554101786573748

Epoch: 5| Step: 1
Training loss: 2.444626489517016
Validation loss: 2.548324128406628

Epoch: 5| Step: 2
Training loss: 2.76451166017405
Validation loss: 2.538818740439604

Epoch: 5| Step: 3
Training loss: 3.192479152865292
Validation loss: 2.5352236054961623

Epoch: 5| Step: 4
Training loss: 2.6870535546244554
Validation loss: 2.5617385881553214

Epoch: 5| Step: 5
Training loss: 2.505408444941197
Validation loss: 2.5456062063992433

Epoch: 5| Step: 6
Training loss: 3.020501339964371
Validation loss: 2.553024005788903

Epoch: 5| Step: 7
Training loss: 2.52285136215234
Validation loss: 2.5347625241119442

Epoch: 5| Step: 8
Training loss: 2.855018173133211
Validation loss: 2.5410645230760083

Epoch: 5| Step: 9
Training loss: 3.100113768181966
Validation loss: 2.534518157802922

Epoch: 5| Step: 10
Training loss: 3.467376712073226
Validation loss: 2.521707090132113

Epoch: 63| Step: 0
Training loss: 3.0479775023543723
Validation loss: 2.535490392855912

Epoch: 5| Step: 1
Training loss: 2.7623725343873864
Validation loss: 2.5508175739459023

Epoch: 5| Step: 2
Training loss: 2.9401291789006563
Validation loss: 2.559139112615234

Epoch: 5| Step: 3
Training loss: 2.731708818040246
Validation loss: 2.536092019466855

Epoch: 5| Step: 4
Training loss: 3.4336123590757097
Validation loss: 2.532008310819998

Epoch: 5| Step: 5
Training loss: 2.6527349214492886
Validation loss: 2.551652048085872

Epoch: 5| Step: 6
Training loss: 2.5366033764570144
Validation loss: 2.5533429638262697

Epoch: 5| Step: 7
Training loss: 2.5535890043189244
Validation loss: 2.5386741261643877

Epoch: 5| Step: 8
Training loss: 2.8063302927063796
Validation loss: 2.5373282892086517

Epoch: 5| Step: 9
Training loss: 2.9881607729492528
Validation loss: 2.5481542900793492

Epoch: 5| Step: 10
Training loss: 2.9818684702161886
Validation loss: 2.5473081762791114

Epoch: 64| Step: 0
Training loss: 2.4055890253022825
Validation loss: 2.560664172869745

Epoch: 5| Step: 1
Training loss: 2.4561082716570235
Validation loss: 2.56207161160582

Epoch: 5| Step: 2
Training loss: 2.4371389463978193
Validation loss: 2.5330504032207584

Epoch: 5| Step: 3
Training loss: 2.7035396015419697
Validation loss: 2.541265590848233

Epoch: 5| Step: 4
Training loss: 3.0491552965470516
Validation loss: 2.532699731907802

Epoch: 5| Step: 5
Training loss: 2.690564205178442
Validation loss: 2.552583831134915

Epoch: 5| Step: 6
Training loss: 3.9809083705189416
Validation loss: 2.532628538679176

Epoch: 5| Step: 7
Training loss: 2.8723429135761953
Validation loss: 2.535594301729054

Epoch: 5| Step: 8
Training loss: 2.951909536676171
Validation loss: 2.565352731049465

Epoch: 5| Step: 9
Training loss: 2.3504039660685856
Validation loss: 2.5391757603477134

Epoch: 5| Step: 10
Training loss: 3.3005681618071248
Validation loss: 2.531023049520443

Epoch: 65| Step: 0
Training loss: 2.986535054554036
Validation loss: 2.5419266169565002

Epoch: 5| Step: 1
Training loss: 2.9879028878625493
Validation loss: 2.539589585085728

Epoch: 5| Step: 2
Training loss: 3.632736238068822
Validation loss: 2.539305330564614

Epoch: 5| Step: 3
Training loss: 2.769308621991317
Validation loss: 2.5499115017504437

Epoch: 5| Step: 4
Training loss: 2.507257417942294
Validation loss: 2.5311893115053565

Epoch: 5| Step: 5
Training loss: 2.8652193970083975
Validation loss: 2.5446858598400834

Epoch: 5| Step: 6
Training loss: 2.564892837933111
Validation loss: 2.5480710710220507

Epoch: 5| Step: 7
Training loss: 2.6902196563322494
Validation loss: 2.546174023939669

Epoch: 5| Step: 8
Training loss: 3.0293423657226675
Validation loss: 2.53593403592427

Epoch: 5| Step: 9
Training loss: 2.5617762218007054
Validation loss: 2.569756435848434

Epoch: 5| Step: 10
Training loss: 2.603351516301946
Validation loss: 2.5216318538866354

Epoch: 66| Step: 0
Training loss: 2.9037172499742208
Validation loss: 2.551172818227395

Epoch: 5| Step: 1
Training loss: 2.6317971646030767
Validation loss: 2.527353599432656

Epoch: 5| Step: 2
Training loss: 2.8121249478663124
Validation loss: 2.54396611825705

Epoch: 5| Step: 3
Training loss: 3.036767872932455
Validation loss: 2.553716519113074

Epoch: 5| Step: 4
Training loss: 2.3929466991299053
Validation loss: 2.5082789044138205

Epoch: 5| Step: 5
Training loss: 2.943448152070252
Validation loss: 2.5339961537902886

Epoch: 5| Step: 6
Training loss: 2.930113575787908
Validation loss: 2.5173141534706893

Epoch: 5| Step: 7
Training loss: 2.8251278839873017
Validation loss: 2.5486703525485543

Epoch: 5| Step: 8
Training loss: 2.976178963048555
Validation loss: 2.5328923726757897

Epoch: 5| Step: 9
Training loss: 2.9436621456440535
Validation loss: 2.526410643956578

Epoch: 5| Step: 10
Training loss: 3.0052744275807495
Validation loss: 2.5418471042513704

Epoch: 67| Step: 0
Training loss: 2.423601679402544
Validation loss: 2.545508956430854

Epoch: 5| Step: 1
Training loss: 3.091356681096625
Validation loss: 2.533678185264335

Epoch: 5| Step: 2
Training loss: 2.5649771463666213
Validation loss: 2.552249013224444

Epoch: 5| Step: 3
Training loss: 2.667585800635785
Validation loss: 2.531296644916636

Epoch: 5| Step: 4
Training loss: 2.9625000064383076
Validation loss: 2.5313218290264756

Epoch: 5| Step: 5
Training loss: 2.8067851178717786
Validation loss: 2.5603960675723485

Epoch: 5| Step: 6
Training loss: 3.186534361333942
Validation loss: 2.5238941363246727

Epoch: 5| Step: 7
Training loss: 3.4245018478435356
Validation loss: 2.5308084329379628

Epoch: 5| Step: 8
Training loss: 3.335577701935881
Validation loss: 2.5608010511353494

Epoch: 5| Step: 9
Training loss: 2.5979328667921515
Validation loss: 2.5301789935198165

Epoch: 5| Step: 10
Training loss: 2.2089491891124537
Validation loss: 2.5441120329184805

Epoch: 68| Step: 0
Training loss: 2.9623372737440086
Validation loss: 2.524909229285715

Epoch: 5| Step: 1
Training loss: 2.547202060036809
Validation loss: 2.537960614198077

Epoch: 5| Step: 2
Training loss: 2.970999096654898
Validation loss: 2.5381947548962795

Epoch: 5| Step: 3
Training loss: 3.1965025385574006
Validation loss: 2.5569646492698967

Epoch: 5| Step: 4
Training loss: 2.6721516962277025
Validation loss: 2.5573080715749894

Epoch: 5| Step: 5
Training loss: 3.1214877418229716
Validation loss: 2.5415572548537066

Epoch: 5| Step: 6
Training loss: 2.5556686337189247
Validation loss: 2.5259205142874235

Epoch: 5| Step: 7
Training loss: 2.443330296527108
Validation loss: 2.537394863641844

Epoch: 5| Step: 8
Training loss: 2.6116246559384946
Validation loss: 2.557342421193404

Epoch: 5| Step: 9
Training loss: 3.3974003053119444
Validation loss: 2.537267471272384

Epoch: 5| Step: 10
Training loss: 2.6531451760323557
Validation loss: 2.5393497770870046

Epoch: 69| Step: 0
Training loss: 2.8430567305060896
Validation loss: 2.554006979058852

Epoch: 5| Step: 1
Training loss: 2.670031709232702
Validation loss: 2.5506948863396475

Epoch: 5| Step: 2
Training loss: 2.345908633209881
Validation loss: 2.540823843286973

Epoch: 5| Step: 3
Training loss: 2.889563453200715
Validation loss: 2.5375067941083786

Epoch: 5| Step: 4
Training loss: 2.617395011304682
Validation loss: 2.5320880312371163

Epoch: 5| Step: 5
Training loss: 3.1165475594783922
Validation loss: 2.531016907365523

Epoch: 5| Step: 6
Training loss: 3.1666660476148567
Validation loss: 2.5432934224331882

Epoch: 5| Step: 7
Training loss: 3.232746691100811
Validation loss: 2.55212118804227

Epoch: 5| Step: 8
Training loss: 3.389408860330272
Validation loss: 2.513870992328463

Epoch: 5| Step: 9
Training loss: 2.379856264438251
Validation loss: 2.5355165317426045

Epoch: 5| Step: 10
Training loss: 2.3609981385174805
Validation loss: 2.5473380232546816

Epoch: 70| Step: 0
Training loss: 2.802173561052164
Validation loss: 2.541373215923521

Epoch: 5| Step: 1
Training loss: 2.68747897472585
Validation loss: 2.522479044284908

Epoch: 5| Step: 2
Training loss: 2.7755257495090446
Validation loss: 2.525908792796494

Epoch: 5| Step: 3
Training loss: 2.314014660471483
Validation loss: 2.525396948337613

Epoch: 5| Step: 4
Training loss: 3.089827237284021
Validation loss: 2.5660734174489246

Epoch: 5| Step: 5
Training loss: 3.1867264949763383
Validation loss: 2.5624871392456314

Epoch: 5| Step: 6
Training loss: 2.7054637868132354
Validation loss: 2.5383875704712926

Epoch: 5| Step: 7
Training loss: 2.9523270755164708
Validation loss: 2.5277309853626173

Epoch: 5| Step: 8
Training loss: 3.2397735801513927
Validation loss: 2.5484279868063404

Epoch: 5| Step: 9
Training loss: 2.810197862874324
Validation loss: 2.5260759688546353

Epoch: 5| Step: 10
Training loss: 2.606474858013275
Validation loss: 2.5403844277633154

Epoch: 71| Step: 0
Training loss: 2.8664336915676762
Validation loss: 2.538471539227861

Epoch: 5| Step: 1
Training loss: 3.409901333049866
Validation loss: 2.5343758522892985

Epoch: 5| Step: 2
Training loss: 2.0294417328666596
Validation loss: 2.510898705583716

Epoch: 5| Step: 3
Training loss: 3.3830512757639726
Validation loss: 2.545325365385716

Epoch: 5| Step: 4
Training loss: 2.582331883736804
Validation loss: 2.5494592735337878

Epoch: 5| Step: 5
Training loss: 2.3277943011445754
Validation loss: 2.5140547001879057

Epoch: 5| Step: 6
Training loss: 2.499775876489375
Validation loss: 2.551995037092548

Epoch: 5| Step: 7
Training loss: 2.7534989725432757
Validation loss: 2.535393785228557

Epoch: 5| Step: 8
Training loss: 3.451328414363302
Validation loss: 2.5431727165680518

Epoch: 5| Step: 9
Training loss: 2.7278320504784888
Validation loss: 2.506846971976235

Epoch: 5| Step: 10
Training loss: 2.906117425991652
Validation loss: 2.5633479210393832

Epoch: 72| Step: 0
Training loss: 2.3458968439081676
Validation loss: 2.5218539841738385

Epoch: 5| Step: 1
Training loss: 3.40661815088691
Validation loss: 2.541195875435364

Epoch: 5| Step: 2
Training loss: 2.5488921994161275
Validation loss: 2.5337754532347176

Epoch: 5| Step: 3
Training loss: 2.742477271264894
Validation loss: 2.5292920280635305

Epoch: 5| Step: 4
Training loss: 2.8867527775184945
Validation loss: 2.5563060014731476

Epoch: 5| Step: 5
Training loss: 2.2977246639750395
Validation loss: 2.5393488866498317

Epoch: 5| Step: 6
Training loss: 2.8297633699623135
Validation loss: 2.5230285563087143

Epoch: 5| Step: 7
Training loss: 3.3413390978098487
Validation loss: 2.5507442953668464

Epoch: 5| Step: 8
Training loss: 3.1974169975525233
Validation loss: 2.540546951958859

Epoch: 5| Step: 9
Training loss: 2.7385708251731864
Validation loss: 2.540763493453927

Epoch: 5| Step: 10
Training loss: 2.6352870642919273
Validation loss: 2.535190246506781

Epoch: 73| Step: 0
Training loss: 2.898625205817625
Validation loss: 2.5065982887048333

Epoch: 5| Step: 1
Training loss: 2.9182037299756085
Validation loss: 2.553778874237843

Epoch: 5| Step: 2
Training loss: 2.750129523261469
Validation loss: 2.5308086730125545

Epoch: 5| Step: 3
Training loss: 2.4723420870103388
Validation loss: 2.5264847541366082

Epoch: 5| Step: 4
Training loss: 3.37719909231103
Validation loss: 2.5256694621396494

Epoch: 5| Step: 5
Training loss: 1.9149299368728125
Validation loss: 2.541476076290584

Epoch: 5| Step: 6
Training loss: 2.734231738016014
Validation loss: 2.5325951353853533

Epoch: 5| Step: 7
Training loss: 3.0170697324063687
Validation loss: 2.5197666575577333

Epoch: 5| Step: 8
Training loss: 2.7249890353480937
Validation loss: 2.529283434914939

Epoch: 5| Step: 9
Training loss: 3.229180924578948
Validation loss: 2.5294679734067964

Epoch: 5| Step: 10
Training loss: 2.9113950409717204
Validation loss: 2.5231368820696387

Epoch: 74| Step: 0
Training loss: 3.1827234119949446
Validation loss: 2.5286856914371563

Epoch: 5| Step: 1
Training loss: 2.536273727644417
Validation loss: 2.5185807649839433

Epoch: 5| Step: 2
Training loss: 2.731449065821295
Validation loss: 2.5399549951749645

Epoch: 5| Step: 3
Training loss: 2.944136569483574
Validation loss: 2.5274502679033852

Epoch: 5| Step: 4
Training loss: 2.963866061315684
Validation loss: 2.5375470127834676

Epoch: 5| Step: 5
Training loss: 2.5004697358379104
Validation loss: 2.5363154891086204

Epoch: 5| Step: 6
Training loss: 3.202067577461457
Validation loss: 2.525052117047744

Epoch: 5| Step: 7
Training loss: 3.2791929063793326
Validation loss: 2.543649583487191

Epoch: 5| Step: 8
Training loss: 2.415269371000048
Validation loss: 2.536009767273835

Epoch: 5| Step: 9
Training loss: 2.1742415673350193
Validation loss: 2.5242477787800723

Epoch: 5| Step: 10
Training loss: 3.1055344676615624
Validation loss: 2.523816858043041

Epoch: 75| Step: 0
Training loss: 2.692486240419102
Validation loss: 2.544218448332177

Epoch: 5| Step: 1
Training loss: 3.1500290097308623
Validation loss: 2.5453987093234263

Epoch: 5| Step: 2
Training loss: 2.537697199031951
Validation loss: 2.524635252411286

Epoch: 5| Step: 3
Training loss: 3.0120357044834325
Validation loss: 2.513349301316385

Epoch: 5| Step: 4
Training loss: 3.571985299450739
Validation loss: 2.545065097759173

Epoch: 5| Step: 5
Training loss: 2.0752389701290044
Validation loss: 2.544888680410417

Epoch: 5| Step: 6
Training loss: 2.3523242008305294
Validation loss: 2.5492932040034724

Epoch: 5| Step: 7
Training loss: 2.3018067022544395
Validation loss: 2.548702844075705

Epoch: 5| Step: 8
Training loss: 3.1732033639893613
Validation loss: 2.512145453595218

Epoch: 5| Step: 9
Training loss: 2.9971606805576116
Validation loss: 2.5230591599353818

Epoch: 5| Step: 10
Training loss: 2.967109467824734
Validation loss: 2.5335058938253527

Epoch: 76| Step: 0
Training loss: 2.943766301913792
Validation loss: 2.523603081306688

Epoch: 5| Step: 1
Training loss: 2.806723363157203
Validation loss: 2.5328230052455156

Epoch: 5| Step: 2
Training loss: 1.9872097283248074
Validation loss: 2.546252879989392

Epoch: 5| Step: 3
Training loss: 2.305850598659289
Validation loss: 2.53928028873325

Epoch: 5| Step: 4
Training loss: 2.6863569445864837
Validation loss: 2.515356307449707

Epoch: 5| Step: 5
Training loss: 2.9483490931159984
Validation loss: 2.535686718102216

Epoch: 5| Step: 6
Training loss: 2.5867887173530617
Validation loss: 2.5171085019797306

Epoch: 5| Step: 7
Training loss: 2.787913622767297
Validation loss: 2.531019110399386

Epoch: 5| Step: 8
Training loss: 3.2966827770439933
Validation loss: 2.5297926420972896

Epoch: 5| Step: 9
Training loss: 3.1247543238390096
Validation loss: 2.533868549066109

Epoch: 5| Step: 10
Training loss: 3.5613737501104925
Validation loss: 2.5334092894529894

Epoch: 77| Step: 0
Training loss: 3.4029342892176744
Validation loss: 2.5416126353066497

Epoch: 5| Step: 1
Training loss: 3.343559188389619
Validation loss: 2.547645283761681

Epoch: 5| Step: 2
Training loss: 3.061146028122991
Validation loss: 2.515012074967533

Epoch: 5| Step: 3
Training loss: 3.1192817250000733
Validation loss: 2.528684776461404

Epoch: 5| Step: 4
Training loss: 2.5526658204380217
Validation loss: 2.5128304514540543

Epoch: 5| Step: 5
Training loss: 2.2403594217429776
Validation loss: 2.5056605213752388

Epoch: 5| Step: 6
Training loss: 2.519702426507097
Validation loss: 2.5375049038382023

Epoch: 5| Step: 7
Training loss: 2.171304421980902
Validation loss: 2.5365750960436

Epoch: 5| Step: 8
Training loss: 2.567501954818026
Validation loss: 2.5358149253674185

Epoch: 5| Step: 9
Training loss: 3.3579167949371813
Validation loss: 2.520355639701894

Epoch: 5| Step: 10
Training loss: 2.6897125008255376
Validation loss: 2.536218332703991

Epoch: 78| Step: 0
Training loss: 2.64948272334893
Validation loss: 2.516995743849483

Epoch: 5| Step: 1
Training loss: 2.9509356421210593
Validation loss: 2.5387298027772456

Epoch: 5| Step: 2
Training loss: 2.6578759097833444
Validation loss: 2.5315596151749484

Epoch: 5| Step: 3
Training loss: 3.28743879602122
Validation loss: 2.530243667858841

Epoch: 5| Step: 4
Training loss: 1.9278731987002473
Validation loss: 2.522079568207807

Epoch: 5| Step: 5
Training loss: 3.4477950859338846
Validation loss: 2.540817191577113

Epoch: 5| Step: 6
Training loss: 2.8223401333698557
Validation loss: 2.529780530660214

Epoch: 5| Step: 7
Training loss: 3.1090518649325123
Validation loss: 2.531889824094375

Epoch: 5| Step: 8
Training loss: 2.7718685386608213
Validation loss: 2.537451598851845

Epoch: 5| Step: 9
Training loss: 2.9058179636764763
Validation loss: 2.5444671645458206

Epoch: 5| Step: 10
Training loss: 2.2588344491900365
Validation loss: 2.5463176313442624

Epoch: 79| Step: 0
Training loss: 2.7239312097039043
Validation loss: 2.518250516087572

Epoch: 5| Step: 1
Training loss: 2.8089056146937903
Validation loss: 2.536977991942783

Epoch: 5| Step: 2
Training loss: 3.1855046627912884
Validation loss: 2.543783232184674

Epoch: 5| Step: 3
Training loss: 3.6605495375934054
Validation loss: 2.530423576242194

Epoch: 5| Step: 4
Training loss: 2.5317807053310273
Validation loss: 2.5281679112762485

Epoch: 5| Step: 5
Training loss: 2.2855875244072528
Validation loss: 2.538271535849781

Epoch: 5| Step: 6
Training loss: 2.666382883787113
Validation loss: 2.5359853762720683

Epoch: 5| Step: 7
Training loss: 2.6830068776671316
Validation loss: 2.5300881978108074

Epoch: 5| Step: 8
Training loss: 2.3629825119488554
Validation loss: 2.52125361095213

Epoch: 5| Step: 9
Training loss: 3.1560304773113743
Validation loss: 2.541398817164444

Epoch: 5| Step: 10
Training loss: 2.611708733861524
Validation loss: 2.5284333006945725

Epoch: 80| Step: 0
Training loss: 2.7697548930660947
Validation loss: 2.5360040708717975

Epoch: 5| Step: 1
Training loss: 1.990753615220239
Validation loss: 2.5257979139764593

Epoch: 5| Step: 2
Training loss: 2.5881377019434635
Validation loss: 2.507234973207633

Epoch: 5| Step: 3
Training loss: 2.439120023294694
Validation loss: 2.530978932837649

Epoch: 5| Step: 4
Training loss: 2.926608082120496
Validation loss: 2.5157920156750535

Epoch: 5| Step: 5
Training loss: 2.9119377674118136
Validation loss: 2.512550098660693

Epoch: 5| Step: 6
Training loss: 2.3665716295457857
Validation loss: 2.510657256619869

Epoch: 5| Step: 7
Training loss: 2.8649908897161853
Validation loss: 2.5289641602026505

Epoch: 5| Step: 8
Training loss: 3.539170421445355
Validation loss: 2.5471767315000537

Epoch: 5| Step: 9
Training loss: 3.1379142943640788
Validation loss: 2.515994120865333

Epoch: 5| Step: 10
Training loss: 3.1877855659171965
Validation loss: 2.505407033889263

Epoch: 81| Step: 0
Training loss: 3.494177471071099
Validation loss: 2.5037517002380434

Epoch: 5| Step: 1
Training loss: 2.741014104557037
Validation loss: 2.5413810771933445

Epoch: 5| Step: 2
Training loss: 2.3861418930671534
Validation loss: 2.5214752834838605

Epoch: 5| Step: 3
Training loss: 3.25329657403576
Validation loss: 2.531069365385793

Epoch: 5| Step: 4
Training loss: 2.6960176734062045
Validation loss: 2.535867785060637

Epoch: 5| Step: 5
Training loss: 2.228599820899357
Validation loss: 2.5108815513614986

Epoch: 5| Step: 6
Training loss: 2.817964543359809
Validation loss: 2.5159101243734097

Epoch: 5| Step: 7
Training loss: 3.0862289097970343
Validation loss: 2.5183982383030368

Epoch: 5| Step: 8
Training loss: 3.109119193898046
Validation loss: 2.5268941225948836

Epoch: 5| Step: 9
Training loss: 2.4895570080587652
Validation loss: 2.5455695563151153

Epoch: 5| Step: 10
Training loss: 2.396681848176385
Validation loss: 2.51520192104407

Epoch: 82| Step: 0
Training loss: 3.215826095358071
Validation loss: 2.53587728192604

Epoch: 5| Step: 1
Training loss: 2.6620209777888197
Validation loss: 2.510668056807194

Epoch: 5| Step: 2
Training loss: 2.7210739551210805
Validation loss: 2.5324076571829597

Epoch: 5| Step: 3
Training loss: 2.6714223734854423
Validation loss: 2.5243989451429334

Epoch: 5| Step: 4
Training loss: 2.77113517513022
Validation loss: 2.5225200249391864

Epoch: 5| Step: 5
Training loss: 3.0938091850399405
Validation loss: 2.5341042009088177

Epoch: 5| Step: 6
Training loss: 2.3079597648273706
Validation loss: 2.5391804430303733

Epoch: 5| Step: 7
Training loss: 2.812620033245873
Validation loss: 2.515478750655414

Epoch: 5| Step: 8
Training loss: 3.2746495299600205
Validation loss: 2.525773507082286

Epoch: 5| Step: 9
Training loss: 2.511626388726363
Validation loss: 2.542775870462365

Epoch: 5| Step: 10
Training loss: 2.8512614888215215
Validation loss: 2.527644735789225

Epoch: 83| Step: 0
Training loss: 3.4774559673096705
Validation loss: 2.530525863536783

Epoch: 5| Step: 1
Training loss: 2.3057170055600933
Validation loss: 2.525957258662382

Epoch: 5| Step: 2
Training loss: 2.96617094748797
Validation loss: 2.5304859486418385

Epoch: 5| Step: 3
Training loss: 2.6604325519307577
Validation loss: 2.5370405881987086

Epoch: 5| Step: 4
Training loss: 2.9397525672852725
Validation loss: 2.531266445732902

Epoch: 5| Step: 5
Training loss: 2.8342232895933654
Validation loss: 2.536598498002084

Epoch: 5| Step: 6
Training loss: 2.723857160528638
Validation loss: 2.5174753747641128

Epoch: 5| Step: 7
Training loss: 2.418347749603579
Validation loss: 2.5202394726839255

Epoch: 5| Step: 8
Training loss: 2.5717905542871975
Validation loss: 2.5258718102175512

Epoch: 5| Step: 9
Training loss: 3.331940805117731
Validation loss: 2.5310288047384892

Epoch: 5| Step: 10
Training loss: 2.557087835098969
Validation loss: 2.528898041004397

Epoch: 84| Step: 0
Training loss: 2.97703568509632
Validation loss: 2.5374606422189965

Epoch: 5| Step: 1
Training loss: 2.6758529764850527
Validation loss: 2.5178392617041134

Epoch: 5| Step: 2
Training loss: 2.7152404934360463
Validation loss: 2.515922269465036

Epoch: 5| Step: 3
Training loss: 3.095660149790427
Validation loss: 2.52940614247089

Epoch: 5| Step: 4
Training loss: 2.6049237689324927
Validation loss: 2.534737046998591

Epoch: 5| Step: 5
Training loss: 3.3437468626774125
Validation loss: 2.5586031685243826

Epoch: 5| Step: 6
Training loss: 2.942366768697962
Validation loss: 2.5369492761287438

Epoch: 5| Step: 7
Training loss: 2.6630175378546395
Validation loss: 2.5399826182373926

Epoch: 5| Step: 8
Training loss: 2.2582967166348555
Validation loss: 2.5296620226891267

Epoch: 5| Step: 9
Training loss: 2.5476077845179725
Validation loss: 2.53035443646667

Epoch: 5| Step: 10
Training loss: 2.9314181086439284
Validation loss: 2.5460135535999027

Epoch: 85| Step: 0
Training loss: 2.491391906984084
Validation loss: 2.527026021808476

Epoch: 5| Step: 1
Training loss: 3.0897784702431697
Validation loss: 2.516584855903849

Epoch: 5| Step: 2
Training loss: 3.488372015546458
Validation loss: 2.536228976534965

Epoch: 5| Step: 3
Training loss: 3.323803885870602
Validation loss: 2.530327837975022

Epoch: 5| Step: 4
Training loss: 2.7642210074476927
Validation loss: 2.520186765841448

Epoch: 5| Step: 5
Training loss: 2.490850299136325
Validation loss: 2.5348421548753737

Epoch: 5| Step: 6
Training loss: 2.6733361426201547
Validation loss: 2.531910084945924

Epoch: 5| Step: 7
Training loss: 3.182295495929485
Validation loss: 2.5067385221366894

Epoch: 5| Step: 8
Training loss: 2.630776996639697
Validation loss: 2.5099993562160297

Epoch: 5| Step: 9
Training loss: 2.3886237650383038
Validation loss: 2.5250510510011397

Epoch: 5| Step: 10
Training loss: 2.089621844052497
Validation loss: 2.53162374285724

Epoch: 86| Step: 0
Training loss: 2.985251892043242
Validation loss: 2.519875766519512

Epoch: 5| Step: 1
Training loss: 3.116252099138602
Validation loss: 2.5348499908887145

Epoch: 5| Step: 2
Training loss: 2.670294935817107
Validation loss: 2.510642524096935

Epoch: 5| Step: 3
Training loss: 3.1627410012794783
Validation loss: 2.5361797032077122

Epoch: 5| Step: 4
Training loss: 2.737556916781755
Validation loss: 2.536386353452922

Epoch: 5| Step: 5
Training loss: 2.5676778258909394
Validation loss: 2.544060256991099

Epoch: 5| Step: 6
Training loss: 2.8083692097471116
Validation loss: 2.5052860171130718

Epoch: 5| Step: 7
Training loss: 2.7171073423542706
Validation loss: 2.5379223395143593

Epoch: 5| Step: 8
Training loss: 2.692921160926789
Validation loss: 2.5246176373402744

Epoch: 5| Step: 9
Training loss: 2.285363655785348
Validation loss: 2.548035388063106

Epoch: 5| Step: 10
Training loss: 3.190915988725954
Validation loss: 2.5215322447181245

Epoch: 87| Step: 0
Training loss: 2.411906344251356
Validation loss: 2.5187778148613034

Epoch: 5| Step: 1
Training loss: 3.3123050848277225
Validation loss: 2.5220274101037647

Epoch: 5| Step: 2
Training loss: 2.3481532124664075
Validation loss: 2.5226348450745846

Epoch: 5| Step: 3
Training loss: 3.0589600948986924
Validation loss: 2.5341363076183208

Epoch: 5| Step: 4
Training loss: 2.426613976496916
Validation loss: 2.5244074442244284

Epoch: 5| Step: 5
Training loss: 3.451441013310147
Validation loss: 2.5251394501179982

Epoch: 5| Step: 6
Training loss: 2.508763593438256
Validation loss: 2.537907063217914

Epoch: 5| Step: 7
Training loss: 2.71649848401796
Validation loss: 2.51930013405683

Epoch: 5| Step: 8
Training loss: 3.0860808906559587
Validation loss: 2.526384105418258

Epoch: 5| Step: 9
Training loss: 2.1264899023530703
Validation loss: 2.5393332706328717

Epoch: 5| Step: 10
Training loss: 3.1183258502108564
Validation loss: 2.5325570326812454

Epoch: 88| Step: 0
Training loss: 3.189443855716016
Validation loss: 2.517497954269542

Epoch: 5| Step: 1
Training loss: 2.6900660662000924
Validation loss: 2.5294255951656104

Epoch: 5| Step: 2
Training loss: 2.4333917279835666
Validation loss: 2.541493320319908

Epoch: 5| Step: 3
Training loss: 3.3142640796669336
Validation loss: 2.5313243168887674

Epoch: 5| Step: 4
Training loss: 3.0345189666381587
Validation loss: 2.5349795971392983

Epoch: 5| Step: 5
Training loss: 2.476460933106829
Validation loss: 2.523718252399564

Epoch: 5| Step: 6
Training loss: 2.4701134501978133
Validation loss: 2.530217417760101

Epoch: 5| Step: 7
Training loss: 2.753133462466377
Validation loss: 2.5228639280536145

Epoch: 5| Step: 8
Training loss: 3.279624609051156
Validation loss: 2.537590025218103

Epoch: 5| Step: 9
Training loss: 2.4103720891375384
Validation loss: 2.515727829887194

Epoch: 5| Step: 10
Training loss: 2.6757173628036894
Validation loss: 2.5257347665594656

Epoch: 89| Step: 0
Training loss: 2.662144750974368
Validation loss: 2.539322706445917

Epoch: 5| Step: 1
Training loss: 2.827095898286638
Validation loss: 2.5162832185872945

Epoch: 5| Step: 2
Training loss: 2.859820555279376
Validation loss: 2.5266290839035452

Epoch: 5| Step: 3
Training loss: 3.1985038359098583
Validation loss: 2.521486794789647

Epoch: 5| Step: 4
Training loss: 2.705649371373347
Validation loss: 2.528084390247348

Epoch: 5| Step: 5
Training loss: 2.4865441599302933
Validation loss: 2.5175390160063107

Epoch: 5| Step: 6
Training loss: 2.8164076044552915
Validation loss: 2.524469574291572

Epoch: 5| Step: 7
Training loss: 2.575173925803542
Validation loss: 2.542213975778703

Epoch: 5| Step: 8
Training loss: 2.798539843623395
Validation loss: 2.5261300263820674

Epoch: 5| Step: 9
Training loss: 2.7369716848651655
Validation loss: 2.5248311017467095

Epoch: 5| Step: 10
Training loss: 3.163939975202913
Validation loss: 2.531729096436255

Epoch: 90| Step: 0
Training loss: 3.1689969239344884
Validation loss: 2.544402578694617

Epoch: 5| Step: 1
Training loss: 2.7415408162583854
Validation loss: 2.5037733888107314

Epoch: 5| Step: 2
Training loss: 2.9015392559313993
Validation loss: 2.514610550178162

Epoch: 5| Step: 3
Training loss: 2.6249159844895344
Validation loss: 2.5329869432088654

Epoch: 5| Step: 4
Training loss: 3.2729395667492507
Validation loss: 2.5352117571339696

Epoch: 5| Step: 5
Training loss: 2.3875427322158527
Validation loss: 2.539825829551563

Epoch: 5| Step: 6
Training loss: 3.1657116771704956
Validation loss: 2.5253411087295383

Epoch: 5| Step: 7
Training loss: 3.077217394250769
Validation loss: 2.521247941202687

Epoch: 5| Step: 8
Training loss: 2.2737780968019052
Validation loss: 2.5213997764741127

Epoch: 5| Step: 9
Training loss: 2.456468672397525
Validation loss: 2.511036658740413

Epoch: 5| Step: 10
Training loss: 2.5819156407316046
Validation loss: 2.5189653324967964

Epoch: 91| Step: 0
Training loss: 3.1243734113029507
Validation loss: 2.5247154483249665

Epoch: 5| Step: 1
Training loss: 3.3581025308538526
Validation loss: 2.530016241044583

Epoch: 5| Step: 2
Training loss: 2.7689842038955983
Validation loss: 2.5120433986120103

Epoch: 5| Step: 3
Training loss: 2.6198962413830045
Validation loss: 2.516470921601044

Epoch: 5| Step: 4
Training loss: 2.665621840910029
Validation loss: 2.504476943995771

Epoch: 5| Step: 5
Training loss: 2.758958443362224
Validation loss: 2.5186631222142477

Epoch: 5| Step: 6
Training loss: 2.0876711381213338
Validation loss: 2.5436878566523893

Epoch: 5| Step: 7
Training loss: 3.0223934135227926
Validation loss: 2.5239507707552247

Epoch: 5| Step: 8
Training loss: 2.912363165395641
Validation loss: 2.5131263795332455

Epoch: 5| Step: 9
Training loss: 3.0184420537158942
Validation loss: 2.5389428672657086

Epoch: 5| Step: 10
Training loss: 2.4294644464975517
Validation loss: 2.5102785304196327

Epoch: 92| Step: 0
Training loss: 2.534479318299219
Validation loss: 2.5055386287141483

Epoch: 5| Step: 1
Training loss: 2.7770561573695414
Validation loss: 2.5277311709623596

Epoch: 5| Step: 2
Training loss: 3.305799364845482
Validation loss: 2.5415354902797493

Epoch: 5| Step: 3
Training loss: 3.663001453846136
Validation loss: 2.523358173775045

Epoch: 5| Step: 4
Training loss: 2.4472802383217926
Validation loss: 2.5092993385668234

Epoch: 5| Step: 5
Training loss: 2.056338151242309
Validation loss: 2.5281429670046203

Epoch: 5| Step: 6
Training loss: 2.9419722905309547
Validation loss: 2.514314277796246

Epoch: 5| Step: 7
Training loss: 2.968055884881349
Validation loss: 2.519842738472605

Epoch: 5| Step: 8
Training loss: 2.467941827920041
Validation loss: 2.5180709892180224

Epoch: 5| Step: 9
Training loss: 2.891741727648129
Validation loss: 2.516576309010862

Epoch: 5| Step: 10
Training loss: 2.505411109463494
Validation loss: 2.5319877034665534

Epoch: 93| Step: 0
Training loss: 3.2443413990965255
Validation loss: 2.5207328600087022

Epoch: 5| Step: 1
Training loss: 3.38100139952086
Validation loss: 2.5427247751021156

Epoch: 5| Step: 2
Training loss: 2.3624505214454916
Validation loss: 2.5322977518441587

Epoch: 5| Step: 3
Training loss: 2.2380176009439716
Validation loss: 2.5169169764841035

Epoch: 5| Step: 4
Training loss: 2.580772751107483
Validation loss: 2.533662186226277

Epoch: 5| Step: 5
Training loss: 2.9660095415739995
Validation loss: 2.5211137923906866

Epoch: 5| Step: 6
Training loss: 2.9470394343497177
Validation loss: 2.5283801907680488

Epoch: 5| Step: 7
Training loss: 2.8482348949132623
Validation loss: 2.545057175342965

Epoch: 5| Step: 8
Training loss: 2.6067720324919716
Validation loss: 2.5146357204403618

Epoch: 5| Step: 9
Training loss: 2.7603545751725935
Validation loss: 2.516569367567712

Epoch: 5| Step: 10
Training loss: 2.760617394016904
Validation loss: 2.5326205591390827

Epoch: 94| Step: 0
Training loss: 2.8812893614490394
Validation loss: 2.5081436684304723

Epoch: 5| Step: 1
Training loss: 2.4359754049252795
Validation loss: 2.5112693784101197

Epoch: 5| Step: 2
Training loss: 3.0496439553986803
Validation loss: 2.530195712690464

Epoch: 5| Step: 3
Training loss: 2.8126751315373855
Validation loss: 2.5487307656165625

Epoch: 5| Step: 4
Training loss: 2.6032326409294066
Validation loss: 2.5050451695300873

Epoch: 5| Step: 5
Training loss: 2.79029840179344
Validation loss: 2.534711301604659

Epoch: 5| Step: 6
Training loss: 3.3995151959514582
Validation loss: 2.526845072048731

Epoch: 5| Step: 7
Training loss: 2.647265239533735
Validation loss: 2.515595884517044

Epoch: 5| Step: 8
Training loss: 2.9882517295825375
Validation loss: 2.529250052313225

Epoch: 5| Step: 9
Training loss: 2.3875537167243253
Validation loss: 2.497244446337497

Epoch: 5| Step: 10
Training loss: 2.897521005180785
Validation loss: 2.5116801182938753

Epoch: 95| Step: 0
Training loss: 2.2569703112041015
Validation loss: 2.531322603287158

Epoch: 5| Step: 1
Training loss: 3.837931252766828
Validation loss: 2.520648410176896

Epoch: 5| Step: 2
Training loss: 3.0706442151438016
Validation loss: 2.5385345147299776

Epoch: 5| Step: 3
Training loss: 2.883216064655627
Validation loss: 2.516505364603807

Epoch: 5| Step: 4
Training loss: 3.1597978650401988
Validation loss: 2.5483965792413974

Epoch: 5| Step: 5
Training loss: 1.9830594846120697
Validation loss: 2.5411598709621606

Epoch: 5| Step: 6
Training loss: 2.7931430575524794
Validation loss: 2.5403165470812756

Epoch: 5| Step: 7
Training loss: 2.6012543733515128
Validation loss: 2.50280733611154

Epoch: 5| Step: 8
Training loss: 2.6942191297396683
Validation loss: 2.5355603440052614

Epoch: 5| Step: 9
Training loss: 2.626052463806383
Validation loss: 2.5218742931238056

Epoch: 5| Step: 10
Training loss: 2.4511323842062844
Validation loss: 2.557598086549861

Epoch: 96| Step: 0
Training loss: 2.6061361164487353
Validation loss: 2.5233626572194447

Epoch: 5| Step: 1
Training loss: 2.3659286928951393
Validation loss: 2.513232000525069

Epoch: 5| Step: 2
Training loss: 2.916467242008052
Validation loss: 2.548255281153287

Epoch: 5| Step: 3
Training loss: 2.726237367848231
Validation loss: 2.526285172754069

Epoch: 5| Step: 4
Training loss: 2.69725475267654
Validation loss: 2.5207677293819253

Epoch: 5| Step: 5
Training loss: 2.5730704957842945
Validation loss: 2.5366969132297457

Epoch: 5| Step: 6
Training loss: 2.9018189480733394
Validation loss: 2.5383279333237025

Epoch: 5| Step: 7
Training loss: 3.099423841032766
Validation loss: 2.5447118387888636

Epoch: 5| Step: 8
Training loss: 2.337149111272696
Validation loss: 2.5422069510571372

Epoch: 5| Step: 9
Training loss: 3.5028606713214314
Validation loss: 2.532705501535679

Epoch: 5| Step: 10
Training loss: 2.6531150718835703
Validation loss: 2.5220167073535023

Epoch: 97| Step: 0
Training loss: 3.0389172046124022
Validation loss: 2.4999049199389796

Epoch: 5| Step: 1
Training loss: 2.8009095315538084
Validation loss: 2.544248212654429

Epoch: 5| Step: 2
Training loss: 2.8908018315844806
Validation loss: 2.5265572954813558

Epoch: 5| Step: 3
Training loss: 2.8764886733677044
Validation loss: 2.5344582525910195

Epoch: 5| Step: 4
Training loss: 2.4606424790610117
Validation loss: 2.5291994834460647

Epoch: 5| Step: 5
Training loss: 2.7203838656736354
Validation loss: 2.527802446341071

Epoch: 5| Step: 6
Training loss: 2.4710721068349533
Validation loss: 2.546923187683776

Epoch: 5| Step: 7
Training loss: 3.0804589422555
Validation loss: 2.5352817240606638

Epoch: 5| Step: 8
Training loss: 3.1590350865283976
Validation loss: 2.539516512506627

Epoch: 5| Step: 9
Training loss: 2.499813549718401
Validation loss: 2.556709203335623

Epoch: 5| Step: 10
Training loss: 2.754509176712967
Validation loss: 2.538263653839405

Epoch: 98| Step: 0
Training loss: 2.1256599523446145
Validation loss: 2.5274827725585993

Epoch: 5| Step: 1
Training loss: 3.4928858525987745
Validation loss: 2.5408944456868974

Epoch: 5| Step: 2
Training loss: 2.8777051720937497
Validation loss: 2.5426496389914255

Epoch: 5| Step: 3
Training loss: 2.8653105951220286
Validation loss: 2.5142235933560837

Epoch: 5| Step: 4
Training loss: 2.5532600544539457
Validation loss: 2.518620471580068

Epoch: 5| Step: 5
Training loss: 3.0936268483320983
Validation loss: 2.5222341475470533

Epoch: 5| Step: 6
Training loss: 2.643903423934769
Validation loss: 2.523294206865546

Epoch: 5| Step: 7
Training loss: 2.5820607978642154
Validation loss: 2.5096977752366474

Epoch: 5| Step: 8
Training loss: 2.3726952060670485
Validation loss: 2.53028858358998

Epoch: 5| Step: 9
Training loss: 2.4689022934694047
Validation loss: 2.5423614652386743

Epoch: 5| Step: 10
Training loss: 3.389612424632231
Validation loss: 2.523164291980795

Epoch: 99| Step: 0
Training loss: 2.6041665547688777
Validation loss: 2.5288646178339347

Epoch: 5| Step: 1
Training loss: 2.5734537055797144
Validation loss: 2.5167769424681454

Epoch: 5| Step: 2
Training loss: 2.76740186260558
Validation loss: 2.5218527622566262

Epoch: 5| Step: 3
Training loss: 2.229229531040487
Validation loss: 2.5086395287189514

Epoch: 5| Step: 4
Training loss: 2.8581523678568543
Validation loss: 2.524438681073542

Epoch: 5| Step: 5
Training loss: 2.8877090180284326
Validation loss: 2.511398405999454

Epoch: 5| Step: 6
Training loss: 3.228756950391613
Validation loss: 2.5055346474809226

Epoch: 5| Step: 7
Training loss: 2.655168919191568
Validation loss: 2.516297286419835

Epoch: 5| Step: 8
Training loss: 3.2602322234774896
Validation loss: 2.516869387712763

Epoch: 5| Step: 9
Training loss: 2.5820702161748668
Validation loss: 2.5214775055298895

Epoch: 5| Step: 10
Training loss: 2.950410756108502
Validation loss: 2.511274096795167

Epoch: 100| Step: 0
Training loss: 2.367342298626966
Validation loss: 2.52383036075965

Epoch: 5| Step: 1
Training loss: 2.7782127855675256
Validation loss: 2.520197476363088

Epoch: 5| Step: 2
Training loss: 2.927733234916806
Validation loss: 2.528713498431507

Epoch: 5| Step: 3
Training loss: 2.7363890287822015
Validation loss: 2.492333509378105

Epoch: 5| Step: 4
Training loss: 2.8187622712876856
Validation loss: 2.5256886502714324

Epoch: 5| Step: 5
Training loss: 2.9337423111725847
Validation loss: 2.5225852066200005

Epoch: 5| Step: 6
Training loss: 2.311628847736483
Validation loss: 2.514974514219224

Epoch: 5| Step: 7
Training loss: 2.673429159823618
Validation loss: 2.5266920654004523

Epoch: 5| Step: 8
Training loss: 3.2333720398254213
Validation loss: 2.546260965818343

Epoch: 5| Step: 9
Training loss: 2.81659806824466
Validation loss: 2.5282034010981222

Epoch: 5| Step: 10
Training loss: 3.1693202662419915
Validation loss: 2.5157087369818942

Epoch: 101| Step: 0
Training loss: 2.3676594178807875
Validation loss: 2.507395091149881

Epoch: 5| Step: 1
Training loss: 2.579532955538129
Validation loss: 2.500048376456192

Epoch: 5| Step: 2
Training loss: 2.8939829524552425
Validation loss: 2.508168505475482

Epoch: 5| Step: 3
Training loss: 3.0232983934205335
Validation loss: 2.5441734097232485

Epoch: 5| Step: 4
Training loss: 2.1814535226393312
Validation loss: 2.5462188791550795

Epoch: 5| Step: 5
Training loss: 2.959267822387271
Validation loss: 2.5161137881473232

Epoch: 5| Step: 6
Training loss: 2.6773815854741065
Validation loss: 2.5129061322966346

Epoch: 5| Step: 7
Training loss: 2.687566445882128
Validation loss: 2.5278972610415185

Epoch: 5| Step: 8
Training loss: 2.6244968658736694
Validation loss: 2.521093323766069

Epoch: 5| Step: 9
Training loss: 2.7426295779973118
Validation loss: 2.524075604877331

Epoch: 5| Step: 10
Training loss: 3.813435299072531
Validation loss: 2.507838311974746

Epoch: 102| Step: 0
Training loss: 2.415221790846694
Validation loss: 2.515584860901651

Epoch: 5| Step: 1
Training loss: 2.816204005584133
Validation loss: 2.524834824094567

Epoch: 5| Step: 2
Training loss: 3.145314373806027
Validation loss: 2.5123424908594565

Epoch: 5| Step: 3
Training loss: 3.182161535587755
Validation loss: 2.5370591759962218

Epoch: 5| Step: 4
Training loss: 2.6493816661998326
Validation loss: 2.5287220438402365

Epoch: 5| Step: 5
Training loss: 3.139554728626811
Validation loss: 2.538132964843243

Epoch: 5| Step: 6
Training loss: 2.555658465077936
Validation loss: 2.5294148588534062

Epoch: 5| Step: 7
Training loss: 2.3502595149467336
Validation loss: 2.5166263002829456

Epoch: 5| Step: 8
Training loss: 2.506106167516406
Validation loss: 2.5039290859907615

Epoch: 5| Step: 9
Training loss: 2.830358813828142
Validation loss: 2.5227180323164844

Epoch: 5| Step: 10
Training loss: 2.860000291837664
Validation loss: 2.513673557799444

Epoch: 103| Step: 0
Training loss: 2.7930035782356435
Validation loss: 2.4961774823142853

Epoch: 5| Step: 1
Training loss: 2.791120608619694
Validation loss: 2.4998859907631337

Epoch: 5| Step: 2
Training loss: 2.8234039017536516
Validation loss: 2.515991794630058

Epoch: 5| Step: 3
Training loss: 2.6553649325388924
Validation loss: 2.49862466533433

Epoch: 5| Step: 4
Training loss: 2.3744763248730894
Validation loss: 2.5206485566329584

Epoch: 5| Step: 5
Training loss: 3.325662624330547
Validation loss: 2.537050956760776

Epoch: 5| Step: 6
Training loss: 3.3204295418249914
Validation loss: 2.5185207575602027

Epoch: 5| Step: 7
Training loss: 2.442551196370981
Validation loss: 2.506725098145002

Epoch: 5| Step: 8
Training loss: 2.162771452696262
Validation loss: 2.5282988142192746

Epoch: 5| Step: 9
Training loss: 2.890951931997507
Validation loss: 2.511882915620248

Epoch: 5| Step: 10
Training loss: 2.8829165390800457
Validation loss: 2.523048711519563

Epoch: 104| Step: 0
Training loss: 2.401984657803976
Validation loss: 2.531434332342859

Epoch: 5| Step: 1
Training loss: 2.8030928219720637
Validation loss: 2.520754056690954

Epoch: 5| Step: 2
Training loss: 2.892933846185101
Validation loss: 2.5161196431860904

Epoch: 5| Step: 3
Training loss: 2.473835596085059
Validation loss: 2.5254467304907577

Epoch: 5| Step: 4
Training loss: 3.0526890766592794
Validation loss: 2.5368868524441925

Epoch: 5| Step: 5
Training loss: 2.1254634351831454
Validation loss: 2.5016865834985684

Epoch: 5| Step: 6
Training loss: 3.4338423255975847
Validation loss: 2.514226664555041

Epoch: 5| Step: 7
Training loss: 2.7125939137598163
Validation loss: 2.520885161563087

Epoch: 5| Step: 8
Training loss: 2.854226234724013
Validation loss: 2.5125355057994945

Epoch: 5| Step: 9
Training loss: 2.783267371791366
Validation loss: 2.543878867168679

Epoch: 5| Step: 10
Training loss: 2.880987814933434
Validation loss: 2.545640642414818

Epoch: 105| Step: 0
Training loss: 2.907750224451518
Validation loss: 2.503382681061507

Epoch: 5| Step: 1
Training loss: 3.1791952816003306
Validation loss: 2.5390262825199463

Epoch: 5| Step: 2
Training loss: 2.453741980066859
Validation loss: 2.5220267910552434

Epoch: 5| Step: 3
Training loss: 2.4791425388561885
Validation loss: 2.5320975640466665

Epoch: 5| Step: 4
Training loss: 3.303040150226574
Validation loss: 2.5359892126541483

Epoch: 5| Step: 5
Training loss: 2.5845681797509714
Validation loss: 2.5386888171937194

Epoch: 5| Step: 6
Training loss: 3.1186325815607026
Validation loss: 2.5196277037455213

Epoch: 5| Step: 7
Training loss: 3.0573408305922873
Validation loss: 2.512853241031759

Epoch: 5| Step: 8
Training loss: 2.891475078234595
Validation loss: 2.5262116260310066

Epoch: 5| Step: 9
Training loss: 2.1939271513008003
Validation loss: 2.520239793108535

Epoch: 5| Step: 10
Training loss: 2.222885468635562
Validation loss: 2.4976175369295084

Epoch: 106| Step: 0
Training loss: 2.8471917086157634
Validation loss: 2.5240487680604513

Epoch: 5| Step: 1
Training loss: 2.781871083380454
Validation loss: 2.5251259422185095

Epoch: 5| Step: 2
Training loss: 2.396110076400911
Validation loss: 2.5171326257316733

Epoch: 5| Step: 3
Training loss: 2.773908977998196
Validation loss: 2.492793673446183

Epoch: 5| Step: 4
Training loss: 2.6778866246719946
Validation loss: 2.5079418965905282

Epoch: 5| Step: 5
Training loss: 3.2869809919503448
Validation loss: 2.53254480033747

Epoch: 5| Step: 6
Training loss: 3.0546934317997074
Validation loss: 2.526768471467261

Epoch: 5| Step: 7
Training loss: 2.604639656346037
Validation loss: 2.527843193640541

Epoch: 5| Step: 8
Training loss: 2.8625040912182484
Validation loss: 2.526484263018589

Epoch: 5| Step: 9
Training loss: 2.770801591810868
Validation loss: 2.5150090546751693

Epoch: 5| Step: 10
Training loss: 2.4727303010238626
Validation loss: 2.5415578005543527

Epoch: 107| Step: 0
Training loss: 2.9985615142573208
Validation loss: 2.5134065854276386

Epoch: 5| Step: 1
Training loss: 3.1234111561506293
Validation loss: 2.539231224038884

Epoch: 5| Step: 2
Training loss: 2.19786168130613
Validation loss: 2.5251919662765863

Epoch: 5| Step: 3
Training loss: 3.150592530243474
Validation loss: 2.5199466394379786

Epoch: 5| Step: 4
Training loss: 2.6469018135379057
Validation loss: 2.5105612537069777

Epoch: 5| Step: 5
Training loss: 3.136005553308747
Validation loss: 2.5158367104838453

Epoch: 5| Step: 6
Training loss: 2.7961506678161867
Validation loss: 2.5248661510998356

Epoch: 5| Step: 7
Training loss: 2.5367890964125763
Validation loss: 2.5240850125500205

Epoch: 5| Step: 8
Training loss: 2.0652295171176553
Validation loss: 2.5197630955960806

Epoch: 5| Step: 9
Training loss: 2.9752165386990637
Validation loss: 2.5227433371244254

Epoch: 5| Step: 10
Training loss: 2.6760720644885363
Validation loss: 2.54341964614357

Epoch: 108| Step: 0
Training loss: 3.452782937623398
Validation loss: 2.5332497901194215

Epoch: 5| Step: 1
Training loss: 3.0650807619889315
Validation loss: 2.525487358456733

Epoch: 5| Step: 2
Training loss: 2.3952481315310075
Validation loss: 2.520090155469202

Epoch: 5| Step: 3
Training loss: 2.380542912468807
Validation loss: 2.5126241728067593

Epoch: 5| Step: 4
Training loss: 2.3584348308527403
Validation loss: 2.520510727543459

Epoch: 5| Step: 5
Training loss: 2.8279662535568035
Validation loss: 2.537120284883784

Epoch: 5| Step: 6
Training loss: 2.3324217605143582
Validation loss: 2.525847409619721

Epoch: 5| Step: 7
Training loss: 3.0897851063097943
Validation loss: 2.4960384298253993

Epoch: 5| Step: 8
Training loss: 3.2443984249050497
Validation loss: 2.5340197808917706

Epoch: 5| Step: 9
Training loss: 2.5413664239201443
Validation loss: 2.5137306570666302

Epoch: 5| Step: 10
Training loss: 2.6558554861267765
Validation loss: 2.5315357808043597

Epoch: 109| Step: 0
Training loss: 2.7847430978078704
Validation loss: 2.5330207532558298

Epoch: 5| Step: 1
Training loss: 2.9442661359367013
Validation loss: 2.526120547674238

Epoch: 5| Step: 2
Training loss: 2.110051025569138
Validation loss: 2.510200399945525

Epoch: 5| Step: 3
Training loss: 2.913568449709912
Validation loss: 2.5123777828041582

Epoch: 5| Step: 4
Training loss: 2.5656874766391957
Validation loss: 2.5261351259933527

Epoch: 5| Step: 5
Training loss: 2.7317526313362004
Validation loss: 2.5454034581096483

Epoch: 5| Step: 6
Training loss: 3.3486558808260543
Validation loss: 2.5133312338198497

Epoch: 5| Step: 7
Training loss: 2.4460102097492378
Validation loss: 2.536759519453268

Epoch: 5| Step: 8
Training loss: 2.4965065389433323
Validation loss: 2.5290240820205705

Epoch: 5| Step: 9
Training loss: 3.146924282320702
Validation loss: 2.5160188411853825

Epoch: 5| Step: 10
Training loss: 2.8552924022153032
Validation loss: 2.5424447128733614

Epoch: 110| Step: 0
Training loss: 3.0767188563798955
Validation loss: 2.5208784791067815

Epoch: 5| Step: 1
Training loss: 2.9295738910263722
Validation loss: 2.5118913621342536

Epoch: 5| Step: 2
Training loss: 3.0965135349918453
Validation loss: 2.5077682336337728

Epoch: 5| Step: 3
Training loss: 2.56934985654012
Validation loss: 2.4944823626934856

Epoch: 5| Step: 4
Training loss: 2.683847029503936
Validation loss: 2.5224609293693985

Epoch: 5| Step: 5
Training loss: 1.94265712912598
Validation loss: 2.535323488719802

Epoch: 5| Step: 6
Training loss: 2.1543808727887095
Validation loss: 2.520299307967281

Epoch: 5| Step: 7
Training loss: 3.016526477851164
Validation loss: 2.5311852075530252

Epoch: 5| Step: 8
Training loss: 2.598626481901111
Validation loss: 2.5344220168570493

Epoch: 5| Step: 9
Training loss: 3.104424758183653
Validation loss: 2.5476179158587358

Epoch: 5| Step: 10
Training loss: 3.0240769609517675
Validation loss: 2.524458910329745

Epoch: 111| Step: 0
Training loss: 2.365483744144321
Validation loss: 2.5239019230374797

Epoch: 5| Step: 1
Training loss: 2.9349532755003302
Validation loss: 2.5161663663453937

Epoch: 5| Step: 2
Training loss: 3.4240250473710634
Validation loss: 2.5277801302785052

Epoch: 5| Step: 3
Training loss: 2.719679925542692
Validation loss: 2.5245654088004885

Epoch: 5| Step: 4
Training loss: 2.564699299161051
Validation loss: 2.541263797192869

Epoch: 5| Step: 5
Training loss: 3.2504996869284084
Validation loss: 2.5280675141295954

Epoch: 5| Step: 6
Training loss: 2.328497120860988
Validation loss: 2.512856696484199

Epoch: 5| Step: 7
Training loss: 2.6074572614347375
Validation loss: 2.5433457925516842

Epoch: 5| Step: 8
Training loss: 2.741751524782109
Validation loss: 2.519461763490084

Epoch: 5| Step: 9
Training loss: 2.7692635948933626
Validation loss: 2.538561450878246

Epoch: 5| Step: 10
Training loss: 2.627337278051286
Validation loss: 2.519614512237983

Epoch: 112| Step: 0
Training loss: 2.7434459091980576
Validation loss: 2.5394669092550286

Epoch: 5| Step: 1
Training loss: 2.80850682236826
Validation loss: 2.515100584334515

Epoch: 5| Step: 2
Training loss: 2.7496328975832696
Validation loss: 2.5208373843060246

Epoch: 5| Step: 3
Training loss: 2.8319059778328017
Validation loss: 2.5297828888050833

Epoch: 5| Step: 4
Training loss: 2.4781726693069928
Validation loss: 2.507685324354771

Epoch: 5| Step: 5
Training loss: 3.0989536488388087
Validation loss: 2.498466820401705

Epoch: 5| Step: 6
Training loss: 2.835925057544158
Validation loss: 2.5444401100625575

Epoch: 5| Step: 7
Training loss: 2.9004771004574956
Validation loss: 2.507500881987919

Epoch: 5| Step: 8
Training loss: 2.809364605673294
Validation loss: 2.513004487868784

Epoch: 5| Step: 9
Training loss: 2.7537696317508873
Validation loss: 2.5331502441120923

Epoch: 5| Step: 10
Training loss: 2.604425453043378
Validation loss: 2.509384115168311

Epoch: 113| Step: 0
Training loss: 2.8164238578679823
Validation loss: 2.527573881956362

Epoch: 5| Step: 1
Training loss: 2.816321764563697
Validation loss: 2.5340145605697333

Epoch: 5| Step: 2
Training loss: 2.2198189323059934
Validation loss: 2.5195621342001036

Epoch: 5| Step: 3
Training loss: 3.270534931023729
Validation loss: 2.515427588485798

Epoch: 5| Step: 4
Training loss: 2.8197932306874147
Validation loss: 2.5032076930241334

Epoch: 5| Step: 5
Training loss: 2.5255049512199483
Validation loss: 2.5318365254338797

Epoch: 5| Step: 6
Training loss: 2.4638040952583897
Validation loss: 2.525917078736145

Epoch: 5| Step: 7
Training loss: 3.5222580113349284
Validation loss: 2.5102042307985064

Epoch: 5| Step: 8
Training loss: 2.6709045075342357
Validation loss: 2.5297444458039586

Epoch: 5| Step: 9
Training loss: 3.015800826723284
Validation loss: 2.5272275024931345

Epoch: 5| Step: 10
Training loss: 1.8610957542026243
Validation loss: 2.528820035685402

Epoch: 114| Step: 0
Training loss: 2.3439363532685413
Validation loss: 2.5267274765200227

Epoch: 5| Step: 1
Training loss: 2.9721292637660746
Validation loss: 2.5215570296973353

Epoch: 5| Step: 2
Training loss: 3.4724022136236914
Validation loss: 2.5191911024370306

Epoch: 5| Step: 3
Training loss: 2.8107504595306843
Validation loss: 2.503881818617519

Epoch: 5| Step: 4
Training loss: 3.0941847678745016
Validation loss: 2.5047759408073995

Epoch: 5| Step: 5
Training loss: 3.004911534789199
Validation loss: 2.522254110906325

Epoch: 5| Step: 6
Training loss: 2.2356530649046222
Validation loss: 2.50471289951808

Epoch: 5| Step: 7
Training loss: 3.0692978752511926
Validation loss: 2.523826059462031

Epoch: 5| Step: 8
Training loss: 2.036064194362362
Validation loss: 2.5192836325959154

Epoch: 5| Step: 9
Training loss: 3.132384615856238
Validation loss: 2.52693665387362

Epoch: 5| Step: 10
Training loss: 1.5499717463717786
Validation loss: 2.518672099703102

Epoch: 115| Step: 0
Training loss: 2.5463992183448005
Validation loss: 2.529451018344159

Epoch: 5| Step: 1
Training loss: 2.841317110758523
Validation loss: 2.5135156277726107

Epoch: 5| Step: 2
Training loss: 2.9843125261887846
Validation loss: 2.542409791408358

Epoch: 5| Step: 3
Training loss: 2.8792278881345124
Validation loss: 2.5192653939598046

Epoch: 5| Step: 4
Training loss: 2.296891400544657
Validation loss: 2.5346194808110267

Epoch: 5| Step: 5
Training loss: 2.862970478423965
Validation loss: 2.5479890415261672

Epoch: 5| Step: 6
Training loss: 2.6862916225867193
Validation loss: 2.5136271804543484

Epoch: 5| Step: 7
Training loss: 2.871051232678383
Validation loss: 2.509058734203635

Epoch: 5| Step: 8
Training loss: 2.9442241894216687
Validation loss: 2.5216318884530535

Epoch: 5| Step: 9
Training loss: 2.972146269955805
Validation loss: 2.540767878581711

Epoch: 5| Step: 10
Training loss: 2.5630713965525196
Validation loss: 2.5175364722612077

Epoch: 116| Step: 0
Training loss: 2.8883362653144737
Validation loss: 2.529381814482962

Epoch: 5| Step: 1
Training loss: 3.3407422379707064
Validation loss: 2.526193750994154

Epoch: 5| Step: 2
Training loss: 2.638412361174359
Validation loss: 2.515902378140377

Epoch: 5| Step: 3
Training loss: 2.5889698700343264
Validation loss: 2.5353552240617168

Epoch: 5| Step: 4
Training loss: 2.6084021850696124
Validation loss: 2.5025641443149538

Epoch: 5| Step: 5
Training loss: 2.778059082622349
Validation loss: 2.5276779570311194

Epoch: 5| Step: 6
Training loss: 2.9170923921418725
Validation loss: 2.524791241007194

Epoch: 5| Step: 7
Training loss: 2.758975035213998
Validation loss: 2.524449526899736

Epoch: 5| Step: 8
Training loss: 2.237037726018087
Validation loss: 2.512268552291248

Epoch: 5| Step: 9
Training loss: 2.811639442183908
Validation loss: 2.532144179682307

Epoch: 5| Step: 10
Training loss: 2.6429296063832033
Validation loss: 2.5163739653202195

Epoch: 117| Step: 0
Training loss: 3.2559391920424177
Validation loss: 2.5148302470055475

Epoch: 5| Step: 1
Training loss: 3.136524465204312
Validation loss: 2.509749377506795

Epoch: 5| Step: 2
Training loss: 3.069575485800097
Validation loss: 2.5180847935580295

Epoch: 5| Step: 3
Training loss: 2.9939364508622686
Validation loss: 2.517265474348381

Epoch: 5| Step: 4
Training loss: 2.014797898562534
Validation loss: 2.545400616896305

Epoch: 5| Step: 5
Training loss: 2.5158598417048252
Validation loss: 2.5400373406475802

Epoch: 5| Step: 6
Training loss: 2.6067319721649618
Validation loss: 2.5306610836204024

Epoch: 5| Step: 7
Training loss: 2.1406115232168723
Validation loss: 2.533792421854836

Epoch: 5| Step: 8
Training loss: 3.0268899294421816
Validation loss: 2.530214586850831

Epoch: 5| Step: 9
Training loss: 2.8180469490455877
Validation loss: 2.540503576862134

Epoch: 5| Step: 10
Training loss: 2.5282403455150266
Validation loss: 2.5280670212909104

Epoch: 118| Step: 0
Training loss: 2.2413030601301185
Validation loss: 2.529223026628123

Epoch: 5| Step: 1
Training loss: 2.7638965960149693
Validation loss: 2.4976049651289567

Epoch: 5| Step: 2
Training loss: 2.4864832731566864
Validation loss: 2.5311446588904327

Epoch: 5| Step: 3
Training loss: 3.3673016380560967
Validation loss: 2.5453666419431555

Epoch: 5| Step: 4
Training loss: 2.3368214239369265
Validation loss: 2.5101764700034965

Epoch: 5| Step: 5
Training loss: 2.32245000647867
Validation loss: 2.5238778375782753

Epoch: 5| Step: 6
Training loss: 2.9298414673083544
Validation loss: 2.5440493617531317

Epoch: 5| Step: 7
Training loss: 2.9317710689875067
Validation loss: 2.522573293814674

Epoch: 5| Step: 8
Training loss: 2.766817342339675
Validation loss: 2.5533746740040915

Epoch: 5| Step: 9
Training loss: 2.697209760237912
Validation loss: 2.523811861420276

Epoch: 5| Step: 10
Training loss: 3.2947696272991007
Validation loss: 2.5275335015437466

Epoch: 119| Step: 0
Training loss: 3.070140571730876
Validation loss: 2.5448094768460967

Epoch: 5| Step: 1
Training loss: 3.6021178638633655
Validation loss: 2.5256946135444402

Epoch: 5| Step: 2
Training loss: 2.9082080438801583
Validation loss: 2.508530187549318

Epoch: 5| Step: 3
Training loss: 2.344945577221748
Validation loss: 2.520548691964188

Epoch: 5| Step: 4
Training loss: 2.1059732770392685
Validation loss: 2.527039273031809

Epoch: 5| Step: 5
Training loss: 1.8603316338540532
Validation loss: 2.531858077717421

Epoch: 5| Step: 6
Training loss: 2.57787095176454
Validation loss: 2.5225226845985507

Epoch: 5| Step: 7
Training loss: 2.246106487735621
Validation loss: 2.525325729931757

Epoch: 5| Step: 8
Training loss: 2.609732414939871
Validation loss: 2.5246737417312146

Epoch: 5| Step: 9
Training loss: 3.3541542826990587
Validation loss: 2.5331556347190443

Epoch: 5| Step: 10
Training loss: 3.3649110112886493
Validation loss: 2.512982754506774

Epoch: 120| Step: 0
Training loss: 2.800169129032045
Validation loss: 2.505149066121571

Epoch: 5| Step: 1
Training loss: 2.472783041742057
Validation loss: 2.541883899786473

Epoch: 5| Step: 2
Training loss: 2.37826213666838
Validation loss: 2.532664138074221

Epoch: 5| Step: 3
Training loss: 2.7501648940154304
Validation loss: 2.5022315791710814

Epoch: 5| Step: 4
Training loss: 3.6317462627995365
Validation loss: 2.525327981582832

Epoch: 5| Step: 5
Training loss: 2.7789903526674045
Validation loss: 2.4989621484619775

Epoch: 5| Step: 6
Training loss: 2.8157758496654313
Validation loss: 2.527423100239319

Epoch: 5| Step: 7
Training loss: 2.8500767011527275
Validation loss: 2.513624769416858

Epoch: 5| Step: 8
Training loss: 2.958781320384369
Validation loss: 2.555634899595968

Epoch: 5| Step: 9
Training loss: 2.574118906032707
Validation loss: 2.5401504192621416

Epoch: 5| Step: 10
Training loss: 2.27792879575751
Validation loss: 2.523458490081537

Epoch: 121| Step: 0
Training loss: 2.543127567705277
Validation loss: 2.519828037780037

Epoch: 5| Step: 1
Training loss: 2.4504281545714726
Validation loss: 2.524521562110583

Epoch: 5| Step: 2
Training loss: 2.791049367114747
Validation loss: 2.510262546668491

Epoch: 5| Step: 3
Training loss: 2.3870973177545887
Validation loss: 2.527934193706501

Epoch: 5| Step: 4
Training loss: 2.5156777891697732
Validation loss: 2.5295082601013994

Epoch: 5| Step: 5
Training loss: 2.5837727962497747
Validation loss: 2.5350364774195273

Epoch: 5| Step: 6
Training loss: 2.925398227285989
Validation loss: 2.5114258000938

Epoch: 5| Step: 7
Training loss: 2.880106495901562
Validation loss: 2.5312248774630977

Epoch: 5| Step: 8
Training loss: 3.187091127986971
Validation loss: 2.5096088321982593

Epoch: 5| Step: 9
Training loss: 2.757382108047906
Validation loss: 2.527336147347823

Epoch: 5| Step: 10
Training loss: 3.3265428677790814
Validation loss: 2.5209341379050274

Epoch: 122| Step: 0
Training loss: 3.1474126089506353
Validation loss: 2.5272759899854584

Epoch: 5| Step: 1
Training loss: 2.4782008579184014
Validation loss: 2.5262797476984575

Epoch: 5| Step: 2
Training loss: 2.3834197724501958
Validation loss: 2.5206474027827084

Epoch: 5| Step: 3
Training loss: 2.9659002178382288
Validation loss: 2.512920170065957

Epoch: 5| Step: 4
Training loss: 3.4961109352754995
Validation loss: 2.515268367658688

Epoch: 5| Step: 5
Training loss: 2.4792094721157425
Validation loss: 2.51842240976005

Epoch: 5| Step: 6
Training loss: 3.1734681286218875
Validation loss: 2.52001158452175

Epoch: 5| Step: 7
Training loss: 2.816286124155636
Validation loss: 2.5211881914771896

Epoch: 5| Step: 8
Training loss: 2.0394732182215627
Validation loss: 2.5290316694617134

Epoch: 5| Step: 9
Training loss: 2.8220738540490964
Validation loss: 2.527868235186311

Epoch: 5| Step: 10
Training loss: 2.2021975898755124
Validation loss: 2.51913231294478

Epoch: 123| Step: 0
Training loss: 2.592527262177402
Validation loss: 2.5374348946546177

Epoch: 5| Step: 1
Training loss: 2.924183795219633
Validation loss: 2.5045850546946244

Epoch: 5| Step: 2
Training loss: 3.3538801809346745
Validation loss: 2.513437064477637

Epoch: 5| Step: 3
Training loss: 2.5503527528203542
Validation loss: 2.5125306387726813

Epoch: 5| Step: 4
Training loss: 2.781355352228016
Validation loss: 2.5186595693794636

Epoch: 5| Step: 5
Training loss: 2.409187901378124
Validation loss: 2.536212823771082

Epoch: 5| Step: 6
Training loss: 3.2755475583863802
Validation loss: 2.517370182332654

Epoch: 5| Step: 7
Training loss: 2.419972121338219
Validation loss: 2.5204190088555367

Epoch: 5| Step: 8
Training loss: 2.395097326479261
Validation loss: 2.519981942897494

Epoch: 5| Step: 9
Training loss: 2.8329960304391335
Validation loss: 2.509327293983253

Epoch: 5| Step: 10
Training loss: 2.5295816268584215
Validation loss: 2.5272772374298635

Epoch: 124| Step: 0
Training loss: 2.1975890603966985
Validation loss: 2.5217669465058115

Epoch: 5| Step: 1
Training loss: 3.030068078613937
Validation loss: 2.5015123684219414

Epoch: 5| Step: 2
Training loss: 3.081835374300839
Validation loss: 2.5282789411765783

Epoch: 5| Step: 3
Training loss: 2.9666171794864304
Validation loss: 2.5141537174678557

Epoch: 5| Step: 4
Training loss: 1.9147416758791538
Validation loss: 2.5197832520002277

Epoch: 5| Step: 5
Training loss: 2.44063190845093
Validation loss: 2.5049079228993416

Epoch: 5| Step: 6
Training loss: 3.2625310377493473
Validation loss: 2.5412689254499337

Epoch: 5| Step: 7
Training loss: 3.1586924252321937
Validation loss: 2.5363565474090604

Epoch: 5| Step: 8
Training loss: 2.4442928247920865
Validation loss: 2.5360683095731584

Epoch: 5| Step: 9
Training loss: 2.959467943417816
Validation loss: 2.519047206975619

Epoch: 5| Step: 10
Training loss: 2.3841720954684766
Validation loss: 2.519127036330071

Epoch: 125| Step: 0
Training loss: 2.260244254526136
Validation loss: 2.515193620190126

Epoch: 5| Step: 1
Training loss: 3.334991535364759
Validation loss: 2.518373313312766

Epoch: 5| Step: 2
Training loss: 2.5131518604861
Validation loss: 2.529352693159189

Epoch: 5| Step: 3
Training loss: 2.4482824065865283
Validation loss: 2.4954350519810733

Epoch: 5| Step: 4
Training loss: 2.985058451580234
Validation loss: 2.528832379312768

Epoch: 5| Step: 5
Training loss: 2.6134149947674765
Validation loss: 2.528966035568657

Epoch: 5| Step: 6
Training loss: 1.7002645847641258
Validation loss: 2.5327328008072576

Epoch: 5| Step: 7
Training loss: 2.988506233871312
Validation loss: 2.5208507077228455

Epoch: 5| Step: 8
Training loss: 3.485251962099562
Validation loss: 2.521613345534999

Epoch: 5| Step: 9
Training loss: 2.5649642260698426
Validation loss: 2.5205303821308997

Epoch: 5| Step: 10
Training loss: 2.7554136222600643
Validation loss: 2.520428570034127

Epoch: 126| Step: 0
Training loss: 2.6955936008873853
Validation loss: 2.5181737416202146

Epoch: 5| Step: 1
Training loss: 2.556854728668726
Validation loss: 2.539400140795575

Epoch: 5| Step: 2
Training loss: 2.3443322030790674
Validation loss: 2.5347040568410826

Epoch: 5| Step: 3
Training loss: 2.6328422024010747
Validation loss: 2.526020339116313

Epoch: 5| Step: 4
Training loss: 2.214140711488544
Validation loss: 2.525702188657151

Epoch: 5| Step: 5
Training loss: 2.811895517451175
Validation loss: 2.5369355466474053

Epoch: 5| Step: 6
Training loss: 3.2025683407783756
Validation loss: 2.534535489630654

Epoch: 5| Step: 7
Training loss: 3.117451209670768
Validation loss: 2.532734625809247

Epoch: 5| Step: 8
Training loss: 2.5596984807904692
Validation loss: 2.5169608986110314

Epoch: 5| Step: 9
Training loss: 2.886478233383525
Validation loss: 2.541773188830162

Epoch: 5| Step: 10
Training loss: 3.0640952179060186
Validation loss: 2.5205460800596513

Epoch: 127| Step: 0
Training loss: 2.6408816782782547
Validation loss: 2.5014495584921748

Epoch: 5| Step: 1
Training loss: 3.0456244616681207
Validation loss: 2.4944482440622493

Epoch: 5| Step: 2
Training loss: 2.765838528595961
Validation loss: 2.525439518027115

Epoch: 5| Step: 3
Training loss: 3.345771677502958
Validation loss: 2.5058464179310938

Epoch: 5| Step: 4
Training loss: 2.506169526670359
Validation loss: 2.530199129260314

Epoch: 5| Step: 5
Training loss: 2.6783663771117507
Validation loss: 2.5189529934404424

Epoch: 5| Step: 6
Training loss: 1.9231744730489826
Validation loss: 2.508309464168526

Epoch: 5| Step: 7
Training loss: 2.981886060506712
Validation loss: 2.52340235155952

Epoch: 5| Step: 8
Training loss: 2.691404123955982
Validation loss: 2.5183010045048144

Epoch: 5| Step: 9
Training loss: 2.473113248275126
Validation loss: 2.5135522668888823

Epoch: 5| Step: 10
Training loss: 3.0537804234878987
Validation loss: 2.527243452999412

Epoch: 128| Step: 0
Training loss: 3.3019914744663907
Validation loss: 2.5184996652504044

Epoch: 5| Step: 1
Training loss: 3.064125563825058
Validation loss: 2.503727319055913

Epoch: 5| Step: 2
Training loss: 3.0418243018981492
Validation loss: 2.5305625704200385

Epoch: 5| Step: 3
Training loss: 2.7909948669900304
Validation loss: 2.51936276963101

Epoch: 5| Step: 4
Training loss: 2.6502305884174393
Validation loss: 2.527542974952763

Epoch: 5| Step: 5
Training loss: 2.369769510577803
Validation loss: 2.5228871035736145

Epoch: 5| Step: 6
Training loss: 2.278822456417418
Validation loss: 2.5215751856736266

Epoch: 5| Step: 7
Training loss: 2.8647205851647897
Validation loss: 2.5430066755315406

Epoch: 5| Step: 8
Training loss: 2.8226699064353595
Validation loss: 2.5229853200135492

Epoch: 5| Step: 9
Training loss: 2.7834658421784115
Validation loss: 2.5275372158545966

Epoch: 5| Step: 10
Training loss: 1.8750476195328338
Validation loss: 2.5338897532267

Epoch: 129| Step: 0
Training loss: 3.4201814824188297
Validation loss: 2.534527391681424

Epoch: 5| Step: 1
Training loss: 2.9450707779104146
Validation loss: 2.5176607787585845

Epoch: 5| Step: 2
Training loss: 2.2293761561941237
Validation loss: 2.5295422929902336

Epoch: 5| Step: 3
Training loss: 2.915171394347655
Validation loss: 2.529600571982094

Epoch: 5| Step: 4
Training loss: 2.961692693621368
Validation loss: 2.495165356556246

Epoch: 5| Step: 5
Training loss: 2.4665596818491005
Validation loss: 2.5162815181754987

Epoch: 5| Step: 6
Training loss: 2.171246334725088
Validation loss: 2.5318981856388656

Epoch: 5| Step: 7
Training loss: 2.4103317320511537
Validation loss: 2.5193616940539783

Epoch: 5| Step: 8
Training loss: 2.6098145440228344
Validation loss: 2.5221246689728236

Epoch: 5| Step: 9
Training loss: 3.027308467536334
Validation loss: 2.525917445127508

Epoch: 5| Step: 10
Training loss: 2.777075044963475
Validation loss: 2.5267353681371083

Epoch: 130| Step: 0
Training loss: 2.9050219822903807
Validation loss: 2.5281498198844456

Epoch: 5| Step: 1
Training loss: 2.436076995985778
Validation loss: 2.5357511508668726

Epoch: 5| Step: 2
Training loss: 2.7778785072712715
Validation loss: 2.53550730854405

Epoch: 5| Step: 3
Training loss: 2.7379150145711537
Validation loss: 2.5243895462567956

Epoch: 5| Step: 4
Training loss: 3.1076847255364433
Validation loss: 2.530327172325262

Epoch: 5| Step: 5
Training loss: 2.6401569019277766
Validation loss: 2.5295147667224573

Epoch: 5| Step: 6
Training loss: 3.30285622649998
Validation loss: 2.526567725329018

Epoch: 5| Step: 7
Training loss: 2.5021091147520416
Validation loss: 2.5313571039812324

Epoch: 5| Step: 8
Training loss: 2.5609002120519353
Validation loss: 2.5422903237507843

Epoch: 5| Step: 9
Training loss: 2.482891479541973
Validation loss: 2.5236769864520374

Epoch: 5| Step: 10
Training loss: 2.578843912401452
Validation loss: 2.525800449908917

Epoch: 131| Step: 0
Training loss: 1.8415294788869108
Validation loss: 2.5338679602277403

Epoch: 5| Step: 1
Training loss: 3.144816320541059
Validation loss: 2.532568649506485

Epoch: 5| Step: 2
Training loss: 2.329033795856387
Validation loss: 2.5247756151656664

Epoch: 5| Step: 3
Training loss: 3.1234769542925367
Validation loss: 2.519934592077729

Epoch: 5| Step: 4
Training loss: 3.4067460845031206
Validation loss: 2.544840335330607

Epoch: 5| Step: 5
Training loss: 2.8321582002796264
Validation loss: 2.5440306688341905

Epoch: 5| Step: 6
Training loss: 2.790182962468643
Validation loss: 2.5064052389787133

Epoch: 5| Step: 7
Training loss: 2.5488342986635004
Validation loss: 2.5148844631801164

Epoch: 5| Step: 8
Training loss: 1.730952141543693
Validation loss: 2.529875683658045

Epoch: 5| Step: 9
Training loss: 2.529694538448313
Validation loss: 2.5087756801364494

Epoch: 5| Step: 10
Training loss: 3.3215404438919838
Validation loss: 2.518372420548685

Epoch: 132| Step: 0
Training loss: 2.9029413894061222
Validation loss: 2.5204575005156236

Epoch: 5| Step: 1
Training loss: 2.0854185223134487
Validation loss: 2.5098752994673403

Epoch: 5| Step: 2
Training loss: 3.2125060619490013
Validation loss: 2.536408729234109

Epoch: 5| Step: 3
Training loss: 2.9110997251558124
Validation loss: 2.535123227881469

Epoch: 5| Step: 4
Training loss: 2.5676674262470796
Validation loss: 2.5501516093610985

Epoch: 5| Step: 5
Training loss: 2.6494062334304247
Validation loss: 2.532715906064729

Epoch: 5| Step: 6
Training loss: 2.607406787614355
Validation loss: 2.527052247232991

Epoch: 5| Step: 7
Training loss: 3.094588657338788
Validation loss: 2.5469767569481605

Epoch: 5| Step: 8
Training loss: 2.87049662290719
Validation loss: 2.536099828368609

Epoch: 5| Step: 9
Training loss: 2.1284947548842643
Validation loss: 2.529467800096611

Epoch: 5| Step: 10
Training loss: 2.942112811209706
Validation loss: 2.5023225783361296

Epoch: 133| Step: 0
Training loss: 2.613118028318954
Validation loss: 2.531200897133497

Epoch: 5| Step: 1
Training loss: 3.036843713260452
Validation loss: 2.5229242965509573

Epoch: 5| Step: 2
Training loss: 3.308152855114947
Validation loss: 2.5196413133611317

Epoch: 5| Step: 3
Training loss: 2.656865755404746
Validation loss: 2.5262257431005612

Epoch: 5| Step: 4
Training loss: 2.4323650950771167
Validation loss: 2.5240057076912477

Epoch: 5| Step: 5
Training loss: 2.836195416452755
Validation loss: 2.536522724599111

Epoch: 5| Step: 6
Training loss: 2.4513118384778525
Validation loss: 2.541993105249696

Epoch: 5| Step: 7
Training loss: 2.8234793933827778
Validation loss: 2.534750741358122

Epoch: 5| Step: 8
Training loss: 2.429415279728724
Validation loss: 2.5349809897075817

Epoch: 5| Step: 9
Training loss: 2.8649710837830606
Validation loss: 2.519625966926996

Epoch: 5| Step: 10
Training loss: 2.3373219141720187
Validation loss: 2.5332538598612113

Epoch: 134| Step: 0
Training loss: 2.4187103268230707
Validation loss: 2.5382091123637256

Epoch: 5| Step: 1
Training loss: 2.5867909293788887
Validation loss: 2.521144775159807

Epoch: 5| Step: 2
Training loss: 2.338572161390005
Validation loss: 2.5349460033466706

Epoch: 5| Step: 3
Training loss: 2.4596922615285313
Validation loss: 2.535173627015598

Epoch: 5| Step: 4
Training loss: 2.889938685416829
Validation loss: 2.497105888047091

Epoch: 5| Step: 5
Training loss: 2.3781409324483236
Validation loss: 2.52528745967781

Epoch: 5| Step: 6
Training loss: 2.6658952511171927
Validation loss: 2.525532813537136

Epoch: 5| Step: 7
Training loss: 2.819209678367646
Validation loss: 2.518475442620252

Epoch: 5| Step: 8
Training loss: 2.8276661031622705
Validation loss: 2.5248350860599214

Epoch: 5| Step: 9
Training loss: 3.4830822107303963
Validation loss: 2.497950298148572

Epoch: 5| Step: 10
Training loss: 3.0464830977927186
Validation loss: 2.529759842328976

Epoch: 135| Step: 0
Training loss: 2.550926870304996
Validation loss: 2.5189297104819994

Epoch: 5| Step: 1
Training loss: 3.0979407084869752
Validation loss: 2.5214275224924294

Epoch: 5| Step: 2
Training loss: 3.1762405194798218
Validation loss: 2.524657026609771

Epoch: 5| Step: 3
Training loss: 2.6592413719663437
Validation loss: 2.5177459855585194

Epoch: 5| Step: 4
Training loss: 2.5895435280066565
Validation loss: 2.5339930225808516

Epoch: 5| Step: 5
Training loss: 3.039965968443468
Validation loss: 2.5249751757548062

Epoch: 5| Step: 6
Training loss: 2.252831055491601
Validation loss: 2.519778201086947

Epoch: 5| Step: 7
Training loss: 2.483967009818262
Validation loss: 2.5188460826143095

Epoch: 5| Step: 8
Training loss: 2.739528747324213
Validation loss: 2.532862220858081

Epoch: 5| Step: 9
Training loss: 2.53700487539795
Validation loss: 2.5455808237141464

Epoch: 5| Step: 10
Training loss: 2.8309185920418245
Validation loss: 2.5431899198601537

Epoch: 136| Step: 0
Training loss: 3.0455677847338567
Validation loss: 2.526863113937872

Epoch: 5| Step: 1
Training loss: 3.6243530222250584
Validation loss: 2.535112637049369

Epoch: 5| Step: 2
Training loss: 2.32223677557117
Validation loss: 2.5320515924999065

Epoch: 5| Step: 3
Training loss: 2.6599061823197214
Validation loss: 2.524465703139511

Epoch: 5| Step: 4
Training loss: 2.156028183294607
Validation loss: 2.526101019800233

Epoch: 5| Step: 5
Training loss: 2.3943460464419726
Validation loss: 2.528564786055188

Epoch: 5| Step: 6
Training loss: 2.8068144232788304
Validation loss: 2.5428373531154893

Epoch: 5| Step: 7
Training loss: 2.720286143622769
Validation loss: 2.5210220295443544

Epoch: 5| Step: 8
Training loss: 2.7240750134036147
Validation loss: 2.5371384406371384

Epoch: 5| Step: 9
Training loss: 2.512979573650659
Validation loss: 2.5251054746434836

Epoch: 5| Step: 10
Training loss: 3.1316218598878818
Validation loss: 2.5363195301780928

Epoch: 137| Step: 0
Training loss: 2.0476708648521034
Validation loss: 2.5087684217889135

Epoch: 5| Step: 1
Training loss: 2.6766481651943046
Validation loss: 2.536403811996742

Epoch: 5| Step: 2
Training loss: 3.567114802458146
Validation loss: 2.4992513499591325

Epoch: 5| Step: 3
Training loss: 2.699759091120067
Validation loss: 2.5257561821194727

Epoch: 5| Step: 4
Training loss: 2.7520777049405347
Validation loss: 2.5269376014398763

Epoch: 5| Step: 5
Training loss: 3.0600231820518653
Validation loss: 2.5210115686179426

Epoch: 5| Step: 6
Training loss: 2.599112333476102
Validation loss: 2.525451729969654

Epoch: 5| Step: 7
Training loss: 2.3637073479510557
Validation loss: 2.5113983804794024

Epoch: 5| Step: 8
Training loss: 2.5500915436113933
Validation loss: 2.5334806034203208

Epoch: 5| Step: 9
Training loss: 3.029511100430133
Validation loss: 2.5219507953418336

Epoch: 5| Step: 10
Training loss: 2.525555551428887
Validation loss: 2.5168039439160212

Epoch: 138| Step: 0
Training loss: 2.623033786266674
Validation loss: 2.529537725231549

Epoch: 5| Step: 1
Training loss: 2.8681880026452355
Validation loss: 2.525780530819165

Epoch: 5| Step: 2
Training loss: 2.153311454772031
Validation loss: 2.5342514481576446

Epoch: 5| Step: 3
Training loss: 2.737550210710015
Validation loss: 2.5067896550641544

Epoch: 5| Step: 4
Training loss: 2.548811474742072
Validation loss: 2.5284612706629286

Epoch: 5| Step: 5
Training loss: 2.8959897957300638
Validation loss: 2.523480877887122

Epoch: 5| Step: 6
Training loss: 2.384179795500274
Validation loss: 2.514314005558168

Epoch: 5| Step: 7
Training loss: 2.5688761956247066
Validation loss: 2.523924229713777

Epoch: 5| Step: 8
Training loss: 2.5943269202901242
Validation loss: 2.5376206350795707

Epoch: 5| Step: 9
Training loss: 3.019791802814994
Validation loss: 2.545194169735184

Epoch: 5| Step: 10
Training loss: 3.5717146976841296
Validation loss: 2.5204177943798305

Epoch: 139| Step: 0
Training loss: 2.56322636427828
Validation loss: 2.5074814698180123

Epoch: 5| Step: 1
Training loss: 1.8104608838381344
Validation loss: 2.5149388183486043

Epoch: 5| Step: 2
Training loss: 2.2712337129268207
Validation loss: 2.5415925092955427

Epoch: 5| Step: 3
Training loss: 3.230517902484666
Validation loss: 2.544797959217771

Epoch: 5| Step: 4
Training loss: 3.1485344277883875
Validation loss: 2.522167814311606

Epoch: 5| Step: 5
Training loss: 2.808256635741085
Validation loss: 2.546469204069008

Epoch: 5| Step: 6
Training loss: 2.325831980179225
Validation loss: 2.5363423058001655

Epoch: 5| Step: 7
Training loss: 2.98611112096821
Validation loss: 2.522822714183524

Epoch: 5| Step: 8
Training loss: 2.822314114797545
Validation loss: 2.5307257537433503

Epoch: 5| Step: 9
Training loss: 3.088883087860198
Validation loss: 2.5191625202063648

Epoch: 5| Step: 10
Training loss: 2.7911525556584578
Validation loss: 2.533488761381941

Epoch: 140| Step: 0
Training loss: 2.582964337565372
Validation loss: 2.5385265926354665

Epoch: 5| Step: 1
Training loss: 2.75542210192519
Validation loss: 2.528295366690029

Epoch: 5| Step: 2
Training loss: 2.8328756822966406
Validation loss: 2.5197604818598047

Epoch: 5| Step: 3
Training loss: 3.1498960992540996
Validation loss: 2.5310189483371905

Epoch: 5| Step: 4
Training loss: 2.7728425583273717
Validation loss: 2.5156632705006152

Epoch: 5| Step: 5
Training loss: 3.113085502491892
Validation loss: 2.5233926868462992

Epoch: 5| Step: 6
Training loss: 2.308034348204167
Validation loss: 2.556829441609661

Epoch: 5| Step: 7
Training loss: 3.1064870640270312
Validation loss: 2.5384850498355727

Epoch: 5| Step: 8
Training loss: 2.698436220629311
Validation loss: 2.5331633868906636

Epoch: 5| Step: 9
Training loss: 1.9350062903959953
Validation loss: 2.5194509440167336

Epoch: 5| Step: 10
Training loss: 2.751983100939031
Validation loss: 2.525900381991885

Epoch: 141| Step: 0
Training loss: 2.1753427827314002
Validation loss: 2.52126026699394

Epoch: 5| Step: 1
Training loss: 2.7012424859715165
Validation loss: 2.5323832896966945

Epoch: 5| Step: 2
Training loss: 2.8906685490807367
Validation loss: 2.5356027481189924

Epoch: 5| Step: 3
Training loss: 2.7427691850722304
Validation loss: 2.519695010391648

Epoch: 5| Step: 4
Training loss: 3.0673501669705017
Validation loss: 2.5354671657032495

Epoch: 5| Step: 5
Training loss: 2.8172109356422177
Validation loss: 2.532462902646293

Epoch: 5| Step: 6
Training loss: 2.7125034701417445
Validation loss: 2.513812642888346

Epoch: 5| Step: 7
Training loss: 2.3927141423390847
Validation loss: 2.5500113203684953

Epoch: 5| Step: 8
Training loss: 3.139166953749543
Validation loss: 2.5291758199300673

Epoch: 5| Step: 9
Training loss: 2.643202838234151
Validation loss: 2.524070085185195

Epoch: 5| Step: 10
Training loss: 2.5693273076572574
Validation loss: 2.5355148937729366

Epoch: 142| Step: 0
Training loss: 2.1999150693278726
Validation loss: 2.5232607805867806

Epoch: 5| Step: 1
Training loss: 2.809671802563879
Validation loss: 2.519793655455835

Epoch: 5| Step: 2
Training loss: 2.495529087056876
Validation loss: 2.4939920714251564

Epoch: 5| Step: 3
Training loss: 3.031759317613541
Validation loss: 2.530625945348823

Epoch: 5| Step: 4
Training loss: 3.3267624622483356
Validation loss: 2.5372503521037064

Epoch: 5| Step: 5
Training loss: 3.4004662081903465
Validation loss: 2.535147769762805

Epoch: 5| Step: 6
Training loss: 2.9592297946388055
Validation loss: 2.5062220577621455

Epoch: 5| Step: 7
Training loss: 2.2442661977351053
Validation loss: 2.521602126591621

Epoch: 5| Step: 8
Training loss: 2.2419223960968964
Validation loss: 2.5286900255283253

Epoch: 5| Step: 9
Training loss: 2.7926581790285434
Validation loss: 2.5240860566598724

Epoch: 5| Step: 10
Training loss: 1.947932550974117
Validation loss: 2.5475916042766777

Epoch: 143| Step: 0
Training loss: 2.071146440789042
Validation loss: 2.508987846979496

Epoch: 5| Step: 1
Training loss: 2.67482590153371
Validation loss: 2.525363083934072

Epoch: 5| Step: 2
Training loss: 2.1808939777125125
Validation loss: 2.5509225508801987

Epoch: 5| Step: 3
Training loss: 2.6678363102537053
Validation loss: 2.5257843065771617

Epoch: 5| Step: 4
Training loss: 2.8293454397927063
Validation loss: 2.511135414226569

Epoch: 5| Step: 5
Training loss: 2.6418507456698306
Validation loss: 2.519455698973925

Epoch: 5| Step: 6
Training loss: 2.890310074430593
Validation loss: 2.528084661002555

Epoch: 5| Step: 7
Training loss: 3.123652358343696
Validation loss: 2.5261630107902646

Epoch: 5| Step: 8
Training loss: 2.9707265598393535
Validation loss: 2.5468730472285688

Epoch: 5| Step: 9
Training loss: 3.3182684137676546
Validation loss: 2.521412566176907

Epoch: 5| Step: 10
Training loss: 2.2619948144544666
Validation loss: 2.5299982601462805

Epoch: 144| Step: 0
Training loss: 2.729172209134674
Validation loss: 2.5372277311422575

Epoch: 5| Step: 1
Training loss: 3.4020149431208813
Validation loss: 2.5120937219532506

Epoch: 5| Step: 2
Training loss: 2.2932965695775978
Validation loss: 2.5189761001161846

Epoch: 5| Step: 3
Training loss: 2.8193054937523265
Validation loss: 2.5189279202591672

Epoch: 5| Step: 4
Training loss: 2.43043138701543
Validation loss: 2.5133225881407175

Epoch: 5| Step: 5
Training loss: 2.7256083308153896
Validation loss: 2.5102912920202134

Epoch: 5| Step: 6
Training loss: 2.293677147480124
Validation loss: 2.5314994839970684

Epoch: 5| Step: 7
Training loss: 2.8493359377677603
Validation loss: 2.5339730383927717

Epoch: 5| Step: 8
Training loss: 2.6620473091799584
Validation loss: 2.512220333596746

Epoch: 5| Step: 9
Training loss: 2.909141988468391
Validation loss: 2.5532941633392388

Epoch: 5| Step: 10
Training loss: 2.5930715385951966
Validation loss: 2.5152553673278963

Epoch: 145| Step: 0
Training loss: 2.4298088803897615
Validation loss: 2.515967632427929

Epoch: 5| Step: 1
Training loss: 2.7481150669510934
Validation loss: 2.5394014390710855

Epoch: 5| Step: 2
Training loss: 2.2864516018454855
Validation loss: 2.517395715038374

Epoch: 5| Step: 3
Training loss: 3.2458616465230214
Validation loss: 2.528500582819611

Epoch: 5| Step: 4
Training loss: 2.4878713129377834
Validation loss: 2.5454982446480092

Epoch: 5| Step: 5
Training loss: 2.703466493060634
Validation loss: 2.5289773039547936

Epoch: 5| Step: 6
Training loss: 2.4485053037835653
Validation loss: 2.5374704422788397

Epoch: 5| Step: 7
Training loss: 1.882503373344408
Validation loss: 2.525683877612833

Epoch: 5| Step: 8
Training loss: 2.7771289597521824
Validation loss: 2.5347760878648593

Epoch: 5| Step: 9
Training loss: 2.9609539263025564
Validation loss: 2.5341767254589875

Epoch: 5| Step: 10
Training loss: 3.8802924162013217
Validation loss: 2.516042876040489

Epoch: 146| Step: 0
Training loss: 2.2805738100143973
Validation loss: 2.505975485353064

Epoch: 5| Step: 1
Training loss: 2.5641649931252464
Validation loss: 2.522600874485186

Epoch: 5| Step: 2
Training loss: 3.2566565788786983
Validation loss: 2.540006105880562

Epoch: 5| Step: 3
Training loss: 2.396902779986402
Validation loss: 2.5199693244752375

Epoch: 5| Step: 4
Training loss: 2.458844751298123
Validation loss: 2.525215195556722

Epoch: 5| Step: 5
Training loss: 2.7905813831026314
Validation loss: 2.491316621167643

Epoch: 5| Step: 6
Training loss: 2.3911860187942406
Validation loss: 2.505958135025437

Epoch: 5| Step: 7
Training loss: 1.9141938456409202
Validation loss: 2.522282830382717

Epoch: 5| Step: 8
Training loss: 3.122303065983834
Validation loss: 2.5154040665124526

Epoch: 5| Step: 9
Training loss: 3.3214599062940158
Validation loss: 2.503568870656326

Epoch: 5| Step: 10
Training loss: 3.1775433483513416
Validation loss: 2.524789581356318

Epoch: 147| Step: 0
Training loss: 2.586509986962875
Validation loss: 2.516034086331545

Epoch: 5| Step: 1
Training loss: 2.8486174124326613
Validation loss: 2.508657616720492

Epoch: 5| Step: 2
Training loss: 2.831056708670372
Validation loss: 2.5288855329642574

Epoch: 5| Step: 3
Training loss: 3.311099350102621
Validation loss: 2.5282148452654374

Epoch: 5| Step: 4
Training loss: 2.5432277846360276
Validation loss: 2.5076176718443715

Epoch: 5| Step: 5
Training loss: 2.8967257069261825
Validation loss: 2.537123977076118

Epoch: 5| Step: 6
Training loss: 2.3321741721671927
Validation loss: 2.5354047469845615

Epoch: 5| Step: 7
Training loss: 2.7754941380074154
Validation loss: 2.537814075126178

Epoch: 5| Step: 8
Training loss: 2.020834408264923
Validation loss: 2.5381137445332596

Epoch: 5| Step: 9
Training loss: 2.5431159426606844
Validation loss: 2.52821204557627

Epoch: 5| Step: 10
Training loss: 3.1002541960817127
Validation loss: 2.5117475225747894

Epoch: 148| Step: 0
Training loss: 2.2366752255195936
Validation loss: 2.5338670961931853

Epoch: 5| Step: 1
Training loss: 2.118233397752938
Validation loss: 2.550096433450006

Epoch: 5| Step: 2
Training loss: 2.329845023506312
Validation loss: 2.502575860926173

Epoch: 5| Step: 3
Training loss: 3.324133543542981
Validation loss: 2.53892218998014

Epoch: 5| Step: 4
Training loss: 3.080892954600188
Validation loss: 2.538472422903384

Epoch: 5| Step: 5
Training loss: 3.3227695125574903
Validation loss: 2.5204117341983383

Epoch: 5| Step: 6
Training loss: 3.2769228059059765
Validation loss: 2.515892061001333

Epoch: 5| Step: 7
Training loss: 2.516216895562689
Validation loss: 2.5280269215448232

Epoch: 5| Step: 8
Training loss: 2.5782424437620133
Validation loss: 2.522465724387185

Epoch: 5| Step: 9
Training loss: 2.100551659921978
Validation loss: 2.5260457722095904

Epoch: 5| Step: 10
Training loss: 2.6904840093483786
Validation loss: 2.539829558185805

Epoch: 149| Step: 0
Training loss: 2.1695045323936664
Validation loss: 2.543725697890932

Epoch: 5| Step: 1
Training loss: 2.7212575989644705
Validation loss: 2.5539363988607926

Epoch: 5| Step: 2
Training loss: 3.620704210186346
Validation loss: 2.5329803108971105

Epoch: 5| Step: 3
Training loss: 2.4638009986650604
Validation loss: 2.5425089431179337

Epoch: 5| Step: 4
Training loss: 3.2920356414923515
Validation loss: 2.5334391068973083

Epoch: 5| Step: 5
Training loss: 3.0373947941649133
Validation loss: 2.5538319834873215

Epoch: 5| Step: 6
Training loss: 2.823294460640036
Validation loss: 2.5168759443236692

Epoch: 5| Step: 7
Training loss: 2.471854274195072
Validation loss: 2.515981224140966

Epoch: 5| Step: 8
Training loss: 1.7316576315993215
Validation loss: 2.540237640976568

Epoch: 5| Step: 9
Training loss: 2.5946802516332013
Validation loss: 2.543091576467379

Epoch: 5| Step: 10
Training loss: 2.525747558671107
Validation loss: 2.5208930694355356

Epoch: 150| Step: 0
Training loss: 2.0463392415565957
Validation loss: 2.5256708090895437

Epoch: 5| Step: 1
Training loss: 2.967442596060299
Validation loss: 2.5217354544308708

Epoch: 5| Step: 2
Training loss: 2.4955893709890957
Validation loss: 2.516498091876635

Epoch: 5| Step: 3
Training loss: 2.559203004006629
Validation loss: 2.532141789819745

Epoch: 5| Step: 4
Training loss: 2.633331157184458
Validation loss: 2.5401815634419473

Epoch: 5| Step: 5
Training loss: 2.62720451428424
Validation loss: 2.5045129629889638

Epoch: 5| Step: 6
Training loss: 2.68677865926909
Validation loss: 2.5290917467975316

Epoch: 5| Step: 7
Training loss: 2.3197941570385665
Validation loss: 2.5354931106979337

Epoch: 5| Step: 8
Training loss: 3.754436030799139
Validation loss: 2.534592395049272

Epoch: 5| Step: 9
Training loss: 2.6280136973867436
Validation loss: 2.538879462650121

Epoch: 5| Step: 10
Training loss: 2.842171534849419
Validation loss: 2.5476145709509246

Epoch: 151| Step: 0
Training loss: 2.7257023632369184
Validation loss: 2.521458259424973

Epoch: 5| Step: 1
Training loss: 2.408032042280665
Validation loss: 2.5007388330637332

Epoch: 5| Step: 2
Training loss: 2.935487260549455
Validation loss: 2.493055067830613

Epoch: 5| Step: 3
Training loss: 2.6647289706573436
Validation loss: 2.5133734142297297

Epoch: 5| Step: 4
Training loss: 2.069973439216673
Validation loss: 2.5259108784930744

Epoch: 5| Step: 5
Training loss: 3.368088179143015
Validation loss: 2.5193959118316105

Epoch: 5| Step: 6
Training loss: 2.45723986687639
Validation loss: 2.5141842889177526

Epoch: 5| Step: 7
Training loss: 2.716312850737609
Validation loss: 2.5251023801266315

Epoch: 5| Step: 8
Training loss: 2.5429107143735177
Validation loss: 2.4965725794977236

Epoch: 5| Step: 9
Training loss: 2.9217230609413387
Validation loss: 2.522448340117598

Epoch: 5| Step: 10
Training loss: 2.9630959728197506
Validation loss: 2.5366104935066076

Epoch: 152| Step: 0
Training loss: 2.0738631983622353
Validation loss: 2.5161096117178885

Epoch: 5| Step: 1
Training loss: 2.747940939872453
Validation loss: 2.527578995897086

Epoch: 5| Step: 2
Training loss: 2.895681875828382
Validation loss: 2.523913134805348

Epoch: 5| Step: 3
Training loss: 2.8353183375193063
Validation loss: 2.530214010333845

Epoch: 5| Step: 4
Training loss: 2.2503393764980704
Validation loss: 2.552016923419993

Epoch: 5| Step: 5
Training loss: 3.0171827335013814
Validation loss: 2.5606109064725064

Epoch: 5| Step: 6
Training loss: 3.0117002256543635
Validation loss: 2.516694714002075

Epoch: 5| Step: 7
Training loss: 2.709001800987121
Validation loss: 2.5554237899005092

Epoch: 5| Step: 8
Training loss: 2.4987017121451003
Validation loss: 2.530014605593689

Epoch: 5| Step: 9
Training loss: 2.8836941495132615
Validation loss: 2.545596852580923

Epoch: 5| Step: 10
Training loss: 2.895048149380498
Validation loss: 2.538684649627567

Epoch: 153| Step: 0
Training loss: 3.1464716451092585
Validation loss: 2.5371958858459216

Epoch: 5| Step: 1
Training loss: 2.9356345888841693
Validation loss: 2.5269707335706255

Epoch: 5| Step: 2
Training loss: 2.3155359495597585
Validation loss: 2.521614122268334

Epoch: 5| Step: 3
Training loss: 2.500582817806868
Validation loss: 2.536886016722587

Epoch: 5| Step: 4
Training loss: 2.097371058905441
Validation loss: 2.5175601927372724

Epoch: 5| Step: 5
Training loss: 2.8707547565072984
Validation loss: 2.5325429579899414

Epoch: 5| Step: 6
Training loss: 2.474364162420371
Validation loss: 2.539067291936778

Epoch: 5| Step: 7
Training loss: 3.1015117650722908
Validation loss: 2.520109983221375

Epoch: 5| Step: 8
Training loss: 2.5659079801036553
Validation loss: 2.521610109482417

Epoch: 5| Step: 9
Training loss: 3.3715658730265416
Validation loss: 2.5191691369964238

Epoch: 5| Step: 10
Training loss: 2.085857146926422
Validation loss: 2.5432614222530665

Epoch: 154| Step: 0
Training loss: 2.553109431131769
Validation loss: 2.540900172492225

Epoch: 5| Step: 1
Training loss: 2.438549793810516
Validation loss: 2.5352638017418947

Epoch: 5| Step: 2
Training loss: 2.3832803235683393
Validation loss: 2.502373282597031

Epoch: 5| Step: 3
Training loss: 2.4497725789921274
Validation loss: 2.5241940160999445

Epoch: 5| Step: 4
Training loss: 2.8544445784282377
Validation loss: 2.51599734783817

Epoch: 5| Step: 5
Training loss: 2.4904677816473235
Validation loss: 2.5370036819984687

Epoch: 5| Step: 6
Training loss: 3.051005375990231
Validation loss: 2.5209448188110253

Epoch: 5| Step: 7
Training loss: 2.22970241870447
Validation loss: 2.5408578618314954

Epoch: 5| Step: 8
Training loss: 3.219480413134113
Validation loss: 2.54918839228143

Epoch: 5| Step: 9
Training loss: 3.1758178858972324
Validation loss: 2.538736767443273

Epoch: 5| Step: 10
Training loss: 2.8682287336831394
Validation loss: 2.5342226356940225

Epoch: 155| Step: 0
Training loss: 2.3478021814370136
Validation loss: 2.536722258481664

Epoch: 5| Step: 1
Training loss: 2.4702021516864834
Validation loss: 2.542178699701974

Epoch: 5| Step: 2
Training loss: 2.055171198895313
Validation loss: 2.5539797054248314

Epoch: 5| Step: 3
Training loss: 3.0482701946357045
Validation loss: 2.541083389146305

Epoch: 5| Step: 4
Training loss: 2.7455267503970884
Validation loss: 2.5308555681591174

Epoch: 5| Step: 5
Training loss: 3.0016514683038085
Validation loss: 2.5294647909839534

Epoch: 5| Step: 6
Training loss: 3.259407512908608
Validation loss: 2.528366559243115

Epoch: 5| Step: 7
Training loss: 2.5123009370356053
Validation loss: 2.5423918532907

Epoch: 5| Step: 8
Training loss: 2.6970343797958494
Validation loss: 2.564984647445087

Epoch: 5| Step: 9
Training loss: 2.7847691249517736
Validation loss: 2.5356399770116314

Epoch: 5| Step: 10
Training loss: 2.725526366886407
Validation loss: 2.541281283765469

Epoch: 156| Step: 0
Training loss: 2.1520654737304477
Validation loss: 2.517174657909316

Epoch: 5| Step: 1
Training loss: 2.730299260850812
Validation loss: 2.535440242687819

Epoch: 5| Step: 2
Training loss: 3.1266446172862654
Validation loss: 2.5155604206494697

Epoch: 5| Step: 3
Training loss: 2.9736555828356432
Validation loss: 2.5426729668933037

Epoch: 5| Step: 4
Training loss: 2.9169688113705963
Validation loss: 2.533925769945369

Epoch: 5| Step: 5
Training loss: 2.441621474888305
Validation loss: 2.5402005925002733

Epoch: 5| Step: 6
Training loss: 2.381608902925016
Validation loss: 2.515497867727382

Epoch: 5| Step: 7
Training loss: 2.137848438618296
Validation loss: 2.5321810868518932

Epoch: 5| Step: 8
Training loss: 2.804719037486989
Validation loss: 2.5120567247872843

Epoch: 5| Step: 9
Training loss: 3.276875077065178
Validation loss: 2.514420109900377

Epoch: 5| Step: 10
Training loss: 2.763645994247193
Validation loss: 2.5293894930832677

Epoch: 157| Step: 0
Training loss: 2.325048372319223
Validation loss: 2.5102620636105977

Epoch: 5| Step: 1
Training loss: 3.814228854054071
Validation loss: 2.5106446357502485

Epoch: 5| Step: 2
Training loss: 2.879133984112952
Validation loss: 2.527866567921168

Epoch: 5| Step: 3
Training loss: 2.6911204591892015
Validation loss: 2.5290326882146026

Epoch: 5| Step: 4
Training loss: 2.5280601276236716
Validation loss: 2.525249582742622

Epoch: 5| Step: 5
Training loss: 2.0539352142160614
Validation loss: 2.5214397721672954

Epoch: 5| Step: 6
Training loss: 2.7681499097835487
Validation loss: 2.5144547079305437

Epoch: 5| Step: 7
Training loss: 2.6853947044956175
Validation loss: 2.5592181932540385

Epoch: 5| Step: 8
Training loss: 2.6335649179279628
Validation loss: 2.5478313579838385

Epoch: 5| Step: 9
Training loss: 2.5610323168627014
Validation loss: 2.5249893190281263

Epoch: 5| Step: 10
Training loss: 2.369233408696841
Validation loss: 2.5249073590292537

Epoch: 158| Step: 0
Training loss: 2.712403090894924
Validation loss: 2.5279754672328183

Epoch: 5| Step: 1
Training loss: 2.756612803137818
Validation loss: 2.551888792263051

Epoch: 5| Step: 2
Training loss: 2.1587472359220463
Validation loss: 2.5178103525574014

Epoch: 5| Step: 3
Training loss: 2.5148353996031565
Validation loss: 2.515951098920465

Epoch: 5| Step: 4
Training loss: 2.953561417560543
Validation loss: 2.5261914625662576

Epoch: 5| Step: 5
Training loss: 2.6850418559244855
Validation loss: 2.5268853041789923

Epoch: 5| Step: 6
Training loss: 2.9767798797448246
Validation loss: 2.5395509695191083

Epoch: 5| Step: 7
Training loss: 3.4670544853671386
Validation loss: 2.5510373900189194

Epoch: 5| Step: 8
Training loss: 1.8781799054490669
Validation loss: 2.534317232454819

Epoch: 5| Step: 9
Training loss: 2.7926616793336256
Validation loss: 2.508350489287256

Epoch: 5| Step: 10
Training loss: 2.690277793182739
Validation loss: 2.523402187992202

Epoch: 159| Step: 0
Training loss: 2.6684459968750587
Validation loss: 2.5236946121519876

Epoch: 5| Step: 1
Training loss: 2.3677685720017236
Validation loss: 2.503785311199803

Epoch: 5| Step: 2
Training loss: 2.2531985753674357
Validation loss: 2.533108894491238

Epoch: 5| Step: 3
Training loss: 1.9649830459756337
Validation loss: 2.517587694930983

Epoch: 5| Step: 4
Training loss: 2.6455780166430123
Validation loss: 2.5197007151760413

Epoch: 5| Step: 5
Training loss: 2.8871102074100747
Validation loss: 2.5289097101182927

Epoch: 5| Step: 6
Training loss: 2.9091947669758564
Validation loss: 2.529944411936591

Epoch: 5| Step: 7
Training loss: 3.364569901163002
Validation loss: 2.5297490831137925

Epoch: 5| Step: 8
Training loss: 2.3666978587750114
Validation loss: 2.5220427226119244

Epoch: 5| Step: 9
Training loss: 2.740687033148034
Validation loss: 2.5280219342573167

Epoch: 5| Step: 10
Training loss: 3.263347945240676
Validation loss: 2.5410888688559266

Epoch: 160| Step: 0
Training loss: 2.510757096082001
Validation loss: 2.5156424818665237

Epoch: 5| Step: 1
Training loss: 2.921038992512145
Validation loss: 2.557741277053651

Epoch: 5| Step: 2
Training loss: 2.8148238012754123
Validation loss: 2.5163345115955678

Epoch: 5| Step: 3
Training loss: 2.6293171395731023
Validation loss: 2.5136390387679404

Epoch: 5| Step: 4
Training loss: 2.772807218902797
Validation loss: 2.5641346571505697

Epoch: 5| Step: 5
Training loss: 2.358283592571242
Validation loss: 2.533436112620832

Epoch: 5| Step: 6
Training loss: 3.0861609269394514
Validation loss: 2.5449338050813046

Epoch: 5| Step: 7
Training loss: 2.8629937957898974
Validation loss: 2.518318022421109

Epoch: 5| Step: 8
Training loss: 2.569481526929163
Validation loss: 2.5384027984286015

Epoch: 5| Step: 9
Training loss: 2.6164445875975018
Validation loss: 2.5336867705906267

Epoch: 5| Step: 10
Training loss: 2.492204241561862
Validation loss: 2.5375733850741335

Epoch: 161| Step: 0
Training loss: 3.141994803887532
Validation loss: 2.513625727100808

Epoch: 5| Step: 1
Training loss: 2.7704291562790297
Validation loss: 2.5438782070803168

Epoch: 5| Step: 2
Training loss: 2.0705255362827395
Validation loss: 2.5051480519829643

Epoch: 5| Step: 3
Training loss: 2.3850123522296665
Validation loss: 2.530340434629181

Epoch: 5| Step: 4
Training loss: 2.6298614172449417
Validation loss: 2.5349337784855948

Epoch: 5| Step: 5
Training loss: 2.931654125083423
Validation loss: 2.5403955677965384

Epoch: 5| Step: 6
Training loss: 2.9786528208668934
Validation loss: 2.5231138058778706

Epoch: 5| Step: 7
Training loss: 3.5295423581702026
Validation loss: 2.519497197881265

Epoch: 5| Step: 8
Training loss: 2.0155438547286617
Validation loss: 2.5164959209548416

Epoch: 5| Step: 9
Training loss: 2.3123756581462294
Validation loss: 2.518972818948574

Epoch: 5| Step: 10
Training loss: 2.560609073305401
Validation loss: 2.5339154229941605

Epoch: 162| Step: 0
Training loss: 3.062200103400593
Validation loss: 2.5156264144923037

Epoch: 5| Step: 1
Training loss: 3.042805462864635
Validation loss: 2.5184769603587105

Epoch: 5| Step: 2
Training loss: 2.898543116863605
Validation loss: 2.536167214401025

Epoch: 5| Step: 3
Training loss: 2.8269924193241156
Validation loss: 2.5231285788713884

Epoch: 5| Step: 4
Training loss: 2.923483013408407
Validation loss: 2.545496290820403

Epoch: 5| Step: 5
Training loss: 2.6036278942996245
Validation loss: 2.508931442912972

Epoch: 5| Step: 6
Training loss: 2.817128082342764
Validation loss: 2.537492489244188

Epoch: 5| Step: 7
Training loss: 2.9647520752254666
Validation loss: 2.5202166156233456

Epoch: 5| Step: 8
Training loss: 1.8328843723003643
Validation loss: 2.516318074228868

Epoch: 5| Step: 9
Training loss: 2.2093698720049666
Validation loss: 2.55447923922088

Epoch: 5| Step: 10
Training loss: 2.281126985760502
Validation loss: 2.4887885985391187

Epoch: 163| Step: 0
Training loss: 3.21567692394986
Validation loss: 2.5411183313616252

Epoch: 5| Step: 1
Training loss: 2.690530620696323
Validation loss: 2.5456496587046984

Epoch: 5| Step: 2
Training loss: 2.4717065032746257
Validation loss: 2.539765789105902

Epoch: 5| Step: 3
Training loss: 1.4022647761047402
Validation loss: 2.52681830066312

Epoch: 5| Step: 4
Training loss: 3.2137757911077514
Validation loss: 2.5260442965694363

Epoch: 5| Step: 5
Training loss: 2.8899992394363827
Validation loss: 2.5148262351181527

Epoch: 5| Step: 6
Training loss: 2.88602588892036
Validation loss: 2.55352846523993

Epoch: 5| Step: 7
Training loss: 2.6756937499949816
Validation loss: 2.519595835427825

Epoch: 5| Step: 8
Training loss: 2.742030125636825
Validation loss: 2.5315301330707265

Epoch: 5| Step: 9
Training loss: 2.447507805193261
Validation loss: 2.5465139503922805

Epoch: 5| Step: 10
Training loss: 2.6386607612471815
Validation loss: 2.5083100651392334

Epoch: 164| Step: 0
Training loss: 2.013188034818804
Validation loss: 2.5563175906037183

Epoch: 5| Step: 1
Training loss: 2.739602982096442
Validation loss: 2.519592711759529

Epoch: 5| Step: 2
Training loss: 2.6984168709572196
Validation loss: 2.5622211357585316

Epoch: 5| Step: 3
Training loss: 2.6050404621284047
Validation loss: 2.522942603236649

Epoch: 5| Step: 4
Training loss: 2.2124856980718235
Validation loss: 2.536090775094658

Epoch: 5| Step: 5
Training loss: 2.489793159442495
Validation loss: 2.526505892415087

Epoch: 5| Step: 6
Training loss: 1.9077084699631175
Validation loss: 2.507831787959326

Epoch: 5| Step: 7
Training loss: 2.977181117536555
Validation loss: 2.541186188608568

Epoch: 5| Step: 8
Training loss: 3.1790226421671837
Validation loss: 2.5187708642180384

Epoch: 5| Step: 9
Training loss: 3.235889172526995
Validation loss: 2.518023779615454

Epoch: 5| Step: 10
Training loss: 3.297830596848148
Validation loss: 2.530100703451739

Epoch: 165| Step: 0
Training loss: 2.503799031501715
Validation loss: 2.5402551013092105

Epoch: 5| Step: 1
Training loss: 2.14600270414582
Validation loss: 2.5206293687822012

Epoch: 5| Step: 2
Training loss: 3.585654918698101
Validation loss: 2.53053816642475

Epoch: 5| Step: 3
Training loss: 2.6586398873710246
Validation loss: 2.5417390847005192

Epoch: 5| Step: 4
Training loss: 2.514643886525008
Validation loss: 2.554905429824776

Epoch: 5| Step: 5
Training loss: 2.9446081639803765
Validation loss: 2.526789661145213

Epoch: 5| Step: 6
Training loss: 2.533966678788698
Validation loss: 2.5280167177752695

Epoch: 5| Step: 7
Training loss: 2.682053302926467
Validation loss: 2.5558459620132994

Epoch: 5| Step: 8
Training loss: 2.643892692881496
Validation loss: 2.528665864015883

Epoch: 5| Step: 9
Training loss: 2.3102700074832825
Validation loss: 2.5559602780390436

Epoch: 5| Step: 10
Training loss: 2.8845011409731476
Validation loss: 2.5260019938418576

Epoch: 166| Step: 0
Training loss: 3.0525066268811476
Validation loss: 2.5310068706228397

Epoch: 5| Step: 1
Training loss: 3.07951981752327
Validation loss: 2.516615267938201

Epoch: 5| Step: 2
Training loss: 3.2791745842947533
Validation loss: 2.53702504891778

Epoch: 5| Step: 3
Training loss: 3.3742903917094935
Validation loss: 2.513270369687322

Epoch: 5| Step: 4
Training loss: 2.9192778932938066
Validation loss: 2.5210567810261812

Epoch: 5| Step: 5
Training loss: 2.0495904280737562
Validation loss: 2.5245586061041925

Epoch: 5| Step: 6
Training loss: 2.3324073475449336
Validation loss: 2.518225164096696

Epoch: 5| Step: 7
Training loss: 2.086512797027769
Validation loss: 2.5275724467648706

Epoch: 5| Step: 8
Training loss: 2.4072051691493592
Validation loss: 2.5461143355757025

Epoch: 5| Step: 9
Training loss: 2.2078458379886334
Validation loss: 2.511540291434462

Epoch: 5| Step: 10
Training loss: 2.408496650077354
Validation loss: 2.516718181638919

Epoch: 167| Step: 0
Training loss: 2.670606434706443
Validation loss: 2.5333777027772837

Epoch: 5| Step: 1
Training loss: 2.9297770168615687
Validation loss: 2.5148243981394987

Epoch: 5| Step: 2
Training loss: 3.36798553568799
Validation loss: 2.551030590582845

Epoch: 5| Step: 3
Training loss: 2.7314744660693906
Validation loss: 2.5039405356471596

Epoch: 5| Step: 4
Training loss: 2.448921927880961
Validation loss: 2.5390923742362146

Epoch: 5| Step: 5
Training loss: 2.6955318638088093
Validation loss: 2.519767027895645

Epoch: 5| Step: 6
Training loss: 2.496134439772517
Validation loss: 2.539321898784556

Epoch: 5| Step: 7
Training loss: 2.401974731884135
Validation loss: 2.5384420020702723

Epoch: 5| Step: 8
Training loss: 2.9086331676565367
Validation loss: 2.540785759048569

Epoch: 5| Step: 9
Training loss: 2.2693281755840773
Validation loss: 2.5385306761351636

Epoch: 5| Step: 10
Training loss: 2.6864264362202253
Validation loss: 2.537990898376207

Epoch: 168| Step: 0
Training loss: 3.0333444497320468
Validation loss: 2.5237420864452793

Epoch: 5| Step: 1
Training loss: 2.14658045184917
Validation loss: 2.5276380093347215

Epoch: 5| Step: 2
Training loss: 2.7236466433070308
Validation loss: 2.54041261531327

Epoch: 5| Step: 3
Training loss: 2.9263180493667815
Validation loss: 2.5304353598957627

Epoch: 5| Step: 4
Training loss: 2.8018627339898092
Validation loss: 2.53733569925214

Epoch: 5| Step: 5
Training loss: 2.3682570858018526
Validation loss: 2.5219048446818952

Epoch: 5| Step: 6
Training loss: 2.6048576862318678
Validation loss: 2.5402628429164764

Epoch: 5| Step: 7
Training loss: 2.870557254828245
Validation loss: 2.532335341132484

Epoch: 5| Step: 8
Training loss: 2.750004594972413
Validation loss: 2.5032253420044603

Epoch: 5| Step: 9
Training loss: 2.9599707643250768
Validation loss: 2.541302976874879

Epoch: 5| Step: 10
Training loss: 2.605647731507083
Validation loss: 2.5066753716826025

Epoch: 169| Step: 0
Training loss: 2.468863375993846
Validation loss: 2.5007685413169596

Epoch: 5| Step: 1
Training loss: 2.5897396291110297
Validation loss: 2.5453294938773365

Epoch: 5| Step: 2
Training loss: 3.0332836133074936
Validation loss: 2.5189427050800894

Epoch: 5| Step: 3
Training loss: 2.4800286811277625
Validation loss: 2.5348924372255834

Epoch: 5| Step: 4
Training loss: 2.5563180368784644
Validation loss: 2.546251510701907

Epoch: 5| Step: 5
Training loss: 2.199818612337176
Validation loss: 2.523806880024366

Epoch: 5| Step: 6
Training loss: 3.278402928006534
Validation loss: 2.524688974217109

Epoch: 5| Step: 7
Training loss: 2.499591603300142
Validation loss: 2.5092891025748396

Epoch: 5| Step: 8
Training loss: 3.305534669042542
Validation loss: 2.505512383767874

Epoch: 5| Step: 9
Training loss: 2.3702463461408483
Validation loss: 2.5131613432232003

Epoch: 5| Step: 10
Training loss: 2.718414198208441
Validation loss: 2.5498872014994864

Epoch: 170| Step: 0
Training loss: 2.5967279826389813
Validation loss: 2.521524867027228

Epoch: 5| Step: 1
Training loss: 2.3613376159729653
Validation loss: 2.5252533471120753

Epoch: 5| Step: 2
Training loss: 2.0887313591737686
Validation loss: 2.5286210289562066

Epoch: 5| Step: 3
Training loss: 3.17319164291045
Validation loss: 2.5307846492329102

Epoch: 5| Step: 4
Training loss: 3.455737062438975
Validation loss: 2.526818861721711

Epoch: 5| Step: 5
Training loss: 2.782036487946136
Validation loss: 2.5312148405322445

Epoch: 5| Step: 6
Training loss: 2.8167762460404213
Validation loss: 2.540900561946805

Epoch: 5| Step: 7
Training loss: 2.6856275733614057
Validation loss: 2.507766116491556

Epoch: 5| Step: 8
Training loss: 2.1116144795983045
Validation loss: 2.551065617132284

Epoch: 5| Step: 9
Training loss: 2.8156715313012235
Validation loss: 2.5224553568521126

Epoch: 5| Step: 10
Training loss: 2.4982018679926945
Validation loss: 2.5294696041435745

Epoch: 171| Step: 0
Training loss: 1.9731243522384396
Validation loss: 2.515464252239646

Epoch: 5| Step: 1
Training loss: 2.6264010732085596
Validation loss: 2.5373635367351133

Epoch: 5| Step: 2
Training loss: 3.2921454326909365
Validation loss: 2.530460036396686

Epoch: 5| Step: 3
Training loss: 2.7617878129624884
Validation loss: 2.5538372446113176

Epoch: 5| Step: 4
Training loss: 2.3700346993268226
Validation loss: 2.5357009462058637

Epoch: 5| Step: 5
Training loss: 2.673184793234792
Validation loss: 2.538638693855146

Epoch: 5| Step: 6
Training loss: 2.7820366593447305
Validation loss: 2.5316806139806816

Epoch: 5| Step: 7
Training loss: 2.13206742795203
Validation loss: 2.5143722862661786

Epoch: 5| Step: 8
Training loss: 2.86946369011641
Validation loss: 2.541213786181987

Epoch: 5| Step: 9
Training loss: 3.2897468634157865
Validation loss: 2.5220231214869977

Epoch: 5| Step: 10
Training loss: 2.752631575569152
Validation loss: 2.5073651868844125

Epoch: 172| Step: 0
Training loss: 2.7096797432940782
Validation loss: 2.5393539395232994

Epoch: 5| Step: 1
Training loss: 2.4223857710197434
Validation loss: 2.526471956625153

Epoch: 5| Step: 2
Training loss: 2.5832640320701263
Validation loss: 2.537927913425457

Epoch: 5| Step: 3
Training loss: 2.979420648876909
Validation loss: 2.5318944817757494

Epoch: 5| Step: 4
Training loss: 2.8582462938882434
Validation loss: 2.522696424303386

Epoch: 5| Step: 5
Training loss: 2.7051812438039367
Validation loss: 2.5113705471362504

Epoch: 5| Step: 6
Training loss: 2.316938213744556
Validation loss: 2.5150357300613666

Epoch: 5| Step: 7
Training loss: 3.1149589966942464
Validation loss: 2.533624324199434

Epoch: 5| Step: 8
Training loss: 3.296137433464455
Validation loss: 2.5228095449713783

Epoch: 5| Step: 9
Training loss: 2.4004200249928593
Validation loss: 2.5376974233014957

Epoch: 5| Step: 10
Training loss: 1.9623439653895993
Validation loss: 2.5073064989311202

Epoch: 173| Step: 0
Training loss: 2.6874490777447657
Validation loss: 2.5355574255367777

Epoch: 5| Step: 1
Training loss: 2.1729824618225586
Validation loss: 2.5060527992442343

Epoch: 5| Step: 2
Training loss: 2.6727760089625345
Validation loss: 2.516150561620013

Epoch: 5| Step: 3
Training loss: 2.251468814804159
Validation loss: 2.5167144090869193

Epoch: 5| Step: 4
Training loss: 2.767839568825054
Validation loss: 2.5478273794461868

Epoch: 5| Step: 5
Training loss: 2.4750465542817204
Validation loss: 2.536195796544083

Epoch: 5| Step: 6
Training loss: 3.2210498482069148
Validation loss: 2.5151967146668768

Epoch: 5| Step: 7
Training loss: 3.5897119807154705
Validation loss: 2.511022563477475

Epoch: 5| Step: 8
Training loss: 2.96552720064389
Validation loss: 2.514761353890673

Epoch: 5| Step: 9
Training loss: 2.111564799408157
Validation loss: 2.532763227457312

Epoch: 5| Step: 10
Training loss: 2.524615317019978
Validation loss: 2.53471661758721

Epoch: 174| Step: 0
Training loss: 2.9282784697579145
Validation loss: 2.5448019928617516

Epoch: 5| Step: 1
Training loss: 2.708078509349216
Validation loss: 2.508924674462394

Epoch: 5| Step: 2
Training loss: 2.6500135025544123
Validation loss: 2.5413424404271936

Epoch: 5| Step: 3
Training loss: 2.8144603255296943
Validation loss: 2.536122338088332

Epoch: 5| Step: 4
Training loss: 3.5858723420516814
Validation loss: 2.560496968287608

Epoch: 5| Step: 5
Training loss: 2.6033190962841335
Validation loss: 2.5279329098242402

Epoch: 5| Step: 6
Training loss: 2.6032537971131124
Validation loss: 2.5159054839730604

Epoch: 5| Step: 7
Training loss: 1.7610558321981908
Validation loss: 2.538438473383543

Epoch: 5| Step: 8
Training loss: 2.7922320481361194
Validation loss: 2.5304996579023564

Epoch: 5| Step: 9
Training loss: 1.9679775236327166
Validation loss: 2.5349393245830845

Epoch: 5| Step: 10
Training loss: 2.5406402850189806
Validation loss: 2.5195721300463276

Epoch: 175| Step: 0
Training loss: 2.7878978017741622
Validation loss: 2.5173100106033113

Epoch: 5| Step: 1
Training loss: 3.0724681634704614
Validation loss: 2.519090075293189

Epoch: 5| Step: 2
Training loss: 2.6249844686866144
Validation loss: 2.5475746541083515

Epoch: 5| Step: 3
Training loss: 2.1677536316415473
Validation loss: 2.5307845266620212

Epoch: 5| Step: 4
Training loss: 2.831877689802298
Validation loss: 2.5457537450430974

Epoch: 5| Step: 5
Training loss: 2.340042029435353
Validation loss: 2.513287268151698

Epoch: 5| Step: 6
Training loss: 2.89643910244668
Validation loss: 2.511433072197736

Epoch: 5| Step: 7
Training loss: 3.2413515782796707
Validation loss: 2.513447040824229

Epoch: 5| Step: 8
Training loss: 2.717253174297975
Validation loss: 2.5293701698451727

Epoch: 5| Step: 9
Training loss: 2.3481375761002474
Validation loss: 2.531730175872573

Epoch: 5| Step: 10
Training loss: 2.233207557704517
Validation loss: 2.5597264035757554

Epoch: 176| Step: 0
Training loss: 2.719971568716343
Validation loss: 2.5487246108134736

Epoch: 5| Step: 1
Training loss: 2.53381998967602
Validation loss: 2.522452230626921

Epoch: 5| Step: 2
Training loss: 3.087358442142498
Validation loss: 2.538759303263002

Epoch: 5| Step: 3
Training loss: 2.500794665876522
Validation loss: 2.501230505142348

Epoch: 5| Step: 4
Training loss: 2.777020699857627
Validation loss: 2.52728763488926

Epoch: 5| Step: 5
Training loss: 2.6520855740050515
Validation loss: 2.528147968249911

Epoch: 5| Step: 6
Training loss: 2.379642065845565
Validation loss: 2.540563264856137

Epoch: 5| Step: 7
Training loss: 2.8052642994220376
Validation loss: 2.5285698878531466

Epoch: 5| Step: 8
Training loss: 1.8299419144755973
Validation loss: 2.543158125028746

Epoch: 5| Step: 9
Training loss: 3.119171046883187
Validation loss: 2.5170961395621294

Epoch: 5| Step: 10
Training loss: 2.8532743149592625
Validation loss: 2.519124403615764

Epoch: 177| Step: 0
Training loss: 2.314748959204999
Validation loss: 2.5280060342972

Epoch: 5| Step: 1
Training loss: 2.8537001031230407
Validation loss: 2.526641659426865

Epoch: 5| Step: 2
Training loss: 2.619200521604921
Validation loss: 2.5492082250095476

Epoch: 5| Step: 3
Training loss: 2.7311925188034745
Validation loss: 2.523267972354083

Epoch: 5| Step: 4
Training loss: 3.2988424351654535
Validation loss: 2.529948463182542

Epoch: 5| Step: 5
Training loss: 2.8463026055461613
Validation loss: 2.5340089699571147

Epoch: 5| Step: 6
Training loss: 2.805492742441703
Validation loss: 2.536201134685549

Epoch: 5| Step: 7
Training loss: 2.7967238571842423
Validation loss: 2.536224379881247

Epoch: 5| Step: 8
Training loss: 1.8548018049899746
Validation loss: 2.5311042625192965

Epoch: 5| Step: 9
Training loss: 2.6112409304638917
Validation loss: 2.514190946850283

Epoch: 5| Step: 10
Training loss: 2.578598903680212
Validation loss: 2.537447065537471

Epoch: 178| Step: 0
Training loss: 2.879336031299601
Validation loss: 2.5193380037683037

Epoch: 5| Step: 1
Training loss: 2.373762109292688
Validation loss: 2.537423367295105

Epoch: 5| Step: 2
Training loss: 3.0722709020337704
Validation loss: 2.515912995833371

Epoch: 5| Step: 3
Training loss: 3.178010465495485
Validation loss: 2.527445545230064

Epoch: 5| Step: 4
Training loss: 2.839980257261184
Validation loss: 2.5268236926034975

Epoch: 5| Step: 5
Training loss: 2.8974795339287787
Validation loss: 2.551353022284796

Epoch: 5| Step: 6
Training loss: 2.516992517138131
Validation loss: 2.533813552793169

Epoch: 5| Step: 7
Training loss: 2.4853477253162914
Validation loss: 2.5255980656618235

Epoch: 5| Step: 8
Training loss: 2.291748762105227
Validation loss: 2.544348230277463

Epoch: 5| Step: 9
Training loss: 2.571126976946022
Validation loss: 2.4992828258246376

Epoch: 5| Step: 10
Training loss: 2.369760757635968
Validation loss: 2.531708458456927

Epoch: 179| Step: 0
Training loss: 3.183904976639308
Validation loss: 2.529082353187387

Epoch: 5| Step: 1
Training loss: 2.156494679597217
Validation loss: 2.5383888733044806

Epoch: 5| Step: 2
Training loss: 2.513993391624633
Validation loss: 2.540396265119215

Epoch: 5| Step: 3
Training loss: 2.3738476818811396
Validation loss: 2.513746445391418

Epoch: 5| Step: 4
Training loss: 2.2098214395806557
Validation loss: 2.55603397075435

Epoch: 5| Step: 5
Training loss: 2.7360339554581414
Validation loss: 2.518223195216789

Epoch: 5| Step: 6
Training loss: 2.377544946897497
Validation loss: 2.4921096932082976

Epoch: 5| Step: 7
Training loss: 3.396523543971603
Validation loss: 2.5411972575353228

Epoch: 5| Step: 8
Training loss: 2.6967877317522406
Validation loss: 2.5113242058648764

Epoch: 5| Step: 9
Training loss: 2.7962108654682316
Validation loss: 2.5372870172751734

Epoch: 5| Step: 10
Training loss: 2.969774892477089
Validation loss: 2.5163717290891294

Epoch: 180| Step: 0
Training loss: 2.6873248730753985
Validation loss: 2.5446828505888326

Epoch: 5| Step: 1
Training loss: 2.8396973290141605
Validation loss: 2.51332748933458

Epoch: 5| Step: 2
Training loss: 2.6690882376888267
Validation loss: 2.5355730339706866

Epoch: 5| Step: 3
Training loss: 2.247983876927344
Validation loss: 2.5283344715357114

Epoch: 5| Step: 4
Training loss: 2.703220696491807
Validation loss: 2.519548187364274

Epoch: 5| Step: 5
Training loss: 2.1117428523137223
Validation loss: 2.547788317201687

Epoch: 5| Step: 6
Training loss: 2.6573615048655883
Validation loss: 2.5070543783679287

Epoch: 5| Step: 7
Training loss: 3.5785720745789855
Validation loss: 2.523623125264026

Epoch: 5| Step: 8
Training loss: 2.5781679092073126
Validation loss: 2.5542528341449846

Epoch: 5| Step: 9
Training loss: 2.5180364388396406
Validation loss: 2.537933513587671

Epoch: 5| Step: 10
Training loss: 2.5866128552466536
Validation loss: 2.5325360658774256

Epoch: 181| Step: 0
Training loss: 2.260713080266185
Validation loss: 2.5267176190662965

Epoch: 5| Step: 1
Training loss: 2.899416529396475
Validation loss: 2.5121791206352175

Epoch: 5| Step: 2
Training loss: 2.7377944927235145
Validation loss: 2.5084276635461644

Epoch: 5| Step: 3
Training loss: 2.8298071816473174
Validation loss: 2.5355904546388426

Epoch: 5| Step: 4
Training loss: 2.9674927307914007
Validation loss: 2.5341387891767835

Epoch: 5| Step: 5
Training loss: 3.332876905345669
Validation loss: 2.515260195447614

Epoch: 5| Step: 6
Training loss: 2.019713284757149
Validation loss: 2.523739901438569

Epoch: 5| Step: 7
Training loss: 2.3330512443961173
Validation loss: 2.5534164499532594

Epoch: 5| Step: 8
Training loss: 2.638819602145944
Validation loss: 2.5358354252912663

Epoch: 5| Step: 9
Training loss: 2.418243737676331
Validation loss: 2.5342417135430173

Epoch: 5| Step: 10
Training loss: 2.6824426306058626
Validation loss: 2.53606952969546

Epoch: 182| Step: 0
Training loss: 2.82099817452681
Validation loss: 2.513807870883215

Epoch: 5| Step: 1
Training loss: 2.53629863844663
Validation loss: 2.4947297093290217

Epoch: 5| Step: 2
Training loss: 3.133625181958726
Validation loss: 2.5262265686488408

Epoch: 5| Step: 3
Training loss: 2.6714102357565896
Validation loss: 2.5225628779400657

Epoch: 5| Step: 4
Training loss: 3.0746111492141686
Validation loss: 2.5231134416195355

Epoch: 5| Step: 5
Training loss: 3.320455678208897
Validation loss: 2.5191016270111755

Epoch: 5| Step: 6
Training loss: 2.374871099889185
Validation loss: 2.5400240280446296

Epoch: 5| Step: 7
Training loss: 2.4412505809746134
Validation loss: 2.5057584052801642

Epoch: 5| Step: 8
Training loss: 2.2423186047866426
Validation loss: 2.5299303206839174

Epoch: 5| Step: 9
Training loss: 1.9195163799202086
Validation loss: 2.5296413506171738

Epoch: 5| Step: 10
Training loss: 2.4909764519324606
Validation loss: 2.5224226014218627

Epoch: 183| Step: 0
Training loss: 1.9279747902895896
Validation loss: 2.527855581589964

Epoch: 5| Step: 1
Training loss: 2.914623707584141
Validation loss: 2.5280056479266206

Epoch: 5| Step: 2
Training loss: 2.1098173454738225
Validation loss: 2.530372234493701

Epoch: 5| Step: 3
Training loss: 3.1625840488769525
Validation loss: 2.54609839454547

Epoch: 5| Step: 4
Training loss: 2.492007736371616
Validation loss: 2.514846871982585

Epoch: 5| Step: 5
Training loss: 3.006268627719705
Validation loss: 2.540364324314949

Epoch: 5| Step: 6
Training loss: 2.877103284968743
Validation loss: 2.529223239485789

Epoch: 5| Step: 7
Training loss: 2.3674227657429108
Validation loss: 2.5272614037922083

Epoch: 5| Step: 8
Training loss: 2.6797556520920773
Validation loss: 2.5478470124730377

Epoch: 5| Step: 9
Training loss: 2.238522608128417
Validation loss: 2.5230262385913975

Epoch: 5| Step: 10
Training loss: 3.2473741340473063
Validation loss: 2.521601158720311

Epoch: 184| Step: 0
Training loss: 2.3287800117069937
Validation loss: 2.536807559730646

Epoch: 5| Step: 1
Training loss: 2.51275072507399
Validation loss: 2.5192938218741054

Epoch: 5| Step: 2
Training loss: 2.6734214902629194
Validation loss: 2.513631273297382

Epoch: 5| Step: 3
Training loss: 2.5057336860467445
Validation loss: 2.5208655483865776

Epoch: 5| Step: 4
Training loss: 3.2762920888375313
Validation loss: 2.513765406343913

Epoch: 5| Step: 5
Training loss: 3.2266254557455283
Validation loss: 2.5215674171928844

Epoch: 5| Step: 6
Training loss: 2.611319177422498
Validation loss: 2.526269964099769

Epoch: 5| Step: 7
Training loss: 2.249200784943396
Validation loss: 2.5283936417380932

Epoch: 5| Step: 8
Training loss: 2.4039263714830614
Validation loss: 2.5446483592982574

Epoch: 5| Step: 9
Training loss: 2.84894598493858
Validation loss: 2.5259920538870015

Epoch: 5| Step: 10
Training loss: 2.6368767132949054
Validation loss: 2.518131563227484

Epoch: 185| Step: 0
Training loss: 2.2304873490768986
Validation loss: 2.5206085778802736

Epoch: 5| Step: 1
Training loss: 2.817603842066453
Validation loss: 2.539904704038647

Epoch: 5| Step: 2
Training loss: 2.751476238399764
Validation loss: 2.5281694526028993

Epoch: 5| Step: 3
Training loss: 3.11716378951078
Validation loss: 2.521628368775929

Epoch: 5| Step: 4
Training loss: 2.740305110483367
Validation loss: 2.519139362321958

Epoch: 5| Step: 5
Training loss: 2.7289210395838137
Validation loss: 2.5034436555313437

Epoch: 5| Step: 6
Training loss: 2.539818378322935
Validation loss: 2.539082763187147

Epoch: 5| Step: 7
Training loss: 2.538244121417337
Validation loss: 2.507957712157428

Epoch: 5| Step: 8
Training loss: 2.139943146529748
Validation loss: 2.548441439599948

Epoch: 5| Step: 9
Training loss: 2.3886519124779095
Validation loss: 2.5391136195840076

Epoch: 5| Step: 10
Training loss: 3.142813152773265
Validation loss: 2.5102910448771563

Epoch: 186| Step: 0
Training loss: 2.105988447210173
Validation loss: 2.5210041197447874

Epoch: 5| Step: 1
Training loss: 2.87900529584964
Validation loss: 2.526697836571178

Epoch: 5| Step: 2
Training loss: 2.102214612790226
Validation loss: 2.5341909084245127

Epoch: 5| Step: 3
Training loss: 2.7896323596918626
Validation loss: 2.5079556391299076

Epoch: 5| Step: 4
Training loss: 3.5358457114273993
Validation loss: 2.521045977550165

Epoch: 5| Step: 5
Training loss: 2.3221578228106132
Validation loss: 2.5157819930906986

Epoch: 5| Step: 6
Training loss: 2.5852929548061687
Validation loss: 2.5154881502233493

Epoch: 5| Step: 7
Training loss: 2.0006196730976877
Validation loss: 2.5405857087936443

Epoch: 5| Step: 8
Training loss: 2.9966131802015776
Validation loss: 2.5174343929484797

Epoch: 5| Step: 9
Training loss: 3.16323291593723
Validation loss: 2.5179367434354187

Epoch: 5| Step: 10
Training loss: 2.607438059553001
Validation loss: 2.5316797426175506

Epoch: 187| Step: 0
Training loss: 3.040397448910071
Validation loss: 2.528903422934958

Epoch: 5| Step: 1
Training loss: 2.494293853426254
Validation loss: 2.4959958208902746

Epoch: 5| Step: 2
Training loss: 2.5053442100639343
Validation loss: 2.544895710825311

Epoch: 5| Step: 3
Training loss: 2.543275782366057
Validation loss: 2.5124885452295906

Epoch: 5| Step: 4
Training loss: 3.3901247279534634
Validation loss: 2.52166890017338

Epoch: 5| Step: 5
Training loss: 2.197857993067439
Validation loss: 2.5275128323488785

Epoch: 5| Step: 6
Training loss: 2.7570297383893116
Validation loss: 2.5278887189295074

Epoch: 5| Step: 7
Training loss: 2.2875441374976258
Validation loss: 2.5426122707492707

Epoch: 5| Step: 8
Training loss: 2.762949711024769
Validation loss: 2.505407777786937

Epoch: 5| Step: 9
Training loss: 2.6177497829116207
Validation loss: 2.4932759447941817

Epoch: 5| Step: 10
Training loss: 2.518643386894115
Validation loss: 2.5397171123233377

Epoch: 188| Step: 0
Training loss: 2.825598330980458
Validation loss: 2.5019645703784583

Epoch: 5| Step: 1
Training loss: 2.5464508077834886
Validation loss: 2.5156317769191605

Epoch: 5| Step: 2
Training loss: 2.7207667445339045
Validation loss: 2.5384139279695423

Epoch: 5| Step: 3
Training loss: 3.066435951040474
Validation loss: 2.493730588155778

Epoch: 5| Step: 4
Training loss: 2.9699887552273254
Validation loss: 2.533987228568566

Epoch: 5| Step: 5
Training loss: 2.7694943187494885
Validation loss: 2.5250258525742058

Epoch: 5| Step: 6
Training loss: 2.4377044200765288
Validation loss: 2.5398986994369213

Epoch: 5| Step: 7
Training loss: 2.4854766514444444
Validation loss: 2.551546670131526

Epoch: 5| Step: 8
Training loss: 2.801007760437222
Validation loss: 2.5397478772324176

Epoch: 5| Step: 9
Training loss: 2.486397166166323
Validation loss: 2.53647644261557

Epoch: 5| Step: 10
Training loss: 2.2264439601463404
Validation loss: 2.5532686532574225

Epoch: 189| Step: 0
Training loss: 3.0930381495395163
Validation loss: 2.541372201108783

Epoch: 5| Step: 1
Training loss: 2.6432981785877456
Validation loss: 2.539225119407061

Epoch: 5| Step: 2
Training loss: 2.3051436667860696
Validation loss: 2.517833432555346

Epoch: 5| Step: 3
Training loss: 3.1049340513538826
Validation loss: 2.561173730840885

Epoch: 5| Step: 4
Training loss: 3.3987551485887586
Validation loss: 2.5378637026016233

Epoch: 5| Step: 5
Training loss: 2.3810983965015016
Validation loss: 2.5319366222648045

Epoch: 5| Step: 6
Training loss: 2.7647466611779072
Validation loss: 2.5299516632366634

Epoch: 5| Step: 7
Training loss: 2.790177408272637
Validation loss: 2.553599557675251

Epoch: 5| Step: 8
Training loss: 2.512351708282425
Validation loss: 2.554709960341028

Epoch: 5| Step: 9
Training loss: 1.913016270785175
Validation loss: 2.5291108714595985

Epoch: 5| Step: 10
Training loss: 2.082410595315364
Validation loss: 2.523105368991053

Epoch: 190| Step: 0
Training loss: 2.504740416883402
Validation loss: 2.5525254044790113

Epoch: 5| Step: 1
Training loss: 2.981460026474501
Validation loss: 2.529812231158707

Epoch: 5| Step: 2
Training loss: 2.9260297804677387
Validation loss: 2.515932013823285

Epoch: 5| Step: 3
Training loss: 1.9249104664776173
Validation loss: 2.535859773253298

Epoch: 5| Step: 4
Training loss: 2.9008400719737972
Validation loss: 2.5376922044834367

Epoch: 5| Step: 5
Training loss: 2.875703476794886
Validation loss: 2.5330648849811475

Epoch: 5| Step: 6
Training loss: 3.011978078876378
Validation loss: 2.5269063092459905

Epoch: 5| Step: 7
Training loss: 2.3557470862576952
Validation loss: 2.525307050693039

Epoch: 5| Step: 8
Training loss: 2.7662966499928596
Validation loss: 2.508359374908425

Epoch: 5| Step: 9
Training loss: 2.5199331037028445
Validation loss: 2.5263052086235485

Epoch: 5| Step: 10
Training loss: 2.338835688560074
Validation loss: 2.522730317389879

Epoch: 191| Step: 0
Training loss: 2.826427814570718
Validation loss: 2.5512507621089258

Epoch: 5| Step: 1
Training loss: 2.430633851258613
Validation loss: 2.5343928806165836

Epoch: 5| Step: 2
Training loss: 2.4122727558850805
Validation loss: 2.5003492808662684

Epoch: 5| Step: 3
Training loss: 2.56474568658554
Validation loss: 2.505627253768586

Epoch: 5| Step: 4
Training loss: 2.766627846487614
Validation loss: 2.5575533447020606

Epoch: 5| Step: 5
Training loss: 2.723922019312844
Validation loss: 2.516945693200073

Epoch: 5| Step: 6
Training loss: 2.593528921977644
Validation loss: 2.534117215824333

Epoch: 5| Step: 7
Training loss: 3.457817789893364
Validation loss: 2.5184945491604167

Epoch: 5| Step: 8
Training loss: 2.4774555317018425
Validation loss: 2.522203272687696

Epoch: 5| Step: 9
Training loss: 2.663973004132746
Validation loss: 2.5048324957626975

Epoch: 5| Step: 10
Training loss: 2.1606174411260763
Validation loss: 2.5035926333588234

Epoch: 192| Step: 0
Training loss: 2.4381985763717124
Validation loss: 2.522459647782982

Epoch: 5| Step: 1
Training loss: 2.4794972837740556
Validation loss: 2.5347563864671416

Epoch: 5| Step: 2
Training loss: 2.493284552023864
Validation loss: 2.541888852816348

Epoch: 5| Step: 3
Training loss: 2.080021845812884
Validation loss: 2.535926192140413

Epoch: 5| Step: 4
Training loss: 3.481766481285067
Validation loss: 2.5410436108318404

Epoch: 5| Step: 5
Training loss: 2.811324234080302
Validation loss: 2.5404635936991453

Epoch: 5| Step: 6
Training loss: 2.387590464530791
Validation loss: 2.530021104832816

Epoch: 5| Step: 7
Training loss: 2.8894053592526827
Validation loss: 2.516443006321792

Epoch: 5| Step: 8
Training loss: 2.4471869064055594
Validation loss: 2.5337236299950674

Epoch: 5| Step: 9
Training loss: 3.0866908445571686
Validation loss: 2.5377426500829263

Epoch: 5| Step: 10
Training loss: 2.509974798250772
Validation loss: 2.540852035042887

Epoch: 193| Step: 0
Training loss: 2.4476383348595285
Validation loss: 2.521703329110208

Epoch: 5| Step: 1
Training loss: 2.314319307345847
Validation loss: 2.5457571377119925

Epoch: 5| Step: 2
Training loss: 2.8508972228011986
Validation loss: 2.5271875476763226

Epoch: 5| Step: 3
Training loss: 2.3520345119098156
Validation loss: 2.5341877597523235

Epoch: 5| Step: 4
Training loss: 2.9953341758393455
Validation loss: 2.5333073513364197

Epoch: 5| Step: 5
Training loss: 2.14722821968341
Validation loss: 2.5450448580269325

Epoch: 5| Step: 6
Training loss: 2.609401588532903
Validation loss: 2.5499069734961624

Epoch: 5| Step: 7
Training loss: 2.5417379046186386
Validation loss: 2.4996342299130005

Epoch: 5| Step: 8
Training loss: 3.149693543800863
Validation loss: 2.516756587291999

Epoch: 5| Step: 9
Training loss: 2.864395819797883
Validation loss: 2.5047116242034337

Epoch: 5| Step: 10
Training loss: 2.6582291354463115
Validation loss: 2.5377848091283934

Epoch: 194| Step: 0
Training loss: 3.002508227537954
Validation loss: 2.5143088799316224

Epoch: 5| Step: 1
Training loss: 2.4984650668209296
Validation loss: 2.534753162642649

Epoch: 5| Step: 2
Training loss: 2.203843993694032
Validation loss: 2.508471295881811

Epoch: 5| Step: 3
Training loss: 2.451285772223074
Validation loss: 2.523919089068778

Epoch: 5| Step: 4
Training loss: 2.841291601612384
Validation loss: 2.5527519898768336

Epoch: 5| Step: 5
Training loss: 2.8581992478427134
Validation loss: 2.5279254985859296

Epoch: 5| Step: 6
Training loss: 2.2159766473675817
Validation loss: 2.5424902550066135

Epoch: 5| Step: 7
Training loss: 2.647458596054387
Validation loss: 2.5569231548783873

Epoch: 5| Step: 8
Training loss: 2.6245954292609297
Validation loss: 2.5434189304987447

Epoch: 5| Step: 9
Training loss: 2.570199424784077
Validation loss: 2.5252585571200865

Epoch: 5| Step: 10
Training loss: 3.3017530466635967
Validation loss: 2.5455187486690196

Epoch: 195| Step: 0
Training loss: 2.3452441729733557
Validation loss: 2.532909473728492

Epoch: 5| Step: 1
Training loss: 2.664178482171058
Validation loss: 2.531108625893221

Epoch: 5| Step: 2
Training loss: 2.5455555686782327
Validation loss: 2.5273189781521728

Epoch: 5| Step: 3
Training loss: 2.9574324261385554
Validation loss: 2.536870541170205

Epoch: 5| Step: 4
Training loss: 3.0588578345243596
Validation loss: 2.5636250477029465

Epoch: 5| Step: 5
Training loss: 3.043927297578629
Validation loss: 2.560398084123722

Epoch: 5| Step: 6
Training loss: 2.323688350391173
Validation loss: 2.5213655025163133

Epoch: 5| Step: 7
Training loss: 2.5453270262508005
Validation loss: 2.53083953203126

Epoch: 5| Step: 8
Training loss: 2.204745082522297
Validation loss: 2.5315198775967884

Epoch: 5| Step: 9
Training loss: 2.286043852224452
Validation loss: 2.519592505210642

Epoch: 5| Step: 10
Training loss: 3.079650965396006
Validation loss: 2.531078145928288

Epoch: 196| Step: 0
Training loss: 2.2610394605780897
Validation loss: 2.5551609236488115

Epoch: 5| Step: 1
Training loss: 3.3063824863245967
Validation loss: 2.5199765302140067

Epoch: 5| Step: 2
Training loss: 2.350450018141369
Validation loss: 2.5324860186677265

Epoch: 5| Step: 3
Training loss: 2.595658852319486
Validation loss: 2.5610885174266853

Epoch: 5| Step: 4
Training loss: 2.469455087145583
Validation loss: 2.5161338978789503

Epoch: 5| Step: 5
Training loss: 2.887513336547174
Validation loss: 2.522807218415276

Epoch: 5| Step: 6
Training loss: 2.4165304956767346
Validation loss: 2.5127320451984274

Epoch: 5| Step: 7
Training loss: 2.4479875162030718
Validation loss: 2.5205261184288195

Epoch: 5| Step: 8
Training loss: 2.9878676184318205
Validation loss: 2.5580807066664986

Epoch: 5| Step: 9
Training loss: 2.667954988011267
Validation loss: 2.5178214809763375

Epoch: 5| Step: 10
Training loss: 2.821982694917497
Validation loss: 2.5235209777428618

Epoch: 197| Step: 0
Training loss: 3.155241550200603
Validation loss: 2.545339354287656

Epoch: 5| Step: 1
Training loss: 2.771628722181347
Validation loss: 2.5164324499531734

Epoch: 5| Step: 2
Training loss: 3.0765176826959597
Validation loss: 2.5522503240488006

Epoch: 5| Step: 3
Training loss: 2.6250528602727363
Validation loss: 2.5478988664011144

Epoch: 5| Step: 4
Training loss: 2.550236548877518
Validation loss: 2.53417647862208

Epoch: 5| Step: 5
Training loss: 2.389146035669364
Validation loss: 2.504294359816708

Epoch: 5| Step: 6
Training loss: 2.8520253263100703
Validation loss: 2.545143908592564

Epoch: 5| Step: 7
Training loss: 2.2468753947335904
Validation loss: 2.5163947830590687

Epoch: 5| Step: 8
Training loss: 1.9156827958366507
Validation loss: 2.5431385334244423

Epoch: 5| Step: 9
Training loss: 2.7009009765358836
Validation loss: 2.5113360954694643

Epoch: 5| Step: 10
Training loss: 2.836641586796499
Validation loss: 2.5225714330054174

Epoch: 198| Step: 0
Training loss: 2.6410680094538086
Validation loss: 2.512770083273577

Epoch: 5| Step: 1
Training loss: 2.942440990805933
Validation loss: 2.5281639119348207

Epoch: 5| Step: 2
Training loss: 2.88002868055262
Validation loss: 2.5428440585193424

Epoch: 5| Step: 3
Training loss: 2.8300506611525424
Validation loss: 2.5480519910742476

Epoch: 5| Step: 4
Training loss: 2.5218425233397137
Validation loss: 2.5330986099870545

Epoch: 5| Step: 5
Training loss: 2.1510587480681185
Validation loss: 2.5086726112683237

Epoch: 5| Step: 6
Training loss: 2.0928464619565323
Validation loss: 2.506761475502781

Epoch: 5| Step: 7
Training loss: 2.405033348946725
Validation loss: 2.533892325071501

Epoch: 5| Step: 8
Training loss: 2.847518771061974
Validation loss: 2.55341168494526

Epoch: 5| Step: 9
Training loss: 2.7956051634356167
Validation loss: 2.5102602804827705

Epoch: 5| Step: 10
Training loss: 3.0842875645079877
Validation loss: 2.5468856435845644

Epoch: 199| Step: 0
Training loss: 2.578453644121851
Validation loss: 2.5266817050607195

Epoch: 5| Step: 1
Training loss: 2.988665786635251
Validation loss: 2.506481913657695

Epoch: 5| Step: 2
Training loss: 2.202987125492441
Validation loss: 2.511338411725711

Epoch: 5| Step: 3
Training loss: 2.6985040759171923
Validation loss: 2.5408448481473807

Epoch: 5| Step: 4
Training loss: 2.330907400693539
Validation loss: 2.525922757288839

Epoch: 5| Step: 5
Training loss: 2.1668660976864107
Validation loss: 2.5612641462501724

Epoch: 5| Step: 6
Training loss: 2.905173810003974
Validation loss: 2.527411800579789

Epoch: 5| Step: 7
Training loss: 2.9382959464369236
Validation loss: 2.5257539349101292

Epoch: 5| Step: 8
Training loss: 2.5862120514303113
Validation loss: 2.550106873585976

Epoch: 5| Step: 9
Training loss: 2.7094560692438083
Validation loss: 2.5445927990882513

Epoch: 5| Step: 10
Training loss: 3.021003157128178
Validation loss: 2.5392422348534516

Epoch: 200| Step: 0
Training loss: 2.132832509600219
Validation loss: 2.524844132986299

Epoch: 5| Step: 1
Training loss: 2.647747028558429
Validation loss: 2.5499426689716467

Epoch: 5| Step: 2
Training loss: 2.5700548037018125
Validation loss: 2.5223701017990567

Epoch: 5| Step: 3
Training loss: 2.438855405687236
Validation loss: 2.5184594406540866

Epoch: 5| Step: 4
Training loss: 3.0544836264087296
Validation loss: 2.516397894396651

Epoch: 5| Step: 5
Training loss: 2.1288168792164384
Validation loss: 2.5242571050707254

Epoch: 5| Step: 6
Training loss: 2.7583848397170243
Validation loss: 2.5391092517924907

Epoch: 5| Step: 7
Training loss: 3.3585448192963954
Validation loss: 2.5439003275053973

Epoch: 5| Step: 8
Training loss: 2.5257208446733155
Validation loss: 2.5309484462462506

Epoch: 5| Step: 9
Training loss: 2.4556236419712816
Validation loss: 2.5269076869872316

Epoch: 5| Step: 10
Training loss: 2.844209571005848
Validation loss: 2.52579592410252

Epoch: 201| Step: 0
Training loss: 2.6992637972792224
Validation loss: 2.5501237576634175

Epoch: 5| Step: 1
Training loss: 2.119046791917406
Validation loss: 2.5237194500497333

Epoch: 5| Step: 2
Training loss: 2.3263832079563396
Validation loss: 2.5074143894770384

Epoch: 5| Step: 3
Training loss: 2.6221142301664244
Validation loss: 2.5330524648196286

Epoch: 5| Step: 4
Training loss: 3.4159355001502183
Validation loss: 2.5445882170395584

Epoch: 5| Step: 5
Training loss: 2.8228472786759164
Validation loss: 2.5472011109509047

Epoch: 5| Step: 6
Training loss: 2.7019017268563554
Validation loss: 2.532059230604849

Epoch: 5| Step: 7
Training loss: 2.2364833461415174
Validation loss: 2.544704677402328

Epoch: 5| Step: 8
Training loss: 2.9439234202625104
Validation loss: 2.5331325845187447

Epoch: 5| Step: 9
Training loss: 2.7250956002360773
Validation loss: 2.5486540553297576

Epoch: 5| Step: 10
Training loss: 2.1682796221020024
Validation loss: 2.527832467848312

Epoch: 202| Step: 0
Training loss: 2.450928792260735
Validation loss: 2.567022043733874

Epoch: 5| Step: 1
Training loss: 2.171862375785309
Validation loss: 2.551777852308915

Epoch: 5| Step: 2
Training loss: 3.074250236844391
Validation loss: 2.5391727395180945

Epoch: 5| Step: 3
Training loss: 2.5005173147937314
Validation loss: 2.542607793016579

Epoch: 5| Step: 4
Training loss: 2.7239661329071434
Validation loss: 2.527115062972323

Epoch: 5| Step: 5
Training loss: 3.188032068462213
Validation loss: 2.551412671491449

Epoch: 5| Step: 6
Training loss: 2.5280343811618593
Validation loss: 2.5127832627614657

Epoch: 5| Step: 7
Training loss: 2.688648067370622
Validation loss: 2.5430274032790505

Epoch: 5| Step: 8
Training loss: 2.246382772674962
Validation loss: 2.5140673488062695

Epoch: 5| Step: 9
Training loss: 2.9588697958942323
Validation loss: 2.5117458976832845

Epoch: 5| Step: 10
Training loss: 2.415347353041783
Validation loss: 2.5190038087141566

Epoch: 203| Step: 0
Training loss: 1.8874272818799578
Validation loss: 2.5236108800920416

Epoch: 5| Step: 1
Training loss: 3.2524044239213494
Validation loss: 2.5595633053651117

Epoch: 5| Step: 2
Training loss: 2.4730049839548007
Validation loss: 2.509256108865629

Epoch: 5| Step: 3
Training loss: 2.7930490762323585
Validation loss: 2.5624580659994276

Epoch: 5| Step: 4
Training loss: 2.525923222127797
Validation loss: 2.5284358106635882

Epoch: 5| Step: 5
Training loss: 2.564517366835051
Validation loss: 2.523638060351883

Epoch: 5| Step: 6
Training loss: 2.9461866139070083
Validation loss: 2.5196735056700548

Epoch: 5| Step: 7
Training loss: 2.572012203074883
Validation loss: 2.553087157537093

Epoch: 5| Step: 8
Training loss: 2.247502636425267
Validation loss: 2.533931162443528

Epoch: 5| Step: 9
Training loss: 2.3243929244743007
Validation loss: 2.5186574730977154

Epoch: 5| Step: 10
Training loss: 3.536908637448211
Validation loss: 2.5324416936249055

Epoch: 204| Step: 0
Training loss: 2.453134330197225
Validation loss: 2.5234274199036277

Epoch: 5| Step: 1
Training loss: 2.516043397216842
Validation loss: 2.5350719535031416

Epoch: 5| Step: 2
Training loss: 3.056300213173109
Validation loss: 2.5294564538279283

Epoch: 5| Step: 3
Training loss: 2.9809816251672805
Validation loss: 2.5217954534709994

Epoch: 5| Step: 4
Training loss: 3.1638282971771012
Validation loss: 2.531350167623363

Epoch: 5| Step: 5
Training loss: 1.6248806396076612
Validation loss: 2.504465084269611

Epoch: 5| Step: 6
Training loss: 2.5760006562107742
Validation loss: 2.5308850591342416

Epoch: 5| Step: 7
Training loss: 3.1129355436098307
Validation loss: 2.525151622902012

Epoch: 5| Step: 8
Training loss: 2.100708288001763
Validation loss: 2.5307273573315046

Epoch: 5| Step: 9
Training loss: 2.398724069778212
Validation loss: 2.5353595204615775

Epoch: 5| Step: 10
Training loss: 2.9181931088968374
Validation loss: 2.5012771697362193

Epoch: 205| Step: 0
Training loss: 2.1264663573652767
Validation loss: 2.527715906098692

Epoch: 5| Step: 1
Training loss: 2.812243471102614
Validation loss: 2.5364443827101395

Epoch: 5| Step: 2
Training loss: 2.94519107477239
Validation loss: 2.5165769854287188

Epoch: 5| Step: 3
Training loss: 2.5050959148813297
Validation loss: 2.5245940852286393

Epoch: 5| Step: 4
Training loss: 2.6098600382200385
Validation loss: 2.521015182711746

Epoch: 5| Step: 5
Training loss: 2.986407800962538
Validation loss: 2.518190839295318

Epoch: 5| Step: 6
Training loss: 2.367348844865009
Validation loss: 2.5184842558734646

Epoch: 5| Step: 7
Training loss: 2.8471756308447107
Validation loss: 2.529742197070452

Epoch: 5| Step: 8
Training loss: 2.734690097637161
Validation loss: 2.5281325690067704

Epoch: 5| Step: 9
Training loss: 2.691899978241927
Validation loss: 2.5162436911023867

Epoch: 5| Step: 10
Training loss: 2.4513038630114172
Validation loss: 2.5477690328842297

Epoch: 206| Step: 0
Training loss: 2.365238810250796
Validation loss: 2.521994469700811

Epoch: 5| Step: 1
Training loss: 2.709149565332281
Validation loss: 2.5240357377929605

Epoch: 5| Step: 2
Training loss: 3.1563886857487087
Validation loss: 2.5246955978472276

Epoch: 5| Step: 3
Training loss: 2.4579202229440194
Validation loss: 2.5331077792085086

Epoch: 5| Step: 4
Training loss: 2.599694758617284
Validation loss: 2.53558854372984

Epoch: 5| Step: 5
Training loss: 2.497614294421036
Validation loss: 2.494719269689164

Epoch: 5| Step: 6
Training loss: 2.745397531042561
Validation loss: 2.516931495560568

Epoch: 5| Step: 7
Training loss: 3.1355218373315172
Validation loss: 2.5272165387441

Epoch: 5| Step: 8
Training loss: 1.8791260144671167
Validation loss: 2.5570363880814666

Epoch: 5| Step: 9
Training loss: 2.774697976387422
Validation loss: 2.5430545977952885

Epoch: 5| Step: 10
Training loss: 2.4638274163518714
Validation loss: 2.550453063790478

Epoch: 207| Step: 0
Training loss: 2.3860416731860363
Validation loss: 2.5242306231089304

Epoch: 5| Step: 1
Training loss: 2.8125069512175385
Validation loss: 2.553980555628634

Epoch: 5| Step: 2
Training loss: 2.365568003604059
Validation loss: 2.530330284768299

Epoch: 5| Step: 3
Training loss: 3.0183916593997027
Validation loss: 2.5381467681459635

Epoch: 5| Step: 4
Training loss: 2.7512975579275865
Validation loss: 2.516379584924519

Epoch: 5| Step: 5
Training loss: 2.318174771010895
Validation loss: 2.526758187535569

Epoch: 5| Step: 6
Training loss: 3.0668263918979815
Validation loss: 2.5405580024798975

Epoch: 5| Step: 7
Training loss: 2.6106821772017534
Validation loss: 2.522156950541968

Epoch: 5| Step: 8
Training loss: 2.3744757224195543
Validation loss: 2.5318846925346272

Epoch: 5| Step: 9
Training loss: 2.516601467761688
Validation loss: 2.498686515148843

Epoch: 5| Step: 10
Training loss: 2.9108181399139905
Validation loss: 2.539653961228655

Epoch: 208| Step: 0
Training loss: 2.5013233497002862
Validation loss: 2.5366792565987195

Epoch: 5| Step: 1
Training loss: 2.7915397349485955
Validation loss: 2.52746377048083

Epoch: 5| Step: 2
Training loss: 3.2302337524109372
Validation loss: 2.536564788198188

Epoch: 5| Step: 3
Training loss: 2.5765092815318607
Validation loss: 2.537531661486687

Epoch: 5| Step: 4
Training loss: 2.265262864208837
Validation loss: 2.5049713686490964

Epoch: 5| Step: 5
Training loss: 3.1027171870589645
Validation loss: 2.5447982987130278

Epoch: 5| Step: 6
Training loss: 2.775751658184467
Validation loss: 2.525357687352382

Epoch: 5| Step: 7
Training loss: 2.2376215896388665
Validation loss: 2.5051117412522705

Epoch: 5| Step: 8
Training loss: 1.9910083946499393
Validation loss: 2.501560340548324

Epoch: 5| Step: 9
Training loss: 3.0505603900828584
Validation loss: 2.5111125161551833

Epoch: 5| Step: 10
Training loss: 2.24669383118429
Validation loss: 2.5238591618442863

Epoch: 209| Step: 0
Training loss: 3.100631612606491
Validation loss: 2.531964983849775

Epoch: 5| Step: 1
Training loss: 2.1323269581324
Validation loss: 2.527416759655618

Epoch: 5| Step: 2
Training loss: 3.107388422490426
Validation loss: 2.511194960077281

Epoch: 5| Step: 3
Training loss: 2.608146698178373
Validation loss: 2.500284005512356

Epoch: 5| Step: 4
Training loss: 2.833979832062606
Validation loss: 2.506016223368336

Epoch: 5| Step: 5
Training loss: 2.5226197231527308
Validation loss: 2.530727244381393

Epoch: 5| Step: 6
Training loss: 1.3672337551466696
Validation loss: 2.5256584652362433

Epoch: 5| Step: 7
Training loss: 2.816910993448928
Validation loss: 2.5136732905913646

Epoch: 5| Step: 8
Training loss: 2.2182315704292077
Validation loss: 2.5266888875982856

Epoch: 5| Step: 9
Training loss: 2.9695656709951757
Validation loss: 2.538477895625287

Epoch: 5| Step: 10
Training loss: 2.825003149655367
Validation loss: 2.5101642103007595

Epoch: 210| Step: 0
Training loss: 2.5279588378319584
Validation loss: 2.5242386083510193

Epoch: 5| Step: 1
Training loss: 2.7818394582788137
Validation loss: 2.5337692175848074

Epoch: 5| Step: 2
Training loss: 3.0478385771010625
Validation loss: 2.5308536425325574

Epoch: 5| Step: 3
Training loss: 2.672788764904424
Validation loss: 2.508236834571847

Epoch: 5| Step: 4
Training loss: 2.71917975525936
Validation loss: 2.523255908847479

Epoch: 5| Step: 5
Training loss: 2.321198779040707
Validation loss: 2.5146472151399184

Epoch: 5| Step: 6
Training loss: 2.6099110127037237
Validation loss: 2.5276966146923714

Epoch: 5| Step: 7
Training loss: 2.4595158421517453
Validation loss: 2.5152931776683807

Epoch: 5| Step: 8
Training loss: 2.8270978379539273
Validation loss: 2.5184871065847685

Epoch: 5| Step: 9
Training loss: 2.451084041161616
Validation loss: 2.503817325973889

Epoch: 5| Step: 10
Training loss: 2.426135051191729
Validation loss: 2.5472469374066655

Epoch: 211| Step: 0
Training loss: 2.429570235093482
Validation loss: 2.570439249002927

Epoch: 5| Step: 1
Training loss: 2.8698368980503286
Validation loss: 2.5489330219400848

Epoch: 5| Step: 2
Training loss: 2.4459217031740534
Validation loss: 2.527609150897264

Epoch: 5| Step: 3
Training loss: 2.5886744280310805
Validation loss: 2.5157797884326794

Epoch: 5| Step: 4
Training loss: 2.357510670125041
Validation loss: 2.5186015105345048

Epoch: 5| Step: 5
Training loss: 2.6950437550009516
Validation loss: 2.5117387081381564

Epoch: 5| Step: 6
Training loss: 3.1602428598877776
Validation loss: 2.5373579413770626

Epoch: 5| Step: 7
Training loss: 2.14320785740533
Validation loss: 2.558387009666603

Epoch: 5| Step: 8
Training loss: 2.9828819814335823
Validation loss: 2.548133773058791

Epoch: 5| Step: 9
Training loss: 2.614814525175374
Validation loss: 2.523085901098559

Epoch: 5| Step: 10
Training loss: 2.401457733271594
Validation loss: 2.5401061078611322

Epoch: 212| Step: 0
Training loss: 3.030719199357876
Validation loss: 2.54191807458847

Epoch: 5| Step: 1
Training loss: 2.099566337676648
Validation loss: 2.523455388465362

Epoch: 5| Step: 2
Training loss: 2.5161429872985632
Validation loss: 2.5162425245344515

Epoch: 5| Step: 3
Training loss: 2.7936263157307493
Validation loss: 2.520532443799142

Epoch: 5| Step: 4
Training loss: 2.789621847360235
Validation loss: 2.518752169956103

Epoch: 5| Step: 5
Training loss: 2.7183201767980987
Validation loss: 2.5002119748683835

Epoch: 5| Step: 6
Training loss: 2.776186158581079
Validation loss: 2.5281932669942564

Epoch: 5| Step: 7
Training loss: 2.4900824287283285
Validation loss: 2.5404893393277295

Epoch: 5| Step: 8
Training loss: 2.37165898682599
Validation loss: 2.5249569802613188

Epoch: 5| Step: 9
Training loss: 2.426154901840987
Validation loss: 2.524039937672166

Epoch: 5| Step: 10
Training loss: 2.6940070045170565
Validation loss: 2.5369017266258527

Epoch: 213| Step: 0
Training loss: 2.673888312387562
Validation loss: 2.548988575046506

Epoch: 5| Step: 1
Training loss: 2.5965609657833073
Validation loss: 2.552655485172443

Epoch: 5| Step: 2
Training loss: 2.2703742400226226
Validation loss: 2.5113587444643137

Epoch: 5| Step: 3
Training loss: 3.028965511927914
Validation loss: 2.533670077492243

Epoch: 5| Step: 4
Training loss: 2.954224719798472
Validation loss: 2.525649392805639

Epoch: 5| Step: 5
Training loss: 2.787362315108981
Validation loss: 2.5382298575739584

Epoch: 5| Step: 6
Training loss: 2.110041986201392
Validation loss: 2.516584263021387

Epoch: 5| Step: 7
Training loss: 2.7613510469776883
Validation loss: 2.5230066755905236

Epoch: 5| Step: 8
Training loss: 2.7554987638356874
Validation loss: 2.540727021732044

Epoch: 5| Step: 9
Training loss: 2.8517392456461885
Validation loss: 2.546337457738287

Epoch: 5| Step: 10
Training loss: 1.9903443430579721
Validation loss: 2.523816530962122

Epoch: 214| Step: 0
Training loss: 3.0205310187942196
Validation loss: 2.528854193404409

Epoch: 5| Step: 1
Training loss: 2.7714033377092755
Validation loss: 2.563758428044207

Epoch: 5| Step: 2
Training loss: 2.6357468007497142
Validation loss: 2.5216764970374217

Epoch: 5| Step: 3
Training loss: 1.897553163369211
Validation loss: 2.539905137047703

Epoch: 5| Step: 4
Training loss: 2.8153121452657364
Validation loss: 2.5429903450403955

Epoch: 5| Step: 5
Training loss: 2.4982281127752715
Validation loss: 2.521448067709298

Epoch: 5| Step: 6
Training loss: 2.2685803310539443
Validation loss: 2.5427955505577025

Epoch: 5| Step: 7
Training loss: 2.9199439210601685
Validation loss: 2.539837141609377

Epoch: 5| Step: 8
Training loss: 2.0517556046587955
Validation loss: 2.5340321588798216

Epoch: 5| Step: 9
Training loss: 2.9229854982572974
Validation loss: 2.5283420083203643

Epoch: 5| Step: 10
Training loss: 2.861869016175805
Validation loss: 2.5558487384534803

Epoch: 215| Step: 0
Training loss: 3.187834479042393
Validation loss: 2.540036070957299

Epoch: 5| Step: 1
Training loss: 2.941705493472647
Validation loss: 2.529915024502808

Epoch: 5| Step: 2
Training loss: 2.0294520710701085
Validation loss: 2.5413944916330378

Epoch: 5| Step: 3
Training loss: 2.599342475792199
Validation loss: 2.530445033169536

Epoch: 5| Step: 4
Training loss: 2.145693049119855
Validation loss: 2.5249222662140913

Epoch: 5| Step: 5
Training loss: 2.417117263177903
Validation loss: 2.540169018617497

Epoch: 5| Step: 6
Training loss: 2.7624836122881162
Validation loss: 2.5474386179794544

Epoch: 5| Step: 7
Training loss: 2.478992990346141
Validation loss: 2.4950879344289336

Epoch: 5| Step: 8
Training loss: 2.6278581953300266
Validation loss: 2.540378095310297

Epoch: 5| Step: 9
Training loss: 3.0313563377369364
Validation loss: 2.529763055289091

Epoch: 5| Step: 10
Training loss: 2.5580668365479786
Validation loss: 2.523194823855094

Epoch: 216| Step: 0
Training loss: 2.3262314232315604
Validation loss: 2.5097851941722484

Epoch: 5| Step: 1
Training loss: 2.4771439023282027
Validation loss: 2.5260108295499815

Epoch: 5| Step: 2
Training loss: 3.0691540112227167
Validation loss: 2.51964439321834

Epoch: 5| Step: 3
Training loss: 1.8279354942760235
Validation loss: 2.5380717308469

Epoch: 5| Step: 4
Training loss: 2.2836059528224464
Validation loss: 2.5255016267645347

Epoch: 5| Step: 5
Training loss: 2.4151129057655285
Validation loss: 2.518607047809722

Epoch: 5| Step: 6
Training loss: 2.4840154807549752
Validation loss: 2.5286648055757697

Epoch: 5| Step: 7
Training loss: 3.018995072109652
Validation loss: 2.5371762987238453

Epoch: 5| Step: 8
Training loss: 2.9492852007724863
Validation loss: 2.4969018802697267

Epoch: 5| Step: 9
Training loss: 2.70375362429506
Validation loss: 2.524592208138178

Epoch: 5| Step: 10
Training loss: 3.0994993790048837
Validation loss: 2.528452258985021

Epoch: 217| Step: 0
Training loss: 2.0068759737870745
Validation loss: 2.531704280412668

Epoch: 5| Step: 1
Training loss: 2.610713044614126
Validation loss: 2.5396229771391394

Epoch: 5| Step: 2
Training loss: 2.0425755430560155
Validation loss: 2.507414053099522

Epoch: 5| Step: 3
Training loss: 2.8336160369156453
Validation loss: 2.517776011918741

Epoch: 5| Step: 4
Training loss: 2.9101926250872645
Validation loss: 2.5303550413199876

Epoch: 5| Step: 5
Training loss: 2.1330349425410438
Validation loss: 2.513144143494744

Epoch: 5| Step: 6
Training loss: 3.172814892351189
Validation loss: 2.511509019714977

Epoch: 5| Step: 7
Training loss: 2.758985750731906
Validation loss: 2.566471628636921

Epoch: 5| Step: 8
Training loss: 3.0643985079681144
Validation loss: 2.5297111047552763

Epoch: 5| Step: 9
Training loss: 2.6713411567496634
Validation loss: 2.555588040837209

Epoch: 5| Step: 10
Training loss: 2.216674048428207
Validation loss: 2.531368765810325

Epoch: 218| Step: 0
Training loss: 2.5981540289040135
Validation loss: 2.549389993861399

Epoch: 5| Step: 1
Training loss: 2.96293069791004
Validation loss: 2.5352446729345397

Epoch: 5| Step: 2
Training loss: 3.02901698970973
Validation loss: 2.551203769566974

Epoch: 5| Step: 3
Training loss: 2.3176801244040597
Validation loss: 2.537367975730597

Epoch: 5| Step: 4
Training loss: 2.8986785048155626
Validation loss: 2.50868182582468

Epoch: 5| Step: 5
Training loss: 2.3782957198713635
Validation loss: 2.5099657988464155

Epoch: 5| Step: 6
Training loss: 2.552360011044007
Validation loss: 2.546078576862278

Epoch: 5| Step: 7
Training loss: 2.153215013900759
Validation loss: 2.5323177497592138

Epoch: 5| Step: 8
Training loss: 2.4979691840073786
Validation loss: 2.553806283026903

Epoch: 5| Step: 9
Training loss: 2.6040584287402173
Validation loss: 2.5441404643379393

Epoch: 5| Step: 10
Training loss: 2.845838513423139
Validation loss: 2.534135816971537

Epoch: 219| Step: 0
Training loss: 2.5918323827753276
Validation loss: 2.5477182358947625

Epoch: 5| Step: 1
Training loss: 2.813185290453523
Validation loss: 2.5244722643934825

Epoch: 5| Step: 2
Training loss: 2.400553214368694
Validation loss: 2.5024243116097535

Epoch: 5| Step: 3
Training loss: 2.8965943432007353
Validation loss: 2.504526167503178

Epoch: 5| Step: 4
Training loss: 2.699484871361429
Validation loss: 2.5363834576659277

Epoch: 5| Step: 5
Training loss: 2.600737844822868
Validation loss: 2.5399883052121326

Epoch: 5| Step: 6
Training loss: 3.003469050901254
Validation loss: 2.52311773955972

Epoch: 5| Step: 7
Training loss: 2.6229766131866246
Validation loss: 2.5237220840654513

Epoch: 5| Step: 8
Training loss: 2.2969030261762717
Validation loss: 2.545333789556191

Epoch: 5| Step: 9
Training loss: 2.121918856767287
Validation loss: 2.535328841843085

Epoch: 5| Step: 10
Training loss: 2.6108804346100083
Validation loss: 2.534932248351165

Epoch: 220| Step: 0
Training loss: 2.5358239750768314
Validation loss: 2.5525374235622307

Epoch: 5| Step: 1
Training loss: 3.6219880317646127
Validation loss: 2.531143087977508

Epoch: 5| Step: 2
Training loss: 2.322116959339379
Validation loss: 2.545993932614836

Epoch: 5| Step: 3
Training loss: 1.8910782956315533
Validation loss: 2.4951946765300876

Epoch: 5| Step: 4
Training loss: 1.907202810832007
Validation loss: 2.542667093851767

Epoch: 5| Step: 5
Training loss: 2.5615719300892645
Validation loss: 2.519077087561447

Epoch: 5| Step: 6
Training loss: 2.309523754981316
Validation loss: 2.521972298434238

Epoch: 5| Step: 7
Training loss: 3.263665008081121
Validation loss: 2.5212122151836667

Epoch: 5| Step: 8
Training loss: 2.606070704890867
Validation loss: 2.523554780516028

Epoch: 5| Step: 9
Training loss: 2.475729817379536
Validation loss: 2.5290068401475017

Epoch: 5| Step: 10
Training loss: 2.8836478493617106
Validation loss: 2.519074615590189

Epoch: 221| Step: 0
Training loss: 2.280850519188258
Validation loss: 2.5399725109444455

Epoch: 5| Step: 1
Training loss: 3.079522294984665
Validation loss: 2.519572389506384

Epoch: 5| Step: 2
Training loss: 2.8618436902198163
Validation loss: 2.5435335742320575

Epoch: 5| Step: 3
Training loss: 2.895232616296283
Validation loss: 2.5119664487214686

Epoch: 5| Step: 4
Training loss: 2.4207969973578933
Validation loss: 2.51880836631916

Epoch: 5| Step: 5
Training loss: 2.4864304394882675
Validation loss: 2.5294407929543556

Epoch: 5| Step: 6
Training loss: 2.6862713866449757
Validation loss: 2.513253802115068

Epoch: 5| Step: 7
Training loss: 2.5645743557121836
Validation loss: 2.545707192818852

Epoch: 5| Step: 8
Training loss: 2.5078629818407654
Validation loss: 2.521448494737318

Epoch: 5| Step: 9
Training loss: 2.418471769353059
Validation loss: 2.5385564050113585

Epoch: 5| Step: 10
Training loss: 2.3366675623494317
Validation loss: 2.5192761450325527

Epoch: 222| Step: 0
Training loss: 2.0244074672770043
Validation loss: 2.5003432212590644

Epoch: 5| Step: 1
Training loss: 2.53190302961132
Validation loss: 2.5479553314973424

Epoch: 5| Step: 2
Training loss: 2.549180841704762
Validation loss: 2.5293459327315357

Epoch: 5| Step: 3
Training loss: 2.9056865186901115
Validation loss: 2.5225272554051417

Epoch: 5| Step: 4
Training loss: 2.652172054975298
Validation loss: 2.5187812265626213

Epoch: 5| Step: 5
Training loss: 2.081059270143945
Validation loss: 2.5342549846975624

Epoch: 5| Step: 6
Training loss: 2.620273239942588
Validation loss: 2.5151924235765537

Epoch: 5| Step: 7
Training loss: 3.2815390323315268
Validation loss: 2.5405784696817904

Epoch: 5| Step: 8
Training loss: 2.706351763285435
Validation loss: 2.5203097433686428

Epoch: 5| Step: 9
Training loss: 2.6281329940692464
Validation loss: 2.527055884136452

Epoch: 5| Step: 10
Training loss: 2.687511976348451
Validation loss: 2.5272483099511844

Epoch: 223| Step: 0
Training loss: 2.2343616618578617
Validation loss: 2.511150228592739

Epoch: 5| Step: 1
Training loss: 3.084657195843509
Validation loss: 2.539131199717433

Epoch: 5| Step: 2
Training loss: 2.659358102316689
Validation loss: 2.520085262341311

Epoch: 5| Step: 3
Training loss: 2.824261043754246
Validation loss: 2.5097774259707104

Epoch: 5| Step: 4
Training loss: 2.6044946591137035
Validation loss: 2.519494630675148

Epoch: 5| Step: 5
Training loss: 2.570882623860476
Validation loss: 2.532385524443002

Epoch: 5| Step: 6
Training loss: 2.3782562219656054
Validation loss: 2.5365877496018374

Epoch: 5| Step: 7
Training loss: 2.48634615260059
Validation loss: 2.534578111160809

Epoch: 5| Step: 8
Training loss: 2.3221997122558333
Validation loss: 2.5362514810652774

Epoch: 5| Step: 9
Training loss: 3.2633364018187807
Validation loss: 2.5503723865106727

Epoch: 5| Step: 10
Training loss: 2.154203687850236
Validation loss: 2.5297867670213665

Epoch: 224| Step: 0
Training loss: 2.2625121732774205
Validation loss: 2.522353762713722

Epoch: 5| Step: 1
Training loss: 2.457815848588936
Validation loss: 2.5277877072549413

Epoch: 5| Step: 2
Training loss: 2.1649941322300004
Validation loss: 2.5409042249357303

Epoch: 5| Step: 3
Training loss: 2.6033073736924317
Validation loss: 2.5356633592915094

Epoch: 5| Step: 4
Training loss: 2.348809033799534
Validation loss: 2.526688951519633

Epoch: 5| Step: 5
Training loss: 2.4688782477566797
Validation loss: 2.5208007834467083

Epoch: 5| Step: 6
Training loss: 3.0639241857984674
Validation loss: 2.545981146570803

Epoch: 5| Step: 7
Training loss: 3.6550524453967927
Validation loss: 2.5531183497531473

Epoch: 5| Step: 8
Training loss: 2.149333420262836
Validation loss: 2.5050160978937757

Epoch: 5| Step: 9
Training loss: 2.51976487606883
Validation loss: 2.5110454889137173

Epoch: 5| Step: 10
Training loss: 2.9347451461022644
Validation loss: 2.5220187484931573

Epoch: 225| Step: 0
Training loss: 2.5520242697180566
Validation loss: 2.5293880315563357

Epoch: 5| Step: 1
Training loss: 2.092258662408834
Validation loss: 2.5462433150981854

Epoch: 5| Step: 2
Training loss: 2.919059335134963
Validation loss: 2.550550422764103

Epoch: 5| Step: 3
Training loss: 1.983788230830472
Validation loss: 2.5161363442106732

Epoch: 5| Step: 4
Training loss: 2.6361170103299436
Validation loss: 2.5175869495407723

Epoch: 5| Step: 5
Training loss: 2.9410914117069877
Validation loss: 2.4915350624048256

Epoch: 5| Step: 6
Training loss: 2.663732424337474
Validation loss: 2.545199431591036

Epoch: 5| Step: 7
Training loss: 2.2560977958422583
Validation loss: 2.5282950523562464

Epoch: 5| Step: 8
Training loss: 2.520911496634649
Validation loss: 2.5310616483283135

Epoch: 5| Step: 9
Training loss: 2.9138016571218257
Validation loss: 2.524985736005357

Epoch: 5| Step: 10
Training loss: 3.1883460025432613
Validation loss: 2.5411408440430163

Epoch: 226| Step: 0
Training loss: 2.230774332415062
Validation loss: 2.561548568131108

Epoch: 5| Step: 1
Training loss: 2.482239483151271
Validation loss: 2.52779795353028

Epoch: 5| Step: 2
Training loss: 1.8415595798802653
Validation loss: 2.5376562129347158

Epoch: 5| Step: 3
Training loss: 2.7625920966491346
Validation loss: 2.528950299684175

Epoch: 5| Step: 4
Training loss: 2.708776398632657
Validation loss: 2.520657912513364

Epoch: 5| Step: 5
Training loss: 2.9579776675481964
Validation loss: 2.53054735809376

Epoch: 5| Step: 6
Training loss: 2.597203357195098
Validation loss: 2.53246701566348

Epoch: 5| Step: 7
Training loss: 2.703982884098936
Validation loss: 2.531491917117605

Epoch: 5| Step: 8
Training loss: 2.4885412825888436
Validation loss: 2.531759098721423

Epoch: 5| Step: 9
Training loss: 2.665685343981712
Validation loss: 2.518082076779491

Epoch: 5| Step: 10
Training loss: 3.129618779083577
Validation loss: 2.5145434897585597

Epoch: 227| Step: 0
Training loss: 2.4871405797108914
Validation loss: 2.5388040531914333

Epoch: 5| Step: 1
Training loss: 3.556066895180754
Validation loss: 2.515369616073523

Epoch: 5| Step: 2
Training loss: 3.0606872487451815
Validation loss: 2.5220573915904034

Epoch: 5| Step: 3
Training loss: 2.36727592882223
Validation loss: 2.5531944208530857

Epoch: 5| Step: 4
Training loss: 2.2323660188564007
Validation loss: 2.5225398966022095

Epoch: 5| Step: 5
Training loss: 3.2623458533575107
Validation loss: 2.5444905201157644

Epoch: 5| Step: 6
Training loss: 2.3727249492287097
Validation loss: 2.544977545883845

Epoch: 5| Step: 7
Training loss: 1.8679231607002456
Validation loss: 2.536716663730342

Epoch: 5| Step: 8
Training loss: 2.5388801391844042
Validation loss: 2.5333570670968255

Epoch: 5| Step: 9
Training loss: 2.6112940692606808
Validation loss: 2.5461481666381487

Epoch: 5| Step: 10
Training loss: 2.0514679839768952
Validation loss: 2.523284075397059

Epoch: 228| Step: 0
Training loss: 2.5545543204307664
Validation loss: 2.530008118497388

Epoch: 5| Step: 1
Training loss: 2.2142405285597704
Validation loss: 2.5002348492150688

Epoch: 5| Step: 2
Training loss: 2.223783344707118
Validation loss: 2.518654710624225

Epoch: 5| Step: 3
Training loss: 2.401491885614516
Validation loss: 2.507194773409579

Epoch: 5| Step: 4
Training loss: 3.185248832863502
Validation loss: 2.553578995054636

Epoch: 5| Step: 5
Training loss: 2.4319620065607483
Validation loss: 2.511075333015727

Epoch: 5| Step: 6
Training loss: 2.979889379662751
Validation loss: 2.523712237728839

Epoch: 5| Step: 7
Training loss: 1.9790069180984977
Validation loss: 2.5368131542815036

Epoch: 5| Step: 8
Training loss: 2.504824084807414
Validation loss: 2.5681413400520245

Epoch: 5| Step: 9
Training loss: 2.8652719860559746
Validation loss: 2.506659535752096

Epoch: 5| Step: 10
Training loss: 3.149076562584893
Validation loss: 2.523477305929461

Epoch: 229| Step: 0
Training loss: 2.640899914758552
Validation loss: 2.554213898269943

Epoch: 5| Step: 1
Training loss: 2.747737473975837
Validation loss: 2.5176872473459926

Epoch: 5| Step: 2
Training loss: 2.5421195538707213
Validation loss: 2.5583530578649225

Epoch: 5| Step: 3
Training loss: 2.2690108682599965
Validation loss: 2.5427452359535

Epoch: 5| Step: 4
Training loss: 3.0367248487987863
Validation loss: 2.5102375174069063

Epoch: 5| Step: 5
Training loss: 2.612230303409433
Validation loss: 2.5276592373638094

Epoch: 5| Step: 6
Training loss: 2.475363648797207
Validation loss: 2.552857962789206

Epoch: 5| Step: 7
Training loss: 2.337367407970195
Validation loss: 2.520450903382

Epoch: 5| Step: 8
Training loss: 2.6004733535055653
Validation loss: 2.534370027794195

Epoch: 5| Step: 9
Training loss: 2.9379650316020616
Validation loss: 2.529677889945379

Epoch: 5| Step: 10
Training loss: 2.0992247331176945
Validation loss: 2.535563075927332

Epoch: 230| Step: 0
Training loss: 2.7390200153022017
Validation loss: 2.515171542883861

Epoch: 5| Step: 1
Training loss: 2.6039460864426234
Validation loss: 2.528718344449256

Epoch: 5| Step: 2
Training loss: 2.1770769572430804
Validation loss: 2.5244148586823583

Epoch: 5| Step: 3
Training loss: 2.0179413489749476
Validation loss: 2.5126883358487064

Epoch: 5| Step: 4
Training loss: 2.8132963112870493
Validation loss: 2.535422993890697

Epoch: 5| Step: 5
Training loss: 2.776574909652602
Validation loss: 2.504389484104779

Epoch: 5| Step: 6
Training loss: 3.0774214661879187
Validation loss: 2.5023845283293196

Epoch: 5| Step: 7
Training loss: 2.6123293296150827
Validation loss: 2.5346050443454065

Epoch: 5| Step: 8
Training loss: 2.5302138857088905
Validation loss: 2.5176598999975246

Epoch: 5| Step: 9
Training loss: 2.2374293651833055
Validation loss: 2.5107932291918704

Epoch: 5| Step: 10
Training loss: 2.8317433177983324
Validation loss: 2.5013309870894878

Epoch: 231| Step: 0
Training loss: 2.965988963296308
Validation loss: 2.536665454911537

Epoch: 5| Step: 1
Training loss: 2.748385822619227
Validation loss: 2.5172828058084873

Epoch: 5| Step: 2
Training loss: 2.3114263774444446
Validation loss: 2.4978943879032873

Epoch: 5| Step: 3
Training loss: 2.386617255667282
Validation loss: 2.531870219212011

Epoch: 5| Step: 4
Training loss: 3.0206857228662707
Validation loss: 2.5531886503331203

Epoch: 5| Step: 5
Training loss: 2.6134159982835166
Validation loss: 2.5404567619262033

Epoch: 5| Step: 6
Training loss: 2.6211594233692246
Validation loss: 2.532736783824323

Epoch: 5| Step: 7
Training loss: 2.8349548823350768
Validation loss: 2.5255113666452003

Epoch: 5| Step: 8
Training loss: 2.309727011399291
Validation loss: 2.517285092150967

Epoch: 5| Step: 9
Training loss: 2.3159230646587674
Validation loss: 2.5053972578049435

Epoch: 5| Step: 10
Training loss: 2.335896163115385
Validation loss: 2.5201501133168436

Epoch: 232| Step: 0
Training loss: 2.725941934438744
Validation loss: 2.5493368167113055

Epoch: 5| Step: 1
Training loss: 2.485329594543139
Validation loss: 2.5369875351775693

Epoch: 5| Step: 2
Training loss: 2.4738509198344336
Validation loss: 2.526170403858179

Epoch: 5| Step: 3
Training loss: 2.3846811440490234
Validation loss: 2.5241581103686817

Epoch: 5| Step: 4
Training loss: 2.9863193429502166
Validation loss: 2.544990879919836

Epoch: 5| Step: 5
Training loss: 2.1651005097420204
Validation loss: 2.5358537166134356

Epoch: 5| Step: 6
Training loss: 3.0461049207123976
Validation loss: 2.5241984279815655

Epoch: 5| Step: 7
Training loss: 2.1520851935359846
Validation loss: 2.511768269448236

Epoch: 5| Step: 8
Training loss: 2.6696079742150216
Validation loss: 2.55013438467264

Epoch: 5| Step: 9
Training loss: 2.7519050415256814
Validation loss: 2.5333363492793386

Epoch: 5| Step: 10
Training loss: 2.5910885496213285
Validation loss: 2.541589180666108

Epoch: 233| Step: 0
Training loss: 1.920900175154908
Validation loss: 2.5444232708893537

Epoch: 5| Step: 1
Training loss: 2.044699174910908
Validation loss: 2.538749962590386

Epoch: 5| Step: 2
Training loss: 1.9520581192564939
Validation loss: 2.5133974208327663

Epoch: 5| Step: 3
Training loss: 2.5621059393856083
Validation loss: 2.5015602852082255

Epoch: 5| Step: 4
Training loss: 3.170507277084058
Validation loss: 2.529351236171998

Epoch: 5| Step: 5
Training loss: 3.012072907243011
Validation loss: 2.5291022873252254

Epoch: 5| Step: 6
Training loss: 3.0816063727062404
Validation loss: 2.540620412651992

Epoch: 5| Step: 7
Training loss: 2.6618581472787257
Validation loss: 2.568049129278084

Epoch: 5| Step: 8
Training loss: 2.3246914935779586
Validation loss: 2.515780047264828

Epoch: 5| Step: 9
Training loss: 2.4257493070750624
Validation loss: 2.5238358296796064

Epoch: 5| Step: 10
Training loss: 2.967454808444626
Validation loss: 2.5409266986520715

Epoch: 234| Step: 0
Training loss: 2.39602818456895
Validation loss: 2.5318071559121234

Epoch: 5| Step: 1
Training loss: 2.0276346751309657
Validation loss: 2.524373125795733

Epoch: 5| Step: 2
Training loss: 2.67980138241974
Validation loss: 2.5039184461486683

Epoch: 5| Step: 3
Training loss: 2.3250413993513126
Validation loss: 2.5484745255448913

Epoch: 5| Step: 4
Training loss: 2.6790584793514514
Validation loss: 2.5300878036521035

Epoch: 5| Step: 5
Training loss: 2.427020703818807
Validation loss: 2.520787456206517

Epoch: 5| Step: 6
Training loss: 2.8635375362258677
Validation loss: 2.552138146688056

Epoch: 5| Step: 7
Training loss: 3.043749492418063
Validation loss: 2.5346010278517705

Epoch: 5| Step: 8
Training loss: 2.0837632180786123
Validation loss: 2.543273039576628

Epoch: 5| Step: 9
Training loss: 3.2957313325693725
Validation loss: 2.545568099042399

Epoch: 5| Step: 10
Training loss: 2.479170524102637
Validation loss: 2.5516834145759746

Epoch: 235| Step: 0
Training loss: 2.9906194577220555
Validation loss: 2.549495820943597

Epoch: 5| Step: 1
Training loss: 1.8154555591301358
Validation loss: 2.5196287273181515

Epoch: 5| Step: 2
Training loss: 2.2310530503487493
Validation loss: 2.512403542774055

Epoch: 5| Step: 3
Training loss: 2.637766176505511
Validation loss: 2.5379697042369984

Epoch: 5| Step: 4
Training loss: 3.002370215921439
Validation loss: 2.547050370888029

Epoch: 5| Step: 5
Training loss: 2.9622490627710047
Validation loss: 2.521843340665426

Epoch: 5| Step: 6
Training loss: 2.4512385993781884
Validation loss: 2.5146120325279377

Epoch: 5| Step: 7
Training loss: 2.4135206287198683
Validation loss: 2.504360305560933

Epoch: 5| Step: 8
Training loss: 2.5429465297308296
Validation loss: 2.5169530552847865

Epoch: 5| Step: 9
Training loss: 2.539030949323205
Validation loss: 2.5510647162129896

Epoch: 5| Step: 10
Training loss: 2.7332158710858026
Validation loss: 2.537905128799685

Epoch: 236| Step: 0
Training loss: 2.542932747440996
Validation loss: 2.517578551666138

Epoch: 5| Step: 1
Training loss: 3.1773536608813187
Validation loss: 2.5587654632909533

Epoch: 5| Step: 2
Training loss: 2.5410379559813334
Validation loss: 2.516662135205355

Epoch: 5| Step: 3
Training loss: 2.387129877770526
Validation loss: 2.5308801200225695

Epoch: 5| Step: 4
Training loss: 2.302298028062825
Validation loss: 2.5343498776488285

Epoch: 5| Step: 5
Training loss: 2.436279969674505
Validation loss: 2.5449471494199254

Epoch: 5| Step: 6
Training loss: 3.010709878090504
Validation loss: 2.524878556701741

Epoch: 5| Step: 7
Training loss: 2.514102641011018
Validation loss: 2.545171737225759

Epoch: 5| Step: 8
Training loss: 2.0011067904257964
Validation loss: 2.550144388356399

Epoch: 5| Step: 9
Training loss: 2.8641133055730608
Validation loss: 2.5180428122065313

Epoch: 5| Step: 10
Training loss: 2.4287676771996063
Validation loss: 2.5202559750069455

Epoch: 237| Step: 0
Training loss: 2.74591706360045
Validation loss: 2.5299030975720163

Epoch: 5| Step: 1
Training loss: 2.7139637350978045
Validation loss: 2.555339239258988

Epoch: 5| Step: 2
Training loss: 2.7132782518418055
Validation loss: 2.517878504506018

Epoch: 5| Step: 3
Training loss: 1.5238869028653699
Validation loss: 2.5134187058972812

Epoch: 5| Step: 4
Training loss: 2.142743920559393
Validation loss: 2.5421097006365043

Epoch: 5| Step: 5
Training loss: 2.5992056219963593
Validation loss: 2.5380422132784934

Epoch: 5| Step: 6
Training loss: 2.3911982827543827
Validation loss: 2.499682839589759

Epoch: 5| Step: 7
Training loss: 3.098365192656513
Validation loss: 2.5097088584195513

Epoch: 5| Step: 8
Training loss: 3.1318130991917514
Validation loss: 2.5396039089262517

Epoch: 5| Step: 9
Training loss: 2.6596810116631033
Validation loss: 2.539290988381625

Epoch: 5| Step: 10
Training loss: 2.439539911636772
Validation loss: 2.5381023384341486

Epoch: 238| Step: 0
Training loss: 3.2847029183814973
Validation loss: 2.5449314136298193

Epoch: 5| Step: 1
Training loss: 2.3441149618189874
Validation loss: 2.5296361060578856

Epoch: 5| Step: 2
Training loss: 2.5199069903632925
Validation loss: 2.5225798203560053

Epoch: 5| Step: 3
Training loss: 2.817101930999717
Validation loss: 2.5291424333738237

Epoch: 5| Step: 4
Training loss: 1.9761288133172163
Validation loss: 2.5222451197402944

Epoch: 5| Step: 5
Training loss: 2.8010206984736095
Validation loss: 2.523214117675477

Epoch: 5| Step: 6
Training loss: 2.2225976812225112
Validation loss: 2.5066607620073276

Epoch: 5| Step: 7
Training loss: 2.8844305526987966
Validation loss: 2.5285238506482397

Epoch: 5| Step: 8
Training loss: 2.568063511505741
Validation loss: 2.5297213776960508

Epoch: 5| Step: 9
Training loss: 2.625070480127108
Validation loss: 2.5438058694278967

Epoch: 5| Step: 10
Training loss: 2.200225437057974
Validation loss: 2.5428423849425297

Epoch: 239| Step: 0
Training loss: 2.4693968685941923
Validation loss: 2.525095933204342

Epoch: 5| Step: 1
Training loss: 2.2111581213771063
Validation loss: 2.525532374003551

Epoch: 5| Step: 2
Training loss: 2.435384566000607
Validation loss: 2.5123974989751634

Epoch: 5| Step: 3
Training loss: 2.2355542038992877
Validation loss: 2.5225035780799967

Epoch: 5| Step: 4
Training loss: 3.22980253460434
Validation loss: 2.5119609927418605

Epoch: 5| Step: 5
Training loss: 2.492830008313423
Validation loss: 2.518091400962966

Epoch: 5| Step: 6
Training loss: 2.4383201197563644
Validation loss: 2.517807196636972

Epoch: 5| Step: 7
Training loss: 2.83975542815286
Validation loss: 2.540510155223927

Epoch: 5| Step: 8
Training loss: 3.0709353681475218
Validation loss: 2.509732047140033

Epoch: 5| Step: 9
Training loss: 1.9459085432244112
Validation loss: 2.544959700911832

Epoch: 5| Step: 10
Training loss: 2.7224442387134493
Validation loss: 2.548491576358219

Epoch: 240| Step: 0
Training loss: 2.3067119435550576
Validation loss: 2.531125588083395

Epoch: 5| Step: 1
Training loss: 2.9431993103043124
Validation loss: 2.5284100472416617

Epoch: 5| Step: 2
Training loss: 2.7036129726019102
Validation loss: 2.561352688799889

Epoch: 5| Step: 3
Training loss: 2.616365491538776
Validation loss: 2.5210012256101133

Epoch: 5| Step: 4
Training loss: 2.2644236569463825
Validation loss: 2.5428552059242646

Epoch: 5| Step: 5
Training loss: 3.160218567109993
Validation loss: 2.538023714498034

Epoch: 5| Step: 6
Training loss: 2.7924262100073802
Validation loss: 2.5247979395074753

Epoch: 5| Step: 7
Training loss: 2.0240324463938753
Validation loss: 2.5377849697483352

Epoch: 5| Step: 8
Training loss: 2.760951776041234
Validation loss: 2.554178322206711

Epoch: 5| Step: 9
Training loss: 2.3829314655561022
Validation loss: 2.518810751021087

Epoch: 5| Step: 10
Training loss: 2.112299156798989
Validation loss: 2.517538429458428

Epoch: 241| Step: 0
Training loss: 2.8025188084021404
Validation loss: 2.520821447674088

Epoch: 5| Step: 1
Training loss: 2.372380669409608
Validation loss: 2.5121610559481407

Epoch: 5| Step: 2
Training loss: 2.1028814891848167
Validation loss: 2.534684883858194

Epoch: 5| Step: 3
Training loss: 2.5665660326496904
Validation loss: 2.542641910700814

Epoch: 5| Step: 4
Training loss: 3.105949009640738
Validation loss: 2.543870053211328

Epoch: 5| Step: 5
Training loss: 2.838340392836615
Validation loss: 2.531186595625505

Epoch: 5| Step: 6
Training loss: 2.3864491212074
Validation loss: 2.525368787081045

Epoch: 5| Step: 7
Training loss: 2.061022373675861
Validation loss: 2.548964512910458

Epoch: 5| Step: 8
Training loss: 2.55475600031204
Validation loss: 2.5479179746621954

Epoch: 5| Step: 9
Training loss: 3.005930125771474
Validation loss: 2.522731639487437

Epoch: 5| Step: 10
Training loss: 2.403886798808462
Validation loss: 2.5480608066637527

Epoch: 242| Step: 0
Training loss: 2.586662905270834
Validation loss: 2.51842803649992

Epoch: 5| Step: 1
Training loss: 2.637486324907621
Validation loss: 2.5461896221661453

Epoch: 5| Step: 2
Training loss: 2.12567273879065
Validation loss: 2.517044593187306

Epoch: 5| Step: 3
Training loss: 2.2260050828778004
Validation loss: 2.5231817800333336

Epoch: 5| Step: 4
Training loss: 2.6449838811603184
Validation loss: 2.5590869204391766

Epoch: 5| Step: 5
Training loss: 2.4293119379469994
Validation loss: 2.5463794311926318

Epoch: 5| Step: 6
Training loss: 2.1048763198943545
Validation loss: 2.5314484547194436

Epoch: 5| Step: 7
Training loss: 2.2095871970483225
Validation loss: 2.5351681198574627

Epoch: 5| Step: 8
Training loss: 3.1264675509149025
Validation loss: 2.5155189617668863

Epoch: 5| Step: 9
Training loss: 2.9855743714388656
Validation loss: 2.521755636242628

Epoch: 5| Step: 10
Training loss: 3.2308916140352135
Validation loss: 2.5133026385000385

Epoch: 243| Step: 0
Training loss: 2.477472084111397
Validation loss: 2.5080164502916635

Epoch: 5| Step: 1
Training loss: 2.2671661396978995
Validation loss: 2.536559739875442

Epoch: 5| Step: 2
Training loss: 2.1274220743936048
Validation loss: 2.522185469870339

Epoch: 5| Step: 3
Training loss: 2.4793947793427886
Validation loss: 2.5421830803717227

Epoch: 5| Step: 4
Training loss: 2.1672894487556342
Validation loss: 2.517722240358694

Epoch: 5| Step: 5
Training loss: 2.3532540456943147
Validation loss: 2.5080908975175533

Epoch: 5| Step: 6
Training loss: 3.6920285623753655
Validation loss: 2.4874865904588983

Epoch: 5| Step: 7
Training loss: 2.234038320931597
Validation loss: 2.525739259995256

Epoch: 5| Step: 8
Training loss: 2.682543953200404
Validation loss: 2.52319475578111

Epoch: 5| Step: 9
Training loss: 2.433291004631426
Validation loss: 2.542540662378656

Epoch: 5| Step: 10
Training loss: 2.985887234203331
Validation loss: 2.529539646792453

Epoch: 244| Step: 0
Training loss: 2.683137680013086
Validation loss: 2.5368436744816063

Epoch: 5| Step: 1
Training loss: 2.144445198709007
Validation loss: 2.5224838494314197

Epoch: 5| Step: 2
Training loss: 2.33208911510862
Validation loss: 2.5437098628317223

Epoch: 5| Step: 3
Training loss: 2.6649531780087625
Validation loss: 2.560348868402139

Epoch: 5| Step: 4
Training loss: 3.0946975952217985
Validation loss: 2.530081248854784

Epoch: 5| Step: 5
Training loss: 2.553611972269756
Validation loss: 2.4930383792850352

Epoch: 5| Step: 6
Training loss: 2.832859691599215
Validation loss: 2.5471373351911173

Epoch: 5| Step: 7
Training loss: 2.324386359830927
Validation loss: 2.541520121717332

Epoch: 5| Step: 8
Training loss: 2.6793502728883096
Validation loss: 2.545956057624293

Epoch: 5| Step: 9
Training loss: 2.3614991585986247
Validation loss: 2.5341098823601427

Epoch: 5| Step: 10
Training loss: 2.666663547355099
Validation loss: 2.5251411191837647

Epoch: 245| Step: 0
Training loss: 2.7716683776293545
Validation loss: 2.525046261908127

Epoch: 5| Step: 1
Training loss: 2.5032822996475077
Validation loss: 2.570534226369579

Epoch: 5| Step: 2
Training loss: 2.772745309336138
Validation loss: 2.512913608234709

Epoch: 5| Step: 3
Training loss: 2.589818249515058
Validation loss: 2.5435350367020364

Epoch: 5| Step: 4
Training loss: 2.8430540469866052
Validation loss: 2.526129116062313

Epoch: 5| Step: 5
Training loss: 2.26343035458621
Validation loss: 2.542763633842684

Epoch: 5| Step: 6
Training loss: 2.9883870423981795
Validation loss: 2.561178536460666

Epoch: 5| Step: 7
Training loss: 2.414116917462868
Validation loss: 2.5512536862393502

Epoch: 5| Step: 8
Training loss: 2.551876002602774
Validation loss: 2.5552113238613385

Epoch: 5| Step: 9
Training loss: 1.9827656264660327
Validation loss: 2.5404265525545986

Epoch: 5| Step: 10
Training loss: 2.451826881949766
Validation loss: 2.5224158351248858

Epoch: 246| Step: 0
Training loss: 3.087632112340657
Validation loss: 2.514962722344504

Epoch: 5| Step: 1
Training loss: 2.3694708862061615
Validation loss: 2.5595622416731474

Epoch: 5| Step: 2
Training loss: 1.891852177613655
Validation loss: 2.5466303342658425

Epoch: 5| Step: 3
Training loss: 2.425421695814329
Validation loss: 2.54577496299601

Epoch: 5| Step: 4
Training loss: 2.5449169097406954
Validation loss: 2.50929569125238

Epoch: 5| Step: 5
Training loss: 2.674140014737525
Validation loss: 2.5022299194129514

Epoch: 5| Step: 6
Training loss: 2.5015257947660428
Validation loss: 2.528006816165043

Epoch: 5| Step: 7
Training loss: 2.5609371722944423
Validation loss: 2.5083680417825778

Epoch: 5| Step: 8
Training loss: 2.3794813541484183
Validation loss: 2.557836057406759

Epoch: 5| Step: 9
Training loss: 2.8326117400765556
Validation loss: 2.5119338575890566

Epoch: 5| Step: 10
Training loss: 3.108928399145858
Validation loss: 2.5134963436516378

Epoch: 247| Step: 0
Training loss: 3.0771958551170435
Validation loss: 2.5361984034507774

Epoch: 5| Step: 1
Training loss: 2.361573969283249
Validation loss: 2.549234867861147

Epoch: 5| Step: 2
Training loss: 2.8791861533919247
Validation loss: 2.5321925535389505

Epoch: 5| Step: 3
Training loss: 2.2826184910313296
Validation loss: 2.4986713775682463

Epoch: 5| Step: 4
Training loss: 2.5697125601425057
Validation loss: 2.5118982869073543

Epoch: 5| Step: 5
Training loss: 2.496481613088413
Validation loss: 2.5316067252708785

Epoch: 5| Step: 6
Training loss: 2.772617616489087
Validation loss: 2.5259017978346114

Epoch: 5| Step: 7
Training loss: 2.8234192704592314
Validation loss: 2.5249618202896014

Epoch: 5| Step: 8
Training loss: 1.8783423832717947
Validation loss: 2.5279630748003132

Epoch: 5| Step: 9
Training loss: 2.724786305579169
Validation loss: 2.541871814191702

Epoch: 5| Step: 10
Training loss: 1.9228151084501546
Validation loss: 2.540957088728985

Epoch: 248| Step: 0
Training loss: 2.64216000335661
Validation loss: 2.5323568739854196

Epoch: 5| Step: 1
Training loss: 2.5198800252178075
Validation loss: 2.536941142431808

Epoch: 5| Step: 2
Training loss: 2.83421067136861
Validation loss: 2.4912733088999817

Epoch: 5| Step: 3
Training loss: 2.3688625540451422
Validation loss: 2.5350953030998933

Epoch: 5| Step: 4
Training loss: 2.836026781426484
Validation loss: 2.5329377819112366

Epoch: 5| Step: 5
Training loss: 2.1713166102248604
Validation loss: 2.561541836629006

Epoch: 5| Step: 6
Training loss: 2.653114532701672
Validation loss: 2.5346944301638223

Epoch: 5| Step: 7
Training loss: 2.9672017125758057
Validation loss: 2.5178141774319487

Epoch: 5| Step: 8
Training loss: 2.7089447700669256
Validation loss: 2.529140012799685

Epoch: 5| Step: 9
Training loss: 2.6590118411267825
Validation loss: 2.529848815646627

Epoch: 5| Step: 10
Training loss: 1.6140369926571607
Validation loss: 2.5156396890765658

Epoch: 249| Step: 0
Training loss: 3.0310627987660492
Validation loss: 2.564325483275644

Epoch: 5| Step: 1
Training loss: 2.1261732565044658
Validation loss: 2.4888092581664676

Epoch: 5| Step: 2
Training loss: 2.7924455913000514
Validation loss: 2.5516946544759866

Epoch: 5| Step: 3
Training loss: 2.6380398527754547
Validation loss: 2.5326176114705703

Epoch: 5| Step: 4
Training loss: 2.31875671221232
Validation loss: 2.5243553007052815

Epoch: 5| Step: 5
Training loss: 2.720827208001416
Validation loss: 2.5073513736271456

Epoch: 5| Step: 6
Training loss: 2.531419536423144
Validation loss: 2.532298965684191

Epoch: 5| Step: 7
Training loss: 2.7906538326401487
Validation loss: 2.5305681423067554

Epoch: 5| Step: 8
Training loss: 2.6260596361848054
Validation loss: 2.5216102736743142

Epoch: 5| Step: 9
Training loss: 2.5405288922101517
Validation loss: 2.536823220601785

Epoch: 5| Step: 10
Training loss: 2.1326906497787737
Validation loss: 2.5071682707407272

Epoch: 250| Step: 0
Training loss: 2.3466606885479875
Validation loss: 2.5443216370147606

Epoch: 5| Step: 1
Training loss: 2.9899163852597495
Validation loss: 2.5165915640230074

Epoch: 5| Step: 2
Training loss: 2.0694162408100194
Validation loss: 2.5168302014970823

Epoch: 5| Step: 3
Training loss: 2.807958199399121
Validation loss: 2.53682375115164

Epoch: 5| Step: 4
Training loss: 2.733232444746583
Validation loss: 2.547996379314489

Epoch: 5| Step: 5
Training loss: 2.4088785256193774
Validation loss: 2.5266749354572515

Epoch: 5| Step: 6
Training loss: 2.5197012910461933
Validation loss: 2.4940855811772074

Epoch: 5| Step: 7
Training loss: 2.444625026602427
Validation loss: 2.532995987334372

Epoch: 5| Step: 8
Training loss: 2.4761836487200863
Validation loss: 2.5684214519870365

Epoch: 5| Step: 9
Training loss: 2.6408238082173074
Validation loss: 2.527471826653003

Epoch: 5| Step: 10
Training loss: 2.560510654033239
Validation loss: 2.525079258442695

Epoch: 251| Step: 0
Training loss: 2.6310697951416526
Validation loss: 2.5423919077420205

Epoch: 5| Step: 1
Training loss: 2.431682099138025
Validation loss: 2.508660784662143

Epoch: 5| Step: 2
Training loss: 2.6286496721641144
Validation loss: 2.5084391764735274

Epoch: 5| Step: 3
Training loss: 2.0998480514776383
Validation loss: 2.5343079755559588

Epoch: 5| Step: 4
Training loss: 2.901890756571262
Validation loss: 2.4988842597114025

Epoch: 5| Step: 5
Training loss: 2.6577227829059953
Validation loss: 2.519776024851878

Epoch: 5| Step: 6
Training loss: 2.5216960744241876
Validation loss: 2.5287165084381846

Epoch: 5| Step: 7
Training loss: 2.836175913811926
Validation loss: 2.530259924586357

Epoch: 5| Step: 8
Training loss: 2.404891980891928
Validation loss: 2.5184281968274664

Epoch: 5| Step: 9
Training loss: 2.5598105296412768
Validation loss: 2.4911925952158445

Epoch: 5| Step: 10
Training loss: 2.386411956198829
Validation loss: 2.520063795537822

Epoch: 252| Step: 0
Training loss: 2.840188615472564
Validation loss: 2.5243226907820717

Epoch: 5| Step: 1
Training loss: 2.8651606492410635
Validation loss: 2.5088001563394298

Epoch: 5| Step: 2
Training loss: 2.397593495730722
Validation loss: 2.541899643348322

Epoch: 5| Step: 3
Training loss: 2.724417293436458
Validation loss: 2.536459378751026

Epoch: 5| Step: 4
Training loss: 2.202206034446336
Validation loss: 2.515152635341482

Epoch: 5| Step: 5
Training loss: 2.5818607891562473
Validation loss: 2.544966740196571

Epoch: 5| Step: 6
Training loss: 2.5193584530740525
Validation loss: 2.5463689395299314

Epoch: 5| Step: 7
Training loss: 2.153311454772031
Validation loss: 2.5329174898072013

Epoch: 5| Step: 8
Training loss: 2.635724186703306
Validation loss: 2.520692655793587

Epoch: 5| Step: 9
Training loss: 2.7822202211778384
Validation loss: 2.5130412731755922

Epoch: 5| Step: 10
Training loss: 2.342635029389002
Validation loss: 2.5109855030538295

Epoch: 253| Step: 0
Training loss: 2.6357598263523814
Validation loss: 2.5491663348401308

Epoch: 5| Step: 1
Training loss: 2.3438726774534477
Validation loss: 2.526408486626244

Epoch: 5| Step: 2
Training loss: 2.3885294387533222
Validation loss: 2.5314496335216425

Epoch: 5| Step: 3
Training loss: 2.7942397846773424
Validation loss: 2.525128117903858

Epoch: 5| Step: 4
Training loss: 2.26872788095332
Validation loss: 2.5065469785127097

Epoch: 5| Step: 5
Training loss: 2.846856066308986
Validation loss: 2.5005096603095303

Epoch: 5| Step: 6
Training loss: 2.470608072374883
Validation loss: 2.4928752443853197

Epoch: 5| Step: 7
Training loss: 2.678943764375801
Validation loss: 2.499519890655425

Epoch: 5| Step: 8
Training loss: 2.156409437394051
Validation loss: 2.528651078256772

Epoch: 5| Step: 9
Training loss: 3.044310130879184
Validation loss: 2.5319380727016467

Epoch: 5| Step: 10
Training loss: 2.238580866760812
Validation loss: 2.528795929145445

Epoch: 254| Step: 0
Training loss: 2.340454021977723
Validation loss: 2.5285410299124784

Epoch: 5| Step: 1
Training loss: 3.012708765924175
Validation loss: 2.5449780576091108

Epoch: 5| Step: 2
Training loss: 2.4412790982513823
Validation loss: 2.5293914907723187

Epoch: 5| Step: 3
Training loss: 2.345784334401645
Validation loss: 2.5446878787666143

Epoch: 5| Step: 4
Training loss: 2.163773169611087
Validation loss: 2.548038929618615

Epoch: 5| Step: 5
Training loss: 2.178394002188762
Validation loss: 2.5286037640577024

Epoch: 5| Step: 6
Training loss: 2.787214933453485
Validation loss: 2.5310508986984654

Epoch: 5| Step: 7
Training loss: 2.9172914834841883
Validation loss: 2.5320963182176657

Epoch: 5| Step: 8
Training loss: 3.0123999551329597
Validation loss: 2.5314689115181985

Epoch: 5| Step: 9
Training loss: 1.8675735904436688
Validation loss: 2.523656328337064

Epoch: 5| Step: 10
Training loss: 3.08342832126877
Validation loss: 2.551119415958611

Epoch: 255| Step: 0
Training loss: 2.8603859036859642
Validation loss: 2.512637776483916

Epoch: 5| Step: 1
Training loss: 2.114148312876055
Validation loss: 2.5180873754366604

Epoch: 5| Step: 2
Training loss: 2.717260369173222
Validation loss: 2.5231510132942927

Epoch: 5| Step: 3
Training loss: 2.3375180616037974
Validation loss: 2.5089847938906225

Epoch: 5| Step: 4
Training loss: 1.7696297388688993
Validation loss: 2.503066857219972

Epoch: 5| Step: 5
Training loss: 2.539624149118649
Validation loss: 2.5099068749827835

Epoch: 5| Step: 6
Training loss: 2.5222299243325796
Validation loss: 2.5285931712658054

Epoch: 5| Step: 7
Training loss: 2.6015107204107126
Validation loss: 2.5205097958690845

Epoch: 5| Step: 8
Training loss: 2.3108918036533295
Validation loss: 2.520511398837009

Epoch: 5| Step: 9
Training loss: 2.855199047037801
Validation loss: 2.4992266935619574

Epoch: 5| Step: 10
Training loss: 3.3454467176357108
Validation loss: 2.5054620551168396

Epoch: 256| Step: 0
Training loss: 2.564126033797814
Validation loss: 2.518420899114406

Epoch: 5| Step: 1
Training loss: 3.1366442603818694
Validation loss: 2.524063553356164

Epoch: 5| Step: 2
Training loss: 2.395966788735221
Validation loss: 2.5310241576187047

Epoch: 5| Step: 3
Training loss: 2.7338925181192955
Validation loss: 2.5041714065974

Epoch: 5| Step: 4
Training loss: 2.474348360069034
Validation loss: 2.523891184559012

Epoch: 5| Step: 5
Training loss: 2.3717398101114875
Validation loss: 2.5504576363077938

Epoch: 5| Step: 6
Training loss: 2.0796494019742453
Validation loss: 2.533776605659656

Epoch: 5| Step: 7
Training loss: 2.9748283400818445
Validation loss: 2.514773922497376

Epoch: 5| Step: 8
Training loss: 2.4668055735817536
Validation loss: 2.527512498646191

Epoch: 5| Step: 9
Training loss: 2.4058398603195097
Validation loss: 2.5232152205669367

Epoch: 5| Step: 10
Training loss: 2.335711924400486
Validation loss: 2.5330283904625315

Epoch: 257| Step: 0
Training loss: 3.305211523907878
Validation loss: 2.551076611537816

Epoch: 5| Step: 1
Training loss: 2.487244490339484
Validation loss: 2.5590461767436508

Epoch: 5| Step: 2
Training loss: 2.2773601865093496
Validation loss: 2.5497348968906763

Epoch: 5| Step: 3
Training loss: 1.6900535619024029
Validation loss: 2.57864248203987

Epoch: 5| Step: 4
Training loss: 3.016525213251601
Validation loss: 2.5311977037227993

Epoch: 5| Step: 5
Training loss: 2.6473517879104462
Validation loss: 2.5296932331642403

Epoch: 5| Step: 6
Training loss: 2.3934900406597786
Validation loss: 2.5213654974324795

Epoch: 5| Step: 7
Training loss: 2.5153514642265056
Validation loss: 2.515602776672486

Epoch: 5| Step: 8
Training loss: 2.4948379629109465
Validation loss: 2.522781606340721

Epoch: 5| Step: 9
Training loss: 2.188941916303779
Validation loss: 2.521841855450222

Epoch: 5| Step: 10
Training loss: 2.672113597545216
Validation loss: 2.5460898218708383

Epoch: 258| Step: 0
Training loss: 2.1574817401206103
Validation loss: 2.5363443536049193

Epoch: 5| Step: 1
Training loss: 2.5078689711442705
Validation loss: 2.5381792640926943

Epoch: 5| Step: 2
Training loss: 2.0899740303287815
Validation loss: 2.5251708301972426

Epoch: 5| Step: 3
Training loss: 2.584760507524073
Validation loss: 2.533455009188402

Epoch: 5| Step: 4
Training loss: 2.3531511079877396
Validation loss: 2.52308332331886

Epoch: 5| Step: 5
Training loss: 3.272367680426648
Validation loss: 2.518810993257008

Epoch: 5| Step: 6
Training loss: 2.65332543432635
Validation loss: 2.5073774592504017

Epoch: 5| Step: 7
Training loss: 2.5428314875109113
Validation loss: 2.526522864197887

Epoch: 5| Step: 8
Training loss: 2.8595894488188156
Validation loss: 2.5340922754848716

Epoch: 5| Step: 9
Training loss: 2.380390875318446
Validation loss: 2.514923582873006

Epoch: 5| Step: 10
Training loss: 2.474732502054732
Validation loss: 2.5139096138220216

Epoch: 259| Step: 0
Training loss: 2.8521771330324768
Validation loss: 2.5354737824098885

Epoch: 5| Step: 1
Training loss: 2.908677594669345
Validation loss: 2.5302768853736604

Epoch: 5| Step: 2
Training loss: 2.4893060846388537
Validation loss: 2.5385561171951725

Epoch: 5| Step: 3
Training loss: 2.7326810112668434
Validation loss: 2.5258982039269005

Epoch: 5| Step: 4
Training loss: 2.1560902190785978
Validation loss: 2.5471864750554247

Epoch: 5| Step: 5
Training loss: 2.3533880808606478
Validation loss: 2.527330590651752

Epoch: 5| Step: 6
Training loss: 2.536694922304457
Validation loss: 2.5299109184833632

Epoch: 5| Step: 7
Training loss: 2.590082079938134
Validation loss: 2.5347736443516866

Epoch: 5| Step: 8
Training loss: 2.9409497072653354
Validation loss: 2.529039044006942

Epoch: 5| Step: 9
Training loss: 2.1988712666410284
Validation loss: 2.490965471683249

Epoch: 5| Step: 10
Training loss: 1.9956286461447676
Validation loss: 2.5358082635515307

Epoch: 260| Step: 0
Training loss: 2.460925971110148
Validation loss: 2.5403538723965258

Epoch: 5| Step: 1
Training loss: 2.8137290812138014
Validation loss: 2.5392436886893273

Epoch: 5| Step: 2
Training loss: 2.816606956240285
Validation loss: 2.548100712872289

Epoch: 5| Step: 3
Training loss: 2.1571644488709376
Validation loss: 2.532590032585746

Epoch: 5| Step: 4
Training loss: 3.452697451260595
Validation loss: 2.525218844242903

Epoch: 5| Step: 5
Training loss: 2.2913679679640753
Validation loss: 2.512407360063821

Epoch: 5| Step: 6
Training loss: 2.3945876047992285
Validation loss: 2.5108108707988226

Epoch: 5| Step: 7
Training loss: 1.839439720288046
Validation loss: 2.506092259344737

Epoch: 5| Step: 8
Training loss: 2.9853887783397517
Validation loss: 2.5179816231459253

Epoch: 5| Step: 9
Training loss: 2.5486639561892757
Validation loss: 2.5289295690806983

Epoch: 5| Step: 10
Training loss: 1.6695755448844758
Validation loss: 2.5296774612662634

Epoch: 261| Step: 0
Training loss: 3.1301565731971768
Validation loss: 2.5277320659991758

Epoch: 5| Step: 1
Training loss: 2.596460144471668
Validation loss: 2.516705900833251

Epoch: 5| Step: 2
Training loss: 2.532630355658342
Validation loss: 2.5287104995731937

Epoch: 5| Step: 3
Training loss: 1.9283477209417725
Validation loss: 2.5373521914215087

Epoch: 5| Step: 4
Training loss: 2.5127723584184727
Validation loss: 2.50316016725415

Epoch: 5| Step: 5
Training loss: 2.607910201960658
Validation loss: 2.530997353456775

Epoch: 5| Step: 6
Training loss: 2.9554957002141578
Validation loss: 2.527110563867195

Epoch: 5| Step: 7
Training loss: 2.982866475179947
Validation loss: 2.5357316001023156

Epoch: 5| Step: 8
Training loss: 1.9598794339524772
Validation loss: 2.520856935666377

Epoch: 5| Step: 9
Training loss: 2.2511337390895036
Validation loss: 2.50707146137774

Epoch: 5| Step: 10
Training loss: 2.3329105221185458
Validation loss: 2.568304735988488

Epoch: 262| Step: 0
Training loss: 2.4837501747810906
Validation loss: 2.5306275039197015

Epoch: 5| Step: 1
Training loss: 3.0994675332818318
Validation loss: 2.528252526691084

Epoch: 5| Step: 2
Training loss: 1.9832327851952825
Validation loss: 2.5244992343092427

Epoch: 5| Step: 3
Training loss: 2.3114834174820365
Validation loss: 2.5154094253366206

Epoch: 5| Step: 4
Training loss: 2.4374825892682472
Validation loss: 2.5277723007461343

Epoch: 5| Step: 5
Training loss: 2.0228291779079135
Validation loss: 2.535303786014367

Epoch: 5| Step: 6
Training loss: 2.9178751122337743
Validation loss: 2.5141845647387244

Epoch: 5| Step: 7
Training loss: 2.6959792929456903
Validation loss: 2.524739413083262

Epoch: 5| Step: 8
Training loss: 3.0357013541835753
Validation loss: 2.5125575715767963

Epoch: 5| Step: 9
Training loss: 2.5246508253030497
Validation loss: 2.551289818568211

Epoch: 5| Step: 10
Training loss: 2.210874091543258
Validation loss: 2.5383875421927353

Epoch: 263| Step: 0
Training loss: 2.7463877101397642
Validation loss: 2.5415786944462146

Epoch: 5| Step: 1
Training loss: 2.294902447557397
Validation loss: 2.5044545889895304

Epoch: 5| Step: 2
Training loss: 3.0065528510882222
Validation loss: 2.542765601868543

Epoch: 5| Step: 3
Training loss: 2.3134042157720205
Validation loss: 2.5440381641569436

Epoch: 5| Step: 4
Training loss: 2.897939140475841
Validation loss: 2.534129046036222

Epoch: 5| Step: 5
Training loss: 1.9106357963239509
Validation loss: 2.537821772660922

Epoch: 5| Step: 6
Training loss: 2.0172311460469845
Validation loss: 2.5443172932776905

Epoch: 5| Step: 7
Training loss: 2.0731534071900355
Validation loss: 2.51362606978635

Epoch: 5| Step: 8
Training loss: 3.253097965007215
Validation loss: 2.5954119105692692

Epoch: 5| Step: 9
Training loss: 1.7653217561640866
Validation loss: 2.4998931677357232

Epoch: 5| Step: 10
Training loss: 3.097370841694847
Validation loss: 2.488618103326468

Epoch: 264| Step: 0
Training loss: 2.6265491274210166
Validation loss: 2.529058370788674

Epoch: 5| Step: 1
Training loss: 3.1574209326285243
Validation loss: 2.534658974587814

Epoch: 5| Step: 2
Training loss: 1.4325881466200618
Validation loss: 2.56390881472339

Epoch: 5| Step: 3
Training loss: 3.035687845579776
Validation loss: 2.502094005050238

Epoch: 5| Step: 4
Training loss: 2.5817579162245794
Validation loss: 2.53188476138751

Epoch: 5| Step: 5
Training loss: 2.969985544188798
Validation loss: 2.5126113404098747

Epoch: 5| Step: 6
Training loss: 2.303181819137255
Validation loss: 2.5432071875500957

Epoch: 5| Step: 7
Training loss: 2.6219159128773026
Validation loss: 2.5514164133364408

Epoch: 5| Step: 8
Training loss: 2.3371100401331475
Validation loss: 2.5461364073742874

Epoch: 5| Step: 9
Training loss: 2.3284640481346948
Validation loss: 2.5101851959667028

Epoch: 5| Step: 10
Training loss: 2.37808539098169
Validation loss: 2.551880100404448

Epoch: 265| Step: 0
Training loss: 2.128081835786558
Validation loss: 2.5208904660234768

Epoch: 5| Step: 1
Training loss: 2.576695086119717
Validation loss: 2.517345352057518

Epoch: 5| Step: 2
Training loss: 2.5034177306569276
Validation loss: 2.536673638508046

Epoch: 5| Step: 3
Training loss: 2.0881161402925703
Validation loss: 2.5135260158623556

Epoch: 5| Step: 4
Training loss: 2.2012513763232806
Validation loss: 2.5589123193549543

Epoch: 5| Step: 5
Training loss: 2.534150145841781
Validation loss: 2.537910181517199

Epoch: 5| Step: 6
Training loss: 2.5594175453981505
Validation loss: 2.5536532423814324

Epoch: 5| Step: 7
Training loss: 2.6951160304016377
Validation loss: 2.5217730273206915

Epoch: 5| Step: 8
Training loss: 2.6886368941703243
Validation loss: 2.5434981168889967

Epoch: 5| Step: 9
Training loss: 2.8942249872241037
Validation loss: 2.517108154675863

Epoch: 5| Step: 10
Training loss: 3.0197242191533338
Validation loss: 2.5296495706185604

Epoch: 266| Step: 0
Training loss: 1.8836423699121527
Validation loss: 2.519672315253818

Epoch: 5| Step: 1
Training loss: 2.0863698447005343
Validation loss: 2.532396806971258

Epoch: 5| Step: 2
Training loss: 2.4958666010079256
Validation loss: 2.5302398947025377

Epoch: 5| Step: 3
Training loss: 2.1610622058337214
Validation loss: 2.508887071626672

Epoch: 5| Step: 4
Training loss: 2.434121455846093
Validation loss: 2.523412199105306

Epoch: 5| Step: 5
Training loss: 2.7535218315471264
Validation loss: 2.529183206725284

Epoch: 5| Step: 6
Training loss: 2.7529170430824372
Validation loss: 2.533560314949916

Epoch: 5| Step: 7
Training loss: 2.90222430593089
Validation loss: 2.5356918631991494

Epoch: 5| Step: 8
Training loss: 3.569787719356305
Validation loss: 2.5344253801877814

Epoch: 5| Step: 9
Training loss: 1.9959526236077463
Validation loss: 2.4954285047847478

Epoch: 5| Step: 10
Training loss: 2.552808438009616
Validation loss: 2.5422626778633997

Epoch: 267| Step: 0
Training loss: 2.8116890479890886
Validation loss: 2.521063265733479

Epoch: 5| Step: 1
Training loss: 2.641209464315015
Validation loss: 2.52474792929592

Epoch: 5| Step: 2
Training loss: 3.0042219653355167
Validation loss: 2.544754098467464

Epoch: 5| Step: 3
Training loss: 3.026940497336223
Validation loss: 2.495085383207045

Epoch: 5| Step: 4
Training loss: 2.4836088714667257
Validation loss: 2.5201499709008437

Epoch: 5| Step: 5
Training loss: 2.6269303444934655
Validation loss: 2.5210697351708067

Epoch: 5| Step: 6
Training loss: 1.8525142235666763
Validation loss: 2.506872393015698

Epoch: 5| Step: 7
Training loss: 2.475967383858559
Validation loss: 2.5200038976927273

Epoch: 5| Step: 8
Training loss: 2.576437658057997
Validation loss: 2.508705161994561

Epoch: 5| Step: 9
Training loss: 1.9086178940859273
Validation loss: 2.527059996823767

Epoch: 5| Step: 10
Training loss: 2.113158612033322
Validation loss: 2.556764081123841

Epoch: 268| Step: 0
Training loss: 2.623308454183109
Validation loss: 2.5100009771313356

Epoch: 5| Step: 1
Training loss: 2.851751117477673
Validation loss: 2.5463642811413187

Epoch: 5| Step: 2
Training loss: 2.891717817603493
Validation loss: 2.532172144602063

Epoch: 5| Step: 3
Training loss: 2.661150551833076
Validation loss: 2.5247267590368696

Epoch: 5| Step: 4
Training loss: 3.07543704020766
Validation loss: 2.555011952571341

Epoch: 5| Step: 5
Training loss: 2.1381478560362615
Validation loss: 2.517042165053041

Epoch: 5| Step: 6
Training loss: 2.3327852241163285
Validation loss: 2.5322040768693066

Epoch: 5| Step: 7
Training loss: 2.111142696456648
Validation loss: 2.5206331990411903

Epoch: 5| Step: 8
Training loss: 2.301189289276431
Validation loss: 2.5442713546051268

Epoch: 5| Step: 9
Training loss: 2.2609055396235247
Validation loss: 2.5263811058244108

Epoch: 5| Step: 10
Training loss: 2.379054974697272
Validation loss: 2.556575965632446

Epoch: 269| Step: 0
Training loss: 2.3422273903491537
Validation loss: 2.5522983087970115

Epoch: 5| Step: 1
Training loss: 2.4006118113804256
Validation loss: 2.5190304117753026

Epoch: 5| Step: 2
Training loss: 2.9264555740986147
Validation loss: 2.5317089262840424

Epoch: 5| Step: 3
Training loss: 2.9719462004113817
Validation loss: 2.501065423012301

Epoch: 5| Step: 4
Training loss: 2.4466646496230013
Validation loss: 2.5450326993028494

Epoch: 5| Step: 5
Training loss: 1.903751048578085
Validation loss: 2.5332177607247877

Epoch: 5| Step: 6
Training loss: 1.945180926794965
Validation loss: 2.5181033868938023

Epoch: 5| Step: 7
Training loss: 3.0114253553082238
Validation loss: 2.5228798116453466

Epoch: 5| Step: 8
Training loss: 2.459773972369759
Validation loss: 2.538120269495844

Epoch: 5| Step: 9
Training loss: 2.543083504755242
Validation loss: 2.5111233756392966

Epoch: 5| Step: 10
Training loss: 2.8366661291723343
Validation loss: 2.5484619199410554

Epoch: 270| Step: 0
Training loss: 2.583334266498356
Validation loss: 2.5153245694916286

Epoch: 5| Step: 1
Training loss: 2.7064345721939955
Validation loss: 2.5205845504546898

Epoch: 5| Step: 2
Training loss: 2.843487905643957
Validation loss: 2.5122418041531924

Epoch: 5| Step: 3
Training loss: 3.1454482358627662
Validation loss: 2.4967216223123154

Epoch: 5| Step: 4
Training loss: 2.4854778025404665
Validation loss: 2.528263142204046

Epoch: 5| Step: 5
Training loss: 2.3467792516851977
Validation loss: 2.5370330358250026

Epoch: 5| Step: 6
Training loss: 2.0978504851078954
Validation loss: 2.5139201114395404

Epoch: 5| Step: 7
Training loss: 1.8830859986782251
Validation loss: 2.5264487267235785

Epoch: 5| Step: 8
Training loss: 2.37858170924835
Validation loss: 2.5430172828849416

Epoch: 5| Step: 9
Training loss: 2.1984402242398278
Validation loss: 2.525946730891321

Epoch: 5| Step: 10
Training loss: 2.8427650506575013
Validation loss: 2.505601405203453

Epoch: 271| Step: 0
Training loss: 2.44515473950715
Validation loss: 2.506429847772261

Epoch: 5| Step: 1
Training loss: 3.3901387933996423
Validation loss: 2.510838217188798

Epoch: 5| Step: 2
Training loss: 2.5340199326451573
Validation loss: 2.555099803754965

Epoch: 5| Step: 3
Training loss: 2.5405653980578973
Validation loss: 2.5245079737268745

Epoch: 5| Step: 4
Training loss: 2.6372036410336244
Validation loss: 2.541907382972637

Epoch: 5| Step: 5
Training loss: 2.568488311432064
Validation loss: 2.5132798264763343

Epoch: 5| Step: 6
Training loss: 1.7849806150350835
Validation loss: 2.4939080719656053

Epoch: 5| Step: 7
Training loss: 2.02991606120688
Validation loss: 2.5478191033837487

Epoch: 5| Step: 8
Training loss: 2.4600377935902396
Validation loss: 2.532641013556126

Epoch: 5| Step: 9
Training loss: 2.6469422567170384
Validation loss: 2.5465280988737353

Epoch: 5| Step: 10
Training loss: 2.541970521451319
Validation loss: 2.486805142176475

Epoch: 272| Step: 0
Training loss: 2.371670447011666
Validation loss: 2.5295210858314054

Epoch: 5| Step: 1
Training loss: 1.8186964466298743
Validation loss: 2.5205488160499283

Epoch: 5| Step: 2
Training loss: 2.472828839442238
Validation loss: 2.5200939651848593

Epoch: 5| Step: 3
Training loss: 2.736143837059086
Validation loss: 2.5190214213154487

Epoch: 5| Step: 4
Training loss: 2.6122517518001045
Validation loss: 2.548843382108955

Epoch: 5| Step: 5
Training loss: 2.7821372684973182
Validation loss: 2.532368778223597

Epoch: 5| Step: 6
Training loss: 3.088221996075719
Validation loss: 2.504374696284356

Epoch: 5| Step: 7
Training loss: 2.3187113673855397
Validation loss: 2.5318550268932234

Epoch: 5| Step: 8
Training loss: 2.009741187062914
Validation loss: 2.4943146663333655

Epoch: 5| Step: 9
Training loss: 2.264892670233674
Validation loss: 2.5083942261898002

Epoch: 5| Step: 10
Training loss: 3.127289353068395
Validation loss: 2.514493997436688

Epoch: 273| Step: 0
Training loss: 2.633268594147228
Validation loss: 2.510371019917442

Epoch: 5| Step: 1
Training loss: 3.0845496038392577
Validation loss: 2.5010150407501888

Epoch: 5| Step: 2
Training loss: 2.5286724011917454
Validation loss: 2.520412371952263

Epoch: 5| Step: 3
Training loss: 2.5222641428387313
Validation loss: 2.503657544445577

Epoch: 5| Step: 4
Training loss: 2.2589102323747277
Validation loss: 2.5066891937786164

Epoch: 5| Step: 5
Training loss: 2.48938270515937
Validation loss: 2.5420790273409515

Epoch: 5| Step: 6
Training loss: 2.519210250869494
Validation loss: 2.56136840077405

Epoch: 5| Step: 7
Training loss: 2.7484334471793597
Validation loss: 2.5218848988889633

Epoch: 5| Step: 8
Training loss: 2.6320285252471796
Validation loss: 2.545281486529016

Epoch: 5| Step: 9
Training loss: 2.220321313785506
Validation loss: 2.521969544166041

Epoch: 5| Step: 10
Training loss: 1.805765267770362
Validation loss: 2.545460933835067

Epoch: 274| Step: 0
Training loss: 2.5228352019820606
Validation loss: 2.537716969023355

Epoch: 5| Step: 1
Training loss: 2.2951045056985175
Validation loss: 2.5344522563274596

Epoch: 5| Step: 2
Training loss: 3.2770491093624674
Validation loss: 2.5271271237808044

Epoch: 5| Step: 3
Training loss: 2.3992653079842845
Validation loss: 2.5464343242041276

Epoch: 5| Step: 4
Training loss: 2.20057194817822
Validation loss: 2.5545576221308606

Epoch: 5| Step: 5
Training loss: 2.5406003080437376
Validation loss: 2.538321228109441

Epoch: 5| Step: 6
Training loss: 2.8725195426043744
Validation loss: 2.5189459761094923

Epoch: 5| Step: 7
Training loss: 2.520021281530951
Validation loss: 2.4929527187771297

Epoch: 5| Step: 8
Training loss: 1.9648999306627069
Validation loss: 2.5162933874130022

Epoch: 5| Step: 9
Training loss: 2.4318562239699286
Validation loss: 2.534057078565792

Epoch: 5| Step: 10
Training loss: 2.4860319933624115
Validation loss: 2.5352153964891433

Epoch: 275| Step: 0
Training loss: 2.6482958010236013
Validation loss: 2.561228103585336

Epoch: 5| Step: 1
Training loss: 1.774463306472195
Validation loss: 2.542550787694794

Epoch: 5| Step: 2
Training loss: 2.9884982560095086
Validation loss: 2.563542817884184

Epoch: 5| Step: 3
Training loss: 2.5083030149670074
Validation loss: 2.513876239183749

Epoch: 5| Step: 4
Training loss: 2.425406557590808
Validation loss: 2.560684259111778

Epoch: 5| Step: 5
Training loss: 2.7046705469121344
Validation loss: 2.5485379947333358

Epoch: 5| Step: 6
Training loss: 2.8353876258620447
Validation loss: 2.5253770930652215

Epoch: 5| Step: 7
Training loss: 2.2249905575326876
Validation loss: 2.5430478616955488

Epoch: 5| Step: 8
Training loss: 2.8161233238720826
Validation loss: 2.524250154280875

Epoch: 5| Step: 9
Training loss: 2.3437330118199426
Validation loss: 2.525340929045136

Epoch: 5| Step: 10
Training loss: 2.168037592164305
Validation loss: 2.535381638868066

Epoch: 276| Step: 0
Training loss: 1.7068262042540594
Validation loss: 2.5171582677859945

Epoch: 5| Step: 1
Training loss: 2.119729855797681
Validation loss: 2.5327315932483683

Epoch: 5| Step: 2
Training loss: 2.4556700509575635
Validation loss: 2.5195546698493323

Epoch: 5| Step: 3
Training loss: 2.6387171427290714
Validation loss: 2.526807868806022

Epoch: 5| Step: 4
Training loss: 2.0559980614946185
Validation loss: 2.5227015013850367

Epoch: 5| Step: 5
Training loss: 2.6640365465080187
Validation loss: 2.507261772714926

Epoch: 5| Step: 6
Training loss: 2.4285544547120934
Validation loss: 2.506192235654589

Epoch: 5| Step: 7
Training loss: 3.191199754737047
Validation loss: 2.5279381670441543

Epoch: 5| Step: 8
Training loss: 2.4671522354205218
Validation loss: 2.5328873858591443

Epoch: 5| Step: 9
Training loss: 2.635125658053925
Validation loss: 2.557889713332652

Epoch: 5| Step: 10
Training loss: 2.749916595581366
Validation loss: 2.5330124095862487

Epoch: 277| Step: 0
Training loss: 2.2702134595824006
Validation loss: 2.5255179820278753

Epoch: 5| Step: 1
Training loss: 2.6898265792648743
Validation loss: 2.5063825805095252

Epoch: 5| Step: 2
Training loss: 2.2895621838227735
Validation loss: 2.504482428562725

Epoch: 5| Step: 3
Training loss: 2.9352664978587804
Validation loss: 2.536681390034822

Epoch: 5| Step: 4
Training loss: 2.7453509827957867
Validation loss: 2.487177045608421

Epoch: 5| Step: 5
Training loss: 2.2997618717816493
Validation loss: 2.5072031968620774

Epoch: 5| Step: 6
Training loss: 2.90435877145921
Validation loss: 2.509696296113236

Epoch: 5| Step: 7
Training loss: 2.224357610829084
Validation loss: 2.523337633950691

Epoch: 5| Step: 8
Training loss: 2.6385695002057026
Validation loss: 2.5389558432416726

Epoch: 5| Step: 9
Training loss: 2.3219639716166656
Validation loss: 2.5151862391953776

Epoch: 5| Step: 10
Training loss: 1.910016762724812
Validation loss: 2.5263377726611416

Epoch: 278| Step: 0
Training loss: 2.64282135221188
Validation loss: 2.516860093124769

Epoch: 5| Step: 1
Training loss: 2.3374522729580884
Validation loss: 2.4937436996710574

Epoch: 5| Step: 2
Training loss: 2.784282017449319
Validation loss: 2.509537716360236

Epoch: 5| Step: 3
Training loss: 2.2734543842331036
Validation loss: 2.510424707932823

Epoch: 5| Step: 4
Training loss: 2.4800336801641496
Validation loss: 2.5252957496865323

Epoch: 5| Step: 5
Training loss: 2.8075183406512703
Validation loss: 2.519325898547717

Epoch: 5| Step: 6
Training loss: 2.4843592613249506
Validation loss: 2.52908668355781

Epoch: 5| Step: 7
Training loss: 1.8908348282322944
Validation loss: 2.5258966632441977

Epoch: 5| Step: 8
Training loss: 2.507628627227781
Validation loss: 2.492506781037382

Epoch: 5| Step: 9
Training loss: 2.5106452320775547
Validation loss: 2.539879958811047

Epoch: 5| Step: 10
Training loss: 3.0684305494808424
Validation loss: 2.5017604013500367

Epoch: 279| Step: 0
Training loss: 2.040003533827769
Validation loss: 2.507863392781658

Epoch: 5| Step: 1
Training loss: 2.9220194245410176
Validation loss: 2.5074134273758752

Epoch: 5| Step: 2
Training loss: 2.266727363484337
Validation loss: 2.5033779764766924

Epoch: 5| Step: 3
Training loss: 2.1896104713546354
Validation loss: 2.528702798634742

Epoch: 5| Step: 4
Training loss: 2.4100096424590514
Validation loss: 2.5309420511868708

Epoch: 5| Step: 5
Training loss: 2.333609655458224
Validation loss: 2.5213191487103486

Epoch: 5| Step: 6
Training loss: 2.7488270772455192
Validation loss: 2.531089445932182

Epoch: 5| Step: 7
Training loss: 3.032920141339075
Validation loss: 2.4978851181578188

Epoch: 5| Step: 8
Training loss: 2.3066549922485984
Validation loss: 2.538674529088308

Epoch: 5| Step: 9
Training loss: 3.212608181211766
Validation loss: 2.50688153953631

Epoch: 5| Step: 10
Training loss: 1.6911406627874004
Validation loss: 2.5097076377395933

Epoch: 280| Step: 0
Training loss: 3.111562601510713
Validation loss: 2.520777449924467

Epoch: 5| Step: 1
Training loss: 2.492402548764523
Validation loss: 2.539550596009336

Epoch: 5| Step: 2
Training loss: 2.3076183239986094
Validation loss: 2.5345158202476665

Epoch: 5| Step: 3
Training loss: 2.2447473885992264
Validation loss: 2.517897213399867

Epoch: 5| Step: 4
Training loss: 2.299475825744022
Validation loss: 2.5208708996686853

Epoch: 5| Step: 5
Training loss: 2.614561853409235
Validation loss: 2.517544113671906

Epoch: 5| Step: 6
Training loss: 1.7729913091430405
Validation loss: 2.515140427415022

Epoch: 5| Step: 7
Training loss: 2.3372397986848497
Validation loss: 2.5243191850125237

Epoch: 5| Step: 8
Training loss: 2.456203885409939
Validation loss: 2.5086840127056593

Epoch: 5| Step: 9
Training loss: 2.8015784242414656
Validation loss: 2.525140811564062

Epoch: 5| Step: 10
Training loss: 2.8532596084244433
Validation loss: 2.495511586024635

Epoch: 281| Step: 0
Training loss: 2.939687198684724
Validation loss: 2.512666099811537

Epoch: 5| Step: 1
Training loss: 2.8252685621769293
Validation loss: 2.53796251726037

Epoch: 5| Step: 2
Training loss: 2.4392655654039954
Validation loss: 2.5115266619081575

Epoch: 5| Step: 3
Training loss: 2.441375683402402
Validation loss: 2.4978740184339467

Epoch: 5| Step: 4
Training loss: 2.0876500104353117
Validation loss: 2.515602249799818

Epoch: 5| Step: 5
Training loss: 2.4035615641052024
Validation loss: 2.5258947835694303

Epoch: 5| Step: 6
Training loss: 2.6081794238359803
Validation loss: 2.5110246472454354

Epoch: 5| Step: 7
Training loss: 2.3387131546682065
Validation loss: 2.512845538936658

Epoch: 5| Step: 8
Training loss: 2.504395721250814
Validation loss: 2.4927957559972476

Epoch: 5| Step: 9
Training loss: 2.4846076136678974
Validation loss: 2.520899896260716

Epoch: 5| Step: 10
Training loss: 2.348740617653144
Validation loss: 2.509793397234182

Epoch: 282| Step: 0
Training loss: 2.2858863642325544
Validation loss: 2.5150008887568664

Epoch: 5| Step: 1
Training loss: 2.1180699610263694
Validation loss: 2.512603911016962

Epoch: 5| Step: 2
Training loss: 2.6103236135919587
Validation loss: 2.4883273455863697

Epoch: 5| Step: 3
Training loss: 2.4701295692035194
Validation loss: 2.5221478655160485

Epoch: 5| Step: 4
Training loss: 3.0783967028919417
Validation loss: 2.5285602956128157

Epoch: 5| Step: 5
Training loss: 2.4250616950612858
Validation loss: 2.5688448484513935

Epoch: 5| Step: 6
Training loss: 2.304368129520298
Validation loss: 2.4970947694597587

Epoch: 5| Step: 7
Training loss: 2.7547497046117626
Validation loss: 2.5142839460106345

Epoch: 5| Step: 8
Training loss: 2.2823499353220145
Validation loss: 2.4975538130142376

Epoch: 5| Step: 9
Training loss: 2.992414421062499
Validation loss: 2.5095296429671166

Epoch: 5| Step: 10
Training loss: 2.140678906284072
Validation loss: 2.560354723914049

Epoch: 283| Step: 0
Training loss: 2.569601685349326
Validation loss: 2.5505866344087385

Epoch: 5| Step: 1
Training loss: 2.0992586916441143
Validation loss: 2.536573161620269

Epoch: 5| Step: 2
Training loss: 2.1549962033550334
Validation loss: 2.5404057566608627

Epoch: 5| Step: 3
Training loss: 3.017355308824664
Validation loss: 2.5384271884205876

Epoch: 5| Step: 4
Training loss: 2.5767853926743496
Validation loss: 2.5380009489141475

Epoch: 5| Step: 5
Training loss: 2.619468310038303
Validation loss: 2.542006120114937

Epoch: 5| Step: 6
Training loss: 2.568943296710273
Validation loss: 2.5166677236062434

Epoch: 5| Step: 7
Training loss: 2.738193918704788
Validation loss: 2.5325942233403937

Epoch: 5| Step: 8
Training loss: 2.3949977799885125
Validation loss: 2.485035203946931

Epoch: 5| Step: 9
Training loss: 2.646750123592297
Validation loss: 2.5327745538514486

Epoch: 5| Step: 10
Training loss: 1.987605251668286
Validation loss: 2.511010340576401

Epoch: 284| Step: 0
Training loss: 2.7828762143534274
Validation loss: 2.5438629987985806

Epoch: 5| Step: 1
Training loss: 2.841903254559586
Validation loss: 2.549076743442127

Epoch: 5| Step: 2
Training loss: 2.082375446088942
Validation loss: 2.510357831850074

Epoch: 5| Step: 3
Training loss: 2.494999175444258
Validation loss: 2.5147342642467883

Epoch: 5| Step: 4
Training loss: 2.4180679424196736
Validation loss: 2.5402788528608786

Epoch: 5| Step: 5
Training loss: 2.4594119232779645
Validation loss: 2.5395378855399833

Epoch: 5| Step: 6
Training loss: 2.335257849169323
Validation loss: 2.513860727020735

Epoch: 5| Step: 7
Training loss: 2.9608610037632563
Validation loss: 2.5308356767004234

Epoch: 5| Step: 8
Training loss: 2.145218091250188
Validation loss: 2.5105718041284626

Epoch: 5| Step: 9
Training loss: 2.0888391094273495
Validation loss: 2.506055402725705

Epoch: 5| Step: 10
Training loss: 2.742533517943928
Validation loss: 2.5439346666908285

Epoch: 285| Step: 0
Training loss: 2.2916576269722873
Validation loss: 2.5291923333787323

Epoch: 5| Step: 1
Training loss: 2.3707744754218543
Validation loss: 2.502503652947469

Epoch: 5| Step: 2
Training loss: 2.4515404899003004
Validation loss: 2.5425780054578557

Epoch: 5| Step: 3
Training loss: 2.544336282336662
Validation loss: 2.5012998606172134

Epoch: 5| Step: 4
Training loss: 2.274551276084547
Validation loss: 2.519119174815139

Epoch: 5| Step: 5
Training loss: 2.719512043408145
Validation loss: 2.5000804457488384

Epoch: 5| Step: 6
Training loss: 2.0845249836962396
Validation loss: 2.5203585508526642

Epoch: 5| Step: 7
Training loss: 2.0111886816830684
Validation loss: 2.5113516558929434

Epoch: 5| Step: 8
Training loss: 2.648595664553805
Validation loss: 2.5161971573577206

Epoch: 5| Step: 9
Training loss: 2.7211635005662496
Validation loss: 2.523696885574901

Epoch: 5| Step: 10
Training loss: 3.1895242882829526
Validation loss: 2.5066208056190833

Epoch: 286| Step: 0
Training loss: 2.4444513236536656
Validation loss: 2.4936804313988334

Epoch: 5| Step: 1
Training loss: 2.2283001460378986
Validation loss: 2.50579247529116

Epoch: 5| Step: 2
Training loss: 2.2911360270804924
Validation loss: 2.542101490167092

Epoch: 5| Step: 3
Training loss: 2.4961259389124133
Validation loss: 2.5503480966823946

Epoch: 5| Step: 4
Training loss: 2.1935669825361064
Validation loss: 2.5177808698241364

Epoch: 5| Step: 5
Training loss: 2.455852571517139
Validation loss: 2.5373798993988084

Epoch: 5| Step: 6
Training loss: 2.902599872773259
Validation loss: 2.4914505139813357

Epoch: 5| Step: 7
Training loss: 2.3789720948977107
Validation loss: 2.498530383026138

Epoch: 5| Step: 8
Training loss: 2.518475073110348
Validation loss: 2.534932042040834

Epoch: 5| Step: 9
Training loss: 2.824113814856603
Validation loss: 2.537799241648235

Epoch: 5| Step: 10
Training loss: 2.416136113387304
Validation loss: 2.5361893464854086

Epoch: 287| Step: 0
Training loss: 2.527684467349414
Validation loss: 2.532936876062386

Epoch: 5| Step: 1
Training loss: 2.478623263860236
Validation loss: 2.5072134955628296

Epoch: 5| Step: 2
Training loss: 2.6543162768330637
Validation loss: 2.5301450816647044

Epoch: 5| Step: 3
Training loss: 2.8840848601153675
Validation loss: 2.5397376599888317

Epoch: 5| Step: 4
Training loss: 1.8501666638332777
Validation loss: 2.542833408099668

Epoch: 5| Step: 5
Training loss: 3.1022199803459327
Validation loss: 2.522812607242322

Epoch: 5| Step: 6
Training loss: 2.664612744019671
Validation loss: 2.5254859139586423

Epoch: 5| Step: 7
Training loss: 2.15508470975068
Validation loss: 2.5086358007349667

Epoch: 5| Step: 8
Training loss: 2.261067825494586
Validation loss: 2.5618916481462874

Epoch: 5| Step: 9
Training loss: 1.780379768749263
Validation loss: 2.501156061274924

Epoch: 5| Step: 10
Training loss: 2.6935921748895963
Validation loss: 2.527488067229742

Epoch: 288| Step: 0
Training loss: 2.6844213728179933
Validation loss: 2.513054531831468

Epoch: 5| Step: 1
Training loss: 2.6115257856018816
Validation loss: 2.5268019101894645

Epoch: 5| Step: 2
Training loss: 1.904001567146714
Validation loss: 2.524566945218237

Epoch: 5| Step: 3
Training loss: 2.5094738742451175
Validation loss: 2.5249080088470737

Epoch: 5| Step: 4
Training loss: 2.7371830064318794
Validation loss: 2.5126106874118586

Epoch: 5| Step: 5
Training loss: 2.3984190104514314
Validation loss: 2.5337795155555867

Epoch: 5| Step: 6
Training loss: 2.930952526362717
Validation loss: 2.503857884600558

Epoch: 5| Step: 7
Training loss: 2.850982690679
Validation loss: 2.5285081383731285

Epoch: 5| Step: 8
Training loss: 1.7688604771919143
Validation loss: 2.5454681212938577

Epoch: 5| Step: 9
Training loss: 2.6219234603022548
Validation loss: 2.5134153052765886

Epoch: 5| Step: 10
Training loss: 1.9615968606348104
Validation loss: 2.498968184232256

Epoch: 289| Step: 0
Training loss: 2.3535781282221655
Validation loss: 2.5132817920885673

Epoch: 5| Step: 1
Training loss: 2.5221189473089884
Validation loss: 2.52945649538202

Epoch: 5| Step: 2
Training loss: 2.954358363063348
Validation loss: 2.5379188353584734

Epoch: 5| Step: 3
Training loss: 2.297768555263373
Validation loss: 2.560592805016169

Epoch: 5| Step: 4
Training loss: 2.1605252990583317
Validation loss: 2.551152162689592

Epoch: 5| Step: 5
Training loss: 2.5029213049752337
Validation loss: 2.4960069207497027

Epoch: 5| Step: 6
Training loss: 2.5627748295481076
Validation loss: 2.514753592905224

Epoch: 5| Step: 7
Training loss: 2.5925169622147775
Validation loss: 2.493180477715104

Epoch: 5| Step: 8
Training loss: 2.321834078154749
Validation loss: 2.4693536463053745

Epoch: 5| Step: 9
Training loss: 2.28665847307893
Validation loss: 2.5178472941980936

Epoch: 5| Step: 10
Training loss: 2.7532142581391486
Validation loss: 2.491655364219961

Epoch: 290| Step: 0
Training loss: 2.199789132502938
Validation loss: 2.50876167027039

Epoch: 5| Step: 1
Training loss: 3.071532960556211
Validation loss: 2.5689841009855505

Epoch: 5| Step: 2
Training loss: 2.7673570629448174
Validation loss: 2.474536187258635

Epoch: 5| Step: 3
Training loss: 2.268240635547629
Validation loss: 2.49701517001635

Epoch: 5| Step: 4
Training loss: 2.016131906568878
Validation loss: 2.5039230493807425

Epoch: 5| Step: 5
Training loss: 2.0729565129372127
Validation loss: 2.5125112624026427

Epoch: 5| Step: 6
Training loss: 2.6450449953055926
Validation loss: 2.525949192077631

Epoch: 5| Step: 7
Training loss: 2.7282704247935583
Validation loss: 2.492469749241192

Epoch: 5| Step: 8
Training loss: 2.51664741542434
Validation loss: 2.53833855921893

Epoch: 5| Step: 9
Training loss: 2.4925567449081103
Validation loss: 2.511467977288976

Epoch: 5| Step: 10
Training loss: 2.5344301191890724
Validation loss: 2.492060417849028

Epoch: 291| Step: 0
Training loss: 2.4757695899272307
Validation loss: 2.4810983507502242

Epoch: 5| Step: 1
Training loss: 2.4309648792800798
Validation loss: 2.5108098936624743

Epoch: 5| Step: 2
Training loss: 3.0828992349030675
Validation loss: 2.508206082799298

Epoch: 5| Step: 3
Training loss: 2.3143184831946733
Validation loss: 2.5115713718877517

Epoch: 5| Step: 4
Training loss: 2.400413072324354
Validation loss: 2.5007265727129386

Epoch: 5| Step: 5
Training loss: 2.233570514249878
Validation loss: 2.5248518893544953

Epoch: 5| Step: 6
Training loss: 2.145321781946872
Validation loss: 2.5034797385445327

Epoch: 5| Step: 7
Training loss: 2.408292423767609
Validation loss: 2.5388744239786707

Epoch: 5| Step: 8
Training loss: 2.3272664580071063
Validation loss: 2.515243901907879

Epoch: 5| Step: 9
Training loss: 2.7751642573775746
Validation loss: 2.5395612531301053

Epoch: 5| Step: 10
Training loss: 2.8625523991348993
Validation loss: 2.5315129472547206

Epoch: 292| Step: 0
Training loss: 2.261649386012665
Validation loss: 2.5102031676378145

Epoch: 5| Step: 1
Training loss: 2.8455700761407265
Validation loss: 2.5027119724280493

Epoch: 5| Step: 2
Training loss: 2.1592386514591384
Validation loss: 2.5076364634259143

Epoch: 5| Step: 3
Training loss: 2.8612537986293525
Validation loss: 2.5236383803449765

Epoch: 5| Step: 4
Training loss: 2.493089662092485
Validation loss: 2.5263304257476014

Epoch: 5| Step: 5
Training loss: 2.619665356442008
Validation loss: 2.525281277177468

Epoch: 5| Step: 6
Training loss: 1.7101907014753055
Validation loss: 2.5118387842659646

Epoch: 5| Step: 7
Training loss: 2.5372409331268435
Validation loss: 2.5303627686376773

Epoch: 5| Step: 8
Training loss: 2.546654814581063
Validation loss: 2.529819081543498

Epoch: 5| Step: 9
Training loss: 2.3893428181469933
Validation loss: 2.520519039874098

Epoch: 5| Step: 10
Training loss: 2.6874485454512875
Validation loss: 2.513555026810704

Epoch: 293| Step: 0
Training loss: 2.198197610505653
Validation loss: 2.5338728054933415

Epoch: 5| Step: 1
Training loss: 2.1686952215746707
Validation loss: 2.5626581424843766

Epoch: 5| Step: 2
Training loss: 2.4882919814329028
Validation loss: 2.568821800136068

Epoch: 5| Step: 3
Training loss: 2.155381400926405
Validation loss: 2.548081273463464

Epoch: 5| Step: 4
Training loss: 3.2963227446785246
Validation loss: 2.5222812376881563

Epoch: 5| Step: 5
Training loss: 2.790159207522093
Validation loss: 2.531215896893239

Epoch: 5| Step: 6
Training loss: 2.026039483102702
Validation loss: 2.5480685285799693

Epoch: 5| Step: 7
Training loss: 2.7643140713943555
Validation loss: 2.511229475953424

Epoch: 5| Step: 8
Training loss: 2.212125856478587
Validation loss: 2.51974961483301

Epoch: 5| Step: 9
Training loss: 2.7919838189036
Validation loss: 2.542085441277614

Epoch: 5| Step: 10
Training loss: 2.2153489816583454
Validation loss: 2.520627102763256

Epoch: 294| Step: 0
Training loss: 2.0698336065375984
Validation loss: 2.496520802421576

Epoch: 5| Step: 1
Training loss: 2.200822858379857
Validation loss: 2.4976580325188253

Epoch: 5| Step: 2
Training loss: 2.155552295090262
Validation loss: 2.505895083578096

Epoch: 5| Step: 3
Training loss: 3.123269173998929
Validation loss: 2.4702184465426877

Epoch: 5| Step: 4
Training loss: 2.235915393306869
Validation loss: 2.522033496897881

Epoch: 5| Step: 5
Training loss: 2.5278473577832403
Validation loss: 2.5071573941255

Epoch: 5| Step: 6
Training loss: 2.2171602598295266
Validation loss: 2.5272328829092006

Epoch: 5| Step: 7
Training loss: 2.6947140236546447
Validation loss: 2.5294977886630217

Epoch: 5| Step: 8
Training loss: 2.677462797075642
Validation loss: 2.512801087301827

Epoch: 5| Step: 9
Training loss: 2.779755951851793
Validation loss: 2.545421514495229

Epoch: 5| Step: 10
Training loss: 2.4060280437404766
Validation loss: 2.509457341882103

Epoch: 295| Step: 0
Training loss: 3.0187396631794425
Validation loss: 2.487047654089235

Epoch: 5| Step: 1
Training loss: 2.3317251339994822
Validation loss: 2.489373819256589

Epoch: 5| Step: 2
Training loss: 2.709171918498025
Validation loss: 2.529603944256999

Epoch: 5| Step: 3
Training loss: 2.57452946409967
Validation loss: 2.550873478967399

Epoch: 5| Step: 4
Training loss: 2.7653325675195695
Validation loss: 2.5100510975732373

Epoch: 5| Step: 5
Training loss: 2.0896354214941857
Validation loss: 2.5200927220686613

Epoch: 5| Step: 6
Training loss: 2.223002160480117
Validation loss: 2.5439410809876604

Epoch: 5| Step: 7
Training loss: 1.881162878408116
Validation loss: 2.504718345697957

Epoch: 5| Step: 8
Training loss: 2.08623751079462
Validation loss: 2.516377151054655

Epoch: 5| Step: 9
Training loss: 2.5347369630521412
Validation loss: 2.5168721501086275

Epoch: 5| Step: 10
Training loss: 2.7758801516460485
Validation loss: 2.5275378325392124

Epoch: 296| Step: 0
Training loss: 2.649659811532757
Validation loss: 2.5339396993618006

Epoch: 5| Step: 1
Training loss: 2.861216634719982
Validation loss: 2.511273770122321

Epoch: 5| Step: 2
Training loss: 2.5162934546548748
Validation loss: 2.5079310938631516

Epoch: 5| Step: 3
Training loss: 2.2586034953363345
Validation loss: 2.5348527316672183

Epoch: 5| Step: 4
Training loss: 2.218380964509642
Validation loss: 2.550883651607863

Epoch: 5| Step: 5
Training loss: 2.7060501060047826
Validation loss: 2.508259599451131

Epoch: 5| Step: 6
Training loss: 2.4661405268568064
Validation loss: 2.5246699805592936

Epoch: 5| Step: 7
Training loss: 2.312281417826258
Validation loss: 2.4942319697177213

Epoch: 5| Step: 8
Training loss: 2.3448867075867943
Validation loss: 2.521394691687533

Epoch: 5| Step: 9
Training loss: 2.499976253396741
Validation loss: 2.5654046878242798

Epoch: 5| Step: 10
Training loss: 2.4324813431184094
Validation loss: 2.5267047156920595

Epoch: 297| Step: 0
Training loss: 2.324402361116669
Validation loss: 2.551384060865832

Epoch: 5| Step: 1
Training loss: 2.647073670264601
Validation loss: 2.5160117433268274

Epoch: 5| Step: 2
Training loss: 2.002500758748381
Validation loss: 2.523525325783779

Epoch: 5| Step: 3
Training loss: 2.939818421030934
Validation loss: 2.520172189734075

Epoch: 5| Step: 4
Training loss: 2.2094180004462265
Validation loss: 2.52636605604929

Epoch: 5| Step: 5
Training loss: 2.0467402290063483
Validation loss: 2.4970096988217505

Epoch: 5| Step: 6
Training loss: 2.908534638907974
Validation loss: 2.557502422282499

Epoch: 5| Step: 7
Training loss: 2.144686556058151
Validation loss: 2.5342365978732255

Epoch: 5| Step: 8
Training loss: 2.317921032236027
Validation loss: 2.5203169369450467

Epoch: 5| Step: 9
Training loss: 2.343671263325885
Validation loss: 2.529934167261183

Epoch: 5| Step: 10
Training loss: 2.897383423666145
Validation loss: 2.5152530348035764

Epoch: 298| Step: 0
Training loss: 2.4616277786244622
Validation loss: 2.50181375154265

Epoch: 5| Step: 1
Training loss: 2.8382224553825575
Validation loss: 2.5226344294261964

Epoch: 5| Step: 2
Training loss: 2.590133903900862
Validation loss: 2.500090314402794

Epoch: 5| Step: 3
Training loss: 2.3101746492294963
Validation loss: 2.5089101739356297

Epoch: 5| Step: 4
Training loss: 2.845635093237062
Validation loss: 2.5234180204467034

Epoch: 5| Step: 5
Training loss: 2.312409888908913
Validation loss: 2.504173343527993

Epoch: 5| Step: 6
Training loss: 2.9272947585743156
Validation loss: 2.5009045630788798

Epoch: 5| Step: 7
Training loss: 2.2298822650706605
Validation loss: 2.4800861136185826

Epoch: 5| Step: 8
Training loss: 2.0663845587531187
Validation loss: 2.5025328326507825

Epoch: 5| Step: 9
Training loss: 2.5668271443949786
Validation loss: 2.517253119830678

Epoch: 5| Step: 10
Training loss: 1.953355821321646
Validation loss: 2.556299726512664

Epoch: 299| Step: 0
Training loss: 2.2139190950437673
Validation loss: 2.547274214691446

Epoch: 5| Step: 1
Training loss: 1.892054687173381
Validation loss: 2.4756255394219298

Epoch: 5| Step: 2
Training loss: 2.594672900625949
Validation loss: 2.507825162723541

Epoch: 5| Step: 3
Training loss: 2.5026286610579938
Validation loss: 2.5286504912449406

Epoch: 5| Step: 4
Training loss: 2.5849400927687136
Validation loss: 2.5180456919246024

Epoch: 5| Step: 5
Training loss: 2.3799204801850826
Validation loss: 2.4946638625141753

Epoch: 5| Step: 6
Training loss: 3.2764601852021404
Validation loss: 2.54510760842904

Epoch: 5| Step: 7
Training loss: 1.7619264061591293
Validation loss: 2.536087647482234

Epoch: 5| Step: 8
Training loss: 2.4101681206656616
Validation loss: 2.52481345148848

Epoch: 5| Step: 9
Training loss: 2.5346209838251417
Validation loss: 2.4961637380931716

Epoch: 5| Step: 10
Training loss: 2.7002346042857783
Validation loss: 2.54186979907985

Epoch: 300| Step: 0
Training loss: 2.5261225631711928
Validation loss: 2.4841661562174013

Epoch: 5| Step: 1
Training loss: 2.182009318316988
Validation loss: 2.522126267865016

Epoch: 5| Step: 2
Training loss: 2.344871354468374
Validation loss: 2.513910892628754

Epoch: 5| Step: 3
Training loss: 2.3711693640126237
Validation loss: 2.50470649274424

Epoch: 5| Step: 4
Training loss: 2.4409223153185695
Validation loss: 2.5039731192214174

Epoch: 5| Step: 5
Training loss: 2.308593336902459
Validation loss: 2.509331615536116

Epoch: 5| Step: 6
Training loss: 1.8588028997347341
Validation loss: 2.514172218534128

Epoch: 5| Step: 7
Training loss: 3.3708753459812972
Validation loss: 2.5259067091280696

Epoch: 5| Step: 8
Training loss: 2.07510110080262
Validation loss: 2.5022023068184978

Epoch: 5| Step: 9
Training loss: 2.7066146287785893
Validation loss: 2.5119489683169793

Epoch: 5| Step: 10
Training loss: 2.7389978186830914
Validation loss: 2.5126948074637405

Epoch: 301| Step: 0
Training loss: 2.484685722449272
Validation loss: 2.508256676302151

Epoch: 5| Step: 1
Training loss: 2.0967117541650095
Validation loss: 2.5084468047169337

Epoch: 5| Step: 2
Training loss: 2.651435168327384
Validation loss: 2.48585333875796

Epoch: 5| Step: 3
Training loss: 2.392128365730724
Validation loss: 2.525962974676647

Epoch: 5| Step: 4
Training loss: 2.9616711193331033
Validation loss: 2.5017616279571437

Epoch: 5| Step: 5
Training loss: 2.459483077159901
Validation loss: 2.512634469694197

Epoch: 5| Step: 6
Training loss: 2.8267122396658038
Validation loss: 2.531274865039845

Epoch: 5| Step: 7
Training loss: 2.197697006265344
Validation loss: 2.501703330150734

Epoch: 5| Step: 8
Training loss: 2.475668375723044
Validation loss: 2.5708932887113223

Epoch: 5| Step: 9
Training loss: 2.2627544238947417
Validation loss: 2.539529735906077

Epoch: 5| Step: 10
Training loss: 2.2874229209712764
Validation loss: 2.5104421101072494

Epoch: 302| Step: 0
Training loss: 2.516355704575946
Validation loss: 2.5266015644546402

Epoch: 5| Step: 1
Training loss: 2.6985254570476047
Validation loss: 2.522811698774077

Epoch: 5| Step: 2
Training loss: 2.4411630738267234
Validation loss: 2.4996987253592717

Epoch: 5| Step: 3
Training loss: 2.619498345782476
Validation loss: 2.497865007248749

Epoch: 5| Step: 4
Training loss: 2.4419914337745277
Validation loss: 2.514329257982433

Epoch: 5| Step: 5
Training loss: 2.3282514704205344
Validation loss: 2.511678837330669

Epoch: 5| Step: 6
Training loss: 2.7881339100787477
Validation loss: 2.56392540393434

Epoch: 5| Step: 7
Training loss: 2.737378111612248
Validation loss: 2.4986457663870065

Epoch: 5| Step: 8
Training loss: 2.202311697107896
Validation loss: 2.541858698790428

Epoch: 5| Step: 9
Training loss: 2.2073302479988057
Validation loss: 2.5023434883847413

Epoch: 5| Step: 10
Training loss: 2.0000880937248517
Validation loss: 2.5036020806241504

Epoch: 303| Step: 0
Training loss: 2.834957741720124
Validation loss: 2.51193826548943

Epoch: 5| Step: 1
Training loss: 1.8794291635059293
Validation loss: 2.514862792957

Epoch: 5| Step: 2
Training loss: 1.983241921677096
Validation loss: 2.503139206674158

Epoch: 5| Step: 3
Training loss: 2.3976805048387266
Validation loss: 2.5190741708589965

Epoch: 5| Step: 4
Training loss: 2.514731206920593
Validation loss: 2.4956204929249832

Epoch: 5| Step: 5
Training loss: 2.90135436823994
Validation loss: 2.5229773353773206

Epoch: 5| Step: 6
Training loss: 2.8635058971921805
Validation loss: 2.4861576420230866

Epoch: 5| Step: 7
Training loss: 2.2215109826056234
Validation loss: 2.487131620859552

Epoch: 5| Step: 8
Training loss: 2.8452570348923647
Validation loss: 2.5017086527384023

Epoch: 5| Step: 9
Training loss: 2.5149080190655786
Validation loss: 2.5007322941149406

Epoch: 5| Step: 10
Training loss: 1.9535399339517765
Validation loss: 2.523487762729816

Epoch: 304| Step: 0
Training loss: 2.4455732017703506
Validation loss: 2.4959040867302655

Epoch: 5| Step: 1
Training loss: 2.684116806740452
Validation loss: 2.507198534211606

Epoch: 5| Step: 2
Training loss: 2.2765003036140223
Validation loss: 2.5202817121796346

Epoch: 5| Step: 3
Training loss: 2.1748593909754064
Validation loss: 2.4960357029164206

Epoch: 5| Step: 4
Training loss: 2.6429885128318626
Validation loss: 2.4973925888261324

Epoch: 5| Step: 5
Training loss: 1.800445326712589
Validation loss: 2.514470286778489

Epoch: 5| Step: 6
Training loss: 2.2564587610095423
Validation loss: 2.4821216732235327

Epoch: 5| Step: 7
Training loss: 2.39222882893527
Validation loss: 2.5232189656190007

Epoch: 5| Step: 8
Training loss: 3.4239231059451094
Validation loss: 2.5456604987544877

Epoch: 5| Step: 9
Training loss: 2.409369490205276
Validation loss: 2.4800386709207807

Epoch: 5| Step: 10
Training loss: 1.974338712923337
Validation loss: 2.5167486618384722

Epoch: 305| Step: 0
Training loss: 2.1461755671005425
Validation loss: 2.527044977468292

Epoch: 5| Step: 1
Training loss: 2.304514215711472
Validation loss: 2.5281433056942935

Epoch: 5| Step: 2
Training loss: 2.170419987762463
Validation loss: 2.5043636488676886

Epoch: 5| Step: 3
Training loss: 2.4962242223700915
Validation loss: 2.523753808857163

Epoch: 5| Step: 4
Training loss: 2.2701781724754415
Validation loss: 2.4976984833163307

Epoch: 5| Step: 5
Training loss: 2.201973905223372
Validation loss: 2.499219532626552

Epoch: 5| Step: 6
Training loss: 3.1802333203977295
Validation loss: 2.5253522379707567

Epoch: 5| Step: 7
Training loss: 3.0967438978974413
Validation loss: 2.4887216375680286

Epoch: 5| Step: 8
Training loss: 2.4447249904187744
Validation loss: 2.5126843118690436

Epoch: 5| Step: 9
Training loss: 2.264228969480099
Validation loss: 2.495126578494311

Epoch: 5| Step: 10
Training loss: 1.8338288880108007
Validation loss: 2.519683198893125

Epoch: 306| Step: 0
Training loss: 2.108795538536643
Validation loss: 2.5239442579162445

Epoch: 5| Step: 1
Training loss: 2.1785248968082014
Validation loss: 2.5489727807280596

Epoch: 5| Step: 2
Training loss: 2.1321318382166496
Validation loss: 2.548606299714257

Epoch: 5| Step: 3
Training loss: 2.592484406706689
Validation loss: 2.5509442765681216

Epoch: 5| Step: 4
Training loss: 3.141320754080533
Validation loss: 2.517618597881719

Epoch: 5| Step: 5
Training loss: 2.7122563828442354
Validation loss: 2.5146586853194464

Epoch: 5| Step: 6
Training loss: 2.00278993562608
Validation loss: 2.530557131233699

Epoch: 5| Step: 7
Training loss: 2.7300004243675713
Validation loss: 2.4957273766025376

Epoch: 5| Step: 8
Training loss: 2.172309996589566
Validation loss: 2.5199988721459894

Epoch: 5| Step: 9
Training loss: 2.263216303616777
Validation loss: 2.530878102236685

Epoch: 5| Step: 10
Training loss: 2.767653502606846
Validation loss: 2.525465342713806

Epoch: 307| Step: 0
Training loss: 1.7918763001100941
Validation loss: 2.5402850351913755

Epoch: 5| Step: 1
Training loss: 1.6272601036078103
Validation loss: 2.518123459865941

Epoch: 5| Step: 2
Training loss: 2.3305754598445296
Validation loss: 2.512398779571955

Epoch: 5| Step: 3
Training loss: 2.3655765704863034
Validation loss: 2.5276317271021504

Epoch: 5| Step: 4
Training loss: 2.379593071970791
Validation loss: 2.518849651979425

Epoch: 5| Step: 5
Training loss: 2.761208752884877
Validation loss: 2.4976058509461723

Epoch: 5| Step: 6
Training loss: 2.7112620046797433
Validation loss: 2.530664716350514

Epoch: 5| Step: 7
Training loss: 2.1127205773036932
Validation loss: 2.505128553016133

Epoch: 5| Step: 8
Training loss: 2.49097827047689
Validation loss: 2.527606579759114

Epoch: 5| Step: 9
Training loss: 2.798589085275846
Validation loss: 2.535361967454713

Epoch: 5| Step: 10
Training loss: 3.4262237089109373
Validation loss: 2.524505475592922

Epoch: 308| Step: 0
Training loss: 2.4378294110845466
Validation loss: 2.502200265906692

Epoch: 5| Step: 1
Training loss: 2.8139902193486463
Validation loss: 2.5209409046196503

Epoch: 5| Step: 2
Training loss: 2.2927963477963456
Validation loss: 2.517182760251435

Epoch: 5| Step: 3
Training loss: 1.919139000045427
Validation loss: 2.5127637363226

Epoch: 5| Step: 4
Training loss: 2.590925678248749
Validation loss: 2.506645324852252

Epoch: 5| Step: 5
Training loss: 2.736490357822331
Validation loss: 2.5023212854111705

Epoch: 5| Step: 6
Training loss: 2.2002370316520174
Validation loss: 2.541404719364382

Epoch: 5| Step: 7
Training loss: 2.3187454017943163
Validation loss: 2.5018023731055727

Epoch: 5| Step: 8
Training loss: 2.533907025613435
Validation loss: 2.505011352370225

Epoch: 5| Step: 9
Training loss: 2.5492883025678705
Validation loss: 2.487019903855148

Epoch: 5| Step: 10
Training loss: 2.4782927332487548
Validation loss: 2.5552149608171564

Epoch: 309| Step: 0
Training loss: 1.6596990696326555
Validation loss: 2.50244220839877

Epoch: 5| Step: 1
Training loss: 2.327192695891562
Validation loss: 2.5249190643547794

Epoch: 5| Step: 2
Training loss: 2.389124280830562
Validation loss: 2.4912536344472924

Epoch: 5| Step: 3
Training loss: 2.662087611825247
Validation loss: 2.5262617584716534

Epoch: 5| Step: 4
Training loss: 2.6196041962808274
Validation loss: 2.536988025272838

Epoch: 5| Step: 5
Training loss: 2.3687823371391024
Validation loss: 2.4782133750528583

Epoch: 5| Step: 6
Training loss: 2.5999822542611986
Validation loss: 2.524136782815183

Epoch: 5| Step: 7
Training loss: 2.37006518004719
Validation loss: 2.513833857129442

Epoch: 5| Step: 8
Training loss: 2.703952552379124
Validation loss: 2.530254690934561

Epoch: 5| Step: 9
Training loss: 2.5027046831780155
Validation loss: 2.525762265002665

Epoch: 5| Step: 10
Training loss: 2.587746439821781
Validation loss: 2.506238565435995

Epoch: 310| Step: 0
Training loss: 2.737819137434571
Validation loss: 2.5199995758757336

Epoch: 5| Step: 1
Training loss: 2.232898357601263
Validation loss: 2.5377959949192825

Epoch: 5| Step: 2
Training loss: 1.8887607431264
Validation loss: 2.5079471297837523

Epoch: 5| Step: 3
Training loss: 2.6535851069506555
Validation loss: 2.4876976359694356

Epoch: 5| Step: 4
Training loss: 2.585301900238105
Validation loss: 2.4837847247244924

Epoch: 5| Step: 5
Training loss: 2.0476666732225888
Validation loss: 2.5427496882387604

Epoch: 5| Step: 6
Training loss: 2.880022885708478
Validation loss: 2.5376121186265896

Epoch: 5| Step: 7
Training loss: 2.209916596888638
Validation loss: 2.536686605374025

Epoch: 5| Step: 8
Training loss: 2.243581305407631
Validation loss: 2.506938905989813

Epoch: 5| Step: 9
Training loss: 2.791957432042553
Validation loss: 2.519535882705025

Epoch: 5| Step: 10
Training loss: 2.195969995616782
Validation loss: 2.5340106392509907

Epoch: 311| Step: 0
Training loss: 2.307516036881982
Validation loss: 2.4893923319902598

Epoch: 5| Step: 1
Training loss: 2.588314105238997
Validation loss: 2.4967892568935732

Epoch: 5| Step: 2
Training loss: 2.4137188811940926
Validation loss: 2.522217924469661

Epoch: 5| Step: 3
Training loss: 2.7170504815694856
Validation loss: 2.5103715121453463

Epoch: 5| Step: 4
Training loss: 2.363935194098644
Validation loss: 2.493957917304146

Epoch: 5| Step: 5
Training loss: 2.6673340161065324
Validation loss: 2.5012836831747234

Epoch: 5| Step: 6
Training loss: 1.8665960803764192
Validation loss: 2.501497882319149

Epoch: 5| Step: 7
Training loss: 2.290613267564066
Validation loss: 2.511046449623265

Epoch: 5| Step: 8
Training loss: 2.343104871336286
Validation loss: 2.521554444254531

Epoch: 5| Step: 9
Training loss: 2.515834444224148
Validation loss: 2.4933170969749576

Epoch: 5| Step: 10
Training loss: 2.451977503936276
Validation loss: 2.506942634451956

Epoch: 312| Step: 0
Training loss: 2.7344098770097145
Validation loss: 2.488318150439683

Epoch: 5| Step: 1
Training loss: 1.9135051928016888
Validation loss: 2.494430316126459

Epoch: 5| Step: 2
Training loss: 2.7291389250376685
Validation loss: 2.52045647778813

Epoch: 5| Step: 3
Training loss: 2.380103600755562
Validation loss: 2.5054367159513133

Epoch: 5| Step: 4
Training loss: 2.111785867282801
Validation loss: 2.54417671633012

Epoch: 5| Step: 5
Training loss: 2.731969330991121
Validation loss: 2.5185753258776344

Epoch: 5| Step: 6
Training loss: 2.41495336989093
Validation loss: 2.55681181622116

Epoch: 5| Step: 7
Training loss: 2.759725971301503
Validation loss: 2.4937075199869216

Epoch: 5| Step: 8
Training loss: 1.9680525134570184
Validation loss: 2.5461125564118516

Epoch: 5| Step: 9
Training loss: 1.9529190565251575
Validation loss: 2.503070344617944

Epoch: 5| Step: 10
Training loss: 3.132577787439659
Validation loss: 2.5064200137961516

Epoch: 313| Step: 0
Training loss: 2.4066956342748447
Validation loss: 2.5392116960308106

Epoch: 5| Step: 1
Training loss: 2.379345381973014
Validation loss: 2.526881177003689

Epoch: 5| Step: 2
Training loss: 2.4134696553474035
Validation loss: 2.497392653497325

Epoch: 5| Step: 3
Training loss: 2.4137407106905298
Validation loss: 2.514068281848396

Epoch: 5| Step: 4
Training loss: 2.9188365131111738
Validation loss: 2.5293768450701544

Epoch: 5| Step: 5
Training loss: 2.0041866589644157
Validation loss: 2.5160465798036245

Epoch: 5| Step: 6
Training loss: 2.4314231438261618
Validation loss: 2.5195918367247194

Epoch: 5| Step: 7
Training loss: 2.4450796583184076
Validation loss: 2.5170779767404245

Epoch: 5| Step: 8
Training loss: 2.3444543415943415
Validation loss: 2.490847367907247

Epoch: 5| Step: 9
Training loss: 2.5395167487294974
Validation loss: 2.5271047205980706

Epoch: 5| Step: 10
Training loss: 2.310106533781118
Validation loss: 2.541889162443429

Epoch: 314| Step: 0
Training loss: 1.9059011421544854
Validation loss: 2.500561568551287

Epoch: 5| Step: 1
Training loss: 2.444003891500069
Validation loss: 2.548375356960501

Epoch: 5| Step: 2
Training loss: 2.230190493568601
Validation loss: 2.5131673025583923

Epoch: 5| Step: 3
Training loss: 2.605162732684761
Validation loss: 2.5080703113680065

Epoch: 5| Step: 4
Training loss: 2.5402036010060707
Validation loss: 2.5192238077882303

Epoch: 5| Step: 5
Training loss: 2.6399391683159004
Validation loss: 2.53541612831303

Epoch: 5| Step: 6
Training loss: 1.9825562079255732
Validation loss: 2.5216513014886064

Epoch: 5| Step: 7
Training loss: 2.60149972284313
Validation loss: 2.5234237767546777

Epoch: 5| Step: 8
Training loss: 2.515806013904325
Validation loss: 2.514567674820756

Epoch: 5| Step: 9
Training loss: 2.815336534835991
Validation loss: 2.4888260281180474

Epoch: 5| Step: 10
Training loss: 2.4157326855659282
Validation loss: 2.5259330902936186

Epoch: 315| Step: 0
Training loss: 2.2344333667734504
Validation loss: 2.4553504575018557

Epoch: 5| Step: 1
Training loss: 2.1963237998864726
Validation loss: 2.518222776804159

Epoch: 5| Step: 2
Training loss: 1.7629917054092101
Validation loss: 2.4925638046488516

Epoch: 5| Step: 3
Training loss: 2.8324699956478123
Validation loss: 2.5061155480210853

Epoch: 5| Step: 4
Training loss: 1.9554583839937305
Validation loss: 2.5428296586654655

Epoch: 5| Step: 5
Training loss: 2.5263381207257143
Validation loss: 2.502642180777576

Epoch: 5| Step: 6
Training loss: 2.5227064839474274
Validation loss: 2.489147971152011

Epoch: 5| Step: 7
Training loss: 2.619229012974239
Validation loss: 2.5432469108561824

Epoch: 5| Step: 8
Training loss: 2.776823400154967
Validation loss: 2.5175735589549904

Epoch: 5| Step: 9
Training loss: 2.7283284499261207
Validation loss: 2.5300606187125054

Epoch: 5| Step: 10
Training loss: 2.414324304896226
Validation loss: 2.514262019270608

Epoch: 316| Step: 0
Training loss: 2.256864989096603
Validation loss: 2.5106714080518664

Epoch: 5| Step: 1
Training loss: 2.6177474148908493
Validation loss: 2.5117540425434477

Epoch: 5| Step: 2
Training loss: 2.49092840359345
Validation loss: 2.5172396062755125

Epoch: 5| Step: 3
Training loss: 1.7438465652253055
Validation loss: 2.4949894346151207

Epoch: 5| Step: 4
Training loss: 2.4074508841031204
Validation loss: 2.5307568822164694

Epoch: 5| Step: 5
Training loss: 2.8392096523848522
Validation loss: 2.506181462210788

Epoch: 5| Step: 6
Training loss: 2.0772245363572472
Validation loss: 2.5390854620388255

Epoch: 5| Step: 7
Training loss: 2.3483434804241865
Validation loss: 2.5011037389698356

Epoch: 5| Step: 8
Training loss: 2.382735954681136
Validation loss: 2.508015614149465

Epoch: 5| Step: 9
Training loss: 2.6527435495810274
Validation loss: 2.510930264400875

Epoch: 5| Step: 10
Training loss: 2.6066086776149913
Validation loss: 2.517469752514181

Epoch: 317| Step: 0
Training loss: 3.1062388490902717
Validation loss: 2.5305169453239293

Epoch: 5| Step: 1
Training loss: 2.4073048050798524
Validation loss: 2.5100566136230746

Epoch: 5| Step: 2
Training loss: 2.1406564083718838
Validation loss: 2.5260160603233417

Epoch: 5| Step: 3
Training loss: 2.7209470794253243
Validation loss: 2.4931172943796156

Epoch: 5| Step: 4
Training loss: 2.0705348633164187
Validation loss: 2.525959588406619

Epoch: 5| Step: 5
Training loss: 2.659681818439133
Validation loss: 2.5454521782309643

Epoch: 5| Step: 6
Training loss: 2.7519083337543195
Validation loss: 2.5306821910669366

Epoch: 5| Step: 7
Training loss: 1.8843582582758054
Validation loss: 2.551870096492633

Epoch: 5| Step: 8
Training loss: 2.3266594901789444
Validation loss: 2.5379059813580835

Epoch: 5| Step: 9
Training loss: 2.0351102095245084
Validation loss: 2.520895908778747

Epoch: 5| Step: 10
Training loss: 2.029219683738512
Validation loss: 2.504695187325171

Epoch: 318| Step: 0
Training loss: 2.3068966380272133
Validation loss: 2.5215820503025728

Epoch: 5| Step: 1
Training loss: 2.526170508385957
Validation loss: 2.5078680971300575

Epoch: 5| Step: 2
Training loss: 1.932769908142784
Validation loss: 2.516329433893156

Epoch: 5| Step: 3
Training loss: 1.7479698121543406
Validation loss: 2.5299489323484248

Epoch: 5| Step: 4
Training loss: 2.512614373787495
Validation loss: 2.4969265144481185

Epoch: 5| Step: 5
Training loss: 2.752025898390158
Validation loss: 2.512007075291625

Epoch: 5| Step: 6
Training loss: 2.305972707562091
Validation loss: 2.515615744635197

Epoch: 5| Step: 7
Training loss: 2.3116832012345188
Validation loss: 2.4925415135316347

Epoch: 5| Step: 8
Training loss: 2.4971548102698513
Validation loss: 2.5285968424518654

Epoch: 5| Step: 9
Training loss: 2.6096163883599774
Validation loss: 2.4943756856926718

Epoch: 5| Step: 10
Training loss: 2.9151028573421347
Validation loss: 2.512153389999418

Epoch: 319| Step: 0
Training loss: 1.9018161049761373
Validation loss: 2.5112466163142657

Epoch: 5| Step: 1
Training loss: 2.7403107657629895
Validation loss: 2.5368150531517233

Epoch: 5| Step: 2
Training loss: 2.315076372275125
Validation loss: 2.51254785800593

Epoch: 5| Step: 3
Training loss: 1.8909303402832134
Validation loss: 2.5250581569547217

Epoch: 5| Step: 4
Training loss: 3.0547995778472
Validation loss: 2.531895438624449

Epoch: 5| Step: 5
Training loss: 2.3677077524296632
Validation loss: 2.508808262224802

Epoch: 5| Step: 6
Training loss: 2.079838326113007
Validation loss: 2.4867156019819725

Epoch: 5| Step: 7
Training loss: 2.5474075977462216
Validation loss: 2.4749723560723926

Epoch: 5| Step: 8
Training loss: 2.4782060530541123
Validation loss: 2.552198710207804

Epoch: 5| Step: 9
Training loss: 2.506369868049468
Validation loss: 2.515516802229661

Epoch: 5| Step: 10
Training loss: 2.7959791911964866
Validation loss: 2.5255140586827145

Epoch: 320| Step: 0
Training loss: 2.302199025648897
Validation loss: 2.5008847793658697

Epoch: 5| Step: 1
Training loss: 1.825700374667964
Validation loss: 2.532306895618977

Epoch: 5| Step: 2
Training loss: 2.2186576931516733
Validation loss: 2.515002539577216

Epoch: 5| Step: 3
Training loss: 3.3157390377125484
Validation loss: 2.472261931179702

Epoch: 5| Step: 4
Training loss: 2.335864930317412
Validation loss: 2.521810610833168

Epoch: 5| Step: 5
Training loss: 2.0896139713778337
Validation loss: 2.48313309037734

Epoch: 5| Step: 6
Training loss: 2.109029444894588
Validation loss: 2.5049603617306317

Epoch: 5| Step: 7
Training loss: 3.035345241009118
Validation loss: 2.541205204091086

Epoch: 5| Step: 8
Training loss: 2.8297296682045774
Validation loss: 2.4846847226575264

Epoch: 5| Step: 9
Training loss: 2.1806734651191153
Validation loss: 2.53241257306876

Epoch: 5| Step: 10
Training loss: 1.8828187681228326
Validation loss: 2.5148218383961956

Epoch: 321| Step: 0
Training loss: 2.2426507451349935
Validation loss: 2.52135300438831

Epoch: 5| Step: 1
Training loss: 2.0653853607832118
Validation loss: 2.550255637612584

Epoch: 5| Step: 2
Training loss: 2.472309395566422
Validation loss: 2.504267119069999

Epoch: 5| Step: 3
Training loss: 2.827771075111078
Validation loss: 2.507060937128548

Epoch: 5| Step: 4
Training loss: 3.1203361804504848
Validation loss: 2.494327140633023

Epoch: 5| Step: 5
Training loss: 1.9738996458430025
Validation loss: 2.532225342550823

Epoch: 5| Step: 6
Training loss: 2.1086983054472244
Validation loss: 2.466622023883824

Epoch: 5| Step: 7
Training loss: 2.192034680565525
Validation loss: 2.5118912743625046

Epoch: 5| Step: 8
Training loss: 2.5976903009692585
Validation loss: 2.4921266574911867

Epoch: 5| Step: 9
Training loss: 2.3169335831304476
Validation loss: 2.521717973630718

Epoch: 5| Step: 10
Training loss: 2.649188810025773
Validation loss: 2.504683481139678

Epoch: 322| Step: 0
Training loss: 2.299177404168683
Validation loss: 2.5045872712539423

Epoch: 5| Step: 1
Training loss: 2.717853529884568
Validation loss: 2.4812938839322545

Epoch: 5| Step: 2
Training loss: 2.2993761833749864
Validation loss: 2.498820433701408

Epoch: 5| Step: 3
Training loss: 2.202532641269626
Validation loss: 2.529419306223983

Epoch: 5| Step: 4
Training loss: 2.4757554336699314
Validation loss: 2.5451579639638653

Epoch: 5| Step: 5
Training loss: 2.01241940639073
Validation loss: 2.4877673707827923

Epoch: 5| Step: 6
Training loss: 3.269919751783745
Validation loss: 2.54933209234739

Epoch: 5| Step: 7
Training loss: 2.591228868441578
Validation loss: 2.491875695503172

Epoch: 5| Step: 8
Training loss: 1.8895673078918007
Validation loss: 2.5079445113996415

Epoch: 5| Step: 9
Training loss: 2.1247267827934433
Validation loss: 2.5285594966802596

Epoch: 5| Step: 10
Training loss: 2.355263974728845
Validation loss: 2.5009208644455696

Epoch: 323| Step: 0
Training loss: 2.5600748449053854
Validation loss: 2.5409978641932476

Epoch: 5| Step: 1
Training loss: 2.729997629716564
Validation loss: 2.4980071030801514

Epoch: 5| Step: 2
Training loss: 2.433411029550703
Validation loss: 2.505992299506424

Epoch: 5| Step: 3
Training loss: 1.9773217955492972
Validation loss: 2.5030473410162357

Epoch: 5| Step: 4
Training loss: 2.445136115687031
Validation loss: 2.5025178494626723

Epoch: 5| Step: 5
Training loss: 1.9110685638871479
Validation loss: 2.4865861648004457

Epoch: 5| Step: 6
Training loss: 2.432247861240359
Validation loss: 2.4976497688155352

Epoch: 5| Step: 7
Training loss: 2.687377926915065
Validation loss: 2.488399452903224

Epoch: 5| Step: 8
Training loss: 2.8525547757184473
Validation loss: 2.5233049946199055

Epoch: 5| Step: 9
Training loss: 1.9697478051616992
Validation loss: 2.5423685923867456

Epoch: 5| Step: 10
Training loss: 2.2238397380526305
Validation loss: 2.5434451693334035

Epoch: 324| Step: 0
Training loss: 2.4226611430424123
Validation loss: 2.5006272985120037

Epoch: 5| Step: 1
Training loss: 2.9580742270317346
Validation loss: 2.51310666602023

Epoch: 5| Step: 2
Training loss: 2.201471343904673
Validation loss: 2.4951209130781216

Epoch: 5| Step: 3
Training loss: 2.149666951133002
Validation loss: 2.5271043158294484

Epoch: 5| Step: 4
Training loss: 2.230654092493699
Validation loss: 2.51461529593726

Epoch: 5| Step: 5
Training loss: 2.1612437926909847
Validation loss: 2.5103933763621438

Epoch: 5| Step: 6
Training loss: 2.751543912115355
Validation loss: 2.4999776470046395

Epoch: 5| Step: 7
Training loss: 1.9362258413148605
Validation loss: 2.4755752172640677

Epoch: 5| Step: 8
Training loss: 2.2677636937218004
Validation loss: 2.49190823416342

Epoch: 5| Step: 9
Training loss: 2.548891170496623
Validation loss: 2.516968826961165

Epoch: 5| Step: 10
Training loss: 2.754394141732932
Validation loss: 2.4998339613259297

Epoch: 325| Step: 0
Training loss: 2.6307544305001542
Validation loss: 2.4918505473996686

Epoch: 5| Step: 1
Training loss: 2.4050267070151836
Validation loss: 2.4990303343269566

Epoch: 5| Step: 2
Training loss: 2.440462024439371
Validation loss: 2.5300449970434418

Epoch: 5| Step: 3
Training loss: 2.0495021355071787
Validation loss: 2.503664921541454

Epoch: 5| Step: 4
Training loss: 2.564344812039317
Validation loss: 2.474972636781214

Epoch: 5| Step: 5
Training loss: 2.71615678608857
Validation loss: 2.503238350533175

Epoch: 5| Step: 6
Training loss: 2.2861718162654197
Validation loss: 2.5103274461320915

Epoch: 5| Step: 7
Training loss: 2.377009996848134
Validation loss: 2.534745067411636

Epoch: 5| Step: 8
Training loss: 2.2222676934782295
Validation loss: 2.506212697091492

Epoch: 5| Step: 9
Training loss: 2.281593087930094
Validation loss: 2.486003713082486

Epoch: 5| Step: 10
Training loss: 2.488303000266743
Validation loss: 2.5336844454220415

Epoch: 326| Step: 0
Training loss: 2.575178092057042
Validation loss: 2.5207069105602145

Epoch: 5| Step: 1
Training loss: 2.2843981406400498
Validation loss: 2.506965332342672

Epoch: 5| Step: 2
Training loss: 2.8988647146091444
Validation loss: 2.5370081463807805

Epoch: 5| Step: 3
Training loss: 2.3914611949671465
Validation loss: 2.4914972382752296

Epoch: 5| Step: 4
Training loss: 2.1632190809757024
Validation loss: 2.499247167918132

Epoch: 5| Step: 5
Training loss: 2.4561566128825816
Validation loss: 2.531768474299339

Epoch: 5| Step: 6
Training loss: 2.7874467374220524
Validation loss: 2.511568587330001

Epoch: 5| Step: 7
Training loss: 2.472496666361198
Validation loss: 2.5187533424864377

Epoch: 5| Step: 8
Training loss: 2.367320242707091
Validation loss: 2.4977666405558185

Epoch: 5| Step: 9
Training loss: 1.488848080162796
Validation loss: 2.5250345515881607

Epoch: 5| Step: 10
Training loss: 2.3957362169125824
Validation loss: 2.528616656220437

Epoch: 327| Step: 0
Training loss: 2.580599065915018
Validation loss: 2.5025790847133154

Epoch: 5| Step: 1
Training loss: 2.0376111708192184
Validation loss: 2.51748418439004

Epoch: 5| Step: 2
Training loss: 1.798471951921109
Validation loss: 2.5245996921205087

Epoch: 5| Step: 3
Training loss: 2.5937340977192767
Validation loss: 2.5039441610706827

Epoch: 5| Step: 4
Training loss: 2.2307165111818392
Validation loss: 2.5226985756620905

Epoch: 5| Step: 5
Training loss: 2.1351658619546727
Validation loss: 2.495695057228097

Epoch: 5| Step: 6
Training loss: 2.6253487491545764
Validation loss: 2.5033375498628

Epoch: 5| Step: 7
Training loss: 2.267413466042038
Validation loss: 2.4799709283986817

Epoch: 5| Step: 8
Training loss: 2.8583240349472843
Validation loss: 2.525557856674251

Epoch: 5| Step: 9
Training loss: 2.6204390184206683
Validation loss: 2.479603647622675

Epoch: 5| Step: 10
Training loss: 2.572653403050353
Validation loss: 2.502070729148101

Epoch: 328| Step: 0
Training loss: 2.867747904297471
Validation loss: 2.495212324651848

Epoch: 5| Step: 1
Training loss: 1.996014259383003
Validation loss: 2.532364389687504

Epoch: 5| Step: 2
Training loss: 2.490961712102309
Validation loss: 2.5299359608423657

Epoch: 5| Step: 3
Training loss: 2.360568925526354
Validation loss: 2.480444326515483

Epoch: 5| Step: 4
Training loss: 2.6480108496970938
Validation loss: 2.5132445441383195

Epoch: 5| Step: 5
Training loss: 2.504576785186329
Validation loss: 2.523842559157161

Epoch: 5| Step: 6
Training loss: 2.3038895599598925
Validation loss: 2.475763600631849

Epoch: 5| Step: 7
Training loss: 2.2131086816108
Validation loss: 2.496882846701922

Epoch: 5| Step: 8
Training loss: 2.831121722214298
Validation loss: 2.522210238266246

Epoch: 5| Step: 9
Training loss: 1.5635390832088847
Validation loss: 2.5326049461745184

Epoch: 5| Step: 10
Training loss: 2.198633254932997
Validation loss: 2.4984963909854168

Epoch: 329| Step: 0
Training loss: 2.7634852693460883
Validation loss: 2.522016761228225

Epoch: 5| Step: 1
Training loss: 2.3377856850077383
Validation loss: 2.506865856260037

Epoch: 5| Step: 2
Training loss: 2.1353585832341366
Validation loss: 2.522326962969614

Epoch: 5| Step: 3
Training loss: 2.404173115722187
Validation loss: 2.536696610043254

Epoch: 5| Step: 4
Training loss: 2.5186764235434596
Validation loss: 2.500579789313283

Epoch: 5| Step: 5
Training loss: 2.6439697931788917
Validation loss: 2.4974297252131503

Epoch: 5| Step: 6
Training loss: 2.9078604225338047
Validation loss: 2.5026566059653486

Epoch: 5| Step: 7
Training loss: 1.9297744793150378
Validation loss: 2.5191797032678602

Epoch: 5| Step: 8
Training loss: 2.183770515507111
Validation loss: 2.510135777037619

Epoch: 5| Step: 9
Training loss: 2.000226127238896
Validation loss: 2.4948170310590214

Epoch: 5| Step: 10
Training loss: 2.5497150149697623
Validation loss: 2.5323084394882387

Epoch: 330| Step: 0
Training loss: 2.2807319784542734
Validation loss: 2.5172315025887118

Epoch: 5| Step: 1
Training loss: 2.693576507999998
Validation loss: 2.4990620950824916

Epoch: 5| Step: 2
Training loss: 2.4101999733295965
Validation loss: 2.5031475515992114

Epoch: 5| Step: 3
Training loss: 2.7963468596420458
Validation loss: 2.5165524600716944

Epoch: 5| Step: 4
Training loss: 2.2333541351981747
Validation loss: 2.5116944640164736

Epoch: 5| Step: 5
Training loss: 1.897619628552885
Validation loss: 2.4972677764624356

Epoch: 5| Step: 6
Training loss: 1.908220804497648
Validation loss: 2.552643511838981

Epoch: 5| Step: 7
Training loss: 2.321802553507327
Validation loss: 2.52824479900766

Epoch: 5| Step: 8
Training loss: 2.6904569814858728
Validation loss: 2.5612106881372534

Epoch: 5| Step: 9
Training loss: 2.6698857090809045
Validation loss: 2.5089041421577605

Epoch: 5| Step: 10
Training loss: 2.1143621194319158
Validation loss: 2.533761774851633

Epoch: 331| Step: 0
Training loss: 2.4231305221246746
Validation loss: 2.516693968347383

Epoch: 5| Step: 1
Training loss: 2.95801232615618
Validation loss: 2.5059345216094275

Epoch: 5| Step: 2
Training loss: 2.9364063580854554
Validation loss: 2.50230772092838

Epoch: 5| Step: 3
Training loss: 2.6296142712991792
Validation loss: 2.4680447695895937

Epoch: 5| Step: 4
Training loss: 2.0425364399791204
Validation loss: 2.543519458481005

Epoch: 5| Step: 5
Training loss: 1.812144277458122
Validation loss: 2.545308189116747

Epoch: 5| Step: 6
Training loss: 2.904064710479325
Validation loss: 2.4989091297976356

Epoch: 5| Step: 7
Training loss: 1.7021186024977795
Validation loss: 2.5234180072394867

Epoch: 5| Step: 8
Training loss: 2.136409200112354
Validation loss: 2.5233643264445003

Epoch: 5| Step: 9
Training loss: 2.1736617388802753
Validation loss: 2.512730384724315

Epoch: 5| Step: 10
Training loss: 2.0740326476438193
Validation loss: 2.512728584473262

Epoch: 332| Step: 0
Training loss: 2.414566333130528
Validation loss: 2.4613417947505716

Epoch: 5| Step: 1
Training loss: 2.563677819480671
Validation loss: 2.5173992874783027

Epoch: 5| Step: 2
Training loss: 2.561652811079542
Validation loss: 2.5200909403016167

Epoch: 5| Step: 3
Training loss: 2.2237157994642907
Validation loss: 2.501297282933657

Epoch: 5| Step: 4
Training loss: 2.5534043195466696
Validation loss: 2.5225644826522062

Epoch: 5| Step: 5
Training loss: 1.8221234657630236
Validation loss: 2.5017965998406018

Epoch: 5| Step: 6
Training loss: 2.5261925930803426
Validation loss: 2.5082504334257196

Epoch: 5| Step: 7
Training loss: 2.227966615416488
Validation loss: 2.526889557151014

Epoch: 5| Step: 8
Training loss: 2.513387976726139
Validation loss: 2.493705062960952

Epoch: 5| Step: 9
Training loss: 2.651136525273806
Validation loss: 2.5055749997647476

Epoch: 5| Step: 10
Training loss: 2.324474365539402
Validation loss: 2.502581512538832

Epoch: 333| Step: 0
Training loss: 1.7696774319189394
Validation loss: 2.5121701801518785

Epoch: 5| Step: 1
Training loss: 2.4486794007168675
Validation loss: 2.4608472384014015

Epoch: 5| Step: 2
Training loss: 2.1258482642442598
Validation loss: 2.5404534620824712

Epoch: 5| Step: 3
Training loss: 2.5331597364911347
Validation loss: 2.4702700318553488

Epoch: 5| Step: 4
Training loss: 2.3053450341480226
Validation loss: 2.502982919564387

Epoch: 5| Step: 5
Training loss: 2.5106473212642273
Validation loss: 2.490180538913766

Epoch: 5| Step: 6
Training loss: 2.5758755204707486
Validation loss: 2.503900263504014

Epoch: 5| Step: 7
Training loss: 2.6716611006931226
Validation loss: 2.4947574200462097

Epoch: 5| Step: 8
Training loss: 2.653933693219209
Validation loss: 2.4927808119836246

Epoch: 5| Step: 9
Training loss: 2.4422898791545613
Validation loss: 2.4937653622023137

Epoch: 5| Step: 10
Training loss: 2.173308523754188
Validation loss: 2.5051955748342256

Epoch: 334| Step: 0
Training loss: 2.7704090185504953
Validation loss: 2.492567953677132

Epoch: 5| Step: 1
Training loss: 2.128417688707971
Validation loss: 2.513179867916207

Epoch: 5| Step: 2
Training loss: 2.0237435952568847
Validation loss: 2.5175956080694646

Epoch: 5| Step: 3
Training loss: 2.3382426341876945
Validation loss: 2.5034251949919866

Epoch: 5| Step: 4
Training loss: 2.2670808521218144
Validation loss: 2.5096349822077246

Epoch: 5| Step: 5
Training loss: 2.155913174623717
Validation loss: 2.530878557049005

Epoch: 5| Step: 6
Training loss: 2.7898328556049976
Validation loss: 2.4986528232878396

Epoch: 5| Step: 7
Training loss: 2.5892068071039853
Validation loss: 2.512833492728926

Epoch: 5| Step: 8
Training loss: 2.7912964836380803
Validation loss: 2.483183515408951

Epoch: 5| Step: 9
Training loss: 2.3269856372385083
Validation loss: 2.481354591192588

Epoch: 5| Step: 10
Training loss: 2.0831062447441537
Validation loss: 2.476293849791727

Epoch: 335| Step: 0
Training loss: 2.3708884136146122
Validation loss: 2.492375181130464

Epoch: 5| Step: 1
Training loss: 2.5114861791504977
Validation loss: 2.5433140188207455

Epoch: 5| Step: 2
Training loss: 1.803943827598266
Validation loss: 2.474464858078262

Epoch: 5| Step: 3
Training loss: 2.5147249495338175
Validation loss: 2.50323798799125

Epoch: 5| Step: 4
Training loss: 2.206326478491704
Validation loss: 2.4815814838012615

Epoch: 5| Step: 5
Training loss: 2.1992406488280265
Validation loss: 2.4647946378592533

Epoch: 5| Step: 6
Training loss: 2.8025749559953197
Validation loss: 2.4529461946698023

Epoch: 5| Step: 7
Training loss: 2.50709956127475
Validation loss: 2.500771569581962

Epoch: 5| Step: 8
Training loss: 2.271707932155037
Validation loss: 2.517415361867701

Epoch: 5| Step: 9
Training loss: 2.33034455661893
Validation loss: 2.503449378916051

Epoch: 5| Step: 10
Training loss: 2.5731701024651876
Validation loss: 2.5264426302723195

Epoch: 336| Step: 0
Training loss: 2.2068132410993253
Validation loss: 2.516620829957386

Epoch: 5| Step: 1
Training loss: 2.7118576203277502
Validation loss: 2.528110274111228

Epoch: 5| Step: 2
Training loss: 1.8532221313546577
Validation loss: 2.498607129552961

Epoch: 5| Step: 3
Training loss: 2.875824851420859
Validation loss: 2.519040482497387

Epoch: 5| Step: 4
Training loss: 2.7416506510752794
Validation loss: 2.4938443293353636

Epoch: 5| Step: 5
Training loss: 2.1279719271062976
Validation loss: 2.4991577288289557

Epoch: 5| Step: 6
Training loss: 2.484355326640609
Validation loss: 2.5253894210363486

Epoch: 5| Step: 7
Training loss: 2.199142752870687
Validation loss: 2.50237012308731

Epoch: 5| Step: 8
Training loss: 2.098356048525673
Validation loss: 2.531874534182492

Epoch: 5| Step: 9
Training loss: 2.6340348210737745
Validation loss: 2.549140872956033

Epoch: 5| Step: 10
Training loss: 2.2729260895204777
Validation loss: 2.4569907212869473

Epoch: 337| Step: 0
Training loss: 2.230411134633768
Validation loss: 2.489666332979681

Epoch: 5| Step: 1
Training loss: 2.780992388849635
Validation loss: 2.499313821248503

Epoch: 5| Step: 2
Training loss: 2.1947939657432514
Validation loss: 2.5171940513418596

Epoch: 5| Step: 3
Training loss: 2.1011461036861867
Validation loss: 2.5237048130800668

Epoch: 5| Step: 4
Training loss: 2.033845973924327
Validation loss: 2.504608961291371

Epoch: 5| Step: 5
Training loss: 2.665460572085119
Validation loss: 2.4984740194073662

Epoch: 5| Step: 6
Training loss: 2.4340064615745316
Validation loss: 2.502597284127751

Epoch: 5| Step: 7
Training loss: 2.139405954567749
Validation loss: 2.532622495569174

Epoch: 5| Step: 8
Training loss: 2.4281594724429656
Validation loss: 2.4896938899940824

Epoch: 5| Step: 9
Training loss: 2.5416848124705793
Validation loss: 2.4886040114007852

Epoch: 5| Step: 10
Training loss: 2.7400129341599855
Validation loss: 2.4714316210489113

Epoch: 338| Step: 0
Training loss: 2.4508510668425494
Validation loss: 2.471752493092485

Epoch: 5| Step: 1
Training loss: 2.2139206027135727
Validation loss: 2.519375495392144

Epoch: 5| Step: 2
Training loss: 3.5957722157352534
Validation loss: 2.4902750596244325

Epoch: 5| Step: 3
Training loss: 2.0599785622157163
Validation loss: 2.5052211958049067

Epoch: 5| Step: 4
Training loss: 2.20500914176032
Validation loss: 2.4919306245642225

Epoch: 5| Step: 5
Training loss: 2.289748365744624
Validation loss: 2.5328678261476276

Epoch: 5| Step: 6
Training loss: 2.625430662158915
Validation loss: 2.505891152528894

Epoch: 5| Step: 7
Training loss: 2.0065921384757965
Validation loss: 2.50569252804857

Epoch: 5| Step: 8
Training loss: 2.5843798045767574
Validation loss: 2.496066720688342

Epoch: 5| Step: 9
Training loss: 1.8289573926264573
Validation loss: 2.450432946167835

Epoch: 5| Step: 10
Training loss: 1.8361738336917286
Validation loss: 2.507941730992674

Epoch: 339| Step: 0
Training loss: 2.575945493571642
Validation loss: 2.5061390809647146

Epoch: 5| Step: 1
Training loss: 2.5368838309101194
Validation loss: 2.4809099183109864

Epoch: 5| Step: 2
Training loss: 2.3650138115832413
Validation loss: 2.503197784940804

Epoch: 5| Step: 3
Training loss: 2.3594841331858665
Validation loss: 2.5075167289631177

Epoch: 5| Step: 4
Training loss: 2.281001874415369
Validation loss: 2.494876184409919

Epoch: 5| Step: 5
Training loss: 2.340404615225746
Validation loss: 2.4939662148269877

Epoch: 5| Step: 6
Training loss: 2.92285743557295
Validation loss: 2.4913832973317045

Epoch: 5| Step: 7
Training loss: 2.3149074704028885
Validation loss: 2.497536064420592

Epoch: 5| Step: 8
Training loss: 1.9990969645773435
Validation loss: 2.5071157134601902

Epoch: 5| Step: 9
Training loss: 2.2588186167273205
Validation loss: 2.5149790747894163

Epoch: 5| Step: 10
Training loss: 1.9015264302377124
Validation loss: 2.503520945209236

Epoch: 340| Step: 0
Training loss: 2.6742437024391403
Validation loss: 2.4965055372107505

Epoch: 5| Step: 1
Training loss: 2.4313844109181924
Validation loss: 2.488316861571029

Epoch: 5| Step: 2
Training loss: 2.3422273903491537
Validation loss: 2.511003710454654

Epoch: 5| Step: 3
Training loss: 1.9294115247565522
Validation loss: 2.4848261721674056

Epoch: 5| Step: 4
Training loss: 2.659832681255148
Validation loss: 2.508426249082558

Epoch: 5| Step: 5
Training loss: 2.4267075101677964
Validation loss: 2.4774942717827386

Epoch: 5| Step: 6
Training loss: 1.491385838732118
Validation loss: 2.5191407483816723

Epoch: 5| Step: 7
Training loss: 3.0709738759178373
Validation loss: 2.4860391768157672

Epoch: 5| Step: 8
Training loss: 1.8877698288609797
Validation loss: 2.5224884675669874

Epoch: 5| Step: 9
Training loss: 2.443352544517331
Validation loss: 2.5362070338191565

Epoch: 5| Step: 10
Training loss: 2.5904237491862023
Validation loss: 2.4756173243689736

Epoch: 341| Step: 0
Training loss: 2.045230233772463
Validation loss: 2.5137995748593567

Epoch: 5| Step: 1
Training loss: 2.4039842912157536
Validation loss: 2.5067755548405484

Epoch: 5| Step: 2
Training loss: 2.726191192090628
Validation loss: 2.4840767273048145

Epoch: 5| Step: 3
Training loss: 2.3905143961533786
Validation loss: 2.496713572165753

Epoch: 5| Step: 4
Training loss: 2.9868055103022813
Validation loss: 2.477419903703427

Epoch: 5| Step: 5
Training loss: 2.1587739629691107
Validation loss: 2.4946536800260817

Epoch: 5| Step: 6
Training loss: 2.2055762972471653
Validation loss: 2.49666831219784

Epoch: 5| Step: 7
Training loss: 1.920514997912865
Validation loss: 2.4714523764967433

Epoch: 5| Step: 8
Training loss: 2.089664629809215
Validation loss: 2.495900868701443

Epoch: 5| Step: 9
Training loss: 2.6857828376450636
Validation loss: 2.495738983555393

Epoch: 5| Step: 10
Training loss: 2.4379760081774746
Validation loss: 2.490162107708705

Epoch: 342| Step: 0
Training loss: 2.902235314049368
Validation loss: 2.5041216724224595

Epoch: 5| Step: 1
Training loss: 1.9385099547462397
Validation loss: 2.4997470153454917

Epoch: 5| Step: 2
Training loss: 2.5709608008275993
Validation loss: 2.491820829018257

Epoch: 5| Step: 3
Training loss: 2.598686392603396
Validation loss: 2.474037689507893

Epoch: 5| Step: 4
Training loss: 1.809539811134097
Validation loss: 2.508044644884316

Epoch: 5| Step: 5
Training loss: 2.1030564227550483
Validation loss: 2.4874069422370217

Epoch: 5| Step: 6
Training loss: 2.1360346450424856
Validation loss: 2.5108241820739803

Epoch: 5| Step: 7
Training loss: 1.5404409054840675
Validation loss: 2.492915980520578

Epoch: 5| Step: 8
Training loss: 2.9278897478994135
Validation loss: 2.4719696711709283

Epoch: 5| Step: 9
Training loss: 2.2814232747362557
Validation loss: 2.4848939551352336

Epoch: 5| Step: 10
Training loss: 2.7410138436113636
Validation loss: 2.5126604484353154

Epoch: 343| Step: 0
Training loss: 2.509247651437601
Validation loss: 2.48218270468296

Epoch: 5| Step: 1
Training loss: 2.4382519662474227
Validation loss: 2.476749772156896

Epoch: 5| Step: 2
Training loss: 2.5943705724110013
Validation loss: 2.5142031950752415

Epoch: 5| Step: 3
Training loss: 2.6979838084306955
Validation loss: 2.4813730382269465

Epoch: 5| Step: 4
Training loss: 2.4804877813615382
Validation loss: 2.5372880963677624

Epoch: 5| Step: 5
Training loss: 1.9738886543262095
Validation loss: 2.5395435285883945

Epoch: 5| Step: 6
Training loss: 1.907060919706276
Validation loss: 2.5388081892583645

Epoch: 5| Step: 7
Training loss: 2.5439993883708003
Validation loss: 2.475014484827591

Epoch: 5| Step: 8
Training loss: 2.36045913545854
Validation loss: 2.5286697287378987

Epoch: 5| Step: 9
Training loss: 2.2186753166416775
Validation loss: 2.4479549247005448

Epoch: 5| Step: 10
Training loss: 2.015849374510435
Validation loss: 2.4986406794141427

Epoch: 344| Step: 0
Training loss: 1.9785360502824951
Validation loss: 2.4988188034817074

Epoch: 5| Step: 1
Training loss: 2.2607229936415236
Validation loss: 2.527786467922376

Epoch: 5| Step: 2
Training loss: 2.110090007398962
Validation loss: 2.5007912931952325

Epoch: 5| Step: 3
Training loss: 2.065099061155482
Validation loss: 2.4958327128962714

Epoch: 5| Step: 4
Training loss: 2.1575040625824586
Validation loss: 2.4864005624968053

Epoch: 5| Step: 5
Training loss: 2.8856313099997677
Validation loss: 2.5059887374032845

Epoch: 5| Step: 6
Training loss: 2.590370826538535
Validation loss: 2.526731741932987

Epoch: 5| Step: 7
Training loss: 2.4455490241665676
Validation loss: 2.526023449761777

Epoch: 5| Step: 8
Training loss: 2.9857470169672133
Validation loss: 2.474832632671092

Epoch: 5| Step: 9
Training loss: 2.20776635814918
Validation loss: 2.5288338665049346

Epoch: 5| Step: 10
Training loss: 2.1632814616061653
Validation loss: 2.475926982878026

Epoch: 345| Step: 0
Training loss: 2.0728693306532593
Validation loss: 2.545205273608144

Epoch: 5| Step: 1
Training loss: 2.819747065029448
Validation loss: 2.5169194699271635

Epoch: 5| Step: 2
Training loss: 2.174920889632464
Validation loss: 2.480533636276541

Epoch: 5| Step: 3
Training loss: 2.1676417748570045
Validation loss: 2.4897839830949096

Epoch: 5| Step: 4
Training loss: 2.1872813524237444
Validation loss: 2.5018157179657874

Epoch: 5| Step: 5
Training loss: 2.187945184366203
Validation loss: 2.555539040454909

Epoch: 5| Step: 6
Training loss: 2.5707920172126255
Validation loss: 2.479171282077242

Epoch: 5| Step: 7
Training loss: 2.491649222764753
Validation loss: 2.518822954389977

Epoch: 5| Step: 8
Training loss: 2.8313553207378455
Validation loss: 2.4499873409818975

Epoch: 5| Step: 9
Training loss: 2.3363187855105085
Validation loss: 2.4917370557608485

Epoch: 5| Step: 10
Training loss: 2.218763109625113
Validation loss: 2.4742158772339575

Epoch: 346| Step: 0
Training loss: 2.490391098775375
Validation loss: 2.4919095942183165

Epoch: 5| Step: 1
Training loss: 3.0256815623303748
Validation loss: 2.483395332305225

Epoch: 5| Step: 2
Training loss: 2.406477186699213
Validation loss: 2.494082872690861

Epoch: 5| Step: 3
Training loss: 1.990873494350611
Validation loss: 2.4914001950455065

Epoch: 5| Step: 4
Training loss: 2.8477552106325885
Validation loss: 2.4960357172956096

Epoch: 5| Step: 5
Training loss: 2.0308736599234334
Validation loss: 2.50168435668342

Epoch: 5| Step: 6
Training loss: 2.445272914578252
Validation loss: 2.498183523162089

Epoch: 5| Step: 7
Training loss: 2.0151302706349767
Validation loss: 2.499453549498813

Epoch: 5| Step: 8
Training loss: 1.7192083961253393
Validation loss: 2.4952148973198245

Epoch: 5| Step: 9
Training loss: 2.3568376450877424
Validation loss: 2.5137006874306262

Epoch: 5| Step: 10
Training loss: 2.5476305256386795
Validation loss: 2.5152573334347643

Epoch: 347| Step: 0
Training loss: 2.2147880964334425
Validation loss: 2.5296777470523484

Epoch: 5| Step: 1
Training loss: 1.8459400812056412
Validation loss: 2.511605820327254

Epoch: 5| Step: 2
Training loss: 2.8077886326730193
Validation loss: 2.5228170977533706

Epoch: 5| Step: 3
Training loss: 2.2811976387598993
Validation loss: 2.540908044306109

Epoch: 5| Step: 4
Training loss: 2.3894894963680366
Validation loss: 2.490041501069652

Epoch: 5| Step: 5
Training loss: 2.0895500761094117
Validation loss: 2.48078899982381

Epoch: 5| Step: 6
Training loss: 2.463042312779014
Validation loss: 2.4924110736383485

Epoch: 5| Step: 7
Training loss: 2.0343409569114232
Validation loss: 2.453311996381666

Epoch: 5| Step: 8
Training loss: 2.892916374340223
Validation loss: 2.461781231767317

Epoch: 5| Step: 9
Training loss: 2.4913983186707402
Validation loss: 2.5192732621417306

Epoch: 5| Step: 10
Training loss: 2.60258486709223
Validation loss: 2.4984756570346067

Epoch: 348| Step: 0
Training loss: 2.9175260276908275
Validation loss: 2.495340842818061

Epoch: 5| Step: 1
Training loss: 2.4176124717009495
Validation loss: 2.522628723146174

Epoch: 5| Step: 2
Training loss: 2.0430553410634644
Validation loss: 2.5071243744081313

Epoch: 5| Step: 3
Training loss: 2.1293400593704948
Validation loss: 2.511336988182846

Epoch: 5| Step: 4
Training loss: 1.9892508610120316
Validation loss: 2.507865154102181

Epoch: 5| Step: 5
Training loss: 2.405049408466597
Validation loss: 2.4801848357839047

Epoch: 5| Step: 6
Training loss: 2.48598835699496
Validation loss: 2.4698551467120646

Epoch: 5| Step: 7
Training loss: 2.3964891503619903
Validation loss: 2.4972791262851195

Epoch: 5| Step: 8
Training loss: 2.145800050150202
Validation loss: 2.490518025235348

Epoch: 5| Step: 9
Training loss: 1.9627455320869838
Validation loss: 2.491913102374645

Epoch: 5| Step: 10
Training loss: 2.8319362861233492
Validation loss: 2.4858215231964835

Epoch: 349| Step: 0
Training loss: 2.570741194480974
Validation loss: 2.4978394278269125

Epoch: 5| Step: 1
Training loss: 1.9973562171187051
Validation loss: 2.5035063621826024

Epoch: 5| Step: 2
Training loss: 2.3805606394886207
Validation loss: 2.5555194364274754

Epoch: 5| Step: 3
Training loss: 2.445052062964445
Validation loss: 2.520087313183596

Epoch: 5| Step: 4
Training loss: 2.698008993525464
Validation loss: 2.4790376991761325

Epoch: 5| Step: 5
Training loss: 2.6518954318102557
Validation loss: 2.4585196930368673

Epoch: 5| Step: 6
Training loss: 2.0006412431795795
Validation loss: 2.4984806786743516

Epoch: 5| Step: 7
Training loss: 2.508095888662324
Validation loss: 2.504834636877493

Epoch: 5| Step: 8
Training loss: 2.06543522827271
Validation loss: 2.527487321716373

Epoch: 5| Step: 9
Training loss: 2.324281528170024
Validation loss: 2.520988855841753

Epoch: 5| Step: 10
Training loss: 2.047846439848147
Validation loss: 2.5060805105826534

Epoch: 350| Step: 0
Training loss: 2.3478374189143287
Validation loss: 2.4747843040461754

Epoch: 5| Step: 1
Training loss: 2.22574340745296
Validation loss: 2.480109511085205

Epoch: 5| Step: 2
Training loss: 2.710438883106645
Validation loss: 2.497410013032699

Epoch: 5| Step: 3
Training loss: 2.143634879799796
Validation loss: 2.496196770826057

Epoch: 5| Step: 4
Training loss: 2.4859250589247632
Validation loss: 2.4839864922245516

Epoch: 5| Step: 5
Training loss: 2.209929003718683
Validation loss: 2.4635181392491545

Epoch: 5| Step: 6
Training loss: 2.5138260943003305
Validation loss: 2.4756956028885364

Epoch: 5| Step: 7
Training loss: 2.436973123583007
Validation loss: 2.4784006331469017

Epoch: 5| Step: 8
Training loss: 2.451856832055298
Validation loss: 2.484798375457477

Epoch: 5| Step: 9
Training loss: 2.1164488682868656
Validation loss: 2.4963341076844268

Epoch: 5| Step: 10
Training loss: 2.251595461284477
Validation loss: 2.4787251175864933

Epoch: 351| Step: 0
Training loss: 2.168211667309959
Validation loss: 2.463321466175752

Epoch: 5| Step: 1
Training loss: 2.110083566972392
Validation loss: 2.4996529451068588

Epoch: 5| Step: 2
Training loss: 2.1132118650881333
Validation loss: 2.4617198882112685

Epoch: 5| Step: 3
Training loss: 2.611747804982762
Validation loss: 2.4497557870992903

Epoch: 5| Step: 4
Training loss: 2.3505818803195146
Validation loss: 2.4508277174601094

Epoch: 5| Step: 5
Training loss: 2.0930064005382523
Validation loss: 2.4986034430319832

Epoch: 5| Step: 6
Training loss: 3.0330448146166824
Validation loss: 2.483248295649058

Epoch: 5| Step: 7
Training loss: 2.5970192952928346
Validation loss: 2.480091821649131

Epoch: 5| Step: 8
Training loss: 2.5330666510438236
Validation loss: 2.496269325044625

Epoch: 5| Step: 9
Training loss: 2.1603434310855243
Validation loss: 2.512438487952553

Epoch: 5| Step: 10
Training loss: 1.8751286780389504
Validation loss: 2.519874128556713

Epoch: 352| Step: 0
Training loss: 2.3458207201802117
Validation loss: 2.507431955682153

Epoch: 5| Step: 1
Training loss: 2.5936119318603503
Validation loss: 2.50846661003856

Epoch: 5| Step: 2
Training loss: 2.3949757796558067
Validation loss: 2.4871460674616834

Epoch: 5| Step: 3
Training loss: 2.709839700272301
Validation loss: 2.527979832964824

Epoch: 5| Step: 4
Training loss: 2.879782803345732
Validation loss: 2.503467249432399

Epoch: 5| Step: 5
Training loss: 2.175506081294032
Validation loss: 2.495399364261959

Epoch: 5| Step: 6
Training loss: 2.380934278328515
Validation loss: 2.5035487122203737

Epoch: 5| Step: 7
Training loss: 1.7451203980760548
Validation loss: 2.4968633519808012

Epoch: 5| Step: 8
Training loss: 1.9441691029589654
Validation loss: 2.5415961667481914

Epoch: 5| Step: 9
Training loss: 1.7780477282422626
Validation loss: 2.470463637370011

Epoch: 5| Step: 10
Training loss: 2.606984122458784
Validation loss: 2.5051969542820656

Epoch: 353| Step: 0
Training loss: 1.9529026973098051
Validation loss: 2.5234636296329973

Epoch: 5| Step: 1
Training loss: 2.3868350236356326
Validation loss: 2.55337789941806

Epoch: 5| Step: 2
Training loss: 2.012972365203385
Validation loss: 2.5212861954952213

Epoch: 5| Step: 3
Training loss: 2.7460284598390623
Validation loss: 2.541360289614846

Epoch: 5| Step: 4
Training loss: 2.948793655984381
Validation loss: 2.5041229337048097

Epoch: 5| Step: 5
Training loss: 2.1856926127124154
Validation loss: 2.458843252010152

Epoch: 5| Step: 6
Training loss: 2.062670382773552
Validation loss: 2.5261932588047373

Epoch: 5| Step: 7
Training loss: 2.142648005497839
Validation loss: 2.487742941664535

Epoch: 5| Step: 8
Training loss: 2.2843434510355087
Validation loss: 2.4921747907392344

Epoch: 5| Step: 9
Training loss: 2.5320664019382395
Validation loss: 2.5154041368356284

Epoch: 5| Step: 10
Training loss: 2.388094191982698
Validation loss: 2.4874924067270414

Epoch: 354| Step: 0
Training loss: 1.9716304718117716
Validation loss: 2.475457441770092

Epoch: 5| Step: 1
Training loss: 1.92460624333772
Validation loss: 2.504982148320693

Epoch: 5| Step: 2
Training loss: 2.3196192261203157
Validation loss: 2.505740999224766

Epoch: 5| Step: 3
Training loss: 2.6606174242889895
Validation loss: 2.4942713321234278

Epoch: 5| Step: 4
Training loss: 1.815909172051159
Validation loss: 2.5348385220638616

Epoch: 5| Step: 5
Training loss: 3.1625256985913155
Validation loss: 2.5003577991522064

Epoch: 5| Step: 6
Training loss: 2.9526618715466806
Validation loss: 2.491757870476009

Epoch: 5| Step: 7
Training loss: 2.3192473256098913
Validation loss: 2.5136359525725056

Epoch: 5| Step: 8
Training loss: 2.2405078724316554
Validation loss: 2.5072408167720837

Epoch: 5| Step: 9
Training loss: 2.1571549437805926
Validation loss: 2.51801595334457

Epoch: 5| Step: 10
Training loss: 2.122386222458748
Validation loss: 2.495771696289767

Epoch: 355| Step: 0
Training loss: 2.0545238821350273
Validation loss: 2.5020715047759183

Epoch: 5| Step: 1
Training loss: 2.3161319357956383
Validation loss: 2.499261394214449

Epoch: 5| Step: 2
Training loss: 2.52540440861716
Validation loss: 2.5006773241351996

Epoch: 5| Step: 3
Training loss: 2.3962991234047903
Validation loss: 2.4887751652857792

Epoch: 5| Step: 4
Training loss: 2.287658781784071
Validation loss: 2.5012007516404253

Epoch: 5| Step: 5
Training loss: 2.0476969459096233
Validation loss: 2.516455807973252

Epoch: 5| Step: 6
Training loss: 2.5827787070535826
Validation loss: 2.487009617398346

Epoch: 5| Step: 7
Training loss: 2.4727023393092997
Validation loss: 2.4529040694136723

Epoch: 5| Step: 8
Training loss: 1.962250167592003
Validation loss: 2.4633578428941587

Epoch: 5| Step: 9
Training loss: 2.7814662720676924
Validation loss: 2.503899673761203

Epoch: 5| Step: 10
Training loss: 2.0479620456680405
Validation loss: 2.477520517589802

Epoch: 356| Step: 0
Training loss: 2.003317822773448
Validation loss: 2.4830887464343774

Epoch: 5| Step: 1
Training loss: 2.8299893298807275
Validation loss: 2.51060811758056

Epoch: 5| Step: 2
Training loss: 2.5986467581127117
Validation loss: 2.5030927580753235

Epoch: 5| Step: 3
Training loss: 2.0044547774708055
Validation loss: 2.5050904027472503

Epoch: 5| Step: 4
Training loss: 2.424570368039248
Validation loss: 2.5001859708546865

Epoch: 5| Step: 5
Training loss: 2.1546267605326377
Validation loss: 2.4885645675946395

Epoch: 5| Step: 6
Training loss: 2.2078190570580043
Validation loss: 2.470459626074005

Epoch: 5| Step: 7
Training loss: 2.7171596391860704
Validation loss: 2.519171582415974

Epoch: 5| Step: 8
Training loss: 2.166446515660727
Validation loss: 2.5349853565216818

Epoch: 5| Step: 9
Training loss: 2.475706897316089
Validation loss: 2.524840662970951

Epoch: 5| Step: 10
Training loss: 1.864693927859298
Validation loss: 2.4655528500361945

Epoch: 357| Step: 0
Training loss: 2.2283689432239995
Validation loss: 2.5148247212928885

Epoch: 5| Step: 1
Training loss: 3.2143082572509805
Validation loss: 2.4756162049325705

Epoch: 5| Step: 2
Training loss: 2.5457479405376544
Validation loss: 2.496171907094242

Epoch: 5| Step: 3
Training loss: 2.33281946200375
Validation loss: 2.4999144611286326

Epoch: 5| Step: 4
Training loss: 2.5583152095223123
Validation loss: 2.4997149884387864

Epoch: 5| Step: 5
Training loss: 2.164950082040473
Validation loss: 2.5036841288188207

Epoch: 5| Step: 6
Training loss: 1.6833273550752041
Validation loss: 2.486452337918572

Epoch: 5| Step: 7
Training loss: 1.8995737827487953
Validation loss: 2.5323052216586843

Epoch: 5| Step: 8
Training loss: 2.1755921094995045
Validation loss: 2.56385612569368

Epoch: 5| Step: 9
Training loss: 2.4066072669408443
Validation loss: 2.5142191048308065

Epoch: 5| Step: 10
Training loss: 2.5240777210354266
Validation loss: 2.5000883604657256

Epoch: 358| Step: 0
Training loss: 2.5292689730603857
Validation loss: 2.500075591390423

Epoch: 5| Step: 1
Training loss: 1.8187019525322932
Validation loss: 2.4735407734129042

Epoch: 5| Step: 2
Training loss: 2.089353585916568
Validation loss: 2.4409720337988827

Epoch: 5| Step: 3
Training loss: 2.0974041380590362
Validation loss: 2.4987747209734095

Epoch: 5| Step: 4
Training loss: 2.641228781748954
Validation loss: 2.4810086242290725

Epoch: 5| Step: 5
Training loss: 2.2846258607568366
Validation loss: 2.4910304318792127

Epoch: 5| Step: 6
Training loss: 2.245191947020852
Validation loss: 2.50910060533053

Epoch: 5| Step: 7
Training loss: 2.473702979043327
Validation loss: 2.4995837147022457

Epoch: 5| Step: 8
Training loss: 2.4361535290348204
Validation loss: 2.530827178947055

Epoch: 5| Step: 9
Training loss: 2.897550627140244
Validation loss: 2.5171992749753613

Epoch: 5| Step: 10
Training loss: 2.3520897562521004
Validation loss: 2.5127260430001828

Epoch: 359| Step: 0
Training loss: 1.8426921201876443
Validation loss: 2.4656985266489166

Epoch: 5| Step: 1
Training loss: 2.050409076162158
Validation loss: 2.5382665202114643

Epoch: 5| Step: 2
Training loss: 2.3184285848517874
Validation loss: 2.5036464718492573

Epoch: 5| Step: 3
Training loss: 2.9221078708484587
Validation loss: 2.479761273815561

Epoch: 5| Step: 4
Training loss: 2.4026340216212305
Validation loss: 2.465695380448589

Epoch: 5| Step: 5
Training loss: 2.4065746732373445
Validation loss: 2.51823472444255

Epoch: 5| Step: 6
Training loss: 2.2346490011738456
Validation loss: 2.466609362701822

Epoch: 5| Step: 7
Training loss: 2.2391790217276544
Validation loss: 2.5290085806605704

Epoch: 5| Step: 8
Training loss: 2.997353658150197
Validation loss: 2.4925212710538456

Epoch: 5| Step: 9
Training loss: 2.0801463227890116
Validation loss: 2.4962308855849162

Epoch: 5| Step: 10
Training loss: 2.0233639736882565
Validation loss: 2.483478723192734

Epoch: 360| Step: 0
Training loss: 2.0848243972416443
Validation loss: 2.486830869115702

Epoch: 5| Step: 1
Training loss: 2.382415938600624
Validation loss: 2.508850723045531

Epoch: 5| Step: 2
Training loss: 1.5801303322068891
Validation loss: 2.476152068175158

Epoch: 5| Step: 3
Training loss: 2.4489481166516693
Validation loss: 2.490964640109306

Epoch: 5| Step: 4
Training loss: 2.4479932624250456
Validation loss: 2.5157291607600114

Epoch: 5| Step: 5
Training loss: 2.512793232521677
Validation loss: 2.5523111470652733

Epoch: 5| Step: 6
Training loss: 2.766596133317343
Validation loss: 2.488527203105442

Epoch: 5| Step: 7
Training loss: 2.6030418611693045
Validation loss: 2.440812755483088

Epoch: 5| Step: 8
Training loss: 2.33136170965911
Validation loss: 2.476578151583161

Epoch: 5| Step: 9
Training loss: 1.6227989696024576
Validation loss: 2.5180195320313605

Epoch: 5| Step: 10
Training loss: 2.6310109843159273
Validation loss: 2.5100097568157116

Epoch: 361| Step: 0
Training loss: 2.155055834938594
Validation loss: 2.4673926798120878

Epoch: 5| Step: 1
Training loss: 3.145797796659191
Validation loss: 2.4469125971964143

Epoch: 5| Step: 2
Training loss: 2.2336800301410187
Validation loss: 2.5061390308404445

Epoch: 5| Step: 3
Training loss: 2.5026416173720203
Validation loss: 2.4807200443977413

Epoch: 5| Step: 4
Training loss: 2.2884382106181365
Validation loss: 2.4730373883850874

Epoch: 5| Step: 5
Training loss: 2.282016050711911
Validation loss: 2.464886074750333

Epoch: 5| Step: 6
Training loss: 2.0968277358798644
Validation loss: 2.4966656434790764

Epoch: 5| Step: 7
Training loss: 1.8379820781660043
Validation loss: 2.49557767936079

Epoch: 5| Step: 8
Training loss: 2.2460280433663633
Validation loss: 2.5005774179817917

Epoch: 5| Step: 9
Training loss: 2.4076844930814953
Validation loss: 2.490084227334881

Epoch: 5| Step: 10
Training loss: 2.171049441406729
Validation loss: 2.502139866663195

Epoch: 362| Step: 0
Training loss: 2.448028713394515
Validation loss: 2.485692611861816

Epoch: 5| Step: 1
Training loss: 1.8336759882827944
Validation loss: 2.4723913364501753

Epoch: 5| Step: 2
Training loss: 2.7907679710202546
Validation loss: 2.5390755823901103

Epoch: 5| Step: 3
Training loss: 2.1118641048623386
Validation loss: 2.489512019595476

Epoch: 5| Step: 4
Training loss: 2.276671007616737
Validation loss: 2.4689593773463265

Epoch: 5| Step: 5
Training loss: 2.659841913817482
Validation loss: 2.5105910106423686

Epoch: 5| Step: 6
Training loss: 2.128342860157475
Validation loss: 2.4566219432416068

Epoch: 5| Step: 7
Training loss: 2.2738089241668056
Validation loss: 2.489959788906466

Epoch: 5| Step: 8
Training loss: 2.5785232698708347
Validation loss: 2.4744855746155743

Epoch: 5| Step: 9
Training loss: 1.9999400368284606
Validation loss: 2.4461929689837527

Epoch: 5| Step: 10
Training loss: 2.3630775553034966
Validation loss: 2.5421263589888468

Epoch: 363| Step: 0
Training loss: 2.23104450123654
Validation loss: 2.4842798162300164

Epoch: 5| Step: 1
Training loss: 2.59046406171234
Validation loss: 2.5189477357872114

Epoch: 5| Step: 2
Training loss: 1.8419218536724111
Validation loss: 2.4903076994956765

Epoch: 5| Step: 3
Training loss: 2.134898859527396
Validation loss: 2.4768502668069137

Epoch: 5| Step: 4
Training loss: 2.1780657460541732
Validation loss: 2.5213097871492933

Epoch: 5| Step: 5
Training loss: 2.7996571944059445
Validation loss: 2.485648414655643

Epoch: 5| Step: 6
Training loss: 2.3116539103241522
Validation loss: 2.5060146520484006

Epoch: 5| Step: 7
Training loss: 2.1351406260148034
Validation loss: 2.489954664625894

Epoch: 5| Step: 8
Training loss: 2.7340098981978107
Validation loss: 2.4670878191599113

Epoch: 5| Step: 9
Training loss: 2.2996889899241633
Validation loss: 2.488405735279168

Epoch: 5| Step: 10
Training loss: 2.137995197422782
Validation loss: 2.505953491030278

Epoch: 364| Step: 0
Training loss: 2.4003059986862305
Validation loss: 2.454879312162098

Epoch: 5| Step: 1
Training loss: 2.728182161306946
Validation loss: 2.477848366004838

Epoch: 5| Step: 2
Training loss: 2.114737130821892
Validation loss: 2.4868941863958955

Epoch: 5| Step: 3
Training loss: 1.9015832277671267
Validation loss: 2.472513614867852

Epoch: 5| Step: 4
Training loss: 1.8739009179025359
Validation loss: 2.4783578057973186

Epoch: 5| Step: 5
Training loss: 2.197889017470554
Validation loss: 2.4544399748241377

Epoch: 5| Step: 6
Training loss: 2.66708783956309
Validation loss: 2.4545008767761125

Epoch: 5| Step: 7
Training loss: 2.343222596910476
Validation loss: 2.5094081864974367

Epoch: 5| Step: 8
Training loss: 2.7892962018575185
Validation loss: 2.500103166974138

Epoch: 5| Step: 9
Training loss: 2.121762333661151
Validation loss: 2.486115726328332

Epoch: 5| Step: 10
Training loss: 1.886750594160614
Validation loss: 2.5059860699146346

Epoch: 365| Step: 0
Training loss: 2.3182567390534
Validation loss: 2.53528100915227

Epoch: 5| Step: 1
Training loss: 2.7682560189428966
Validation loss: 2.462553811233625

Epoch: 5| Step: 2
Training loss: 1.7763195398450973
Validation loss: 2.5185474909259975

Epoch: 5| Step: 3
Training loss: 2.781675156233144
Validation loss: 2.507689615000985

Epoch: 5| Step: 4
Training loss: 2.278704961100417
Validation loss: 2.4987034358061364

Epoch: 5| Step: 5
Training loss: 2.2320691995729276
Validation loss: 2.5246334012454246

Epoch: 5| Step: 6
Training loss: 1.9870732503059112
Validation loss: 2.4812157644650723

Epoch: 5| Step: 7
Training loss: 2.4369461212776646
Validation loss: 2.4518828041489242

Epoch: 5| Step: 8
Training loss: 2.391125794857622
Validation loss: 2.4628771035780854

Epoch: 5| Step: 9
Training loss: 1.389293375985281
Validation loss: 2.5059738490463404

Epoch: 5| Step: 10
Training loss: 2.621913821418351
Validation loss: 2.4731469459887543

Epoch: 366| Step: 0
Training loss: 1.7370144117478914
Validation loss: 2.543955965318786

Epoch: 5| Step: 1
Training loss: 2.6902535105656566
Validation loss: 2.464918160532985

Epoch: 5| Step: 2
Training loss: 2.3916545969517853
Validation loss: 2.5411948706381335

Epoch: 5| Step: 3
Training loss: 2.5080923714548473
Validation loss: 2.4583008063409872

Epoch: 5| Step: 4
Training loss: 2.038241283160889
Validation loss: 2.485255289690153

Epoch: 5| Step: 5
Training loss: 2.148830974266994
Validation loss: 2.4779417111003554

Epoch: 5| Step: 6
Training loss: 2.1938451023626078
Validation loss: 2.5268609803286144

Epoch: 5| Step: 7
Training loss: 2.6737934386160935
Validation loss: 2.5095866117799766

Epoch: 5| Step: 8
Training loss: 2.613591151543787
Validation loss: 2.481497768815188

Epoch: 5| Step: 9
Training loss: 2.1319251817239624
Validation loss: 2.496372174827054

Epoch: 5| Step: 10
Training loss: 2.531920450217433
Validation loss: 2.493543614600228

Epoch: 367| Step: 0
Training loss: 2.148926391462791
Validation loss: 2.5079859268320392

Epoch: 5| Step: 1
Training loss: 2.1636814925060444
Validation loss: 2.5422316151461404

Epoch: 5| Step: 2
Training loss: 2.47033399153933
Validation loss: 2.5324615107179698

Epoch: 5| Step: 3
Training loss: 3.184537969879462
Validation loss: 2.4949121272516543

Epoch: 5| Step: 4
Training loss: 2.4252022806334623
Validation loss: 2.4948337919683072

Epoch: 5| Step: 5
Training loss: 2.221574087462857
Validation loss: 2.542331235643569

Epoch: 5| Step: 6
Training loss: 2.4051620203589708
Validation loss: 2.4460051548024055

Epoch: 5| Step: 7
Training loss: 1.7646945763674569
Validation loss: 2.5011007223820694

Epoch: 5| Step: 8
Training loss: 1.9288863399276845
Validation loss: 2.442152139503505

Epoch: 5| Step: 9
Training loss: 2.5111266486869694
Validation loss: 2.5005684913800015

Epoch: 5| Step: 10
Training loss: 2.167836778077796
Validation loss: 2.459354094509114

Epoch: 368| Step: 0
Training loss: 2.4449539652978207
Validation loss: 2.4602350422932595

Epoch: 5| Step: 1
Training loss: 3.2718058960474923
Validation loss: 2.4694213629304156

Epoch: 5| Step: 2
Training loss: 1.9483760625212627
Validation loss: 2.472637380236926

Epoch: 5| Step: 3
Training loss: 2.9712489803506643
Validation loss: 2.4735624822853395

Epoch: 5| Step: 4
Training loss: 2.1287259366730034
Validation loss: 2.5141419166449794

Epoch: 5| Step: 5
Training loss: 2.169178668045255
Validation loss: 2.5369331360352594

Epoch: 5| Step: 6
Training loss: 2.184422862738718
Validation loss: 2.494206645971017

Epoch: 5| Step: 7
Training loss: 2.635694788152905
Validation loss: 2.4721407483611078

Epoch: 5| Step: 8
Training loss: 1.8666877237335713
Validation loss: 2.4901004054695957

Epoch: 5| Step: 9
Training loss: 1.8945683111911413
Validation loss: 2.488256063507856

Epoch: 5| Step: 10
Training loss: 1.3972938165761413
Validation loss: 2.5051945341078667

Epoch: 369| Step: 0
Training loss: 1.6957685147331587
Validation loss: 2.507943894497112

Epoch: 5| Step: 1
Training loss: 2.747544319274387
Validation loss: 2.510246515837133

Epoch: 5| Step: 2
Training loss: 2.168474567963456
Validation loss: 2.4629848357106727

Epoch: 5| Step: 3
Training loss: 1.8065685031001268
Validation loss: 2.4826558746698426

Epoch: 5| Step: 4
Training loss: 2.247513881042183
Validation loss: 2.552107675288531

Epoch: 5| Step: 5
Training loss: 2.6665545579073817
Validation loss: 2.4568984495327513

Epoch: 5| Step: 6
Training loss: 2.1462515512948492
Validation loss: 2.4782529929183976

Epoch: 5| Step: 7
Training loss: 2.3382689409731774
Validation loss: 2.467027435246684

Epoch: 5| Step: 8
Training loss: 2.4162245269237825
Validation loss: 2.5068820222229644

Epoch: 5| Step: 9
Training loss: 2.5783402786539438
Validation loss: 2.502214414964847

Epoch: 5| Step: 10
Training loss: 2.5370545884252587
Validation loss: 2.528399827781328

Epoch: 370| Step: 0
Training loss: 2.506657224376967
Validation loss: 2.512170351593931

Epoch: 5| Step: 1
Training loss: 2.08680655559982
Validation loss: 2.481608210140963

Epoch: 5| Step: 2
Training loss: 1.8145767678960665
Validation loss: 2.506416873707245

Epoch: 5| Step: 3
Training loss: 2.770852703131946
Validation loss: 2.5289117102138094

Epoch: 5| Step: 4
Training loss: 2.7737395645914877
Validation loss: 2.49085712802215

Epoch: 5| Step: 5
Training loss: 2.236994135269574
Validation loss: 2.4573672083665064

Epoch: 5| Step: 6
Training loss: 2.69784144194805
Validation loss: 2.4821023316239943

Epoch: 5| Step: 7
Training loss: 1.3106901315087218
Validation loss: 2.4584006791634585

Epoch: 5| Step: 8
Training loss: 2.358756281081723
Validation loss: 2.494141507936148

Epoch: 5| Step: 9
Training loss: 2.0255344674542606
Validation loss: 2.459981818908568

Epoch: 5| Step: 10
Training loss: 2.2544983296069847
Validation loss: 2.5007068957468577

Epoch: 371| Step: 0
Training loss: 1.9777487109164031
Validation loss: 2.455526958203175

Epoch: 5| Step: 1
Training loss: 1.7872301738331422
Validation loss: 2.4421012546117935

Epoch: 5| Step: 2
Training loss: 2.526527143147252
Validation loss: 2.459254823326779

Epoch: 5| Step: 3
Training loss: 2.056850440760518
Validation loss: 2.512756775165038

Epoch: 5| Step: 4
Training loss: 2.651745645900175
Validation loss: 2.4407489055411826

Epoch: 5| Step: 5
Training loss: 2.146585005674006
Validation loss: 2.452732192417872

Epoch: 5| Step: 6
Training loss: 2.167410233870646
Validation loss: 2.4795609537478343

Epoch: 5| Step: 7
Training loss: 2.1503727744987424
Validation loss: 2.5074917203457967

Epoch: 5| Step: 8
Training loss: 3.084674818314114
Validation loss: 2.474974602777897

Epoch: 5| Step: 9
Training loss: 2.330459653022651
Validation loss: 2.510456858102894

Epoch: 5| Step: 10
Training loss: 2.5104734380061804
Validation loss: 2.5153246021063063

Epoch: 372| Step: 0
Training loss: 2.0394588392199924
Validation loss: 2.505833666432058

Epoch: 5| Step: 1
Training loss: 1.884210787524671
Validation loss: 2.4733430010185935

Epoch: 5| Step: 2
Training loss: 2.0768856856248887
Validation loss: 2.4684897644281008

Epoch: 5| Step: 3
Training loss: 2.221807173540054
Validation loss: 2.4925879828751634

Epoch: 5| Step: 4
Training loss: 2.005047151767396
Validation loss: 2.4842814869487357

Epoch: 5| Step: 5
Training loss: 2.098844110174912
Validation loss: 2.5227253978801194

Epoch: 5| Step: 6
Training loss: 2.8257120703512606
Validation loss: 2.4985714604643157

Epoch: 5| Step: 7
Training loss: 2.2676306955846264
Validation loss: 2.4960847652538773

Epoch: 5| Step: 8
Training loss: 2.490129918918296
Validation loss: 2.4982562331252898

Epoch: 5| Step: 9
Training loss: 2.5916670743036305
Validation loss: 2.487414438127594

Epoch: 5| Step: 10
Training loss: 2.500840903955746
Validation loss: 2.4763344746137763

Epoch: 373| Step: 0
Training loss: 1.8418287838773892
Validation loss: 2.4637072999037564

Epoch: 5| Step: 1
Training loss: 2.658572359808942
Validation loss: 2.522629303429202

Epoch: 5| Step: 2
Training loss: 2.1033716745806537
Validation loss: 2.5101747164325574

Epoch: 5| Step: 3
Training loss: 1.5520549319269437
Validation loss: 2.476197601678511

Epoch: 5| Step: 4
Training loss: 2.063072009672941
Validation loss: 2.471148326872637

Epoch: 5| Step: 5
Training loss: 2.031382160288934
Validation loss: 2.4825138853303357

Epoch: 5| Step: 6
Training loss: 2.7289485601579297
Validation loss: 2.4891505356688524

Epoch: 5| Step: 7
Training loss: 2.7235191871355178
Validation loss: 2.4975387742944166

Epoch: 5| Step: 8
Training loss: 2.94489704308323
Validation loss: 2.493587370637825

Epoch: 5| Step: 9
Training loss: 2.0349939907650714
Validation loss: 2.469505647170895

Epoch: 5| Step: 10
Training loss: 2.5598704174147326
Validation loss: 2.4563893021267047

Epoch: 374| Step: 0
Training loss: 2.180526298548052
Validation loss: 2.4922921372882794

Epoch: 5| Step: 1
Training loss: 2.8207498565297833
Validation loss: 2.4987819411500647

Epoch: 5| Step: 2
Training loss: 2.6640326982100966
Validation loss: 2.513100340321523

Epoch: 5| Step: 3
Training loss: 2.808433220404733
Validation loss: 2.4778037888419018

Epoch: 5| Step: 4
Training loss: 2.030084715893609
Validation loss: 2.484934602363294

Epoch: 5| Step: 5
Training loss: 2.09992366833697
Validation loss: 2.45137451913506

Epoch: 5| Step: 6
Training loss: 2.1579176493886343
Validation loss: 2.473670063073769

Epoch: 5| Step: 7
Training loss: 1.733286954491586
Validation loss: 2.4577873469066076

Epoch: 5| Step: 8
Training loss: 1.914793785685346
Validation loss: 2.464606600841513

Epoch: 5| Step: 9
Training loss: 1.8309201773510768
Validation loss: 2.477289650706917

Epoch: 5| Step: 10
Training loss: 2.516372380092372
Validation loss: 2.478942231873498

Epoch: 375| Step: 0
Training loss: 2.9475367720286267
Validation loss: 2.4520718621085065

Epoch: 5| Step: 1
Training loss: 2.106727124968288
Validation loss: 2.474298491654041

Epoch: 5| Step: 2
Training loss: 1.9073891909750325
Validation loss: 2.460816401770393

Epoch: 5| Step: 3
Training loss: 2.227816579908364
Validation loss: 2.485851529871152

Epoch: 5| Step: 4
Training loss: 2.0178562796068817
Validation loss: 2.4859844877945583

Epoch: 5| Step: 5
Training loss: 1.9275173695099541
Validation loss: 2.476230479908874

Epoch: 5| Step: 6
Training loss: 2.082411625739761
Validation loss: 2.4793658908521037

Epoch: 5| Step: 7
Training loss: 2.215955451889388
Validation loss: 2.4488190491630566

Epoch: 5| Step: 8
Training loss: 2.151094548373567
Validation loss: 2.505666232534271

Epoch: 5| Step: 9
Training loss: 3.027784746105641
Validation loss: 2.4584520160801033

Epoch: 5| Step: 10
Training loss: 2.16997582259531
Validation loss: 2.4456310209576486

Epoch: 376| Step: 0
Training loss: 2.4553933316821244
Validation loss: 2.493153777228872

Epoch: 5| Step: 1
Training loss: 2.567766963967699
Validation loss: 2.480047467776802

Epoch: 5| Step: 2
Training loss: 2.0504484942092023
Validation loss: 2.521451740147906

Epoch: 5| Step: 3
Training loss: 1.7442029576627633
Validation loss: 2.458215915788116

Epoch: 5| Step: 4
Training loss: 2.611008367027572
Validation loss: 2.470991643171664

Epoch: 5| Step: 5
Training loss: 1.9740355249240347
Validation loss: 2.469024452394471

Epoch: 5| Step: 6
Training loss: 2.230063593688972
Validation loss: 2.484082358051764

Epoch: 5| Step: 7
Training loss: 1.9120848111531885
Validation loss: 2.480418181927357

Epoch: 5| Step: 8
Training loss: 2.1733012833406615
Validation loss: 2.474559432073188

Epoch: 5| Step: 9
Training loss: 2.9893725029553915
Validation loss: 2.5158054198193045

Epoch: 5| Step: 10
Training loss: 2.3539825862606665
Validation loss: 2.509279192451991

Epoch: 377| Step: 0
Training loss: 2.142132082388581
Validation loss: 2.4609578897621485

Epoch: 5| Step: 1
Training loss: 2.7553741655158537
Validation loss: 2.4951738987042456

Epoch: 5| Step: 2
Training loss: 2.449530136050719
Validation loss: 2.5117432531496995

Epoch: 5| Step: 3
Training loss: 1.6527003992312523
Validation loss: 2.536005515444173

Epoch: 5| Step: 4
Training loss: 1.7842460999972019
Validation loss: 2.4483606606543145

Epoch: 5| Step: 5
Training loss: 2.303382943976703
Validation loss: 2.4730320289689662

Epoch: 5| Step: 6
Training loss: 2.3433331945463585
Validation loss: 2.476588553827333

Epoch: 5| Step: 7
Training loss: 2.081705321947445
Validation loss: 2.488363232605204

Epoch: 5| Step: 8
Training loss: 2.7761110126548867
Validation loss: 2.505032954305546

Epoch: 5| Step: 9
Training loss: 2.232021666370362
Validation loss: 2.46444051450779

Epoch: 5| Step: 10
Training loss: 2.251314944570123
Validation loss: 2.4589037284745383

Epoch: 378| Step: 0
Training loss: 2.6639990477402735
Validation loss: 2.4962918385915476

Epoch: 5| Step: 1
Training loss: 2.2058149649106626
Validation loss: 2.45018900434603

Epoch: 5| Step: 2
Training loss: 2.1497708708629144
Validation loss: 2.5063132168572055

Epoch: 5| Step: 3
Training loss: 2.8234869930993867
Validation loss: 2.4947898743654466

Epoch: 5| Step: 4
Training loss: 2.201458672808994
Validation loss: 2.484626833100842

Epoch: 5| Step: 5
Training loss: 1.9771028164652944
Validation loss: 2.4666768387144367

Epoch: 5| Step: 6
Training loss: 1.9835548207751315
Validation loss: 2.5017752199943057

Epoch: 5| Step: 7
Training loss: 1.9215618591754196
Validation loss: 2.4890994415498877

Epoch: 5| Step: 8
Training loss: 2.543649140029104
Validation loss: 2.4569847342118796

Epoch: 5| Step: 9
Training loss: 2.2528747631268233
Validation loss: 2.5149734520557345

Epoch: 5| Step: 10
Training loss: 2.1065455922612712
Validation loss: 2.4640296231303527

Epoch: 379| Step: 0
Training loss: 2.203773889958483
Validation loss: 2.461619247123209

Epoch: 5| Step: 1
Training loss: 2.470990383652825
Validation loss: 2.4560215318739433

Epoch: 5| Step: 2
Training loss: 1.6095378015635002
Validation loss: 2.462469002750506

Epoch: 5| Step: 3
Training loss: 2.447146084751281
Validation loss: 2.4735022927090804

Epoch: 5| Step: 4
Training loss: 2.607343053876521
Validation loss: 2.4791074459468465

Epoch: 5| Step: 5
Training loss: 2.5032894428162678
Validation loss: 2.428569011724946

Epoch: 5| Step: 6
Training loss: 2.0539499561858534
Validation loss: 2.4534163230534296

Epoch: 5| Step: 7
Training loss: 2.6680385418500085
Validation loss: 2.464780395685848

Epoch: 5| Step: 8
Training loss: 2.240095059387336
Validation loss: 2.4732566179419835

Epoch: 5| Step: 9
Training loss: 2.0929538864399664
Validation loss: 2.48962301465583

Epoch: 5| Step: 10
Training loss: 1.9658308033632452
Validation loss: 2.50244431314167

Epoch: 380| Step: 0
Training loss: 1.9462798137990551
Validation loss: 2.480297185203125

Epoch: 5| Step: 1
Training loss: 2.223832018895897
Validation loss: 2.5009636795649994

Epoch: 5| Step: 2
Training loss: 2.586600043022163
Validation loss: 2.4798608664366433

Epoch: 5| Step: 3
Training loss: 2.620964399577014
Validation loss: 2.4735965179645913

Epoch: 5| Step: 4
Training loss: 2.664019721350544
Validation loss: 2.4620686259994287

Epoch: 5| Step: 5
Training loss: 1.6690308095283644
Validation loss: 2.484101094435468

Epoch: 5| Step: 6
Training loss: 2.3050549230424395
Validation loss: 2.4980084454466507

Epoch: 5| Step: 7
Training loss: 2.141955553849222
Validation loss: 2.5349563157518866

Epoch: 5| Step: 8
Training loss: 1.8195842546839647
Validation loss: 2.492568933850628

Epoch: 5| Step: 9
Training loss: 3.174057684005835
Validation loss: 2.4452918214725337

Epoch: 5| Step: 10
Training loss: 1.8989421032581508
Validation loss: 2.483284011398374

Epoch: 381| Step: 0
Training loss: 2.2763946282800154
Validation loss: 2.5114368705468526

Epoch: 5| Step: 1
Training loss: 2.4851854070196016
Validation loss: 2.4539754195695336

Epoch: 5| Step: 2
Training loss: 2.2012617741041898
Validation loss: 2.480081726633623

Epoch: 5| Step: 3
Training loss: 1.8250207220494878
Validation loss: 2.446502150839565

Epoch: 5| Step: 4
Training loss: 2.802409997970035
Validation loss: 2.469212649780103

Epoch: 5| Step: 5
Training loss: 2.504551654065957
Validation loss: 2.496483942099858

Epoch: 5| Step: 6
Training loss: 2.072319123551005
Validation loss: 2.4550095193374815

Epoch: 5| Step: 7
Training loss: 2.5722579318708516
Validation loss: 2.505936969713714

Epoch: 5| Step: 8
Training loss: 1.6613800923291888
Validation loss: 2.50476195155691

Epoch: 5| Step: 9
Training loss: 2.2379921398553213
Validation loss: 2.4743383923721614

Epoch: 5| Step: 10
Training loss: 2.340866654963409
Validation loss: 2.4517509787931417

Epoch: 382| Step: 0
Training loss: 2.5718958653360304
Validation loss: 2.4590956825236225

Epoch: 5| Step: 1
Training loss: 2.3831169559273966
Validation loss: 2.5163910044189604

Epoch: 5| Step: 2
Training loss: 1.7186343067418557
Validation loss: 2.4819007342585233

Epoch: 5| Step: 3
Training loss: 1.7278182352418971
Validation loss: 2.528531288521541

Epoch: 5| Step: 4
Training loss: 2.6005246953453596
Validation loss: 2.4948864199366945

Epoch: 5| Step: 5
Training loss: 2.5267841833996743
Validation loss: 2.495243159505699

Epoch: 5| Step: 6
Training loss: 1.786149898257537
Validation loss: 2.4797613244730146

Epoch: 5| Step: 7
Training loss: 2.685105254882453
Validation loss: 2.489224312288288

Epoch: 5| Step: 8
Training loss: 2.2027031244319613
Validation loss: 2.522687893035288

Epoch: 5| Step: 9
Training loss: 1.8772565614263312
Validation loss: 2.469981536298027

Epoch: 5| Step: 10
Training loss: 2.636042936395064
Validation loss: 2.5155061716538882

Epoch: 383| Step: 0
Training loss: 2.5892462177740696
Validation loss: 2.4985575011164123

Epoch: 5| Step: 1
Training loss: 2.2752147950438495
Validation loss: 2.46911392343743

Epoch: 5| Step: 2
Training loss: 2.6629188745384353
Validation loss: 2.4729358612395704

Epoch: 5| Step: 3
Training loss: 1.9498403165921383
Validation loss: 2.4928701466654672

Epoch: 5| Step: 4
Training loss: 1.6963336172275427
Validation loss: 2.44542867140885

Epoch: 5| Step: 5
Training loss: 2.457921677945526
Validation loss: 2.4842872740867934

Epoch: 5| Step: 6
Training loss: 2.0909808621509063
Validation loss: 2.4988101506804217

Epoch: 5| Step: 7
Training loss: 2.6374946413392504
Validation loss: 2.4562111540113265

Epoch: 5| Step: 8
Training loss: 2.6793686034738204
Validation loss: 2.487651621463883

Epoch: 5| Step: 9
Training loss: 2.006102193456156
Validation loss: 2.483041297610661

Epoch: 5| Step: 10
Training loss: 1.9840684188343518
Validation loss: 2.442566999770407

Epoch: 384| Step: 0
Training loss: 1.919001408113775
Validation loss: 2.503450173573548

Epoch: 5| Step: 1
Training loss: 2.4229334333456864
Validation loss: 2.472591762508233

Epoch: 5| Step: 2
Training loss: 2.5299654868565025
Validation loss: 2.474527258920885

Epoch: 5| Step: 3
Training loss: 2.5393249376090936
Validation loss: 2.4503418707550932

Epoch: 5| Step: 4
Training loss: 1.897259758980856
Validation loss: 2.4646068229204645

Epoch: 5| Step: 5
Training loss: 1.8790776101245035
Validation loss: 2.4629370984666736

Epoch: 5| Step: 6
Training loss: 2.543837438481797
Validation loss: 2.472151897774893

Epoch: 5| Step: 7
Training loss: 2.4762769468286496
Validation loss: 2.457052390023948

Epoch: 5| Step: 8
Training loss: 1.8682514176232528
Validation loss: 2.5053712881689836

Epoch: 5| Step: 9
Training loss: 2.542391147440148
Validation loss: 2.488867169412442

Epoch: 5| Step: 10
Training loss: 2.4559032476919618
Validation loss: 2.4792793981108883

Epoch: 385| Step: 0
Training loss: 2.5472094544405373
Validation loss: 2.4888784390892793

Epoch: 5| Step: 1
Training loss: 1.887367531883744
Validation loss: 2.4863517802702284

Epoch: 5| Step: 2
Training loss: 1.6934415275946797
Validation loss: 2.5198015687885436

Epoch: 5| Step: 3
Training loss: 2.5859812937367503
Validation loss: 2.5107906847387365

Epoch: 5| Step: 4
Training loss: 2.753877680204198
Validation loss: 2.4768649855583154

Epoch: 5| Step: 5
Training loss: 2.054957034921839
Validation loss: 2.4900285492039678

Epoch: 5| Step: 6
Training loss: 2.3965214832918043
Validation loss: 2.4503277485946597

Epoch: 5| Step: 7
Training loss: 1.8090064482144326
Validation loss: 2.4632379491674086

Epoch: 5| Step: 8
Training loss: 2.4399009030965075
Validation loss: 2.475077873414412

Epoch: 5| Step: 9
Training loss: 2.432482323263773
Validation loss: 2.4878056102286066

Epoch: 5| Step: 10
Training loss: 1.9839293212222113
Validation loss: 2.4878677815643364

Epoch: 386| Step: 0
Training loss: 2.2816594422669305
Validation loss: 2.466656284724767

Epoch: 5| Step: 1
Training loss: 2.7564640368615785
Validation loss: 2.4893055398420314

Epoch: 5| Step: 2
Training loss: 1.925002430010166
Validation loss: 2.4918824917438407

Epoch: 5| Step: 3
Training loss: 2.2233549873445213
Validation loss: 2.4920834343980203

Epoch: 5| Step: 4
Training loss: 2.67512680180991
Validation loss: 2.4834601328273025

Epoch: 5| Step: 5
Training loss: 2.174051415559236
Validation loss: 2.490083149406554

Epoch: 5| Step: 6
Training loss: 1.9529572071479318
Validation loss: 2.4611375369645643

Epoch: 5| Step: 7
Training loss: 2.6543785234234343
Validation loss: 2.5149140191143537

Epoch: 5| Step: 8
Training loss: 1.6075272554546063
Validation loss: 2.5019730032424254

Epoch: 5| Step: 9
Training loss: 2.234467084360951
Validation loss: 2.4724985316772674

Epoch: 5| Step: 10
Training loss: 2.432564164008387
Validation loss: 2.485381673871451

Epoch: 387| Step: 0
Training loss: 2.0892117409192337
Validation loss: 2.4380550010351727

Epoch: 5| Step: 1
Training loss: 2.080594538668786
Validation loss: 2.4743742890511546

Epoch: 5| Step: 2
Training loss: 2.832737991018025
Validation loss: 2.4689130177589194

Epoch: 5| Step: 3
Training loss: 2.1588006896852807
Validation loss: 2.493045054099682

Epoch: 5| Step: 4
Training loss: 2.4482086873206628
Validation loss: 2.466730287743565

Epoch: 5| Step: 5
Training loss: 2.177879212357045
Validation loss: 2.455322564633628

Epoch: 5| Step: 6
Training loss: 1.8163778979642125
Validation loss: 2.495024900092468

Epoch: 5| Step: 7
Training loss: 1.7512370914089668
Validation loss: 2.4567719853293584

Epoch: 5| Step: 8
Training loss: 2.349285556869591
Validation loss: 2.489552306180416

Epoch: 5| Step: 9
Training loss: 2.4068515570605116
Validation loss: 2.4722377066015753

Epoch: 5| Step: 10
Training loss: 2.348687426275104
Validation loss: 2.4791563417656244

Epoch: 388| Step: 0
Training loss: 1.499321863744301
Validation loss: 2.4041559360267732

Epoch: 5| Step: 1
Training loss: 1.8166780001902159
Validation loss: 2.4536323657554013

Epoch: 5| Step: 2
Training loss: 2.5654441853416
Validation loss: 2.4648441369106275

Epoch: 5| Step: 3
Training loss: 2.5182914108163215
Validation loss: 2.4716305348631593

Epoch: 5| Step: 4
Training loss: 2.5862541811555717
Validation loss: 2.4905202846804055

Epoch: 5| Step: 5
Training loss: 2.5185372691090184
Validation loss: 2.5339521424908806

Epoch: 5| Step: 6
Training loss: 2.359873939962092
Validation loss: 2.468302399950121

Epoch: 5| Step: 7
Training loss: 1.839854959182448
Validation loss: 2.4802252454464866

Epoch: 5| Step: 8
Training loss: 2.054680537803081
Validation loss: 2.4872227236012137

Epoch: 5| Step: 9
Training loss: 2.7513865529982833
Validation loss: 2.454764196861734

Epoch: 5| Step: 10
Training loss: 2.0540649863477176
Validation loss: 2.4646113139038768

Epoch: 389| Step: 0
Training loss: 1.5081889418869014
Validation loss: 2.4569267857081685

Epoch: 5| Step: 1
Training loss: 2.4661668228067213
Validation loss: 2.506457070639421

Epoch: 5| Step: 2
Training loss: 2.1176773011472814
Validation loss: 2.463925899832769

Epoch: 5| Step: 3
Training loss: 2.3591923927225875
Validation loss: 2.456360119071228

Epoch: 5| Step: 4
Training loss: 2.578555816720877
Validation loss: 2.4683318027300682

Epoch: 5| Step: 5
Training loss: 2.7911700665807113
Validation loss: 2.4364889439160007

Epoch: 5| Step: 6
Training loss: 2.31936441183179
Validation loss: 2.455914620615581

Epoch: 5| Step: 7
Training loss: 1.9468276163522409
Validation loss: 2.4662883552899397

Epoch: 5| Step: 8
Training loss: 2.0263624811692336
Validation loss: 2.4781021668918832

Epoch: 5| Step: 9
Training loss: 2.154057369250971
Validation loss: 2.4664657823045

Epoch: 5| Step: 10
Training loss: 2.2290097356680345
Validation loss: 2.458031095763056

Epoch: 390| Step: 0
Training loss: 2.2751026675355
Validation loss: 2.4470330204984228

Epoch: 5| Step: 1
Training loss: 2.476291196385117
Validation loss: 2.4707863632137212

Epoch: 5| Step: 2
Training loss: 2.24375281373594
Validation loss: 2.47194632111037

Epoch: 5| Step: 3
Training loss: 2.5263611476783234
Validation loss: 2.489024183210917

Epoch: 5| Step: 4
Training loss: 2.6099309271957116
Validation loss: 2.4633309789057405

Epoch: 5| Step: 5
Training loss: 2.1887188376883087
Validation loss: 2.449006880985267

Epoch: 5| Step: 6
Training loss: 2.3249020381511802
Validation loss: 2.5216219861715463

Epoch: 5| Step: 7
Training loss: 2.121614227226218
Validation loss: 2.4969039614477833

Epoch: 5| Step: 8
Training loss: 2.3071546894227297
Validation loss: 2.512474725447476

Epoch: 5| Step: 9
Training loss: 1.9984089960447067
Validation loss: 2.4957309019909792

Epoch: 5| Step: 10
Training loss: 1.9055779319848227
Validation loss: 2.479355613999793

Epoch: 391| Step: 0
Training loss: 2.6936059829201344
Validation loss: 2.4463027468704617

Epoch: 5| Step: 1
Training loss: 2.1694022173590524
Validation loss: 2.4839153219246715

Epoch: 5| Step: 2
Training loss: 2.0168577460989328
Validation loss: 2.455228690058314

Epoch: 5| Step: 3
Training loss: 2.255163836902857
Validation loss: 2.4911071264588633

Epoch: 5| Step: 4
Training loss: 2.1643034322320056
Validation loss: 2.435567840465673

Epoch: 5| Step: 5
Training loss: 2.688479488993367
Validation loss: 2.4799202535184626

Epoch: 5| Step: 6
Training loss: 1.9442025813799493
Validation loss: 2.4838375646116133

Epoch: 5| Step: 7
Training loss: 2.4417431408186374
Validation loss: 2.492514951738454

Epoch: 5| Step: 8
Training loss: 2.241127534532026
Validation loss: 2.4648039623662408

Epoch: 5| Step: 9
Training loss: 1.7430942382505432
Validation loss: 2.5052960637796478

Epoch: 5| Step: 10
Training loss: 2.18604884378742
Validation loss: 2.4780358378997582

Epoch: 392| Step: 0
Training loss: 2.470620135093356
Validation loss: 2.4786425338330917

Epoch: 5| Step: 1
Training loss: 2.149018808963308
Validation loss: 2.452225543149103

Epoch: 5| Step: 2
Training loss: 1.8048088396243132
Validation loss: 2.5024955650569676

Epoch: 5| Step: 3
Training loss: 1.8371721377430426
Validation loss: 2.477701478194127

Epoch: 5| Step: 4
Training loss: 2.283358709746563
Validation loss: 2.5087286665589543

Epoch: 5| Step: 5
Training loss: 2.249688020905621
Validation loss: 2.4682645277560713

Epoch: 5| Step: 6
Training loss: 2.8864884755710514
Validation loss: 2.48239985220057

Epoch: 5| Step: 7
Training loss: 2.675498691313995
Validation loss: 2.4606228743364644

Epoch: 5| Step: 8
Training loss: 2.226448136454404
Validation loss: 2.478983724380843

Epoch: 5| Step: 9
Training loss: 2.133588824271708
Validation loss: 2.4861804662568416

Epoch: 5| Step: 10
Training loss: 2.2104246805125465
Validation loss: 2.4433393142063036

Epoch: 393| Step: 0
Training loss: 2.8962639321563
Validation loss: 2.4480359968442547

Epoch: 5| Step: 1
Training loss: 2.435200217629973
Validation loss: 2.465946963986978

Epoch: 5| Step: 2
Training loss: 1.6402237174334484
Validation loss: 2.5206099621150084

Epoch: 5| Step: 3
Training loss: 2.2961842023614563
Validation loss: 2.449978157851844

Epoch: 5| Step: 4
Training loss: 2.2304415993449322
Validation loss: 2.4632092031960573

Epoch: 5| Step: 5
Training loss: 2.5369089236814246
Validation loss: 2.486402911261802

Epoch: 5| Step: 6
Training loss: 1.763445631620596
Validation loss: 2.4622800655623434

Epoch: 5| Step: 7
Training loss: 1.5165919729151909
Validation loss: 2.487681888430033

Epoch: 5| Step: 8
Training loss: 1.904640080751211
Validation loss: 2.5067329586443625

Epoch: 5| Step: 9
Training loss: 2.7755258354093737
Validation loss: 2.48566842328915

Epoch: 5| Step: 10
Training loss: 2.4170956614530152
Validation loss: 2.5007124716205817

Epoch: 394| Step: 0
Training loss: 2.168034073129049
Validation loss: 2.497825860558649

Epoch: 5| Step: 1
Training loss: 2.3951145476023634
Validation loss: 2.4983741344293406

Epoch: 5| Step: 2
Training loss: 2.3011597611835817
Validation loss: 2.448562443333707

Epoch: 5| Step: 3
Training loss: 2.7578769924846664
Validation loss: 2.462282193703357

Epoch: 5| Step: 4
Training loss: 1.5434238197352177
Validation loss: 2.4990856816229243

Epoch: 5| Step: 5
Training loss: 1.0408808350750522
Validation loss: 2.455855630113763

Epoch: 5| Step: 6
Training loss: 2.3566139688854317
Validation loss: 2.4898597549068273

Epoch: 5| Step: 7
Training loss: 2.1459313774549282
Validation loss: 2.447349601699155

Epoch: 5| Step: 8
Training loss: 2.734826274145372
Validation loss: 2.51853870639549

Epoch: 5| Step: 9
Training loss: 2.0875729211188694
Validation loss: 2.4856795331680672

Epoch: 5| Step: 10
Training loss: 2.500693892502688
Validation loss: 2.475893066212305

Epoch: 395| Step: 0
Training loss: 2.6649517465771257
Validation loss: 2.4477874084641074

Epoch: 5| Step: 1
Training loss: 1.8219373270304082
Validation loss: 2.4812444082453204

Epoch: 5| Step: 2
Training loss: 2.485318178803297
Validation loss: 2.4568468980685148

Epoch: 5| Step: 3
Training loss: 1.7952155370067135
Validation loss: 2.4635584159308928

Epoch: 5| Step: 4
Training loss: 2.2186506007320443
Validation loss: 2.499184362606413

Epoch: 5| Step: 5
Training loss: 2.425063268091793
Validation loss: 2.460346082755245

Epoch: 5| Step: 6
Training loss: 2.767605864198054
Validation loss: 2.460095215503849

Epoch: 5| Step: 7
Training loss: 2.198185896692925
Validation loss: 2.4916899439621982

Epoch: 5| Step: 8
Training loss: 1.7395205762435122
Validation loss: 2.457782559217391

Epoch: 5| Step: 9
Training loss: 2.422555840068148
Validation loss: 2.4696638877647876

Epoch: 5| Step: 10
Training loss: 2.1480443976305272
Validation loss: 2.5088048665952014

Epoch: 396| Step: 0
Training loss: 2.456890738977109
Validation loss: 2.5003509264931005

Epoch: 5| Step: 1
Training loss: 2.4519626269020427
Validation loss: 2.4577183344376077

Epoch: 5| Step: 2
Training loss: 2.9279638484223907
Validation loss: 2.425178200158164

Epoch: 5| Step: 3
Training loss: 1.9882480341267132
Validation loss: 2.4853876564929314

Epoch: 5| Step: 4
Training loss: 1.779247429806599
Validation loss: 2.440131900274416

Epoch: 5| Step: 5
Training loss: 2.594834985502883
Validation loss: 2.44726241110505

Epoch: 5| Step: 6
Training loss: 2.2199917394467192
Validation loss: 2.5006913172700247

Epoch: 5| Step: 7
Training loss: 2.1910355470281773
Validation loss: 2.4903681582040225

Epoch: 5| Step: 8
Training loss: 1.9807772123448053
Validation loss: 2.455021464992058

Epoch: 5| Step: 9
Training loss: 1.7842469685547326
Validation loss: 2.476809893165921

Epoch: 5| Step: 10
Training loss: 2.2371633777258415
Validation loss: 2.454291477125522

Epoch: 397| Step: 0
Training loss: 2.04844791459172
Validation loss: 2.4582647130144384

Epoch: 5| Step: 1
Training loss: 2.0615624117363343
Validation loss: 2.442447849882884

Epoch: 5| Step: 2
Training loss: 2.7472838079142874
Validation loss: 2.5068764304150912

Epoch: 5| Step: 3
Training loss: 2.024552203929137
Validation loss: 2.451916286803274

Epoch: 5| Step: 4
Training loss: 2.3512170480855765
Validation loss: 2.5307668064957385

Epoch: 5| Step: 5
Training loss: 2.0685171728124403
Validation loss: 2.3905150943001092

Epoch: 5| Step: 6
Training loss: 2.2036090075677315
Validation loss: 2.4877462722662824

Epoch: 5| Step: 7
Training loss: 1.9201082622284553
Validation loss: 2.4911975275994047

Epoch: 5| Step: 8
Training loss: 2.0645514025433944
Validation loss: 2.4829944301986786

Epoch: 5| Step: 9
Training loss: 2.663856476007682
Validation loss: 2.448878055278665

Epoch: 5| Step: 10
Training loss: 2.087188459893897
Validation loss: 2.4687414079596843

Epoch: 398| Step: 0
Training loss: 1.8447886628683468
Validation loss: 2.5028782928813205

Epoch: 5| Step: 1
Training loss: 2.2425658009889333
Validation loss: 2.503055327775996

Epoch: 5| Step: 2
Training loss: 2.8024410506351702
Validation loss: 2.449848288634856

Epoch: 5| Step: 3
Training loss: 2.6468693864891217
Validation loss: 2.4463273121040525

Epoch: 5| Step: 4
Training loss: 2.157160027903894
Validation loss: 2.4977287866005278

Epoch: 5| Step: 5
Training loss: 2.468887035574429
Validation loss: 2.485750042406113

Epoch: 5| Step: 6
Training loss: 2.176771065494929
Validation loss: 2.4974199846253837

Epoch: 5| Step: 7
Training loss: 1.65007891899723
Validation loss: 2.4815109604612484

Epoch: 5| Step: 8
Training loss: 2.0424610331260618
Validation loss: 2.4500861427334835

Epoch: 5| Step: 9
Training loss: 2.2379313090523723
Validation loss: 2.503836072869681

Epoch: 5| Step: 10
Training loss: 1.8248574815258476
Validation loss: 2.48328262597219

Epoch: 399| Step: 0
Training loss: 1.450158649841697
Validation loss: 2.4638995102543944

Epoch: 5| Step: 1
Training loss: 2.0548401982530042
Validation loss: 2.491331750956241

Epoch: 5| Step: 2
Training loss: 1.699659829606193
Validation loss: 2.4710064160139726

Epoch: 5| Step: 3
Training loss: 2.3408367107073205
Validation loss: 2.464294657415727

Epoch: 5| Step: 4
Training loss: 2.1904539599287696
Validation loss: 2.4858078501146856

Epoch: 5| Step: 5
Training loss: 2.2112831950782446
Validation loss: 2.4499874163219286

Epoch: 5| Step: 6
Training loss: 2.2665411839626106
Validation loss: 2.4476589024405024

Epoch: 5| Step: 7
Training loss: 2.7330037193510317
Validation loss: 2.4837033553237884

Epoch: 5| Step: 8
Training loss: 2.855537215428349
Validation loss: 2.4977557999789965

Epoch: 5| Step: 9
Training loss: 2.343136719892877
Validation loss: 2.470621898061213

Epoch: 5| Step: 10
Training loss: 1.6272733631927117
Validation loss: 2.4569647669293464

Epoch: 400| Step: 0
Training loss: 2.1775228491721945
Validation loss: 2.494004330446918

Epoch: 5| Step: 1
Training loss: 2.4321238577636572
Validation loss: 2.483154730875553

Epoch: 5| Step: 2
Training loss: 1.8883687577904824
Validation loss: 2.4672700811000134

Epoch: 5| Step: 3
Training loss: 2.2818502394190374
Validation loss: 2.4678277644786952

Epoch: 5| Step: 4
Training loss: 2.48055352494158
Validation loss: 2.463828959950123

Epoch: 5| Step: 5
Training loss: 2.0171902515442843
Validation loss: 2.4577195173102506

Epoch: 5| Step: 6
Training loss: 2.3899204986309925
Validation loss: 2.4177924501817656

Epoch: 5| Step: 7
Training loss: 1.8046618042805083
Validation loss: 2.454616870768581

Epoch: 5| Step: 8
Training loss: 2.8097366320905284
Validation loss: 2.425362576192361

Epoch: 5| Step: 9
Training loss: 2.430475236080274
Validation loss: 2.480961854300494

Epoch: 5| Step: 10
Training loss: 1.6465396894437663
Validation loss: 2.4925876660954267

Epoch: 401| Step: 0
Training loss: 1.895094451563478
Validation loss: 2.469884377908251

Epoch: 5| Step: 1
Training loss: 2.4814176411532705
Validation loss: 2.4836782091125094

Epoch: 5| Step: 2
Training loss: 2.4743573211708996
Validation loss: 2.474016090500371

Epoch: 5| Step: 3
Training loss: 2.5554185107245377
Validation loss: 2.4500249709463406

Epoch: 5| Step: 4
Training loss: 1.8926214600946851
Validation loss: 2.4917303548074425

Epoch: 5| Step: 5
Training loss: 2.1819452877300707
Validation loss: 2.458479138808012

Epoch: 5| Step: 6
Training loss: 2.177376783631613
Validation loss: 2.464626481769294

Epoch: 5| Step: 7
Training loss: 1.7122061567827003
Validation loss: 2.439431768355548

Epoch: 5| Step: 8
Training loss: 2.341637842220409
Validation loss: 2.467521950835148

Epoch: 5| Step: 9
Training loss: 2.4601501174031415
Validation loss: 2.4777978499879256

Epoch: 5| Step: 10
Training loss: 2.1216823260060806
Validation loss: 2.4756495465230586

Epoch: 402| Step: 0
Training loss: 2.0701192513729216
Validation loss: 2.46392824192931

Epoch: 5| Step: 1
Training loss: 2.2272417069859953
Validation loss: 2.456505769978528

Epoch: 5| Step: 2
Training loss: 2.458229730869148
Validation loss: 2.485470920711153

Epoch: 5| Step: 3
Training loss: 2.0829025077402177
Validation loss: 2.460008912264179

Epoch: 5| Step: 4
Training loss: 2.0323237002253256
Validation loss: 2.448172452536492

Epoch: 5| Step: 5
Training loss: 2.229765077884051
Validation loss: 2.501599015804118

Epoch: 5| Step: 6
Training loss: 2.2042401244452927
Validation loss: 2.478118092723947

Epoch: 5| Step: 7
Training loss: 2.883262040978628
Validation loss: 2.5333775175912163

Epoch: 5| Step: 8
Training loss: 2.2197899328971213
Validation loss: 2.4981722231144974

Epoch: 5| Step: 9
Training loss: 2.3078309427091344
Validation loss: 2.5019961499246626

Epoch: 5| Step: 10
Training loss: 1.7787819880533278
Validation loss: 2.4789346307332547

Epoch: 403| Step: 0
Training loss: 2.368074156637462
Validation loss: 2.475807305380243

Epoch: 5| Step: 1
Training loss: 2.4768073024156645
Validation loss: 2.480018123783108

Epoch: 5| Step: 2
Training loss: 2.530321680961349
Validation loss: 2.443172839305045

Epoch: 5| Step: 3
Training loss: 2.106127237879676
Validation loss: 2.471629432291167

Epoch: 5| Step: 4
Training loss: 2.0589007559485375
Validation loss: 2.442078292920449

Epoch: 5| Step: 5
Training loss: 3.1451832350098634
Validation loss: 2.5082234553945075

Epoch: 5| Step: 6
Training loss: 2.123237552169303
Validation loss: 2.46583699811539

Epoch: 5| Step: 7
Training loss: 1.6933888011518368
Validation loss: 2.4908329988653666

Epoch: 5| Step: 8
Training loss: 2.0926485723073944
Validation loss: 2.448374832902763

Epoch: 5| Step: 9
Training loss: 2.208307973848041
Validation loss: 2.455535812855682

Epoch: 5| Step: 10
Training loss: 1.848820648081523
Validation loss: 2.469759855924772

Epoch: 404| Step: 0
Training loss: 1.8996091139407079
Validation loss: 2.468638055022702

Epoch: 5| Step: 1
Training loss: 2.514269540629447
Validation loss: 2.42390187819528

Epoch: 5| Step: 2
Training loss: 2.056960904214882
Validation loss: 2.4917994674909543

Epoch: 5| Step: 3
Training loss: 2.5698271411992932
Validation loss: 2.463717845978865

Epoch: 5| Step: 4
Training loss: 1.7958444044770563
Validation loss: 2.4964431080939913

Epoch: 5| Step: 5
Training loss: 2.3262278360318436
Validation loss: 2.4831298609599743

Epoch: 5| Step: 6
Training loss: 2.254694808979837
Validation loss: 2.4069564756012785

Epoch: 5| Step: 7
Training loss: 2.099269821743947
Validation loss: 2.494773366465656

Epoch: 5| Step: 8
Training loss: 2.4082749998778423
Validation loss: 2.430104882745868

Epoch: 5| Step: 9
Training loss: 2.488574048216575
Validation loss: 2.4777534695368604

Epoch: 5| Step: 10
Training loss: 2.108449662307618
Validation loss: 2.513513157471411

Epoch: 405| Step: 0
Training loss: 1.3644224270988485
Validation loss: 2.4496386948241193

Epoch: 5| Step: 1
Training loss: 2.7238628499574675
Validation loss: 2.438375884124518

Epoch: 5| Step: 2
Training loss: 1.7815306592985571
Validation loss: 2.470454366913146

Epoch: 5| Step: 3
Training loss: 2.471184507977306
Validation loss: 2.4681887026854885

Epoch: 5| Step: 4
Training loss: 2.122217488352948
Validation loss: 2.4415115379682533

Epoch: 5| Step: 5
Training loss: 2.3281936123357516
Validation loss: 2.422091847164881

Epoch: 5| Step: 6
Training loss: 2.8141251001591425
Validation loss: 2.4951653647757914

Epoch: 5| Step: 7
Training loss: 2.302323710001964
Validation loss: 2.4707581429283763

Epoch: 5| Step: 8
Training loss: 1.5475824453056808
Validation loss: 2.4467355528767656

Epoch: 5| Step: 9
Training loss: 1.7071399086734644
Validation loss: 2.4720141730895326

Epoch: 5| Step: 10
Training loss: 2.8783500061553617
Validation loss: 2.4811179001121912

Epoch: 406| Step: 0
Training loss: 2.574439726680552
Validation loss: 2.4637883136213174

Epoch: 5| Step: 1
Training loss: 1.6052510283294126
Validation loss: 2.4848881756002514

Epoch: 5| Step: 2
Training loss: 2.1279105281448736
Validation loss: 2.4544107456119946

Epoch: 5| Step: 3
Training loss: 2.1432402291056833
Validation loss: 2.495260629543679

Epoch: 5| Step: 4
Training loss: 1.8677574149847374
Validation loss: 2.4547655910727464

Epoch: 5| Step: 5
Training loss: 2.898133953476354
Validation loss: 2.504681717581766

Epoch: 5| Step: 6
Training loss: 2.2298123384211497
Validation loss: 2.4537415485693

Epoch: 5| Step: 7
Training loss: 2.3462541553946243
Validation loss: 2.435670523695406

Epoch: 5| Step: 8
Training loss: 2.5242828748250483
Validation loss: 2.490110542231256

Epoch: 5| Step: 9
Training loss: 1.831979388903985
Validation loss: 2.4404867881755465

Epoch: 5| Step: 10
Training loss: 2.0016184457290342
Validation loss: 2.44168740085611

Epoch: 407| Step: 0
Training loss: 1.7932584773657942
Validation loss: 2.4735057336927477

Epoch: 5| Step: 1
Training loss: 2.3759047140287515
Validation loss: 2.4936810687928523

Epoch: 5| Step: 2
Training loss: 1.7669179452778356
Validation loss: 2.435504861423523

Epoch: 5| Step: 3
Training loss: 2.1829680226981245
Validation loss: 2.4607206869904377

Epoch: 5| Step: 4
Training loss: 1.5528750949648635
Validation loss: 2.459790809474789

Epoch: 5| Step: 5
Training loss: 2.710520775506191
Validation loss: 2.5030128126843936

Epoch: 5| Step: 6
Training loss: 2.118013115407304
Validation loss: 2.4604157008725007

Epoch: 5| Step: 7
Training loss: 2.2120844693257924
Validation loss: 2.4563409101653266

Epoch: 5| Step: 8
Training loss: 2.633997438283267
Validation loss: 2.4700869840173834

Epoch: 5| Step: 9
Training loss: 2.3247834873907354
Validation loss: 2.4811410305695367

Epoch: 5| Step: 10
Training loss: 2.440484298577392
Validation loss: 2.4467932031500466

Epoch: 408| Step: 0
Training loss: 2.106273377059567
Validation loss: 2.4736485971402766

Epoch: 5| Step: 1
Training loss: 2.985417847900441
Validation loss: 2.4487006840355305

Epoch: 5| Step: 2
Training loss: 2.1139364020273663
Validation loss: 2.486944201175895

Epoch: 5| Step: 3
Training loss: 2.2325168165575278
Validation loss: 2.4589798502821094

Epoch: 5| Step: 4
Training loss: 2.131589545892216
Validation loss: 2.478763907119483

Epoch: 5| Step: 5
Training loss: 2.324011221203658
Validation loss: 2.44595554806243

Epoch: 5| Step: 6
Training loss: 1.957327022777921
Validation loss: 2.4621820576781843

Epoch: 5| Step: 7
Training loss: 2.228458922030041
Validation loss: 2.473600115313678

Epoch: 5| Step: 8
Training loss: 1.908871708295366
Validation loss: 2.450260409314857

Epoch: 5| Step: 9
Training loss: 2.209285590751609
Validation loss: 2.426590226974304

Epoch: 5| Step: 10
Training loss: 1.9848748726406618
Validation loss: 2.487598489011319

Epoch: 409| Step: 0
Training loss: 2.2967698468901365
Validation loss: 2.456523926685297

Epoch: 5| Step: 1
Training loss: 2.0254226431895446
Validation loss: 2.4361799645062385

Epoch: 5| Step: 2
Training loss: 2.4597431493819424
Validation loss: 2.403558857070902

Epoch: 5| Step: 3
Training loss: 1.8952533407641023
Validation loss: 2.4462667867214387

Epoch: 5| Step: 4
Training loss: 2.77227801948648
Validation loss: 2.4789192960082285

Epoch: 5| Step: 5
Training loss: 1.845029936240046
Validation loss: 2.4856686336883578

Epoch: 5| Step: 6
Training loss: 2.808236514572329
Validation loss: 2.4717513490878633

Epoch: 5| Step: 7
Training loss: 1.6882188466837158
Validation loss: 2.4927610116189296

Epoch: 5| Step: 8
Training loss: 2.3422072355674426
Validation loss: 2.457855995503209

Epoch: 5| Step: 9
Training loss: 2.2112219529774912
Validation loss: 2.489990355221784

Epoch: 5| Step: 10
Training loss: 1.7476873784709956
Validation loss: 2.4743121091849365

Epoch: 410| Step: 0
Training loss: 1.7455353323978207
Validation loss: 2.4621445510421265

Epoch: 5| Step: 1
Training loss: 2.304143912655425
Validation loss: 2.452046247229676

Epoch: 5| Step: 2
Training loss: 2.3938472200177823
Validation loss: 2.464932947933965

Epoch: 5| Step: 3
Training loss: 1.9993250423198856
Validation loss: 2.419340516905968

Epoch: 5| Step: 4
Training loss: 2.518166437061385
Validation loss: 2.5151541020826045

Epoch: 5| Step: 5
Training loss: 2.166029934367184
Validation loss: 2.4428302298844984

Epoch: 5| Step: 6
Training loss: 2.4109585754304046
Validation loss: 2.466985282327877

Epoch: 5| Step: 7
Training loss: 2.423314116120608
Validation loss: 2.4470636573369893

Epoch: 5| Step: 8
Training loss: 1.8756653558850673
Validation loss: 2.4930300817590587

Epoch: 5| Step: 9
Training loss: 1.7491295557082036
Validation loss: 2.456500412065049

Epoch: 5| Step: 10
Training loss: 2.4792811156283348
Validation loss: 2.5006969300846213

Epoch: 411| Step: 0
Training loss: 1.8187558308406317
Validation loss: 2.458856898312054

Epoch: 5| Step: 1
Training loss: 2.175413035494639
Validation loss: 2.450100396555977

Epoch: 5| Step: 2
Training loss: 2.303693033094635
Validation loss: 2.462918885468906

Epoch: 5| Step: 3
Training loss: 1.71938249914322
Validation loss: 2.4682454416160207

Epoch: 5| Step: 4
Training loss: 2.0647649177689487
Validation loss: 2.4250833849265874

Epoch: 5| Step: 5
Training loss: 2.735602141631898
Validation loss: 2.439661813777744

Epoch: 5| Step: 6
Training loss: 2.3352937863974677
Validation loss: 2.4400691703625914

Epoch: 5| Step: 7
Training loss: 2.7559562682293377
Validation loss: 2.480556428028435

Epoch: 5| Step: 8
Training loss: 2.0701169479431885
Validation loss: 2.417239233413475

Epoch: 5| Step: 9
Training loss: 2.0185679161697636
Validation loss: 2.467550940065667

Epoch: 5| Step: 10
Training loss: 2.127122716386534
Validation loss: 2.4615660880808212

Epoch: 412| Step: 0
Training loss: 2.1476645449037246
Validation loss: 2.447785540550741

Epoch: 5| Step: 1
Training loss: 2.3262289634380653
Validation loss: 2.4549594387448557

Epoch: 5| Step: 2
Training loss: 2.10330411649907
Validation loss: 2.4576583827023235

Epoch: 5| Step: 3
Training loss: 1.929183708868146
Validation loss: 2.447268945741001

Epoch: 5| Step: 4
Training loss: 2.29851945410697
Validation loss: 2.450051810273656

Epoch: 5| Step: 5
Training loss: 2.1684941385483283
Validation loss: 2.431667042059253

Epoch: 5| Step: 6
Training loss: 2.2532233355646
Validation loss: 2.44041675806684

Epoch: 5| Step: 7
Training loss: 2.2500303054464057
Validation loss: 2.479879072857788

Epoch: 5| Step: 8
Training loss: 2.4781710337804483
Validation loss: 2.4525773008043106

Epoch: 5| Step: 9
Training loss: 2.3697844005654973
Validation loss: 2.4410192401590822

Epoch: 5| Step: 10
Training loss: 2.086680762067644
Validation loss: 2.4576593183837137

Epoch: 413| Step: 0
Training loss: 2.143346907640408
Validation loss: 2.473872778319355

Epoch: 5| Step: 1
Training loss: 2.342775472852388
Validation loss: 2.4558297143891377

Epoch: 5| Step: 2
Training loss: 1.8997970547958565
Validation loss: 2.4339550377385937

Epoch: 5| Step: 3
Training loss: 2.2644881980960743
Validation loss: 2.4905669296678945

Epoch: 5| Step: 4
Training loss: 2.6560961510477816
Validation loss: 2.441071567491652

Epoch: 5| Step: 5
Training loss: 2.2433094580382633
Validation loss: 2.474958535525832

Epoch: 5| Step: 6
Training loss: 2.085538333032938
Validation loss: 2.476315772639775

Epoch: 5| Step: 7
Training loss: 1.7800970528964741
Validation loss: 2.4108164552792015

Epoch: 5| Step: 8
Training loss: 2.441636024343578
Validation loss: 2.4600819772778513

Epoch: 5| Step: 9
Training loss: 2.142842167847397
Validation loss: 2.448877252334359

Epoch: 5| Step: 10
Training loss: 1.8309518851349758
Validation loss: 2.4966605293680635

Epoch: 414| Step: 0
Training loss: 2.071842765269663
Validation loss: 2.4690552970929778

Epoch: 5| Step: 1
Training loss: 1.7699251052399279
Validation loss: 2.4549259921134627

Epoch: 5| Step: 2
Training loss: 1.490701064882124
Validation loss: 2.4438154359685225

Epoch: 5| Step: 3
Training loss: 2.631721247790924
Validation loss: 2.453347660993031

Epoch: 5| Step: 4
Training loss: 2.9583722680623596
Validation loss: 2.4527903868708285

Epoch: 5| Step: 5
Training loss: 2.139886770550626
Validation loss: 2.4650561714390964

Epoch: 5| Step: 6
Training loss: 2.498930416187107
Validation loss: 2.477721081259213

Epoch: 5| Step: 7
Training loss: 1.8784972953259083
Validation loss: 2.434564933690128

Epoch: 5| Step: 8
Training loss: 2.333683419077248
Validation loss: 2.4475476383231825

Epoch: 5| Step: 9
Training loss: 2.078383465518919
Validation loss: 2.4670079955705644

Epoch: 5| Step: 10
Training loss: 2.1284198170264688
Validation loss: 2.4834719070512516

Epoch: 415| Step: 0
Training loss: 1.686162135703156
Validation loss: 2.4493038791184354

Epoch: 5| Step: 1
Training loss: 2.292566111369528
Validation loss: 2.5140104101473915

Epoch: 5| Step: 2
Training loss: 2.0240356268253894
Validation loss: 2.4594197379848577

Epoch: 5| Step: 3
Training loss: 2.6477333415568767
Validation loss: 2.450732443146858

Epoch: 5| Step: 4
Training loss: 2.1493355278701243
Validation loss: 2.423324397895601

Epoch: 5| Step: 5
Training loss: 2.329085184035704
Validation loss: 2.4478081030893137

Epoch: 5| Step: 6
Training loss: 1.8919479533490773
Validation loss: 2.4539922567611563

Epoch: 5| Step: 7
Training loss: 2.4014405576365143
Validation loss: 2.4768920836475194

Epoch: 5| Step: 8
Training loss: 1.9712477688483443
Validation loss: 2.4724397906536932

Epoch: 5| Step: 9
Training loss: 2.738622015674812
Validation loss: 2.4608473936253574

Epoch: 5| Step: 10
Training loss: 2.0250830127042954
Validation loss: 2.451654936755846

Epoch: 416| Step: 0
Training loss: 1.4544914957626007
Validation loss: 2.4817115900492848

Epoch: 5| Step: 1
Training loss: 1.98782151458716
Validation loss: 2.517952554257729

Epoch: 5| Step: 2
Training loss: 1.7785611521127997
Validation loss: 2.456913821008071

Epoch: 5| Step: 3
Training loss: 1.9651918501054668
Validation loss: 2.455412417491342

Epoch: 5| Step: 4
Training loss: 2.224279042570043
Validation loss: 2.4439622184144225

Epoch: 5| Step: 5
Training loss: 1.9252976744727555
Validation loss: 2.43455468569763

Epoch: 5| Step: 6
Training loss: 1.7826317898689406
Validation loss: 2.4881566451555917

Epoch: 5| Step: 7
Training loss: 2.989819418771032
Validation loss: 2.445652672532427

Epoch: 5| Step: 8
Training loss: 2.308784386627107
Validation loss: 2.5186919274222466

Epoch: 5| Step: 9
Training loss: 2.6346840927143615
Validation loss: 2.4878188003721653

Epoch: 5| Step: 10
Training loss: 2.457815266563377
Validation loss: 2.426170502439541

Epoch: 417| Step: 0
Training loss: 1.7332731991509294
Validation loss: 2.441760607263965

Epoch: 5| Step: 1
Training loss: 2.4537738500787563
Validation loss: 2.4523208436086046

Epoch: 5| Step: 2
Training loss: 2.5183615638199153
Validation loss: 2.4682991765752593

Epoch: 5| Step: 3
Training loss: 2.425078408458252
Validation loss: 2.4733942382044014

Epoch: 5| Step: 4
Training loss: 1.9796183488987853
Validation loss: 2.4522048837126302

Epoch: 5| Step: 5
Training loss: 2.082838635520335
Validation loss: 2.4545671910618303

Epoch: 5| Step: 6
Training loss: 2.2281828756426454
Validation loss: 2.465362220237316

Epoch: 5| Step: 7
Training loss: 2.5866666843309436
Validation loss: 2.4369233687522724

Epoch: 5| Step: 8
Training loss: 1.9450479350520564
Validation loss: 2.447744720501251

Epoch: 5| Step: 9
Training loss: 2.2283390921397275
Validation loss: 2.426889355734104

Epoch: 5| Step: 10
Training loss: 1.9237082197127646
Validation loss: 2.4875591683901277

Epoch: 418| Step: 0
Training loss: 2.127896074482113
Validation loss: 2.4376997023170968

Epoch: 5| Step: 1
Training loss: 2.166491305761898
Validation loss: 2.455484295948516

Epoch: 5| Step: 2
Training loss: 2.4383024215390217
Validation loss: 2.4019341803399303

Epoch: 5| Step: 3
Training loss: 1.9413132827573973
Validation loss: 2.419902339772595

Epoch: 5| Step: 4
Training loss: 2.1839216119231812
Validation loss: 2.461778050881348

Epoch: 5| Step: 5
Training loss: 2.305912739560949
Validation loss: 2.4841656216452717

Epoch: 5| Step: 6
Training loss: 2.337779871867555
Validation loss: 2.4359858048014975

Epoch: 5| Step: 7
Training loss: 2.3742951802902295
Validation loss: 2.442707698731069

Epoch: 5| Step: 8
Training loss: 2.086709326205438
Validation loss: 2.499163274307634

Epoch: 5| Step: 9
Training loss: 2.1944597970717887
Validation loss: 2.4538426214042306

Epoch: 5| Step: 10
Training loss: 2.0137581866883485
Validation loss: 2.4528602774436417

Epoch: 419| Step: 0
Training loss: 2.1334979733872514
Validation loss: 2.4407243524358684

Epoch: 5| Step: 1
Training loss: 2.2242874033080007
Validation loss: 2.5104359625305097

Epoch: 5| Step: 2
Training loss: 1.684700657321369
Validation loss: 2.4304325304279986

Epoch: 5| Step: 3
Training loss: 2.4577877171959077
Validation loss: 2.482976539330929

Epoch: 5| Step: 4
Training loss: 2.0794786905864826
Validation loss: 2.4700673188093907

Epoch: 5| Step: 5
Training loss: 2.554799022048565
Validation loss: 2.434787445174063

Epoch: 5| Step: 6
Training loss: 2.5020897714075185
Validation loss: 2.4646675776652365

Epoch: 5| Step: 7
Training loss: 2.1361083112394352
Validation loss: 2.477855170715545

Epoch: 5| Step: 8
Training loss: 2.295521551470999
Validation loss: 2.4307799383876127

Epoch: 5| Step: 9
Training loss: 2.044694510775392
Validation loss: 2.478088260865501

Epoch: 5| Step: 10
Training loss: 2.1699349499985803
Validation loss: 2.4587805959693028

Epoch: 420| Step: 0
Training loss: 1.8067847282056455
Validation loss: 2.444460829587907

Epoch: 5| Step: 1
Training loss: 2.2761812727396844
Validation loss: 2.4067660352247287

Epoch: 5| Step: 2
Training loss: 1.7765485724420742
Validation loss: 2.464960713852965

Epoch: 5| Step: 3
Training loss: 2.322775615520921
Validation loss: 2.4577900494956832

Epoch: 5| Step: 4
Training loss: 2.0407371221470405
Validation loss: 2.453113827334747

Epoch: 5| Step: 5
Training loss: 2.8561877220926863
Validation loss: 2.437552473346404

Epoch: 5| Step: 6
Training loss: 1.9147881825484248
Validation loss: 2.480159155157769

Epoch: 5| Step: 7
Training loss: 1.4801345488242923
Validation loss: 2.4326158111570084

Epoch: 5| Step: 8
Training loss: 2.244974033061107
Validation loss: 2.3892080070092607

Epoch: 5| Step: 9
Training loss: 3.028687642739058
Validation loss: 2.495294552136826

Epoch: 5| Step: 10
Training loss: 1.9146752447150428
Validation loss: 2.450669846492708

Epoch: 421| Step: 0
Training loss: 2.110678489036649
Validation loss: 2.4175021502817624

Epoch: 5| Step: 1
Training loss: 2.183282874400305
Validation loss: 2.4255162584227397

Epoch: 5| Step: 2
Training loss: 1.8790295851866825
Validation loss: 2.479478783494454

Epoch: 5| Step: 3
Training loss: 2.879377267057049
Validation loss: 2.5033778873823804

Epoch: 5| Step: 4
Training loss: 2.228796550887178
Validation loss: 2.46352702631647

Epoch: 5| Step: 5
Training loss: 2.175294448389648
Validation loss: 2.4676457348873257

Epoch: 5| Step: 6
Training loss: 1.7313295724808586
Validation loss: 2.4266743412857688

Epoch: 5| Step: 7
Training loss: 2.094593618304583
Validation loss: 2.4859660110421142

Epoch: 5| Step: 8
Training loss: 2.2447345369390352
Validation loss: 2.4558778335320555

Epoch: 5| Step: 9
Training loss: 1.7373931330467307
Validation loss: 2.5057681733053694

Epoch: 5| Step: 10
Training loss: 2.1399594128405184
Validation loss: 2.429096791769067

Epoch: 422| Step: 0
Training loss: 2.52114811552883
Validation loss: 2.4591414401038154

Epoch: 5| Step: 1
Training loss: 1.8537468238438093
Validation loss: 2.443814504427914

Epoch: 5| Step: 2
Training loss: 1.7356371711631138
Validation loss: 2.4430146782972026

Epoch: 5| Step: 3
Training loss: 2.228498828232876
Validation loss: 2.5119939652116954

Epoch: 5| Step: 4
Training loss: 2.2853276636764783
Validation loss: 2.4686333517250776

Epoch: 5| Step: 5
Training loss: 2.2122186514208617
Validation loss: 2.4374959276205814

Epoch: 5| Step: 6
Training loss: 2.107008447124131
Validation loss: 2.4379898937850744

Epoch: 5| Step: 7
Training loss: 2.561787296831125
Validation loss: 2.4273899107789965

Epoch: 5| Step: 8
Training loss: 2.3133219727265617
Validation loss: 2.491684979629289

Epoch: 5| Step: 9
Training loss: 2.187216822142466
Validation loss: 2.424936593582816

Epoch: 5| Step: 10
Training loss: 2.1722271311325883
Validation loss: 2.4552072268253244

Epoch: 423| Step: 0
Training loss: 2.832622850385207
Validation loss: 2.442903421984282

Epoch: 5| Step: 1
Training loss: 2.289981071860368
Validation loss: 2.4479050286268813

Epoch: 5| Step: 2
Training loss: 1.8418697532887156
Validation loss: 2.452236290712626

Epoch: 5| Step: 3
Training loss: 2.13989947198779
Validation loss: 2.481122440253813

Epoch: 5| Step: 4
Training loss: 2.0543311212517343
Validation loss: 2.493939192267606

Epoch: 5| Step: 5
Training loss: 1.8781843483889442
Validation loss: 2.484432164852333

Epoch: 5| Step: 6
Training loss: 1.7350156133647283
Validation loss: 2.4904511004696204

Epoch: 5| Step: 7
Training loss: 2.372368308171798
Validation loss: 2.443817288558362

Epoch: 5| Step: 8
Training loss: 2.5177316315509013
Validation loss: 2.4550260941199027

Epoch: 5| Step: 9
Training loss: 2.4123622005025642
Validation loss: 2.498176020075754

Epoch: 5| Step: 10
Training loss: 1.5620586534885548
Validation loss: 2.4422237437613292

Epoch: 424| Step: 0
Training loss: 2.648048304765481
Validation loss: 2.459318826600468

Epoch: 5| Step: 1
Training loss: 2.8681013849233468
Validation loss: 2.464176652795089

Epoch: 5| Step: 2
Training loss: 1.975612547804133
Validation loss: 2.4556214485523515

Epoch: 5| Step: 3
Training loss: 1.5620966581459896
Validation loss: 2.4940983773220715

Epoch: 5| Step: 4
Training loss: 2.472340254757955
Validation loss: 2.445765378173986

Epoch: 5| Step: 5
Training loss: 1.7232397911957016
Validation loss: 2.501220097760415

Epoch: 5| Step: 6
Training loss: 1.760497217385786
Validation loss: 2.4238808784568953

Epoch: 5| Step: 7
Training loss: 1.7857271779821127
Validation loss: 2.4952052528835162

Epoch: 5| Step: 8
Training loss: 2.176952327698692
Validation loss: 2.456256471648504

Epoch: 5| Step: 9
Training loss: 2.4252270543092687
Validation loss: 2.4736009174876097

Epoch: 5| Step: 10
Training loss: 2.272275227760352
Validation loss: 2.4988828460013512

Epoch: 425| Step: 0
Training loss: 1.702232336886318
Validation loss: 2.4545102231547453

Epoch: 5| Step: 1
Training loss: 1.8314425370483263
Validation loss: 2.479520256683837

Epoch: 5| Step: 2
Training loss: 1.9334959063194388
Validation loss: 2.4525696534921853

Epoch: 5| Step: 3
Training loss: 2.2001224917210087
Validation loss: 2.4187832228162285

Epoch: 5| Step: 4
Training loss: 2.8029153904959916
Validation loss: 2.4753023502978446

Epoch: 5| Step: 5
Training loss: 1.7121249045682307
Validation loss: 2.4870699114684354

Epoch: 5| Step: 6
Training loss: 2.2845414337187955
Validation loss: 2.452977150095171

Epoch: 5| Step: 7
Training loss: 2.231825647975537
Validation loss: 2.415937682568187

Epoch: 5| Step: 8
Training loss: 2.547966658776814
Validation loss: 2.490022409913734

Epoch: 5| Step: 9
Training loss: 2.3164986757620065
Validation loss: 2.3971368779051776

Epoch: 5| Step: 10
Training loss: 2.246550989465096
Validation loss: 2.483204066294137

Epoch: 426| Step: 0
Training loss: 2.2010174132667415
Validation loss: 2.451311630358877

Epoch: 5| Step: 1
Training loss: 1.607116076836667
Validation loss: 2.4633630891066485

Epoch: 5| Step: 2
Training loss: 2.5901277366275046
Validation loss: 2.4501073913359495

Epoch: 5| Step: 3
Training loss: 1.9770635640717282
Validation loss: 2.448858732199086

Epoch: 5| Step: 4
Training loss: 2.1269536854696804
Validation loss: 2.4310190364994853

Epoch: 5| Step: 5
Training loss: 2.0477538806355513
Validation loss: 2.42152582219025

Epoch: 5| Step: 6
Training loss: 2.213679362483477
Validation loss: 2.483078826729147

Epoch: 5| Step: 7
Training loss: 2.15995613813018
Validation loss: 2.429439779345984

Epoch: 5| Step: 8
Training loss: 2.5531689157992976
Validation loss: 2.4311963827984937

Epoch: 5| Step: 9
Training loss: 2.1079641639660154
Validation loss: 2.485097338336371

Epoch: 5| Step: 10
Training loss: 2.246058933154064
Validation loss: 2.4299015346171715

Epoch: 427| Step: 0
Training loss: 2.419784529685439
Validation loss: 2.47395803627483

Epoch: 5| Step: 1
Training loss: 1.4730446705464064
Validation loss: 2.4708227633572664

Epoch: 5| Step: 2
Training loss: 2.074804076171601
Validation loss: 2.495371051963632

Epoch: 5| Step: 3
Training loss: 2.567618120219457
Validation loss: 2.464382626891882

Epoch: 5| Step: 4
Training loss: 2.454932646283911
Validation loss: 2.451678135003514

Epoch: 5| Step: 5
Training loss: 1.8459779887927759
Validation loss: 2.451454690729862

Epoch: 5| Step: 6
Training loss: 2.435807838928604
Validation loss: 2.4478883338882174

Epoch: 5| Step: 7
Training loss: 1.9661740602062872
Validation loss: 2.4190954034408083

Epoch: 5| Step: 8
Training loss: 2.298651494813991
Validation loss: 2.4228973390299307

Epoch: 5| Step: 9
Training loss: 2.330028498233828
Validation loss: 2.4741847326616817

Epoch: 5| Step: 10
Training loss: 1.7518845356556838
Validation loss: 2.4951500702097476

Epoch: 428| Step: 0
Training loss: 2.309336173385448
Validation loss: 2.4672695096171733

Epoch: 5| Step: 1
Training loss: 2.7614854770757167
Validation loss: 2.463728982002465

Epoch: 5| Step: 2
Training loss: 1.8315257206640865
Validation loss: 2.4312775867150918

Epoch: 5| Step: 3
Training loss: 2.556146887306139
Validation loss: 2.427117969389559

Epoch: 5| Step: 4
Training loss: 1.8038902338175213
Validation loss: 2.4451719425360197

Epoch: 5| Step: 5
Training loss: 2.0164421849852734
Validation loss: 2.4630520743068334

Epoch: 5| Step: 6
Training loss: 2.148198228863706
Validation loss: 2.455734185242272

Epoch: 5| Step: 7
Training loss: 2.487958231369904
Validation loss: 2.4260118374340904

Epoch: 5| Step: 8
Training loss: 1.7509562740650448
Validation loss: 2.458691400056451

Epoch: 5| Step: 9
Training loss: 2.087156589608724
Validation loss: 2.41530867332974

Epoch: 5| Step: 10
Training loss: 2.172419198556156
Validation loss: 2.474382201553445

Epoch: 429| Step: 0
Training loss: 2.2842913694506826
Validation loss: 2.481556358444732

Epoch: 5| Step: 1
Training loss: 2.0056898719612146
Validation loss: 2.461450588728347

Epoch: 5| Step: 2
Training loss: 1.902308782925875
Validation loss: 2.429649113472966

Epoch: 5| Step: 3
Training loss: 2.037432256403234
Validation loss: 2.4419401570941606

Epoch: 5| Step: 4
Training loss: 2.2914736810979415
Validation loss: 2.4304639086655646

Epoch: 5| Step: 5
Training loss: 2.350893248567895
Validation loss: 2.4697215893119404

Epoch: 5| Step: 6
Training loss: 2.4498730140007265
Validation loss: 2.476854466480668

Epoch: 5| Step: 7
Training loss: 1.8892661010976817
Validation loss: 2.431926457368454

Epoch: 5| Step: 8
Training loss: 2.104079317099059
Validation loss: 2.3995690722215763

Epoch: 5| Step: 9
Training loss: 2.6626092521209306
Validation loss: 2.4558508094297093

Epoch: 5| Step: 10
Training loss: 2.157807934512447
Validation loss: 2.420832748106614

Epoch: 430| Step: 0
Training loss: 2.4186395506567275
Validation loss: 2.4333051750968906

Epoch: 5| Step: 1
Training loss: 2.3960777379195286
Validation loss: 2.4608961865925196

Epoch: 5| Step: 2
Training loss: 2.8688828447952774
Validation loss: 2.466789351793267

Epoch: 5| Step: 3
Training loss: 1.9082980800965033
Validation loss: 2.4492876104965338

Epoch: 5| Step: 4
Training loss: 2.250209374752627
Validation loss: 2.4542042354471043

Epoch: 5| Step: 5
Training loss: 1.9330151241241682
Validation loss: 2.439101758054219

Epoch: 5| Step: 6
Training loss: 2.0137821023040954
Validation loss: 2.4741265539069244

Epoch: 5| Step: 7
Training loss: 1.9897049339109407
Validation loss: 2.4914376280923642

Epoch: 5| Step: 8
Training loss: 1.7122824621210369
Validation loss: 2.4527306852125834

Epoch: 5| Step: 9
Training loss: 2.195127540480468
Validation loss: 2.4439578211323654

Epoch: 5| Step: 10
Training loss: 2.1187369107092455
Validation loss: 2.3958005313491446

Epoch: 431| Step: 0
Training loss: 2.43155689018729
Validation loss: 2.4784986141011576

Epoch: 5| Step: 1
Training loss: 1.9275280688427752
Validation loss: 2.4152650286838684

Epoch: 5| Step: 2
Training loss: 1.993723737011554
Validation loss: 2.4889940098168046

Epoch: 5| Step: 3
Training loss: 2.6477529715765464
Validation loss: 2.466774878987068

Epoch: 5| Step: 4
Training loss: 2.547940271309292
Validation loss: 2.508541027553398

Epoch: 5| Step: 5
Training loss: 1.9292584764730123
Validation loss: 2.4290430771813183

Epoch: 5| Step: 6
Training loss: 2.268650218776581
Validation loss: 2.4436503589110985

Epoch: 5| Step: 7
Training loss: 2.89947589873855
Validation loss: 2.4545514957355983

Epoch: 5| Step: 8
Training loss: 1.7237709227821212
Validation loss: 2.39547224029968

Epoch: 5| Step: 9
Training loss: 1.7098304267370494
Validation loss: 2.459291912237416

Epoch: 5| Step: 10
Training loss: 1.7518958993536806
Validation loss: 2.45904012009233

Epoch: 432| Step: 0
Training loss: 2.2385259098464676
Validation loss: 2.3976559512149747

Epoch: 5| Step: 1
Training loss: 1.7783003360269394
Validation loss: 2.4386340338535497

Epoch: 5| Step: 2
Training loss: 2.4224352773045337
Validation loss: 2.4386110006182187

Epoch: 5| Step: 3
Training loss: 2.0232072500362195
Validation loss: 2.3958253340478355

Epoch: 5| Step: 4
Training loss: 2.4130038313442
Validation loss: 2.3971630880735613

Epoch: 5| Step: 5
Training loss: 2.0062723746790083
Validation loss: 2.4549430483494454

Epoch: 5| Step: 6
Training loss: 2.412916781935384
Validation loss: 2.475589748391852

Epoch: 5| Step: 7
Training loss: 1.5248191632472616
Validation loss: 2.4574564215675947

Epoch: 5| Step: 8
Training loss: 2.5958160995801216
Validation loss: 2.5002308154458412

Epoch: 5| Step: 9
Training loss: 2.224413882433152
Validation loss: 2.4353446402490793

Epoch: 5| Step: 10
Training loss: 2.0900616398532916
Validation loss: 2.4806727094175556

Epoch: 433| Step: 0
Training loss: 2.078013940582656
Validation loss: 2.4262462881789917

Epoch: 5| Step: 1
Training loss: 1.7109722673898669
Validation loss: 2.4459636447308863

Epoch: 5| Step: 2
Training loss: 2.042517646890745
Validation loss: 2.480497361065717

Epoch: 5| Step: 3
Training loss: 2.8388146132150167
Validation loss: 2.4758124496228575

Epoch: 5| Step: 4
Training loss: 2.1920856911693027
Validation loss: 2.431497126341242

Epoch: 5| Step: 5
Training loss: 2.1500648577466426
Validation loss: 2.461997474993463

Epoch: 5| Step: 6
Training loss: 2.087757816497857
Validation loss: 2.4397182136822626

Epoch: 5| Step: 7
Training loss: 2.2106764137128345
Validation loss: 2.432948705398275

Epoch: 5| Step: 8
Training loss: 2.2932092387794634
Validation loss: 2.5032210170787037

Epoch: 5| Step: 9
Training loss: 1.8464976381875042
Validation loss: 2.4535280545392832

Epoch: 5| Step: 10
Training loss: 1.585387035732937
Validation loss: 2.4390587693491925

Epoch: 434| Step: 0
Training loss: 2.3990602958324225
Validation loss: 2.460796929176462

Epoch: 5| Step: 1
Training loss: 1.7408093531180255
Validation loss: 2.4514456898311137

Epoch: 5| Step: 2
Training loss: 1.8981345606096172
Validation loss: 2.4689578987374063

Epoch: 5| Step: 3
Training loss: 2.2359402382631948
Validation loss: 2.443393655130928

Epoch: 5| Step: 4
Training loss: 2.460745086413881
Validation loss: 2.441825904792857

Epoch: 5| Step: 5
Training loss: 1.9889829705124489
Validation loss: 2.4792404460260826

Epoch: 5| Step: 6
Training loss: 2.340060063267806
Validation loss: 2.448771181809828

Epoch: 5| Step: 7
Training loss: 2.265838192084227
Validation loss: 2.4661375724951347

Epoch: 5| Step: 8
Training loss: 1.9943728559261933
Validation loss: 2.464877000170397

Epoch: 5| Step: 9
Training loss: 1.9859454331138264
Validation loss: 2.4735541836855877

Epoch: 5| Step: 10
Training loss: 2.4936446471176748
Validation loss: 2.4288974800843808

Epoch: 435| Step: 0
Training loss: 2.1805367951554624
Validation loss: 2.4101348560621787

Epoch: 5| Step: 1
Training loss: 1.8311208972404496
Validation loss: 2.489256381994149

Epoch: 5| Step: 2
Training loss: 2.034725326634087
Validation loss: 2.4384947809086985

Epoch: 5| Step: 3
Training loss: 3.0256227782236764
Validation loss: 2.462211783952117

Epoch: 5| Step: 4
Training loss: 1.7899418298230785
Validation loss: 2.444352213906999

Epoch: 5| Step: 5
Training loss: 2.2866737999911395
Validation loss: 2.485146223587535

Epoch: 5| Step: 6
Training loss: 2.4859105768669956
Validation loss: 2.433167829783655

Epoch: 5| Step: 7
Training loss: 2.1571076387545753
Validation loss: 2.458953971654963

Epoch: 5| Step: 8
Training loss: 2.3658044381665273
Validation loss: 2.442021850941494

Epoch: 5| Step: 9
Training loss: 1.9242573680402086
Validation loss: 2.474625898792414

Epoch: 5| Step: 10
Training loss: 1.2877302741626664
Validation loss: 2.409523905755726

Epoch: 436| Step: 0
Training loss: 2.1733779646770484
Validation loss: 2.4434436060822984

Epoch: 5| Step: 1
Training loss: 2.2506995703015575
Validation loss: 2.475567567469183

Epoch: 5| Step: 2
Training loss: 2.168316457547376
Validation loss: 2.4070688427769533

Epoch: 5| Step: 3
Training loss: 1.8065700867775185
Validation loss: 2.505967087441208

Epoch: 5| Step: 4
Training loss: 1.8350670881766504
Validation loss: 2.469850061681706

Epoch: 5| Step: 5
Training loss: 1.8821401068154702
Validation loss: 2.4585403155717804

Epoch: 5| Step: 6
Training loss: 1.847056382207304
Validation loss: 2.4493737001412503

Epoch: 5| Step: 7
Training loss: 2.0958488534563804
Validation loss: 2.4203794606018443

Epoch: 5| Step: 8
Training loss: 2.5983968267050286
Validation loss: 2.487610173569494

Epoch: 5| Step: 9
Training loss: 2.063180637759248
Validation loss: 2.4537268452185015

Epoch: 5| Step: 10
Training loss: 2.8763575458604618
Validation loss: 2.421804740804923

Epoch: 437| Step: 0
Training loss: 2.0336885340053246
Validation loss: 2.4092138719037437

Epoch: 5| Step: 1
Training loss: 2.325996501214765
Validation loss: 2.4266320568272906

Epoch: 5| Step: 2
Training loss: 2.3589194154773114
Validation loss: 2.4582044513081076

Epoch: 5| Step: 3
Training loss: 1.5718452223275998
Validation loss: 2.4893275643686366

Epoch: 5| Step: 4
Training loss: 2.0593679517703505
Validation loss: 2.487417188916915

Epoch: 5| Step: 5
Training loss: 2.3645538686745478
Validation loss: 2.4342387669021366

Epoch: 5| Step: 6
Training loss: 2.3636731540044154
Validation loss: 2.447461769358221

Epoch: 5| Step: 7
Training loss: 1.7408949500550213
Validation loss: 2.4357440830259165

Epoch: 5| Step: 8
Training loss: 1.5697576197235596
Validation loss: 2.483692682510923

Epoch: 5| Step: 9
Training loss: 2.3161064069677835
Validation loss: 2.4791867641490235

Epoch: 5| Step: 10
Training loss: 2.721225444663638
Validation loss: 2.431636397316115

Epoch: 438| Step: 0
Training loss: 2.289722855134747
Validation loss: 2.434201732281723

Epoch: 5| Step: 1
Training loss: 2.5152714640819394
Validation loss: 2.504194201268101

Epoch: 5| Step: 2
Training loss: 2.4679211540982138
Validation loss: 2.4423277116907083

Epoch: 5| Step: 3
Training loss: 2.3807371012682657
Validation loss: 2.4555984744332084

Epoch: 5| Step: 4
Training loss: 1.8640742184061998
Validation loss: 2.4417510036742693

Epoch: 5| Step: 5
Training loss: 2.1295144988312296
Validation loss: 2.3938044595789933

Epoch: 5| Step: 6
Training loss: 2.0465906542642633
Validation loss: 2.440559870931107

Epoch: 5| Step: 7
Training loss: 2.4163180680808627
Validation loss: 2.4356337692478136

Epoch: 5| Step: 8
Training loss: 1.8793283095552584
Validation loss: 2.424743336240844

Epoch: 5| Step: 9
Training loss: 1.6711641030217577
Validation loss: 2.4558907723849317

Epoch: 5| Step: 10
Training loss: 1.8720223625143435
Validation loss: 2.44687783202827

Epoch: 439| Step: 0
Training loss: 1.7417097591319692
Validation loss: 2.4221094722556016

Epoch: 5| Step: 1
Training loss: 1.8825663172171925
Validation loss: 2.4600531928654865

Epoch: 5| Step: 2
Training loss: 2.477712851425906
Validation loss: 2.4609904132343

Epoch: 5| Step: 3
Training loss: 2.1299703475965632
Validation loss: 2.4787207664603668

Epoch: 5| Step: 4
Training loss: 1.8895076886576079
Validation loss: 2.4387587449003356

Epoch: 5| Step: 5
Training loss: 2.264300149748671
Validation loss: 2.4861627416481245

Epoch: 5| Step: 6
Training loss: 1.7276761015194586
Validation loss: 2.4265251375115358

Epoch: 5| Step: 7
Training loss: 2.495503004967683
Validation loss: 2.4224310049813598

Epoch: 5| Step: 8
Training loss: 2.5895883655780523
Validation loss: 2.478548202520994

Epoch: 5| Step: 9
Training loss: 1.8047928552301833
Validation loss: 2.446476972249737

Epoch: 5| Step: 10
Training loss: 2.5793091799835213
Validation loss: 2.428403562280973

Epoch: 440| Step: 0
Training loss: 2.1232647262021236
Validation loss: 2.4452473092060725

Epoch: 5| Step: 1
Training loss: 2.049247706009085
Validation loss: 2.456827529664541

Epoch: 5| Step: 2
Training loss: 2.3205235446293577
Validation loss: 2.4529580160743722

Epoch: 5| Step: 3
Training loss: 1.8493155837239748
Validation loss: 2.4311139771888515

Epoch: 5| Step: 4
Training loss: 2.4935600304010377
Validation loss: 2.4676935280450043

Epoch: 5| Step: 5
Training loss: 2.5261722072154567
Validation loss: 2.464785707517539

Epoch: 5| Step: 6
Training loss: 1.5171515259175645
Validation loss: 2.468449976364424

Epoch: 5| Step: 7
Training loss: 1.9087148271179133
Validation loss: 2.468088981843761

Epoch: 5| Step: 8
Training loss: 2.130563633270844
Validation loss: 2.454780874079976

Epoch: 5| Step: 9
Training loss: 2.5303059454021852
Validation loss: 2.476281208038524

Epoch: 5| Step: 10
Training loss: 2.0043408255404014
Validation loss: 2.463241250977722

Epoch: 441| Step: 0
Training loss: 1.7483383191488842
Validation loss: 2.4669610880311432

Epoch: 5| Step: 1
Training loss: 2.2505311339046328
Validation loss: 2.4225547066949904

Epoch: 5| Step: 2
Training loss: 1.7593649557311393
Validation loss: 2.467660740733679

Epoch: 5| Step: 3
Training loss: 1.809522419196386
Validation loss: 2.4345643697989368

Epoch: 5| Step: 4
Training loss: 2.4629391235126277
Validation loss: 2.4528048016149993

Epoch: 5| Step: 5
Training loss: 2.6916006873422584
Validation loss: 2.454933680122334

Epoch: 5| Step: 6
Training loss: 2.3295932722576165
Validation loss: 2.4427531136573086

Epoch: 5| Step: 7
Training loss: 2.0180535877927337
Validation loss: 2.4646836740384948

Epoch: 5| Step: 8
Training loss: 2.7266917375688204
Validation loss: 2.4121315081469663

Epoch: 5| Step: 9
Training loss: 1.929086815628116
Validation loss: 2.4080401333840036

Epoch: 5| Step: 10
Training loss: 1.374555559289189
Validation loss: 2.4414314808844098

Epoch: 442| Step: 0
Training loss: 2.2494862287769246
Validation loss: 2.4802561498225955

Epoch: 5| Step: 1
Training loss: 2.5695030537928782
Validation loss: 2.504355586433882

Epoch: 5| Step: 2
Training loss: 1.1070970062131307
Validation loss: 2.449067537538815

Epoch: 5| Step: 3
Training loss: 2.3072245453887312
Validation loss: 2.459375259389602

Epoch: 5| Step: 4
Training loss: 2.1895696386120136
Validation loss: 2.4304763436093753

Epoch: 5| Step: 5
Training loss: 2.5911526831659355
Validation loss: 2.475642551422729

Epoch: 5| Step: 6
Training loss: 1.8369627342285908
Validation loss: 2.396131772094263

Epoch: 5| Step: 7
Training loss: 1.8508278721307012
Validation loss: 2.4715113136949824

Epoch: 5| Step: 8
Training loss: 2.252887568350867
Validation loss: 2.443682108754477

Epoch: 5| Step: 9
Training loss: 2.1213282109567757
Validation loss: 2.4640795974085736

Epoch: 5| Step: 10
Training loss: 2.312014554595014
Validation loss: 2.4434362412703927

Epoch: 443| Step: 0
Training loss: 2.0932009532198057
Validation loss: 2.452396134316917

Epoch: 5| Step: 1
Training loss: 1.8935679080653098
Validation loss: 2.433102466373575

Epoch: 5| Step: 2
Training loss: 1.8603206762117828
Validation loss: 2.4351416665037413

Epoch: 5| Step: 3
Training loss: 2.2157044260614303
Validation loss: 2.4001960609793103

Epoch: 5| Step: 4
Training loss: 2.516229592404941
Validation loss: 2.4397696043106385

Epoch: 5| Step: 5
Training loss: 2.629127073720342
Validation loss: 2.46973580818118

Epoch: 5| Step: 6
Training loss: 1.8067362991981115
Validation loss: 2.477885160054171

Epoch: 5| Step: 7
Training loss: 2.1719415229105206
Validation loss: 2.4179472470641508

Epoch: 5| Step: 8
Training loss: 2.144301549842679
Validation loss: 2.4336184256114506

Epoch: 5| Step: 9
Training loss: 2.2598857930114153
Validation loss: 2.484600244476694

Epoch: 5| Step: 10
Training loss: 2.0021608123977384
Validation loss: 2.431606221349986

Epoch: 444| Step: 0
Training loss: 2.378849824383413
Validation loss: 2.4533699289067394

Epoch: 5| Step: 1
Training loss: 2.358485376244359
Validation loss: 2.4260639970894973

Epoch: 5| Step: 2
Training loss: 2.2874837906059917
Validation loss: 2.4198630580209763

Epoch: 5| Step: 3
Training loss: 2.0463711649932352
Validation loss: 2.38183681686116

Epoch: 5| Step: 4
Training loss: 2.6663144793003797
Validation loss: 2.4681397059226935

Epoch: 5| Step: 5
Training loss: 2.0921845918254856
Validation loss: 2.4372102582574935

Epoch: 5| Step: 6
Training loss: 1.8450050608247648
Validation loss: 2.4546855615566274

Epoch: 5| Step: 7
Training loss: 1.6300221378484707
Validation loss: 2.4950511413030814

Epoch: 5| Step: 8
Training loss: 1.949872902860797
Validation loss: 2.451346156950621

Epoch: 5| Step: 9
Training loss: 1.8310506510372175
Validation loss: 2.417287681683003

Epoch: 5| Step: 10
Training loss: 2.145903157206173
Validation loss: 2.5041154827110694

Epoch: 445| Step: 0
Training loss: 2.455763448811003
Validation loss: 2.4330387312487187

Epoch: 5| Step: 1
Training loss: 1.9955678347778794
Validation loss: 2.4293802187922675

Epoch: 5| Step: 2
Training loss: 1.8956832686617848
Validation loss: 2.455473637251128

Epoch: 5| Step: 3
Training loss: 2.185787947844414
Validation loss: 2.4487235438453703

Epoch: 5| Step: 4
Training loss: 2.5423929292082366
Validation loss: 2.4761976099610186

Epoch: 5| Step: 5
Training loss: 1.8983050013642027
Validation loss: 2.4276115064248227

Epoch: 5| Step: 6
Training loss: 2.0274658166376756
Validation loss: 2.4596425472741514

Epoch: 5| Step: 7
Training loss: 2.1433083081996864
Validation loss: 2.486286787024494

Epoch: 5| Step: 8
Training loss: 2.2206019879261882
Validation loss: 2.434861955093895

Epoch: 5| Step: 9
Training loss: 1.9871695957616038
Validation loss: 2.4770802354289145

Epoch: 5| Step: 10
Training loss: 1.7402378685912954
Validation loss: 2.4185877969107588

Epoch: 446| Step: 0
Training loss: 2.0978608271400723
Validation loss: 2.4250394003098488

Epoch: 5| Step: 1
Training loss: 2.099172601735608
Validation loss: 2.438931930385443

Epoch: 5| Step: 2
Training loss: 2.129345769745959
Validation loss: 2.415250256683526

Epoch: 5| Step: 3
Training loss: 1.930082026008473
Validation loss: 2.4562478416458644

Epoch: 5| Step: 4
Training loss: 2.184276712850453
Validation loss: 2.4448536731988297

Epoch: 5| Step: 5
Training loss: 1.4291318220037283
Validation loss: 2.4904450456016707

Epoch: 5| Step: 6
Training loss: 2.7334775487334686
Validation loss: 2.4062599604890162

Epoch: 5| Step: 7
Training loss: 2.6327639612087244
Validation loss: 2.424169454249553

Epoch: 5| Step: 8
Training loss: 1.571477050157317
Validation loss: 2.4319700686484897

Epoch: 5| Step: 9
Training loss: 1.8491906535017006
Validation loss: 2.357040864255798

Epoch: 5| Step: 10
Training loss: 2.616597305480041
Validation loss: 2.4519427854882108

Epoch: 447| Step: 0
Training loss: 1.973414450999386
Validation loss: 2.4330808063753424

Epoch: 5| Step: 1
Training loss: 2.064080990658684
Validation loss: 2.4351831947118305

Epoch: 5| Step: 2
Training loss: 2.7433132023804685
Validation loss: 2.4280670410381244

Epoch: 5| Step: 3
Training loss: 2.099195884974738
Validation loss: 2.398637406336549

Epoch: 5| Step: 4
Training loss: 2.1840288141676387
Validation loss: 2.4839439995961143

Epoch: 5| Step: 5
Training loss: 1.8083191368612248
Validation loss: 2.441108836051071

Epoch: 5| Step: 6
Training loss: 2.122283995008385
Validation loss: 2.470910357240517

Epoch: 5| Step: 7
Training loss: 1.965538493619627
Validation loss: 2.4193053025469187

Epoch: 5| Step: 8
Training loss: 1.9223077798495147
Validation loss: 2.42696577394171

Epoch: 5| Step: 9
Training loss: 2.286431476779974
Validation loss: 2.4050978954994706

Epoch: 5| Step: 10
Training loss: 2.1492470065845155
Validation loss: 2.4393098452595527

Epoch: 448| Step: 0
Training loss: 2.344451392446209
Validation loss: 2.4866236182524695

Epoch: 5| Step: 1
Training loss: 1.893019365932029
Validation loss: 2.4301906255091295

Epoch: 5| Step: 2
Training loss: 2.510470209038775
Validation loss: 2.424986492864037

Epoch: 5| Step: 3
Training loss: 2.092787678038635
Validation loss: 2.4181229599199443

Epoch: 5| Step: 4
Training loss: 1.8193609673113855
Validation loss: 2.4346310265191873

Epoch: 5| Step: 5
Training loss: 2.858420624433452
Validation loss: 2.440763742740316

Epoch: 5| Step: 6
Training loss: 1.5307720761960204
Validation loss: 2.406697266178799

Epoch: 5| Step: 7
Training loss: 2.002123658895177
Validation loss: 2.411499293827797

Epoch: 5| Step: 8
Training loss: 1.7869993749232125
Validation loss: 2.434483001194924

Epoch: 5| Step: 9
Training loss: 2.5615837506004624
Validation loss: 2.4871509377788317

Epoch: 5| Step: 10
Training loss: 1.4478170603057228
Validation loss: 2.4166849724058954

Epoch: 449| Step: 0
Training loss: 2.124852007032097
Validation loss: 2.437075365225132

Epoch: 5| Step: 1
Training loss: 1.9720754240441456
Validation loss: 2.419206257294545

Epoch: 5| Step: 2
Training loss: 2.3465605097117197
Validation loss: 2.4527495941960447

Epoch: 5| Step: 3
Training loss: 2.1572480034442627
Validation loss: 2.4301849690569393

Epoch: 5| Step: 4
Training loss: 1.8344587498641127
Validation loss: 2.4544742934272983

Epoch: 5| Step: 5
Training loss: 1.7701595445700429
Validation loss: 2.4309214429578394

Epoch: 5| Step: 6
Training loss: 2.8722582473151714
Validation loss: 2.444630515411499

Epoch: 5| Step: 7
Training loss: 1.8286006993267079
Validation loss: 2.3886584228542524

Epoch: 5| Step: 8
Training loss: 1.9236765535548155
Validation loss: 2.462991317183478

Epoch: 5| Step: 9
Training loss: 2.1386669698859273
Validation loss: 2.44508929287052

Epoch: 5| Step: 10
Training loss: 2.273695154435606
Validation loss: 2.427568170620953

Epoch: 450| Step: 0
Training loss: 1.8901188701002725
Validation loss: 2.446384739357219

Epoch: 5| Step: 1
Training loss: 1.9416522171508839
Validation loss: 2.4691962538057286

Epoch: 5| Step: 2
Training loss: 1.9972529619256791
Validation loss: 2.458371625309701

Epoch: 5| Step: 3
Training loss: 2.296093658623786
Validation loss: 2.439347623122369

Epoch: 5| Step: 4
Training loss: 1.8426853921028976
Validation loss: 2.4751002648442597

Epoch: 5| Step: 5
Training loss: 2.5251646007104473
Validation loss: 2.478538370145657

Epoch: 5| Step: 6
Training loss: 2.0435305929673064
Validation loss: 2.4521229070593207

Epoch: 5| Step: 7
Training loss: 2.159760312980954
Validation loss: 2.425294630950637

Epoch: 5| Step: 8
Training loss: 2.0028268386868
Validation loss: 2.471334741920351

Epoch: 5| Step: 9
Training loss: 2.570441060199109
Validation loss: 2.4357464174879992

Epoch: 5| Step: 10
Training loss: 1.7912049770620013
Validation loss: 2.4773318963156563

Epoch: 451| Step: 0
Training loss: 2.2265657123743274
Validation loss: 2.4719473654637816

Epoch: 5| Step: 1
Training loss: 2.4174306144343105
Validation loss: 2.5026248698278657

Epoch: 5| Step: 2
Training loss: 2.1831251812008428
Validation loss: 2.459135816883787

Epoch: 5| Step: 3
Training loss: 1.406348797188643
Validation loss: 2.4167419636855665

Epoch: 5| Step: 4
Training loss: 2.4536403430566085
Validation loss: 2.426641066304724

Epoch: 5| Step: 5
Training loss: 1.266244006957081
Validation loss: 2.4642264878066404

Epoch: 5| Step: 6
Training loss: 2.360039219611197
Validation loss: 2.46011029762626

Epoch: 5| Step: 7
Training loss: 2.2938112339088934
Validation loss: 2.452772612767905

Epoch: 5| Step: 8
Training loss: 1.5433906847443442
Validation loss: 2.375691471356041

Epoch: 5| Step: 9
Training loss: 2.4466364874611273
Validation loss: 2.4144482722355094

Epoch: 5| Step: 10
Training loss: 1.999091418833689
Validation loss: 2.4164544794907132

Epoch: 452| Step: 0
Training loss: 1.9930946229597748
Validation loss: 2.408039489290589

Epoch: 5| Step: 1
Training loss: 2.515935748318456
Validation loss: 2.428035750080275

Epoch: 5| Step: 2
Training loss: 2.222333351641439
Validation loss: 2.434054954216024

Epoch: 5| Step: 3
Training loss: 1.7905286753045908
Validation loss: 2.4451018054642937

Epoch: 5| Step: 4
Training loss: 2.077885778898866
Validation loss: 2.414858905350567

Epoch: 5| Step: 5
Training loss: 2.0407611889105435
Validation loss: 2.471923049649129

Epoch: 5| Step: 6
Training loss: 2.0759837656580746
Validation loss: 2.454345455962795

Epoch: 5| Step: 7
Training loss: 1.971836818803012
Validation loss: 2.4454609045056706

Epoch: 5| Step: 8
Training loss: 2.364893843867253
Validation loss: 2.4375576604602407

Epoch: 5| Step: 9
Training loss: 2.578663532769325
Validation loss: 2.428446953907168

Epoch: 5| Step: 10
Training loss: 1.6507484905785432
Validation loss: 2.3978971483433296

Epoch: 453| Step: 0
Training loss: 2.1689254162119287
Validation loss: 2.382608356469232

Epoch: 5| Step: 1
Training loss: 1.484685684617301
Validation loss: 2.4632849931720187

Epoch: 5| Step: 2
Training loss: 2.3823462858778908
Validation loss: 2.3990040461007998

Epoch: 5| Step: 3
Training loss: 1.886265986673751
Validation loss: 2.4143390868093113

Epoch: 5| Step: 4
Training loss: 1.8346976781116717
Validation loss: 2.4738673626808696

Epoch: 5| Step: 5
Training loss: 1.6692905593476228
Validation loss: 2.4558604899341137

Epoch: 5| Step: 6
Training loss: 2.9544481008002603
Validation loss: 2.4454222660432055

Epoch: 5| Step: 7
Training loss: 1.8215802407361998
Validation loss: 2.4466514597502855

Epoch: 5| Step: 8
Training loss: 2.448513385738453
Validation loss: 2.4160460848212946

Epoch: 5| Step: 9
Training loss: 2.3148410391558336
Validation loss: 2.469022202870388

Epoch: 5| Step: 10
Training loss: 1.636081544205594
Validation loss: 2.439256781224316

Epoch: 454| Step: 0
Training loss: 1.9042743388089567
Validation loss: 2.4623469293331115

Epoch: 5| Step: 1
Training loss: 2.4648559376555856
Validation loss: 2.4576195594340176

Epoch: 5| Step: 2
Training loss: 1.7173219470220227
Validation loss: 2.4907546567447008

Epoch: 5| Step: 3
Training loss: 2.0814867864405158
Validation loss: 2.4331218713850613

Epoch: 5| Step: 4
Training loss: 1.8756602714194082
Validation loss: 2.421163557539293

Epoch: 5| Step: 5
Training loss: 2.7736641804646127
Validation loss: 2.445652875891746

Epoch: 5| Step: 6
Training loss: 2.5859749321662964
Validation loss: 2.4176127580094207

Epoch: 5| Step: 7
Training loss: 1.5829956129522977
Validation loss: 2.4718473253969244

Epoch: 5| Step: 8
Training loss: 2.1012744351610935
Validation loss: 2.4593214180527

Epoch: 5| Step: 9
Training loss: 2.0113165888600877
Validation loss: 2.452335356242996

Epoch: 5| Step: 10
Training loss: 2.0100058364783426
Validation loss: 2.4194202983368336

Epoch: 455| Step: 0
Training loss: 2.736589679293361
Validation loss: 2.469593488120717

Epoch: 5| Step: 1
Training loss: 2.23910927893361
Validation loss: 2.4482958379325783

Epoch: 5| Step: 2
Training loss: 1.9026257816997536
Validation loss: 2.4620559965882536

Epoch: 5| Step: 3
Training loss: 1.9414650737362857
Validation loss: 2.4459539706501165

Epoch: 5| Step: 4
Training loss: 1.829358328797724
Validation loss: 2.3887383650941345

Epoch: 5| Step: 5
Training loss: 2.127736629500997
Validation loss: 2.4606411256867897

Epoch: 5| Step: 6
Training loss: 2.0611787667813704
Validation loss: 2.4199158174013427

Epoch: 5| Step: 7
Training loss: 1.935863788423742
Validation loss: 2.4404302127436432

Epoch: 5| Step: 8
Training loss: 2.194563116682181
Validation loss: 2.4363078827323617

Epoch: 5| Step: 9
Training loss: 1.8923451818637629
Validation loss: 2.3979169077442033

Epoch: 5| Step: 10
Training loss: 2.3011886676362754
Validation loss: 2.448931116541982

Epoch: 456| Step: 0
Training loss: 1.7614595006769314
Validation loss: 2.4165028183088397

Epoch: 5| Step: 1
Training loss: 1.3516295311683477
Validation loss: 2.447866079994227

Epoch: 5| Step: 2
Training loss: 2.1874464845923254
Validation loss: 2.452360423983534

Epoch: 5| Step: 3
Training loss: 2.2546234312094864
Validation loss: 2.417688905315419

Epoch: 5| Step: 4
Training loss: 2.1640857461169674
Validation loss: 2.4317138565962146

Epoch: 5| Step: 5
Training loss: 2.1482294155268007
Validation loss: 2.4472476028305055

Epoch: 5| Step: 6
Training loss: 2.629622068628098
Validation loss: 2.4635506039461474

Epoch: 5| Step: 7
Training loss: 1.5662456629906938
Validation loss: 2.4278606110423113

Epoch: 5| Step: 8
Training loss: 1.9551808637973371
Validation loss: 2.46288623340852

Epoch: 5| Step: 9
Training loss: 2.2054005227530165
Validation loss: 2.41296403780947

Epoch: 5| Step: 10
Training loss: 2.7386890494086007
Validation loss: 2.4646008704695035

Epoch: 457| Step: 0
Training loss: 2.183001224603104
Validation loss: 2.423668189665118

Epoch: 5| Step: 1
Training loss: 2.0694525318144184
Validation loss: 2.45921583761797

Epoch: 5| Step: 2
Training loss: 3.2183342822597165
Validation loss: 2.398342388148839

Epoch: 5| Step: 3
Training loss: 1.7277520686189514
Validation loss: 2.4793206192352284

Epoch: 5| Step: 4
Training loss: 2.185187507108519
Validation loss: 2.4715748486579057

Epoch: 5| Step: 5
Training loss: 1.6469712810131747
Validation loss: 2.430025623013761

Epoch: 5| Step: 6
Training loss: 1.835501306540103
Validation loss: 2.3938824361910886

Epoch: 5| Step: 7
Training loss: 1.6548863681379984
Validation loss: 2.4240594513394034

Epoch: 5| Step: 8
Training loss: 2.433447182835574
Validation loss: 2.4019936209810795

Epoch: 5| Step: 9
Training loss: 1.7136110641510542
Validation loss: 2.435208697462233

Epoch: 5| Step: 10
Training loss: 1.7320877664763346
Validation loss: 2.3950233060298958

Epoch: 458| Step: 0
Training loss: 1.7414901774784102
Validation loss: 2.4258208068588942

Epoch: 5| Step: 1
Training loss: 1.9840874050522173
Validation loss: 2.426243897027836

Epoch: 5| Step: 2
Training loss: 2.066604921997789
Validation loss: 2.4538263149648363

Epoch: 5| Step: 3
Training loss: 1.8144022726071638
Validation loss: 2.424447750448738

Epoch: 5| Step: 4
Training loss: 2.2799174976480057
Validation loss: 2.435170362700627

Epoch: 5| Step: 5
Training loss: 1.1635841564501666
Validation loss: 2.43482156486401

Epoch: 5| Step: 6
Training loss: 2.4574744669304773
Validation loss: 2.4795705876941176

Epoch: 5| Step: 7
Training loss: 2.096188505905139
Validation loss: 2.400946576115992

Epoch: 5| Step: 8
Training loss: 2.7779194467234163
Validation loss: 2.4325498891066974

Epoch: 5| Step: 9
Training loss: 1.8034297639280634
Validation loss: 2.4830902827049157

Epoch: 5| Step: 10
Training loss: 2.6786679477558835
Validation loss: 2.427074151565512

Epoch: 459| Step: 0
Training loss: 2.393244685529855
Validation loss: 2.411817492861186

Epoch: 5| Step: 1
Training loss: 2.336498996779302
Validation loss: 2.48757437565276

Epoch: 5| Step: 2
Training loss: 1.9769559928422502
Validation loss: 2.4527920210303638

Epoch: 5| Step: 3
Training loss: 1.9372220609104673
Validation loss: 2.425160517558566

Epoch: 5| Step: 4
Training loss: 1.9845934770116505
Validation loss: 2.4286358535787205

Epoch: 5| Step: 5
Training loss: 2.252951805029329
Validation loss: 2.395837761516857

Epoch: 5| Step: 6
Training loss: 2.2766226253280357
Validation loss: 2.4119498434722804

Epoch: 5| Step: 7
Training loss: 2.092919369948404
Validation loss: 2.4941975598656083

Epoch: 5| Step: 8
Training loss: 1.9779028889854111
Validation loss: 2.4275530954176507

Epoch: 5| Step: 9
Training loss: 1.996496111485081
Validation loss: 2.4185499364563885

Epoch: 5| Step: 10
Training loss: 1.8885745444559612
Validation loss: 2.4629557370495956

Epoch: 460| Step: 0
Training loss: 2.269161647305895
Validation loss: 2.447440733992876

Epoch: 5| Step: 1
Training loss: 1.646266719237913
Validation loss: 2.4386495320182795

Epoch: 5| Step: 2
Training loss: 1.877957745053336
Validation loss: 2.4853230279461442

Epoch: 5| Step: 3
Training loss: 1.8553417925315696
Validation loss: 2.4400806359984286

Epoch: 5| Step: 4
Training loss: 1.8174749040510763
Validation loss: 2.3792434782852347

Epoch: 5| Step: 5
Training loss: 2.1195897061603546
Validation loss: 2.441046651104398

Epoch: 5| Step: 6
Training loss: 2.75112796538796
Validation loss: 2.413715050682859

Epoch: 5| Step: 7
Training loss: 2.448395561827981
Validation loss: 2.4553774531781207

Epoch: 5| Step: 8
Training loss: 1.7171327091045934
Validation loss: 2.4976854813457194

Epoch: 5| Step: 9
Training loss: 1.886104001815399
Validation loss: 2.4355606639348806

Epoch: 5| Step: 10
Training loss: 2.402819082484542
Validation loss: 2.4882870546229676

Epoch: 461| Step: 0
Training loss: 1.910409485426634
Validation loss: 2.4395788455543306

Epoch: 5| Step: 1
Training loss: 1.9101387756422081
Validation loss: 2.432196144089834

Epoch: 5| Step: 2
Training loss: 1.5766399685868853
Validation loss: 2.4267699188784153

Epoch: 5| Step: 3
Training loss: 2.0580606946679225
Validation loss: 2.384298706812443

Epoch: 5| Step: 4
Training loss: 1.7690617020468233
Validation loss: 2.4337837343785114

Epoch: 5| Step: 5
Training loss: 2.636254028056772
Validation loss: 2.4687921591321373

Epoch: 5| Step: 6
Training loss: 2.6717403444858316
Validation loss: 2.441161933339875

Epoch: 5| Step: 7
Training loss: 2.1143517453512954
Validation loss: 2.418455270007021

Epoch: 5| Step: 8
Training loss: 1.7012169325037234
Validation loss: 2.405815521038695

Epoch: 5| Step: 9
Training loss: 2.0793673594924624
Validation loss: 2.450069528254041

Epoch: 5| Step: 10
Training loss: 2.282436323666936
Validation loss: 2.4369653736532233

Epoch: 462| Step: 0
Training loss: 1.6329022980596681
Validation loss: 2.432740454510238

Epoch: 5| Step: 1
Training loss: 1.9340454806924057
Validation loss: 2.4622517956790344

Epoch: 5| Step: 2
Training loss: 1.6634837832220808
Validation loss: 2.4600323506210757

Epoch: 5| Step: 3
Training loss: 2.198747655575594
Validation loss: 2.4431255339469597

Epoch: 5| Step: 4
Training loss: 2.4688429030309305
Validation loss: 2.416244725304457

Epoch: 5| Step: 5
Training loss: 1.7189726251860196
Validation loss: 2.4913135309873193

Epoch: 5| Step: 6
Training loss: 1.8654436723010959
Validation loss: 2.466677337582939

Epoch: 5| Step: 7
Training loss: 2.1349646361234087
Validation loss: 2.4309630759520475

Epoch: 5| Step: 8
Training loss: 2.1991087105088702
Validation loss: 2.439179742255918

Epoch: 5| Step: 9
Training loss: 2.277970556562877
Validation loss: 2.4274861329260493

Epoch: 5| Step: 10
Training loss: 2.335018934218037
Validation loss: 2.4439157087837855

Epoch: 463| Step: 0
Training loss: 1.896217460981803
Validation loss: 2.460980733637214

Epoch: 5| Step: 1
Training loss: 1.825539676353153
Validation loss: 2.4614866915807414

Epoch: 5| Step: 2
Training loss: 2.5816369383224735
Validation loss: 2.463492245336565

Epoch: 5| Step: 3
Training loss: 2.025936748334447
Validation loss: 2.425561567177088

Epoch: 5| Step: 4
Training loss: 2.0221825909602873
Validation loss: 2.4641329883318925

Epoch: 5| Step: 5
Training loss: 3.187245826310927
Validation loss: 2.4717900312931027

Epoch: 5| Step: 6
Training loss: 1.6166047598924826
Validation loss: 2.4008152009006376

Epoch: 5| Step: 7
Training loss: 1.8255576339981239
Validation loss: 2.45584362014165

Epoch: 5| Step: 8
Training loss: 2.2812473871921575
Validation loss: 2.371610208534515

Epoch: 5| Step: 9
Training loss: 1.8192117661975493
Validation loss: 2.4360209337859198

Epoch: 5| Step: 10
Training loss: 1.807077122100694
Validation loss: 2.4215750909170577

Epoch: 464| Step: 0
Training loss: 2.0717906353605273
Validation loss: 2.440259082383044

Epoch: 5| Step: 1
Training loss: 2.3659602342167587
Validation loss: 2.388118094614727

Epoch: 5| Step: 2
Training loss: 1.8884884450335264
Validation loss: 2.3902883746408614

Epoch: 5| Step: 3
Training loss: 2.2140849325676393
Validation loss: 2.474624540634077

Epoch: 5| Step: 4
Training loss: 1.5053250048693918
Validation loss: 2.414969668111157

Epoch: 5| Step: 5
Training loss: 2.0291296823043723
Validation loss: 2.4515447250886027

Epoch: 5| Step: 6
Training loss: 1.99688275116325
Validation loss: 2.448460024344699

Epoch: 5| Step: 7
Training loss: 1.8913327618408595
Validation loss: 2.4226801978307857

Epoch: 5| Step: 8
Training loss: 2.7303746197674084
Validation loss: 2.417818404625727

Epoch: 5| Step: 9
Training loss: 2.334878534755975
Validation loss: 2.4134616148443575

Epoch: 5| Step: 10
Training loss: 1.8950390951104599
Validation loss: 2.439019957990969

Epoch: 465| Step: 0
Training loss: 2.1018845548931533
Validation loss: 2.3884138497638774

Epoch: 5| Step: 1
Training loss: 1.771749154925128
Validation loss: 2.4627554095220967

Epoch: 5| Step: 2
Training loss: 2.2603656628270348
Validation loss: 2.458634518325165

Epoch: 5| Step: 3
Training loss: 2.1264857539693787
Validation loss: 2.42419311750576

Epoch: 5| Step: 4
Training loss: 2.4663001350917857
Validation loss: 2.426109903738445

Epoch: 5| Step: 5
Training loss: 1.9186530048320487
Validation loss: 2.4253981861823894

Epoch: 5| Step: 6
Training loss: 1.6644523772797766
Validation loss: 2.4317025244087023

Epoch: 5| Step: 7
Training loss: 2.635405307991569
Validation loss: 2.456079565407549

Epoch: 5| Step: 8
Training loss: 1.9160217845012806
Validation loss: 2.3930353362930576

Epoch: 5| Step: 9
Training loss: 1.944692367309776
Validation loss: 2.4102161218078026

Epoch: 5| Step: 10
Training loss: 2.0303255765310912
Validation loss: 2.4239061447529795

Epoch: 466| Step: 0
Training loss: 1.9698477212752836
Validation loss: 2.424342186690373

Epoch: 5| Step: 1
Training loss: 1.5530707612727321
Validation loss: 2.4269834048332872

Epoch: 5| Step: 2
Training loss: 2.1501067733865895
Validation loss: 2.437566421314789

Epoch: 5| Step: 3
Training loss: 2.4718950736174796
Validation loss: 2.4040342724175128

Epoch: 5| Step: 4
Training loss: 1.9078931136911899
Validation loss: 2.4063279547207608

Epoch: 5| Step: 5
Training loss: 1.925106464265791
Validation loss: 2.429204639518017

Epoch: 5| Step: 6
Training loss: 2.2369734586762036
Validation loss: 2.4247956299662294

Epoch: 5| Step: 7
Training loss: 1.930176522380012
Validation loss: 2.401012630196762

Epoch: 5| Step: 8
Training loss: 2.221727012717104
Validation loss: 2.458628185423167

Epoch: 5| Step: 9
Training loss: 2.216997340880827
Validation loss: 2.4451779239362157

Epoch: 5| Step: 10
Training loss: 2.2197547034759304
Validation loss: 2.436860050221552

Epoch: 467| Step: 0
Training loss: 1.8017658181775056
Validation loss: 2.440233267865502

Epoch: 5| Step: 1
Training loss: 1.6366179442032542
Validation loss: 2.419348679333028

Epoch: 5| Step: 2
Training loss: 2.254907025537226
Validation loss: 2.418180401799216

Epoch: 5| Step: 3
Training loss: 2.2596091543508194
Validation loss: 2.398217330649643

Epoch: 5| Step: 4
Training loss: 2.084921828892578
Validation loss: 2.48957361954057

Epoch: 5| Step: 5
Training loss: 2.424607243144403
Validation loss: 2.4489759989000324

Epoch: 5| Step: 6
Training loss: 2.144318672581125
Validation loss: 2.4376925331141415

Epoch: 5| Step: 7
Training loss: 2.678750099399296
Validation loss: 2.438006864508505

Epoch: 5| Step: 8
Training loss: 1.9318455046491185
Validation loss: 2.4253092318412066

Epoch: 5| Step: 9
Training loss: 1.6694058161854384
Validation loss: 2.449903747697026

Epoch: 5| Step: 10
Training loss: 2.577674225210765
Validation loss: 2.4500283643324985

Epoch: 468| Step: 0
Training loss: 2.4433780123627225
Validation loss: 2.4511953162785995

Epoch: 5| Step: 1
Training loss: 1.9483477953441255
Validation loss: 2.4507799740692677

Epoch: 5| Step: 2
Training loss: 1.817244207230489
Validation loss: 2.425623128144907

Epoch: 5| Step: 3
Training loss: 2.021031425703628
Validation loss: 2.4405967213608175

Epoch: 5| Step: 4
Training loss: 1.7562828359554807
Validation loss: 2.464343388370272

Epoch: 5| Step: 5
Training loss: 1.9649429447927726
Validation loss: 2.4203589249477013

Epoch: 5| Step: 6
Training loss: 2.1802396997734026
Validation loss: 2.393401398913564

Epoch: 5| Step: 7
Training loss: 2.2959638591939138
Validation loss: 2.4633622107492954

Epoch: 5| Step: 8
Training loss: 2.3270095098585584
Validation loss: 2.4708550516681367

Epoch: 5| Step: 9
Training loss: 2.298584386305893
Validation loss: 2.4273394794934093

Epoch: 5| Step: 10
Training loss: 1.6189388798279731
Validation loss: 2.4108811638827987

Epoch: 469| Step: 0
Training loss: 2.172897866430784
Validation loss: 2.407422046511255

Epoch: 5| Step: 1
Training loss: 2.012473548099283
Validation loss: 2.4122420050446483

Epoch: 5| Step: 2
Training loss: 2.483242808575914
Validation loss: 2.4387324041348637

Epoch: 5| Step: 3
Training loss: 2.0926696495277137
Validation loss: 2.4549541730074407

Epoch: 5| Step: 4
Training loss: 1.7460977097707477
Validation loss: 2.470755913140638

Epoch: 5| Step: 5
Training loss: 2.010756416140769
Validation loss: 2.485977476400342

Epoch: 5| Step: 6
Training loss: 2.0535047484761657
Validation loss: 2.3809212174698073

Epoch: 5| Step: 7
Training loss: 2.029112057535244
Validation loss: 2.4315291392551623

Epoch: 5| Step: 8
Training loss: 2.161202203366605
Validation loss: 2.4135951706119614

Epoch: 5| Step: 9
Training loss: 2.3786212017492683
Validation loss: 2.457312367918264

Epoch: 5| Step: 10
Training loss: 1.7268439706450744
Validation loss: 2.4550817254664534

Epoch: 470| Step: 0
Training loss: 1.8202094183946924
Validation loss: 2.39677796077083

Epoch: 5| Step: 1
Training loss: 1.9593286487505606
Validation loss: 2.5056936821350906

Epoch: 5| Step: 2
Training loss: 2.273847090801444
Validation loss: 2.406104478829247

Epoch: 5| Step: 3
Training loss: 2.72243626935924
Validation loss: 2.4380579915294365

Epoch: 5| Step: 4
Training loss: 1.951069536093222
Validation loss: 2.409712442994467

Epoch: 5| Step: 5
Training loss: 1.7110382468465253
Validation loss: 2.4124188427942936

Epoch: 5| Step: 6
Training loss: 2.238519519420026
Validation loss: 2.433282551303678

Epoch: 5| Step: 7
Training loss: 1.957801043087986
Validation loss: 2.420836599654632

Epoch: 5| Step: 8
Training loss: 1.8481995507621074
Validation loss: 2.454144025504886

Epoch: 5| Step: 9
Training loss: 1.9816688648379588
Validation loss: 2.450284260031816

Epoch: 5| Step: 10
Training loss: 2.20504752611769
Validation loss: 2.439776923974695

Epoch: 471| Step: 0
Training loss: 1.9587083281640827
Validation loss: 2.4241587149721173

Epoch: 5| Step: 1
Training loss: 1.807162944449431
Validation loss: 2.4302048330411594

Epoch: 5| Step: 2
Training loss: 1.902511557946205
Validation loss: 2.429743994267162

Epoch: 5| Step: 3
Training loss: 2.5623134452088023
Validation loss: 2.4141799601363596

Epoch: 5| Step: 4
Training loss: 1.8435796885306168
Validation loss: 2.421569692240782

Epoch: 5| Step: 5
Training loss: 1.6719202053340196
Validation loss: 2.4440203463409995

Epoch: 5| Step: 6
Training loss: 2.2903691751341184
Validation loss: 2.4347938174481003

Epoch: 5| Step: 7
Training loss: 2.111239704041477
Validation loss: 2.4678261729986186

Epoch: 5| Step: 8
Training loss: 2.4265460837276214
Validation loss: 2.42056664605147

Epoch: 5| Step: 9
Training loss: 2.227573098504219
Validation loss: 2.408349344335366

Epoch: 5| Step: 10
Training loss: 1.802673933972903
Validation loss: 2.4346822855030066

Epoch: 472| Step: 0
Training loss: 1.6919955665178585
Validation loss: 2.4827711568116015

Epoch: 5| Step: 1
Training loss: 1.5617187835861286
Validation loss: 2.4130763037197327

Epoch: 5| Step: 2
Training loss: 2.0261175015598067
Validation loss: 2.424614578976576

Epoch: 5| Step: 3
Training loss: 2.090851442763638
Validation loss: 2.459493946755111

Epoch: 5| Step: 4
Training loss: 2.3682917169202584
Validation loss: 2.461456367053379

Epoch: 5| Step: 5
Training loss: 1.5601883382080965
Validation loss: 2.4923161437993624

Epoch: 5| Step: 6
Training loss: 2.788427114906625
Validation loss: 2.4112318151330263

Epoch: 5| Step: 7
Training loss: 1.886339990732961
Validation loss: 2.4506523650879726

Epoch: 5| Step: 8
Training loss: 1.3561385201198544
Validation loss: 2.417356488816916

Epoch: 5| Step: 9
Training loss: 2.675572653108902
Validation loss: 2.382223240455935

Epoch: 5| Step: 10
Training loss: 2.407185162232735
Validation loss: 2.435458271336481

Epoch: 473| Step: 0
Training loss: 1.4714825959545508
Validation loss: 2.444478015476471

Epoch: 5| Step: 1
Training loss: 2.3144209847451926
Validation loss: 2.4261439948877257

Epoch: 5| Step: 2
Training loss: 2.8684782610078314
Validation loss: 2.3922202181831005

Epoch: 5| Step: 3
Training loss: 2.4049894325515893
Validation loss: 2.448843730483246

Epoch: 5| Step: 4
Training loss: 1.9667291108686855
Validation loss: 2.430559749648484

Epoch: 5| Step: 5
Training loss: 2.025730792387058
Validation loss: 2.478192046231614

Epoch: 5| Step: 6
Training loss: 1.6980704801510162
Validation loss: 2.422669881576172

Epoch: 5| Step: 7
Training loss: 1.6444693438188533
Validation loss: 2.435518850065026

Epoch: 5| Step: 8
Training loss: 2.1151800456867464
Validation loss: 2.4387661191073606

Epoch: 5| Step: 9
Training loss: 1.9845800218775442
Validation loss: 2.4278263417848436

Epoch: 5| Step: 10
Training loss: 1.8130586355188483
Validation loss: 2.4780341826265273

Epoch: 474| Step: 0
Training loss: 1.8013751525603545
Validation loss: 2.4251115885970407

Epoch: 5| Step: 1
Training loss: 2.2366064706349404
Validation loss: 2.4777722517089544

Epoch: 5| Step: 2
Training loss: 2.719399297943664
Validation loss: 2.444217566878043

Epoch: 5| Step: 3
Training loss: 1.5373324047018546
Validation loss: 2.4567800880905533

Epoch: 5| Step: 4
Training loss: 1.8115475223718995
Validation loss: 2.4200118165849327

Epoch: 5| Step: 5
Training loss: 2.1601325201072847
Validation loss: 2.4029855989380184

Epoch: 5| Step: 6
Training loss: 2.0620249576852707
Validation loss: 2.4157353566726907

Epoch: 5| Step: 7
Training loss: 2.391879083147992
Validation loss: 2.4120152518564417

Epoch: 5| Step: 8
Training loss: 1.9040150908304876
Validation loss: 2.412114373442252

Epoch: 5| Step: 9
Training loss: 1.8574870807100874
Validation loss: 2.4230640743389293

Epoch: 5| Step: 10
Training loss: 2.2578623413075745
Validation loss: 2.3972638739177623

Epoch: 475| Step: 0
Training loss: 2.1355309680648427
Validation loss: 2.4195201793012644

Epoch: 5| Step: 1
Training loss: 1.4926143654819113
Validation loss: 2.4074091043309425

Epoch: 5| Step: 2
Training loss: 1.61967755699749
Validation loss: 2.372455197480104

Epoch: 5| Step: 3
Training loss: 2.2383164008610956
Validation loss: 2.373714447751435

Epoch: 5| Step: 4
Training loss: 2.202707562230132
Validation loss: 2.4459273473403713

Epoch: 5| Step: 5
Training loss: 1.735892928930719
Validation loss: 2.446754017834447

Epoch: 5| Step: 6
Training loss: 2.2272191200457825
Validation loss: 2.415557246706727

Epoch: 5| Step: 7
Training loss: 2.100644389695512
Validation loss: 2.39676444501971

Epoch: 5| Step: 8
Training loss: 2.653288772614577
Validation loss: 2.4081106589737793

Epoch: 5| Step: 9
Training loss: 1.74337221927685
Validation loss: 2.4055887279714523

Epoch: 5| Step: 10
Training loss: 2.577624555625191
Validation loss: 2.4166327980845024

Epoch: 476| Step: 0
Training loss: 1.8532437445693102
Validation loss: 2.394685339834839

Epoch: 5| Step: 1
Training loss: 2.1282274756497133
Validation loss: 2.4312637840331766

Epoch: 5| Step: 2
Training loss: 2.170193137614051
Validation loss: 2.403384052782128

Epoch: 5| Step: 3
Training loss: 1.7072800513899005
Validation loss: 2.4901508994350245

Epoch: 5| Step: 4
Training loss: 2.150553933576632
Validation loss: 2.455023240727784

Epoch: 5| Step: 5
Training loss: 1.7612562566338785
Validation loss: 2.462983922869789

Epoch: 5| Step: 6
Training loss: 1.6608504695107187
Validation loss: 2.411763819682748

Epoch: 5| Step: 7
Training loss: 2.0804511796851823
Validation loss: 2.4378993083555254

Epoch: 5| Step: 8
Training loss: 2.774427210485609
Validation loss: 2.3780359122544894

Epoch: 5| Step: 9
Training loss: 2.110799350919293
Validation loss: 2.4151205379333667

Epoch: 5| Step: 10
Training loss: 2.038899966956777
Validation loss: 2.4243914066422323

Epoch: 477| Step: 0
Training loss: 1.9795868544673363
Validation loss: 2.3842385959983186

Epoch: 5| Step: 1
Training loss: 2.01279208983882
Validation loss: 2.419679991499283

Epoch: 5| Step: 2
Training loss: 2.2122218846239368
Validation loss: 2.3926842662245478

Epoch: 5| Step: 3
Training loss: 1.778589905901651
Validation loss: 2.4696498013529444

Epoch: 5| Step: 4
Training loss: 1.9671872915399637
Validation loss: 2.4144786021190043

Epoch: 5| Step: 5
Training loss: 2.352860710226705
Validation loss: 2.3947617531742273

Epoch: 5| Step: 6
Training loss: 1.8864787642160552
Validation loss: 2.4367410242257237

Epoch: 5| Step: 7
Training loss: 2.2442490939095676
Validation loss: 2.429981390254166

Epoch: 5| Step: 8
Training loss: 1.8524017363521812
Validation loss: 2.4249981973740957

Epoch: 5| Step: 9
Training loss: 1.8248674109383627
Validation loss: 2.399388828914767

Epoch: 5| Step: 10
Training loss: 2.4792164923098117
Validation loss: 2.488479434050147

Epoch: 478| Step: 0
Training loss: 1.797623320953092
Validation loss: 2.416032695959479

Epoch: 5| Step: 1
Training loss: 1.9806933153431883
Validation loss: 2.417728471573807

Epoch: 5| Step: 2
Training loss: 2.2574723459368617
Validation loss: 2.447862485671322

Epoch: 5| Step: 3
Training loss: 2.1240775406184143
Validation loss: 2.411015958325966

Epoch: 5| Step: 4
Training loss: 1.8661675640277908
Validation loss: 2.4336784428512095

Epoch: 5| Step: 5
Training loss: 1.854638807497963
Validation loss: 2.37549213994706

Epoch: 5| Step: 6
Training loss: 1.4111163814868442
Validation loss: 2.4389358258794047

Epoch: 5| Step: 7
Training loss: 2.9330826681135806
Validation loss: 2.433504638030581

Epoch: 5| Step: 8
Training loss: 1.9238983296398846
Validation loss: 2.4778787651318113

Epoch: 5| Step: 9
Training loss: 2.3346209946694176
Validation loss: 2.419792984081391

Epoch: 5| Step: 10
Training loss: 2.093869333994315
Validation loss: 2.4399214418627904

Epoch: 479| Step: 0
Training loss: 1.9253885050518356
Validation loss: 2.4363689327834286

Epoch: 5| Step: 1
Training loss: 1.8760964684440087
Validation loss: 2.4494386387721776

Epoch: 5| Step: 2
Training loss: 2.089830287996945
Validation loss: 2.438894612793669

Epoch: 5| Step: 3
Training loss: 2.032548460060098
Validation loss: 2.4651422958971665

Epoch: 5| Step: 4
Training loss: 0.9836989609348603
Validation loss: 2.4109742934990313

Epoch: 5| Step: 5
Training loss: 1.904354466276575
Validation loss: 2.4684286940411546

Epoch: 5| Step: 6
Training loss: 2.834901562685588
Validation loss: 2.396187468719424

Epoch: 5| Step: 7
Training loss: 2.0935885239009284
Validation loss: 2.4091311251267573

Epoch: 5| Step: 8
Training loss: 2.2431736282855463
Validation loss: 2.417767527164196

Epoch: 5| Step: 9
Training loss: 1.7396025361544993
Validation loss: 2.4791859772253186

Epoch: 5| Step: 10
Training loss: 2.354490871104723
Validation loss: 2.3982132054614897

Epoch: 480| Step: 0
Training loss: 2.128602844456309
Validation loss: 2.497419900451101

Epoch: 5| Step: 1
Training loss: 1.966384495950916
Validation loss: 2.3949768211785716

Epoch: 5| Step: 2
Training loss: 1.703195622930999
Validation loss: 2.4388798714412805

Epoch: 5| Step: 3
Training loss: 2.459710484347514
Validation loss: 2.3723337344018307

Epoch: 5| Step: 4
Training loss: 1.9979660897876026
Validation loss: 2.3909268066384657

Epoch: 5| Step: 5
Training loss: 2.066346021614599
Validation loss: 2.4026956098628163

Epoch: 5| Step: 6
Training loss: 2.177028332856637
Validation loss: 2.446890721034669

Epoch: 5| Step: 7
Training loss: 1.962473537485334
Validation loss: 2.452439714530228

Epoch: 5| Step: 8
Training loss: 1.8213682592114209
Validation loss: 2.4383709941874785

Epoch: 5| Step: 9
Training loss: 1.7980368133866729
Validation loss: 2.451155392863836

Epoch: 5| Step: 10
Training loss: 2.6855127838745276
Validation loss: 2.428778177603639

Epoch: 481| Step: 0
Training loss: 2.06939907436717
Validation loss: 2.4414276586762953

Epoch: 5| Step: 1
Training loss: 1.7241584992004042
Validation loss: 2.412499356289675

Epoch: 5| Step: 2
Training loss: 2.106710602064757
Validation loss: 2.42732581548211

Epoch: 5| Step: 3
Training loss: 1.9516587513872539
Validation loss: 2.4566009972704435

Epoch: 5| Step: 4
Training loss: 2.5730598399575633
Validation loss: 2.430790995393173

Epoch: 5| Step: 5
Training loss: 1.7805464844349452
Validation loss: 2.4784799346633584

Epoch: 5| Step: 6
Training loss: 1.7993710637687197
Validation loss: 2.4422810050821213

Epoch: 5| Step: 7
Training loss: 1.7932366065111884
Validation loss: 2.435052285863692

Epoch: 5| Step: 8
Training loss: 1.462969341121099
Validation loss: 2.414491591878011

Epoch: 5| Step: 9
Training loss: 2.108507444206197
Validation loss: 2.372657419353444

Epoch: 5| Step: 10
Training loss: 3.0268139972482597
Validation loss: 2.409400286219344

Epoch: 482| Step: 0
Training loss: 1.9085523116008487
Validation loss: 2.468136280309199

Epoch: 5| Step: 1
Training loss: 1.992613025173433
Validation loss: 2.386552649711308

Epoch: 5| Step: 2
Training loss: 1.5707314155495558
Validation loss: 2.4593255673935976

Epoch: 5| Step: 3
Training loss: 1.8200414889710421
Validation loss: 2.4289153394478

Epoch: 5| Step: 4
Training loss: 2.063813918086795
Validation loss: 2.4564967406247953

Epoch: 5| Step: 5
Training loss: 2.035188231701985
Validation loss: 2.4153126897141495

Epoch: 5| Step: 6
Training loss: 1.739606168067893
Validation loss: 2.4393913980908297

Epoch: 5| Step: 7
Training loss: 2.620331927814923
Validation loss: 2.425406011124751

Epoch: 5| Step: 8
Training loss: 2.168201001073168
Validation loss: 2.420973853515874

Epoch: 5| Step: 9
Training loss: 2.7004477412240875
Validation loss: 2.433947955461212

Epoch: 5| Step: 10
Training loss: 1.560163658556208
Validation loss: 2.4193389751228715

Epoch: 483| Step: 0
Training loss: 2.1202677599038626
Validation loss: 2.4095926026416654

Epoch: 5| Step: 1
Training loss: 2.014883688700591
Validation loss: 2.414890454098436

Epoch: 5| Step: 2
Training loss: 1.9173389651680488
Validation loss: 2.442438846231115

Epoch: 5| Step: 3
Training loss: 2.09581563599574
Validation loss: 2.373807819452423

Epoch: 5| Step: 4
Training loss: 1.361289463014146
Validation loss: 2.4279332502585858

Epoch: 5| Step: 5
Training loss: 1.7361884044819398
Validation loss: 2.4660123436001786

Epoch: 5| Step: 6
Training loss: 1.7108405098793227
Validation loss: 2.414726493357311

Epoch: 5| Step: 7
Training loss: 2.4794349738931256
Validation loss: 2.4314252937032648

Epoch: 5| Step: 8
Training loss: 2.25147622741985
Validation loss: 2.4203055148017043

Epoch: 5| Step: 9
Training loss: 2.2024465828522115
Validation loss: 2.4410029866884377

Epoch: 5| Step: 10
Training loss: 2.218679292653605
Validation loss: 2.4727683541596317

Epoch: 484| Step: 0
Training loss: 1.848096089503026
Validation loss: 2.403778479583496

Epoch: 5| Step: 1
Training loss: 1.699559390245507
Validation loss: 2.39269101152609

Epoch: 5| Step: 2
Training loss: 2.134202888218131
Validation loss: 2.3842185501214646

Epoch: 5| Step: 3
Training loss: 1.7967256566887007
Validation loss: 2.4267523297485445

Epoch: 5| Step: 4
Training loss: 2.3281048255404726
Validation loss: 2.449593743500024

Epoch: 5| Step: 5
Training loss: 1.9062897099595668
Validation loss: 2.4049645687305183

Epoch: 5| Step: 6
Training loss: 2.207113780951365
Validation loss: 2.435722439151002

Epoch: 5| Step: 7
Training loss: 2.271329656368258
Validation loss: 2.4320898783688323

Epoch: 5| Step: 8
Training loss: 1.9774703402410876
Validation loss: 2.3412661652852016

Epoch: 5| Step: 9
Training loss: 1.8863298161369322
Validation loss: 2.4098974272982843

Epoch: 5| Step: 10
Training loss: 1.8395104237930167
Validation loss: 2.4355776505322235

Epoch: 485| Step: 0
Training loss: 2.402213438888256
Validation loss: 2.388482964904608

Epoch: 5| Step: 1
Training loss: 1.9743804346138405
Validation loss: 2.411613195909274

Epoch: 5| Step: 2
Training loss: 2.371534730731334
Validation loss: 2.4229727806478185

Epoch: 5| Step: 3
Training loss: 2.392111222778392
Validation loss: 2.4362947783194326

Epoch: 5| Step: 4
Training loss: 1.8114656588241924
Validation loss: 2.378750854591702

Epoch: 5| Step: 5
Training loss: 1.5483173620155128
Validation loss: 2.4529168274665634

Epoch: 5| Step: 6
Training loss: 1.9837973647541518
Validation loss: 2.4431889870514225

Epoch: 5| Step: 7
Training loss: 1.7841657231895711
Validation loss: 2.3941575551536287

Epoch: 5| Step: 8
Training loss: 1.9635823563492354
Validation loss: 2.458558676228902

Epoch: 5| Step: 9
Training loss: 1.9712787917797558
Validation loss: 2.4262181468326887

Epoch: 5| Step: 10
Training loss: 2.1537643701358444
Validation loss: 2.441451286966991

Epoch: 486| Step: 0
Training loss: 1.9383947705902458
Validation loss: 2.4433122768075917

Epoch: 5| Step: 1
Training loss: 1.860500604060956
Validation loss: 2.4136254060624327

Epoch: 5| Step: 2
Training loss: 1.739311136052669
Validation loss: 2.439196414619874

Epoch: 5| Step: 3
Training loss: 2.1139918911145905
Validation loss: 2.460906381144988

Epoch: 5| Step: 4
Training loss: 2.0291574116312363
Validation loss: 2.400224232923124

Epoch: 5| Step: 5
Training loss: 1.7901554683747858
Validation loss: 2.456223378239019

Epoch: 5| Step: 6
Training loss: 2.1327943905910436
Validation loss: 2.4623471781648556

Epoch: 5| Step: 7
Training loss: 2.1222573701279153
Validation loss: 2.406156248565716

Epoch: 5| Step: 8
Training loss: 2.5712085773578948
Validation loss: 2.421767100015558

Epoch: 5| Step: 9
Training loss: 2.5550886766606293
Validation loss: 2.4488083310945763

Epoch: 5| Step: 10
Training loss: 1.279101314594165
Validation loss: 2.42172567414639

Epoch: 487| Step: 0
Training loss: 2.203285698407025
Validation loss: 2.410084107856757

Epoch: 5| Step: 1
Training loss: 1.7362599482845211
Validation loss: 2.4196218287222733

Epoch: 5| Step: 2
Training loss: 1.6637588008931579
Validation loss: 2.4098300476086996

Epoch: 5| Step: 3
Training loss: 1.9801887750704883
Validation loss: 2.408527543468301

Epoch: 5| Step: 4
Training loss: 1.749517714937045
Validation loss: 2.406867066555026

Epoch: 5| Step: 5
Training loss: 1.9264195991726247
Validation loss: 2.4520961677673383

Epoch: 5| Step: 6
Training loss: 2.579689984288628
Validation loss: 2.4198687184718226

Epoch: 5| Step: 7
Training loss: 1.9204829687402687
Validation loss: 2.4195330927239893

Epoch: 5| Step: 8
Training loss: 2.233164319242877
Validation loss: 2.383067049429738

Epoch: 5| Step: 9
Training loss: 2.1296804157903315
Validation loss: 2.443675482697778

Epoch: 5| Step: 10
Training loss: 1.700938369719917
Validation loss: 2.3767380672024427

Epoch: 488| Step: 0
Training loss: 1.9898546509880353
Validation loss: 2.3915904765259324

Epoch: 5| Step: 1
Training loss: 1.998697452772917
Validation loss: 2.4249997371448897

Epoch: 5| Step: 2
Training loss: 1.894584733660334
Validation loss: 2.3392779765177414

Epoch: 5| Step: 3
Training loss: 2.3050460277934977
Validation loss: 2.4255836304203373

Epoch: 5| Step: 4
Training loss: 1.5362571282838302
Validation loss: 2.445147490461786

Epoch: 5| Step: 5
Training loss: 2.3491349474590195
Validation loss: 2.4422197831807373

Epoch: 5| Step: 6
Training loss: 2.2391542127078594
Validation loss: 2.410909009995923

Epoch: 5| Step: 7
Training loss: 2.067063802073081
Validation loss: 2.4087378525970045

Epoch: 5| Step: 8
Training loss: 2.045733533964305
Validation loss: 2.435962017729232

Epoch: 5| Step: 9
Training loss: 2.202951627362153
Validation loss: 2.41951457101342

Epoch: 5| Step: 10
Training loss: 1.5771980206989171
Validation loss: 2.3843437601669994

Epoch: 489| Step: 0
Training loss: 1.5726064750878654
Validation loss: 2.3848576261190724

Epoch: 5| Step: 1
Training loss: 1.9004821441662638
Validation loss: 2.4391048944132137

Epoch: 5| Step: 2
Training loss: 2.086997687842528
Validation loss: 2.4457307298212796

Epoch: 5| Step: 3
Training loss: 2.1617368465061793
Validation loss: 2.396366150734073

Epoch: 5| Step: 4
Training loss: 1.4682851826930845
Validation loss: 2.4097531205676237

Epoch: 5| Step: 5
Training loss: 2.488594071443371
Validation loss: 2.368888122538628

Epoch: 5| Step: 6
Training loss: 2.1671597457681746
Validation loss: 2.3951429163202937

Epoch: 5| Step: 7
Training loss: 2.5464655073051783
Validation loss: 2.435348836231283

Epoch: 5| Step: 8
Training loss: 2.096523099612368
Validation loss: 2.4110786892002554

Epoch: 5| Step: 9
Training loss: 1.6861591663596607
Validation loss: 2.4087841857660224

Epoch: 5| Step: 10
Training loss: 1.7063885838607362
Validation loss: 2.4654696254174278

Epoch: 490| Step: 0
Training loss: 1.9908190047316783
Validation loss: 2.396365093768089

Epoch: 5| Step: 1
Training loss: 2.1301053371261753
Validation loss: 2.4036891176409667

Epoch: 5| Step: 2
Training loss: 2.048453152129012
Validation loss: 2.421342251944733

Epoch: 5| Step: 3
Training loss: 2.243273003755598
Validation loss: 2.4370577999802436

Epoch: 5| Step: 4
Training loss: 2.018844871535683
Validation loss: 2.394820799659717

Epoch: 5| Step: 5
Training loss: 1.8148061616234272
Validation loss: 2.426576596249178

Epoch: 5| Step: 6
Training loss: 2.3088839327592052
Validation loss: 2.394897050249004

Epoch: 5| Step: 7
Training loss: 2.3512339821918347
Validation loss: 2.425806706809679

Epoch: 5| Step: 8
Training loss: 1.5462360025572577
Validation loss: 2.404738977902001

Epoch: 5| Step: 9
Training loss: 1.9606900609962061
Validation loss: 2.473601279191274

Epoch: 5| Step: 10
Training loss: 2.2363524323187716
Validation loss: 2.4516135296648085

Epoch: 491| Step: 0
Training loss: 2.1560504102245486
Validation loss: 2.4693576100843595

Epoch: 5| Step: 1
Training loss: 2.3407005308057993
Validation loss: 2.4392263421889817

Epoch: 5| Step: 2
Training loss: 2.2976452839838406
Validation loss: 2.4175447736412976

Epoch: 5| Step: 3
Training loss: 2.293974829524869
Validation loss: 2.415063885230807

Epoch: 5| Step: 4
Training loss: 1.3417026092422042
Validation loss: 2.405619568122942

Epoch: 5| Step: 5
Training loss: 2.005422869678729
Validation loss: 2.406374222961291

Epoch: 5| Step: 6
Training loss: 1.9046920914003955
Validation loss: 2.393919559114182

Epoch: 5| Step: 7
Training loss: 2.0416007777371017
Validation loss: 2.4277084130477764

Epoch: 5| Step: 8
Training loss: 1.6254136952745701
Validation loss: 2.391326894554131

Epoch: 5| Step: 9
Training loss: 1.417412262476965
Validation loss: 2.417909148621427

Epoch: 5| Step: 10
Training loss: 2.421602098409741
Validation loss: 2.422640212972473

Epoch: 492| Step: 0
Training loss: 1.9044902996718542
Validation loss: 2.4426129746229144

Epoch: 5| Step: 1
Training loss: 2.124292592501016
Validation loss: 2.408421275523133

Epoch: 5| Step: 2
Training loss: 1.9938073846576752
Validation loss: 2.419856902808603

Epoch: 5| Step: 3
Training loss: 2.107252168937361
Validation loss: 2.4664241654800865

Epoch: 5| Step: 4
Training loss: 2.1381465179511348
Validation loss: 2.410996609322624

Epoch: 5| Step: 5
Training loss: 1.862051568035296
Validation loss: 2.413670153642861

Epoch: 5| Step: 6
Training loss: 2.3785270802157332
Validation loss: 2.4244676857590304

Epoch: 5| Step: 7
Training loss: 2.0136238040606753
Validation loss: 2.4301985668841675

Epoch: 5| Step: 8
Training loss: 2.063166077305151
Validation loss: 2.422001752073701

Epoch: 5| Step: 9
Training loss: 1.217384404720426
Validation loss: 2.4517132357255838

Epoch: 5| Step: 10
Training loss: 1.8664956826283483
Validation loss: 2.391784368612021

Epoch: 493| Step: 0
Training loss: 2.0328402823001825
Validation loss: 2.4087983503363075

Epoch: 5| Step: 1
Training loss: 2.431080703921005
Validation loss: 2.3693325536904255

Epoch: 5| Step: 2
Training loss: 2.1588973227782446
Validation loss: 2.3883185018912116

Epoch: 5| Step: 3
Training loss: 1.792206778917326
Validation loss: 2.458435170848862

Epoch: 5| Step: 4
Training loss: 1.868798937204627
Validation loss: 2.3883237368972074

Epoch: 5| Step: 5
Training loss: 1.9240746664480723
Validation loss: 2.4053257335184166

Epoch: 5| Step: 6
Training loss: 2.1553139244186608
Validation loss: 2.421707542364721

Epoch: 5| Step: 7
Training loss: 1.731961263258676
Validation loss: 2.3979254691741234

Epoch: 5| Step: 8
Training loss: 2.864277290410673
Validation loss: 2.457596860608387

Epoch: 5| Step: 9
Training loss: 1.5206860858580946
Validation loss: 2.4058732029549703

Epoch: 5| Step: 10
Training loss: 1.8810126855149694
Validation loss: 2.4226479759235997

Epoch: 494| Step: 0
Training loss: 2.0200607815653124
Validation loss: 2.4178049831656336

Epoch: 5| Step: 1
Training loss: 1.8727623621000828
Validation loss: 2.441013423957997

Epoch: 5| Step: 2
Training loss: 1.721889056859733
Validation loss: 2.4392491688736033

Epoch: 5| Step: 3
Training loss: 2.268387261599131
Validation loss: 2.3994994521293838

Epoch: 5| Step: 4
Training loss: 2.0873482609039113
Validation loss: 2.4637008827484435

Epoch: 5| Step: 5
Training loss: 2.2639914049315655
Validation loss: 2.403730773296005

Epoch: 5| Step: 6
Training loss: 1.8709789392212628
Validation loss: 2.4428698485920393

Epoch: 5| Step: 7
Training loss: 1.9541578079814519
Validation loss: 2.3766703665858904

Epoch: 5| Step: 8
Training loss: 2.388436306607873
Validation loss: 2.4328841833125465

Epoch: 5| Step: 9
Training loss: 1.6437328874400716
Validation loss: 2.424031699182182

Epoch: 5| Step: 10
Training loss: 1.9399924090325409
Validation loss: 2.3815378180457363

Epoch: 495| Step: 0
Training loss: 2.062403127533855
Validation loss: 2.3988711575769712

Epoch: 5| Step: 1
Training loss: 2.411070219133615
Validation loss: 2.3693632901385677

Epoch: 5| Step: 2
Training loss: 1.490013416153077
Validation loss: 2.4380196016744056

Epoch: 5| Step: 3
Training loss: 1.9243799028275301
Validation loss: 2.4114797966795103

Epoch: 5| Step: 4
Training loss: 2.5000884994101336
Validation loss: 2.370449752511679

Epoch: 5| Step: 5
Training loss: 2.125656811802244
Validation loss: 2.4073655315817217

Epoch: 5| Step: 6
Training loss: 1.5469952160909866
Validation loss: 2.4321973483342814

Epoch: 5| Step: 7
Training loss: 2.05893040027048
Validation loss: 2.407203715442559

Epoch: 5| Step: 8
Training loss: 1.9374013691070058
Validation loss: 2.374155830393672

Epoch: 5| Step: 9
Training loss: 2.2875011965222174
Validation loss: 2.4029357324842504

Epoch: 5| Step: 10
Training loss: 1.5390875180142616
Validation loss: 2.37502787713452

Epoch: 496| Step: 0
Training loss: 2.1847030196895694
Validation loss: 2.4380989837488674

Epoch: 5| Step: 1
Training loss: 1.6199675911145868
Validation loss: 2.382484073151611

Epoch: 5| Step: 2
Training loss: 1.6295514555605723
Validation loss: 2.4165849726669726

Epoch: 5| Step: 3
Training loss: 1.5549155216636363
Validation loss: 2.4274957897817675

Epoch: 5| Step: 4
Training loss: 2.556303394011816
Validation loss: 2.4390369278537762

Epoch: 5| Step: 5
Training loss: 1.5822731869162348
Validation loss: 2.4831606166537368

Epoch: 5| Step: 6
Training loss: 2.1782065114951727
Validation loss: 2.4481431830511338

Epoch: 5| Step: 7
Training loss: 2.2969333355820125
Validation loss: 2.418301950559513

Epoch: 5| Step: 8
Training loss: 2.0424120055052395
Validation loss: 2.4402865501922206

Epoch: 5| Step: 9
Training loss: 2.0751127051572946
Validation loss: 2.438762164474805

Epoch: 5| Step: 10
Training loss: 2.143177487633667
Validation loss: 2.436342393085185

Epoch: 497| Step: 0
Training loss: 2.1519789481781877
Validation loss: 2.405171040977885

Epoch: 5| Step: 1
Training loss: 2.085310366649614
Validation loss: 2.4658735825616365

Epoch: 5| Step: 2
Training loss: 1.965733593971849
Validation loss: 2.4183147575873947

Epoch: 5| Step: 3
Training loss: 1.7395131749912152
Validation loss: 2.399485295702471

Epoch: 5| Step: 4
Training loss: 1.9298445294075701
Validation loss: 2.3783728484152507

Epoch: 5| Step: 5
Training loss: 1.9232494122836397
Validation loss: 2.401226496151749

Epoch: 5| Step: 6
Training loss: 1.573108972915648
Validation loss: 2.360545919569844

Epoch: 5| Step: 7
Training loss: 2.5655197400746395
Validation loss: 2.484934456897357

Epoch: 5| Step: 8
Training loss: 2.1305878043963435
Validation loss: 2.4535073919644477

Epoch: 5| Step: 9
Training loss: 1.9291884668929697
Validation loss: 2.432727599076077

Epoch: 5| Step: 10
Training loss: 2.329292567836241
Validation loss: 2.4410055702791422

Epoch: 498| Step: 0
Training loss: 1.9133620245484786
Validation loss: 2.4071241798353196

Epoch: 5| Step: 1
Training loss: 2.014767249889681
Validation loss: 2.427165072351276

Epoch: 5| Step: 2
Training loss: 1.6825577453634524
Validation loss: 2.4298210538622493

Epoch: 5| Step: 3
Training loss: 1.9969474624118324
Validation loss: 2.4541966276761547

Epoch: 5| Step: 4
Training loss: 1.9775680577542525
Validation loss: 2.3945222535366093

Epoch: 5| Step: 5
Training loss: 2.2347210169345644
Validation loss: 2.420392743861561

Epoch: 5| Step: 6
Training loss: 1.608216841173367
Validation loss: 2.432939565435483

Epoch: 5| Step: 7
Training loss: 2.2811227005229924
Validation loss: 2.4414495921891985

Epoch: 5| Step: 8
Training loss: 1.7409539067774544
Validation loss: 2.4347440428265985

Epoch: 5| Step: 9
Training loss: 1.9510078246655311
Validation loss: 2.4334988270457125

Epoch: 5| Step: 10
Training loss: 2.7331467840066743
Validation loss: 2.3950346629810304

Epoch: 499| Step: 0
Training loss: 1.7685306911852172
Validation loss: 2.4077633553652475

Epoch: 5| Step: 1
Training loss: 1.666380508016569
Validation loss: 2.4272053623765952

Epoch: 5| Step: 2
Training loss: 1.73987175688108
Validation loss: 2.383825337077341

Epoch: 5| Step: 3
Training loss: 2.424080515100306
Validation loss: 2.4734323899551076

Epoch: 5| Step: 4
Training loss: 1.991245780217165
Validation loss: 2.4381307874825384

Epoch: 5| Step: 5
Training loss: 2.0713034507471146
Validation loss: 2.4427026505936253

Epoch: 5| Step: 6
Training loss: 2.0261857505368828
Validation loss: 2.4549733186210947

Epoch: 5| Step: 7
Training loss: 2.606933182199935
Validation loss: 2.3874638441359317

Epoch: 5| Step: 8
Training loss: 1.7510012078665027
Validation loss: 2.4256256583617444

Epoch: 5| Step: 9
Training loss: 2.011011091781219
Validation loss: 2.406788711748985

Epoch: 5| Step: 10
Training loss: 1.8582403102680116
Validation loss: 2.390279944590688

Epoch: 500| Step: 0
Training loss: 2.4005111030905044
Validation loss: 2.420726349979155

Epoch: 5| Step: 1
Training loss: 2.370035705297447
Validation loss: 2.4315864362896136

Epoch: 5| Step: 2
Training loss: 1.9295172983082647
Validation loss: 2.4330322779861

Epoch: 5| Step: 3
Training loss: 2.216468497670511
Validation loss: 2.438491837733939

Epoch: 5| Step: 4
Training loss: 1.6251052675630002
Validation loss: 2.3855574611481316

Epoch: 5| Step: 5
Training loss: 1.5046614695166154
Validation loss: 2.4393493372267336

Epoch: 5| Step: 6
Training loss: 2.4127543340044637
Validation loss: 2.3943490770733376

Epoch: 5| Step: 7
Training loss: 2.0605211880063607
Validation loss: 2.400929408569696

Epoch: 5| Step: 8
Training loss: 1.8787487067080455
Validation loss: 2.377558551958134

Epoch: 5| Step: 9
Training loss: 1.71641492880501
Validation loss: 2.407320779160852

Epoch: 5| Step: 10
Training loss: 1.8841158838977206
Validation loss: 2.4093486085324156

Epoch: 501| Step: 0
Training loss: 1.9396175380139669
Validation loss: 2.4185242909034694

Epoch: 5| Step: 1
Training loss: 2.457627168083525
Validation loss: 2.451639022536892

Epoch: 5| Step: 2
Training loss: 1.9533263446023186
Validation loss: 2.378122606344041

Epoch: 5| Step: 3
Training loss: 1.2753636122175238
Validation loss: 2.3825604608584015

Epoch: 5| Step: 4
Training loss: 1.998702403176374
Validation loss: 2.43335626710487

Epoch: 5| Step: 5
Training loss: 1.3824700400225889
Validation loss: 2.415246611699508

Epoch: 5| Step: 6
Training loss: 1.2952263705673936
Validation loss: 2.3611241923176345

Epoch: 5| Step: 7
Training loss: 2.46227797073681
Validation loss: 2.4440006271619827

Epoch: 5| Step: 8
Training loss: 2.325331580195656
Validation loss: 2.3786459712595533

Epoch: 5| Step: 9
Training loss: 2.2278630256461596
Validation loss: 2.418154522271625

Epoch: 5| Step: 10
Training loss: 2.1005872995114134
Validation loss: 2.40226492963815

Epoch: 502| Step: 0
Training loss: 2.133182926637689
Validation loss: 2.3870448103298214

Epoch: 5| Step: 1
Training loss: 1.9659383163244286
Validation loss: 2.4207851353929524

Epoch: 5| Step: 2
Training loss: 1.8715918401614862
Validation loss: 2.4301225383351883

Epoch: 5| Step: 3
Training loss: 1.7825195740511828
Validation loss: 2.405638300718999

Epoch: 5| Step: 4
Training loss: 1.2427369827758263
Validation loss: 2.3536374372683144

Epoch: 5| Step: 5
Training loss: 2.2038226815288686
Validation loss: 2.4302498223080393

Epoch: 5| Step: 6
Training loss: 2.0301658671599108
Validation loss: 2.3824593801302223

Epoch: 5| Step: 7
Training loss: 1.4937507469781899
Validation loss: 2.4004540329704778

Epoch: 5| Step: 8
Training loss: 2.4191032053207575
Validation loss: 2.4184393991517634

Epoch: 5| Step: 9
Training loss: 2.611778751086873
Validation loss: 2.3750520904476127

Epoch: 5| Step: 10
Training loss: 1.811723674390778
Validation loss: 2.3986756740715984

Epoch: 503| Step: 0
Training loss: 1.7146672394787636
Validation loss: 2.3998924685045857

Epoch: 5| Step: 1
Training loss: 2.0830914802040454
Validation loss: 2.3829071418825993

Epoch: 5| Step: 2
Training loss: 2.3470551646396847
Validation loss: 2.399964835821979

Epoch: 5| Step: 3
Training loss: 2.1143072038832433
Validation loss: 2.411204670220174

Epoch: 5| Step: 4
Training loss: 1.2727091291763049
Validation loss: 2.418845286973503

Epoch: 5| Step: 5
Training loss: 1.9540747812734076
Validation loss: 2.372882903285596

Epoch: 5| Step: 6
Training loss: 1.6218571715211094
Validation loss: 2.3737104009421577

Epoch: 5| Step: 7
Training loss: 2.606094765556884
Validation loss: 2.391944408021864

Epoch: 5| Step: 8
Training loss: 1.8414391729545807
Validation loss: 2.424290779398914

Epoch: 5| Step: 9
Training loss: 1.5314108316465649
Validation loss: 2.4016017458556878

Epoch: 5| Step: 10
Training loss: 2.421251869023694
Validation loss: 2.3978828285026204

Epoch: 504| Step: 0
Training loss: 2.026764244796517
Validation loss: 2.3983496824606014

Epoch: 5| Step: 1
Training loss: 1.7396204215415245
Validation loss: 2.4605297734650624

Epoch: 5| Step: 2
Training loss: 1.7404669917657931
Validation loss: 2.4054605227400785

Epoch: 5| Step: 3
Training loss: 1.979854571270973
Validation loss: 2.3966032438101337

Epoch: 5| Step: 4
Training loss: 2.3617592190105965
Validation loss: 2.395291740619119

Epoch: 5| Step: 5
Training loss: 2.189401290783615
Validation loss: 2.415436573424537

Epoch: 5| Step: 6
Training loss: 2.0783362030319474
Validation loss: 2.43224838087272

Epoch: 5| Step: 7
Training loss: 1.5761574291905003
Validation loss: 2.4257774728833437

Epoch: 5| Step: 8
Training loss: 2.089799256608721
Validation loss: 2.426583461275945

Epoch: 5| Step: 9
Training loss: 2.233050507339424
Validation loss: 2.429914262573093

Epoch: 5| Step: 10
Training loss: 1.6361938946039176
Validation loss: 2.454838840865746

Epoch: 505| Step: 0
Training loss: 1.752927647326459
Validation loss: 2.4316541693532163

Epoch: 5| Step: 1
Training loss: 2.352543623030293
Validation loss: 2.4300255660445695

Epoch: 5| Step: 2
Training loss: 1.5769639985033532
Validation loss: 2.3838499266896314

Epoch: 5| Step: 3
Training loss: 1.8114961935704057
Validation loss: 2.4177701981435686

Epoch: 5| Step: 4
Training loss: 1.8781733679612802
Validation loss: 2.4167823834520803

Epoch: 5| Step: 5
Training loss: 2.092144592620081
Validation loss: 2.3959855700505206

Epoch: 5| Step: 6
Training loss: 2.14485652381493
Validation loss: 2.398673881738021

Epoch: 5| Step: 7
Training loss: 1.8556416320569349
Validation loss: 2.4423416229412003

Epoch: 5| Step: 8
Training loss: 2.288796887920121
Validation loss: 2.377871389188818

Epoch: 5| Step: 9
Training loss: 2.131374223506384
Validation loss: 2.4072111011159936

Epoch: 5| Step: 10
Training loss: 1.9797507043397202
Validation loss: 2.4541675961302896

Epoch: 506| Step: 0
Training loss: 2.3507068382648937
Validation loss: 2.4359512688746894

Epoch: 5| Step: 1
Training loss: 2.059631433570307
Validation loss: 2.3952635748794417

Epoch: 5| Step: 2
Training loss: 1.9807551250283129
Validation loss: 2.3596223015533253

Epoch: 5| Step: 3
Training loss: 1.8551833405079383
Validation loss: 2.476889427776704

Epoch: 5| Step: 4
Training loss: 1.8393894938741389
Validation loss: 2.3915250540829143

Epoch: 5| Step: 5
Training loss: 1.4098311707354951
Validation loss: 2.4212067009156804

Epoch: 5| Step: 6
Training loss: 2.2922141606003223
Validation loss: 2.3955816852796716

Epoch: 5| Step: 7
Training loss: 2.1569285776253815
Validation loss: 2.4250210591433508

Epoch: 5| Step: 8
Training loss: 2.01674993306334
Validation loss: 2.3653981908665576

Epoch: 5| Step: 9
Training loss: 2.01316967833764
Validation loss: 2.4194890850859077

Epoch: 5| Step: 10
Training loss: 1.8497675955912907
Validation loss: 2.3891404623118575

Epoch: 507| Step: 0
Training loss: 1.3168972016768206
Validation loss: 2.3889273655260173

Epoch: 5| Step: 1
Training loss: 1.9987959813882015
Validation loss: 2.3969187442436994

Epoch: 5| Step: 2
Training loss: 1.644691078954793
Validation loss: 2.4459977636401096

Epoch: 5| Step: 3
Training loss: 2.269599322612932
Validation loss: 2.4214605399536855

Epoch: 5| Step: 4
Training loss: 1.8444749085975782
Validation loss: 2.394939126971694

Epoch: 5| Step: 5
Training loss: 2.081521492511043
Validation loss: 2.420102270711689

Epoch: 5| Step: 6
Training loss: 2.236950650253582
Validation loss: 2.3742102706230774

Epoch: 5| Step: 7
Training loss: 1.660090546149271
Validation loss: 2.4060357441499844

Epoch: 5| Step: 8
Training loss: 1.8408886317519455
Validation loss: 2.430377582727975

Epoch: 5| Step: 9
Training loss: 1.8551652840858859
Validation loss: 2.4272156250276096

Epoch: 5| Step: 10
Training loss: 2.740180256337024
Validation loss: 2.401760087174875

Epoch: 508| Step: 0
Training loss: 1.6207861918807365
Validation loss: 2.4577824309197083

Epoch: 5| Step: 1
Training loss: 1.8020012088945168
Validation loss: 2.402994449562656

Epoch: 5| Step: 2
Training loss: 1.7690943837624007
Validation loss: 2.365859078694859

Epoch: 5| Step: 3
Training loss: 2.319613161885981
Validation loss: 2.3939136996955352

Epoch: 5| Step: 4
Training loss: 1.5042659180813163
Validation loss: 2.3735711668445774

Epoch: 5| Step: 5
Training loss: 2.452633559452599
Validation loss: 2.405011403107913

Epoch: 5| Step: 6
Training loss: 2.141364088342754
Validation loss: 2.3959781936430966

Epoch: 5| Step: 7
Training loss: 1.9559200581438616
Validation loss: 2.426174032742594

Epoch: 5| Step: 8
Training loss: 2.15750638322118
Validation loss: 2.371808322692

Epoch: 5| Step: 9
Training loss: 1.6000165223222234
Validation loss: 2.4251384076529545

Epoch: 5| Step: 10
Training loss: 2.229820785334708
Validation loss: 2.369790979002364

Epoch: 509| Step: 0
Training loss: 2.3484572886269675
Validation loss: 2.3902745830204086

Epoch: 5| Step: 1
Training loss: 2.4927141834854063
Validation loss: 2.4107397825020547

Epoch: 5| Step: 2
Training loss: 1.8150150344688334
Validation loss: 2.4203850605305255

Epoch: 5| Step: 3
Training loss: 1.874113063845741
Validation loss: 2.4324588059896186

Epoch: 5| Step: 4
Training loss: 1.4037173458795427
Validation loss: 2.408672733053692

Epoch: 5| Step: 5
Training loss: 1.9701674671929288
Validation loss: 2.4277419210263274

Epoch: 5| Step: 6
Training loss: 1.944430476849964
Validation loss: 2.4269400815812836

Epoch: 5| Step: 7
Training loss: 1.8696543466904874
Validation loss: 2.389237213050659

Epoch: 5| Step: 8
Training loss: 1.90135007124663
Validation loss: 2.4142606240891937

Epoch: 5| Step: 9
Training loss: 2.0667516639622483
Validation loss: 2.4464374612092405

Epoch: 5| Step: 10
Training loss: 2.245297922871522
Validation loss: 2.3691270468043713

Epoch: 510| Step: 0
Training loss: 2.6312416892022386
Validation loss: 2.3136884035270704

Epoch: 5| Step: 1
Training loss: 1.9859738854557374
Validation loss: 2.441694828689863

Epoch: 5| Step: 2
Training loss: 2.04039852176591
Validation loss: 2.431623793292231

Epoch: 5| Step: 3
Training loss: 2.3727719748181495
Validation loss: 2.34042800921457

Epoch: 5| Step: 4
Training loss: 1.6198588987996034
Validation loss: 2.42209444881077

Epoch: 5| Step: 5
Training loss: 2.1057397105402518
Validation loss: 2.377870457689346

Epoch: 5| Step: 6
Training loss: 1.8151625446434714
Validation loss: 2.405363791463889

Epoch: 5| Step: 7
Training loss: 1.2392644498751035
Validation loss: 2.3945689870281046

Epoch: 5| Step: 8
Training loss: 2.251876578307942
Validation loss: 2.371048319453785

Epoch: 5| Step: 9
Training loss: 1.2441499670806784
Validation loss: 2.4216017543469133

Epoch: 5| Step: 10
Training loss: 2.147658994248997
Validation loss: 2.4278865201551927

Epoch: 511| Step: 0
Training loss: 1.9613930222517968
Validation loss: 2.400471840428907

Epoch: 5| Step: 1
Training loss: 1.9683515963028788
Validation loss: 2.4350563949668174

Epoch: 5| Step: 2
Training loss: 2.055583337934002
Validation loss: 2.3909177312056835

Epoch: 5| Step: 3
Training loss: 2.146166013341869
Validation loss: 2.441589384302417

Epoch: 5| Step: 4
Training loss: 1.721397303176847
Validation loss: 2.376870395176114

Epoch: 5| Step: 5
Training loss: 1.8402281276634895
Validation loss: 2.4690900687551633

Epoch: 5| Step: 6
Training loss: 2.450276853528096
Validation loss: 2.441592912261036

Epoch: 5| Step: 7
Training loss: 2.222455650891199
Validation loss: 2.348939909791238

Epoch: 5| Step: 8
Training loss: 1.5258603905126606
Validation loss: 2.359071990952617

Epoch: 5| Step: 9
Training loss: 2.107386464129918
Validation loss: 2.4101720881779767

Epoch: 5| Step: 10
Training loss: 1.917792286883669
Validation loss: 2.4742450051051317

Epoch: 512| Step: 0
Training loss: 1.6678363113175967
Validation loss: 2.4289278308707862

Epoch: 5| Step: 1
Training loss: 1.9595108617864578
Validation loss: 2.3905075337140644

Epoch: 5| Step: 2
Training loss: 1.4821882503603332
Validation loss: 2.3630261111186566

Epoch: 5| Step: 3
Training loss: 1.789796703557845
Validation loss: 2.447889432491608

Epoch: 5| Step: 4
Training loss: 1.8899003839226878
Validation loss: 2.417223612293146

Epoch: 5| Step: 5
Training loss: 1.8271147716199645
Validation loss: 2.4100954682974023

Epoch: 5| Step: 6
Training loss: 1.7681994953016293
Validation loss: 2.3891380587035296

Epoch: 5| Step: 7
Training loss: 1.863690345367201
Validation loss: 2.480241387627763

Epoch: 5| Step: 8
Training loss: 2.157471904903231
Validation loss: 2.4333813265024573

Epoch: 5| Step: 9
Training loss: 1.8103005448454814
Validation loss: 2.4142105604575463

Epoch: 5| Step: 10
Training loss: 2.921758965620887
Validation loss: 2.4429273455781795

Epoch: 513| Step: 0
Training loss: 1.598683146137018
Validation loss: 2.450924082174946

Epoch: 5| Step: 1
Training loss: 2.2030009782661693
Validation loss: 2.3696972175139224

Epoch: 5| Step: 2
Training loss: 2.023237417336784
Validation loss: 2.4383127662798882

Epoch: 5| Step: 3
Training loss: 2.002320255018164
Validation loss: 2.4408952191134006

Epoch: 5| Step: 4
Training loss: 2.256530292039711
Validation loss: 2.374298431409439

Epoch: 5| Step: 5
Training loss: 1.9810545166039621
Validation loss: 2.3993837612241

Epoch: 5| Step: 6
Training loss: 1.6053592245420922
Validation loss: 2.374848269185555

Epoch: 5| Step: 7
Training loss: 1.4611037868578876
Validation loss: 2.4546882503283807

Epoch: 5| Step: 8
Training loss: 2.46790975444014
Validation loss: 2.389381163928426

Epoch: 5| Step: 9
Training loss: 2.0367690009563764
Validation loss: 2.4114140144343206

Epoch: 5| Step: 10
Training loss: 1.9101168076513202
Validation loss: 2.4037121698163033

Epoch: 514| Step: 0
Training loss: 2.0965737048202366
Validation loss: 2.3918618537353797

Epoch: 5| Step: 1
Training loss: 1.9558299750207264
Validation loss: 2.4321061777016264

Epoch: 5| Step: 2
Training loss: 1.8487859582918262
Validation loss: 2.366310823955444

Epoch: 5| Step: 3
Training loss: 2.11939566347651
Validation loss: 2.4395837766806787

Epoch: 5| Step: 4
Training loss: 2.002604814850926
Validation loss: 2.3964415760351954

Epoch: 5| Step: 5
Training loss: 1.583197119358135
Validation loss: 2.4131037261083197

Epoch: 5| Step: 6
Training loss: 1.9638384148734502
Validation loss: 2.355773754749477

Epoch: 5| Step: 7
Training loss: 2.214609177427552
Validation loss: 2.3752534371661715

Epoch: 5| Step: 8
Training loss: 1.6170226671922008
Validation loss: 2.4075035789968395

Epoch: 5| Step: 9
Training loss: 1.8398749800378258
Validation loss: 2.4109977911931653

Epoch: 5| Step: 10
Training loss: 2.481831622238172
Validation loss: 2.4036273650063356

Epoch: 515| Step: 0
Training loss: 1.7857094873636354
Validation loss: 2.36900032044002

Epoch: 5| Step: 1
Training loss: 2.2068775224738775
Validation loss: 2.3680363508572504

Epoch: 5| Step: 2
Training loss: 1.9504247080634363
Validation loss: 2.3981266347361028

Epoch: 5| Step: 3
Training loss: 2.4812482435090506
Validation loss: 2.412620318872056

Epoch: 5| Step: 4
Training loss: 1.8374545708055396
Validation loss: 2.4042872145329164

Epoch: 5| Step: 5
Training loss: 1.3572835508715306
Validation loss: 2.4068215400837496

Epoch: 5| Step: 6
Training loss: 1.9833417590413782
Validation loss: 2.3864755900194012

Epoch: 5| Step: 7
Training loss: 2.8158095490902872
Validation loss: 2.475413613817202

Epoch: 5| Step: 8
Training loss: 1.3231786733776805
Validation loss: 2.3809328516512265

Epoch: 5| Step: 9
Training loss: 1.8812634439083067
Validation loss: 2.423734672717209

Epoch: 5| Step: 10
Training loss: 1.8303144988299347
Validation loss: 2.3558102822361544

Epoch: 516| Step: 0
Training loss: 2.0613472925176564
Validation loss: 2.458522957388056

Epoch: 5| Step: 1
Training loss: 1.9323810512162474
Validation loss: 2.453660227163659

Epoch: 5| Step: 2
Training loss: 1.727276615544556
Validation loss: 2.3660595190749456

Epoch: 5| Step: 3
Training loss: 2.3361034525734277
Validation loss: 2.394489737297398

Epoch: 5| Step: 4
Training loss: 1.7654488399701997
Validation loss: 2.36933252501717

Epoch: 5| Step: 5
Training loss: 1.7100337877337513
Validation loss: 2.4009429563985054

Epoch: 5| Step: 6
Training loss: 1.9159435760297172
Validation loss: 2.4189290145711877

Epoch: 5| Step: 7
Training loss: 1.5206517498706815
Validation loss: 2.3203650668471463

Epoch: 5| Step: 8
Training loss: 2.3839137799909853
Validation loss: 2.4271599042203045

Epoch: 5| Step: 9
Training loss: 2.131293793617082
Validation loss: 2.395028388298946

Epoch: 5| Step: 10
Training loss: 2.0081225204096285
Validation loss: 2.417505603105144

Epoch: 517| Step: 0
Training loss: 1.9336491124097712
Validation loss: 2.3938407858671362

Epoch: 5| Step: 1
Training loss: 1.9011077939033119
Validation loss: 2.4063776528663596

Epoch: 5| Step: 2
Training loss: 1.7742878223917398
Validation loss: 2.3282681454383534

Epoch: 5| Step: 3
Training loss: 2.2659289978229786
Validation loss: 2.374268972500463

Epoch: 5| Step: 4
Training loss: 1.8648979163164
Validation loss: 2.438232118404823

Epoch: 5| Step: 5
Training loss: 2.0798603355761487
Validation loss: 2.380971308111443

Epoch: 5| Step: 6
Training loss: 2.8052358277271128
Validation loss: 2.3621598761243314

Epoch: 5| Step: 7
Training loss: 1.722292353456621
Validation loss: 2.3796173142449915

Epoch: 5| Step: 8
Training loss: 1.3926861949788372
Validation loss: 2.419864913056953

Epoch: 5| Step: 9
Training loss: 2.3520136302241648
Validation loss: 2.368953096070324

Epoch: 5| Step: 10
Training loss: 1.141302508496842
Validation loss: 2.4003889354079515

Epoch: 518| Step: 0
Training loss: 2.0659556483468564
Validation loss: 2.426982993929657

Epoch: 5| Step: 1
Training loss: 1.6380627924442812
Validation loss: 2.4009768715492883

Epoch: 5| Step: 2
Training loss: 2.1850392260838585
Validation loss: 2.4478148855301978

Epoch: 5| Step: 3
Training loss: 2.340203208250985
Validation loss: 2.4083000392282266

Epoch: 5| Step: 4
Training loss: 1.7753301931237184
Validation loss: 2.4248512211432343

Epoch: 5| Step: 5
Training loss: 1.7860511189628197
Validation loss: 2.3887201224391177

Epoch: 5| Step: 6
Training loss: 1.6268534727134116
Validation loss: 2.4416043828162604

Epoch: 5| Step: 7
Training loss: 2.0029853950687926
Validation loss: 2.36037417011707

Epoch: 5| Step: 8
Training loss: 2.1875702982914724
Validation loss: 2.421244472709729

Epoch: 5| Step: 9
Training loss: 2.007246123112156
Validation loss: 2.3751796504425813

Epoch: 5| Step: 10
Training loss: 1.9929305542112328
Validation loss: 2.3980282274683815

Epoch: 519| Step: 0
Training loss: 2.169998675754688
Validation loss: 2.395702881402545

Epoch: 5| Step: 1
Training loss: 1.7164599334280906
Validation loss: 2.46023569460444

Epoch: 5| Step: 2
Training loss: 2.1260385780243505
Validation loss: 2.3989133991666884

Epoch: 5| Step: 3
Training loss: 1.7240409562363992
Validation loss: 2.4390326436183516

Epoch: 5| Step: 4
Training loss: 1.7792489038024244
Validation loss: 2.419946898717907

Epoch: 5| Step: 5
Training loss: 2.5268424007059425
Validation loss: 2.4601075163035153

Epoch: 5| Step: 6
Training loss: 2.054794598823267
Validation loss: 2.39017566369845

Epoch: 5| Step: 7
Training loss: 1.71386120569647
Validation loss: 2.445382396196118

Epoch: 5| Step: 8
Training loss: 1.6699092396040613
Validation loss: 2.3858981354356743

Epoch: 5| Step: 9
Training loss: 1.9420943393598655
Validation loss: 2.3938250313350413

Epoch: 5| Step: 10
Training loss: 2.108674335671219
Validation loss: 2.3875423027130283

Epoch: 520| Step: 0
Training loss: 2.4732671046408305
Validation loss: 2.388598528010506

Epoch: 5| Step: 1
Training loss: 2.418524510323878
Validation loss: 2.3985018414063473

Epoch: 5| Step: 2
Training loss: 1.584996287103871
Validation loss: 2.446279381322327

Epoch: 5| Step: 3
Training loss: 1.6552340432489594
Validation loss: 2.3952552929159276

Epoch: 5| Step: 4
Training loss: 2.257749352017545
Validation loss: 2.420631206274219

Epoch: 5| Step: 5
Training loss: 2.0139548309170494
Validation loss: 2.4411385269875012

Epoch: 5| Step: 6
Training loss: 1.7775859290490568
Validation loss: 2.441793244697773

Epoch: 5| Step: 7
Training loss: 1.8172707746060588
Validation loss: 2.425163299319367

Epoch: 5| Step: 8
Training loss: 1.5056319998977874
Validation loss: 2.3660805866796246

Epoch: 5| Step: 9
Training loss: 1.6327849755408483
Validation loss: 2.4081597087369535

Epoch: 5| Step: 10
Training loss: 2.2211090226911265
Validation loss: 2.4304415764479557

Epoch: 521| Step: 0
Training loss: 1.8999051045260718
Validation loss: 2.385963933507508

Epoch: 5| Step: 1
Training loss: 1.6253581752714548
Validation loss: 2.37464718014131

Epoch: 5| Step: 2
Training loss: 1.6876354163195728
Validation loss: 2.4517675630568507

Epoch: 5| Step: 3
Training loss: 1.9585623776297374
Validation loss: 2.371417509069274

Epoch: 5| Step: 4
Training loss: 2.1710924893238626
Validation loss: 2.420621537919106

Epoch: 5| Step: 5
Training loss: 2.083035053516788
Validation loss: 2.403919878994169

Epoch: 5| Step: 6
Training loss: 2.0262577624822393
Validation loss: 2.451134448813042

Epoch: 5| Step: 7
Training loss: 1.8359906777330146
Validation loss: 2.396741227078912

Epoch: 5| Step: 8
Training loss: 2.174435540102374
Validation loss: 2.3903077046551138

Epoch: 5| Step: 9
Training loss: 1.2905103932774682
Validation loss: 2.36776276317858

Epoch: 5| Step: 10
Training loss: 2.2404305090348617
Validation loss: 2.3858732204040005

Epoch: 522| Step: 0
Training loss: 2.019751531219563
Validation loss: 2.37603138666172

Epoch: 5| Step: 1
Training loss: 1.8300501150554598
Validation loss: 2.3742683170860603

Epoch: 5| Step: 2
Training loss: 2.2786915685514355
Validation loss: 2.3694029768635048

Epoch: 5| Step: 3
Training loss: 1.6229924492369492
Validation loss: 2.3585664553120105

Epoch: 5| Step: 4
Training loss: 1.810190966211085
Validation loss: 2.3976670807823455

Epoch: 5| Step: 5
Training loss: 1.6072033053338035
Validation loss: 2.403032833542185

Epoch: 5| Step: 6
Training loss: 1.6096161596848346
Validation loss: 2.440574738691354

Epoch: 5| Step: 7
Training loss: 2.239626814545811
Validation loss: 2.473173384088208

Epoch: 5| Step: 8
Training loss: 1.768334394497183
Validation loss: 2.462100894272471

Epoch: 5| Step: 9
Training loss: 2.4038110237488985
Validation loss: 2.39188604240214

Epoch: 5| Step: 10
Training loss: 2.0505184913514003
Validation loss: 2.4030699825993835

Epoch: 523| Step: 0
Training loss: 2.4606026557913614
Validation loss: 2.4252077267200955

Epoch: 5| Step: 1
Training loss: 1.5446243096016907
Validation loss: 2.4266522245722215

Epoch: 5| Step: 2
Training loss: 1.945661030995425
Validation loss: 2.427009176485459

Epoch: 5| Step: 3
Training loss: 1.9281383272820214
Validation loss: 2.3787489168403266

Epoch: 5| Step: 4
Training loss: 1.9983155786755853
Validation loss: 2.395194599741327

Epoch: 5| Step: 5
Training loss: 1.7803669798732709
Validation loss: 2.422035072797768

Epoch: 5| Step: 6
Training loss: 2.1179354426889865
Validation loss: 2.3848934571509193

Epoch: 5| Step: 7
Training loss: 1.6507352751266215
Validation loss: 2.390861864102841

Epoch: 5| Step: 8
Training loss: 1.4983057149110401
Validation loss: 2.402961099522088

Epoch: 5| Step: 9
Training loss: 1.7456090881736952
Validation loss: 2.3520041822850137

Epoch: 5| Step: 10
Training loss: 2.2315147606124084
Validation loss: 2.397315666138707

Epoch: 524| Step: 0
Training loss: 1.6779075950577524
Validation loss: 2.409797786859554

Epoch: 5| Step: 1
Training loss: 2.0212791449262686
Validation loss: 2.3644319227157666

Epoch: 5| Step: 2
Training loss: 1.874221640199418
Validation loss: 2.4268066837096365

Epoch: 5| Step: 3
Training loss: 2.117171283955426
Validation loss: 2.418595022744003

Epoch: 5| Step: 4
Training loss: 1.9412433394731083
Validation loss: 2.3462466379328917

Epoch: 5| Step: 5
Training loss: 1.7348111953024046
Validation loss: 2.4010449919009047

Epoch: 5| Step: 6
Training loss: 2.0903609446909774
Validation loss: 2.382800437107279

Epoch: 5| Step: 7
Training loss: 2.1912739487692283
Validation loss: 2.4183675666467197

Epoch: 5| Step: 8
Training loss: 1.2926033633164014
Validation loss: 2.474487676719015

Epoch: 5| Step: 9
Training loss: 2.140406687943887
Validation loss: 2.42325278848372

Epoch: 5| Step: 10
Training loss: 2.054307793753071
Validation loss: 2.3828462772809647

Epoch: 525| Step: 0
Training loss: 1.7742595363885805
Validation loss: 2.3920209308913947

Epoch: 5| Step: 1
Training loss: 1.9632234054040736
Validation loss: 2.376623088053594

Epoch: 5| Step: 2
Training loss: 2.233886238473309
Validation loss: 2.417280640719242

Epoch: 5| Step: 3
Training loss: 1.4411410783037388
Validation loss: 2.4107534969218576

Epoch: 5| Step: 4
Training loss: 1.8978054423007042
Validation loss: 2.406309344685624

Epoch: 5| Step: 5
Training loss: 2.2266861730661494
Validation loss: 2.395767253447754

Epoch: 5| Step: 6
Training loss: 1.884350287178915
Validation loss: 2.40623614751107

Epoch: 5| Step: 7
Training loss: 1.8266816921845956
Validation loss: 2.471582520145874

Epoch: 5| Step: 8
Training loss: 1.6887367802574753
Validation loss: 2.4173714261118318

Epoch: 5| Step: 9
Training loss: 2.0230836301102744
Validation loss: 2.344955223055334

Epoch: 5| Step: 10
Training loss: 2.2921274126598417
Validation loss: 2.426458690767614

Epoch: 526| Step: 0
Training loss: 1.9224045186831435
Validation loss: 2.380667307893778

Epoch: 5| Step: 1
Training loss: 1.1861368687731668
Validation loss: 2.334337408134831

Epoch: 5| Step: 2
Training loss: 2.015877877865928
Validation loss: 2.44915988502889

Epoch: 5| Step: 3
Training loss: 2.137780074124723
Validation loss: 2.383591997908363

Epoch: 5| Step: 4
Training loss: 1.850949664844763
Validation loss: 2.387798579759541

Epoch: 5| Step: 5
Training loss: 1.3610217325660265
Validation loss: 2.3890383003867957

Epoch: 5| Step: 6
Training loss: 1.8297671254306884
Validation loss: 2.411541791536703

Epoch: 5| Step: 7
Training loss: 1.6511463489098077
Validation loss: 2.43612050875918

Epoch: 5| Step: 8
Training loss: 2.9879131015566056
Validation loss: 2.3778013203271033

Epoch: 5| Step: 9
Training loss: 1.7004214886266154
Validation loss: 2.364795666788191

Epoch: 5| Step: 10
Training loss: 2.16921307020152
Validation loss: 2.43747577387001

Epoch: 527| Step: 0
Training loss: 1.4908663508321935
Validation loss: 2.389774165471832

Epoch: 5| Step: 1
Training loss: 1.9903172708754544
Validation loss: 2.395851103800124

Epoch: 5| Step: 2
Training loss: 1.5884364692584079
Validation loss: 2.382439628111376

Epoch: 5| Step: 3
Training loss: 1.9803818903728945
Validation loss: 2.4275307807767463

Epoch: 5| Step: 4
Training loss: 1.7260606821870195
Validation loss: 2.3921478019776865

Epoch: 5| Step: 5
Training loss: 1.7630136809962318
Validation loss: 2.3666463432568836

Epoch: 5| Step: 6
Training loss: 2.5268236241199915
Validation loss: 2.468857117615256

Epoch: 5| Step: 7
Training loss: 2.1522327539863206
Validation loss: 2.4537418217814437

Epoch: 5| Step: 8
Training loss: 2.3612687550693994
Validation loss: 2.4390614685156056

Epoch: 5| Step: 9
Training loss: 1.9661467158502952
Validation loss: 2.4327579086646436

Epoch: 5| Step: 10
Training loss: 1.5047660450622597
Validation loss: 2.3931574840808763

Epoch: 528| Step: 0
Training loss: 1.7564599152684077
Validation loss: 2.393408921437741

Epoch: 5| Step: 1
Training loss: 1.9299777658199713
Validation loss: 2.4216724936947727

Epoch: 5| Step: 2
Training loss: 2.386968271850875
Validation loss: 2.403938225988831

Epoch: 5| Step: 3
Training loss: 1.9119713395416678
Validation loss: 2.423999601001098

Epoch: 5| Step: 4
Training loss: 2.2603267411540577
Validation loss: 2.421291302944441

Epoch: 5| Step: 5
Training loss: 1.406708790854979
Validation loss: 2.3734766473870987

Epoch: 5| Step: 6
Training loss: 1.1958372672057271
Validation loss: 2.3673242527911778

Epoch: 5| Step: 7
Training loss: 1.5948847302725189
Validation loss: 2.356553279172444

Epoch: 5| Step: 8
Training loss: 2.3072518258425982
Validation loss: 2.3804395738400563

Epoch: 5| Step: 9
Training loss: 2.0277730674630883
Validation loss: 2.3676520950672533

Epoch: 5| Step: 10
Training loss: 1.553761805691295
Validation loss: 2.457152632850688

Epoch: 529| Step: 0
Training loss: 1.8655850864658385
Validation loss: 2.4142162692039117

Epoch: 5| Step: 1
Training loss: 1.9397791101606956
Validation loss: 2.396755497049953

Epoch: 5| Step: 2
Training loss: 1.3817219717524618
Validation loss: 2.4108958329239383

Epoch: 5| Step: 3
Training loss: 1.9216096896514085
Validation loss: 2.394220524289462

Epoch: 5| Step: 4
Training loss: 2.0036661402049516
Validation loss: 2.414171049106341

Epoch: 5| Step: 5
Training loss: 1.947900605432998
Validation loss: 2.448870038909595

Epoch: 5| Step: 6
Training loss: 1.6042188247854532
Validation loss: 2.3981086807781624

Epoch: 5| Step: 7
Training loss: 2.1698988012809015
Validation loss: 2.3873093590616925

Epoch: 5| Step: 8
Training loss: 1.9691071564793319
Validation loss: 2.387792237211515

Epoch: 5| Step: 9
Training loss: 1.5194407144361637
Validation loss: 2.3859074416369372

Epoch: 5| Step: 10
Training loss: 2.6940716084604084
Validation loss: 2.39704515075393

Epoch: 530| Step: 0
Training loss: 2.0720478198671386
Validation loss: 2.3822910587571102

Epoch: 5| Step: 1
Training loss: 1.9566790783546433
Validation loss: 2.383174696167044

Epoch: 5| Step: 2
Training loss: 1.5461952951016513
Validation loss: 2.3757957308522277

Epoch: 5| Step: 3
Training loss: 1.444385540608893
Validation loss: 2.467778570440899

Epoch: 5| Step: 4
Training loss: 1.5517068786136254
Validation loss: 2.404802615552751

Epoch: 5| Step: 5
Training loss: 2.1465497965817897
Validation loss: 2.3859292311792086

Epoch: 5| Step: 6
Training loss: 2.0273691517556034
Validation loss: 2.3702767711421795

Epoch: 5| Step: 7
Training loss: 2.4380645098268316
Validation loss: 2.3916910116317167

Epoch: 5| Step: 8
Training loss: 2.0109797926029302
Validation loss: 2.3640363695873323

Epoch: 5| Step: 9
Training loss: 1.9301008021264119
Validation loss: 2.4211628004639043

Epoch: 5| Step: 10
Training loss: 2.412498881285413
Validation loss: 2.394691871277583

Epoch: 531| Step: 0
Training loss: 1.8889701473609501
Validation loss: 2.333787011137583

Epoch: 5| Step: 1
Training loss: 1.7893964963761626
Validation loss: 2.4440517232033856

Epoch: 5| Step: 2
Training loss: 1.6965736585254676
Validation loss: 2.4127295055757627

Epoch: 5| Step: 3
Training loss: 1.2108774477927144
Validation loss: 2.421090435670174

Epoch: 5| Step: 4
Training loss: 2.2192995707123924
Validation loss: 2.4336402509517647

Epoch: 5| Step: 5
Training loss: 2.6209230097320897
Validation loss: 2.4303655417881003

Epoch: 5| Step: 6
Training loss: 1.7177010543172417
Validation loss: 2.3476512186045877

Epoch: 5| Step: 7
Training loss: 1.7303453664085073
Validation loss: 2.4082935207403215

Epoch: 5| Step: 8
Training loss: 1.7689922935141151
Validation loss: 2.3805640058951836

Epoch: 5| Step: 9
Training loss: 2.0716264118182175
Validation loss: 2.416764373699451

Epoch: 5| Step: 10
Training loss: 1.959310152723823
Validation loss: 2.3572429326680826

Epoch: 532| Step: 0
Training loss: 1.783161743619827
Validation loss: 2.3129594729412517

Epoch: 5| Step: 1
Training loss: 2.386115514563436
Validation loss: 2.372767866977094

Epoch: 5| Step: 2
Training loss: 1.9793350616559742
Validation loss: 2.4461119740982995

Epoch: 5| Step: 3
Training loss: 2.1045184706885416
Validation loss: 2.373826309542488

Epoch: 5| Step: 4
Training loss: 1.8471080136028903
Validation loss: 2.388427975209935

Epoch: 5| Step: 5
Training loss: 1.5837509039655724
Validation loss: 2.441302331037662

Epoch: 5| Step: 6
Training loss: 1.5936408660024663
Validation loss: 2.4245283207475534

Epoch: 5| Step: 7
Training loss: 1.83875004035125
Validation loss: 2.394092806266906

Epoch: 5| Step: 8
Training loss: 2.1492900474644836
Validation loss: 2.338150201764792

Epoch: 5| Step: 9
Training loss: 1.7610321398590592
Validation loss: 2.4067888970886666

Epoch: 5| Step: 10
Training loss: 2.1956671703186124
Validation loss: 2.405629413997026

Epoch: 533| Step: 0
Training loss: 2.0614627194916135
Validation loss: 2.4052382383545154

Epoch: 5| Step: 1
Training loss: 1.5889387624570757
Validation loss: 2.424640964738878

Epoch: 5| Step: 2
Training loss: 1.816744471409161
Validation loss: 2.39420211995947

Epoch: 5| Step: 3
Training loss: 2.470877877188242
Validation loss: 2.3805500610150054

Epoch: 5| Step: 4
Training loss: 2.0020062397679546
Validation loss: 2.404426870615957

Epoch: 5| Step: 5
Training loss: 1.518084153261494
Validation loss: 2.410458748655787

Epoch: 5| Step: 6
Training loss: 1.4177335013295822
Validation loss: 2.3614963718648796

Epoch: 5| Step: 7
Training loss: 1.7898395300173953
Validation loss: 2.4239490235581567

Epoch: 5| Step: 8
Training loss: 1.7201669574258418
Validation loss: 2.366096646214012

Epoch: 5| Step: 9
Training loss: 2.3483949536843225
Validation loss: 2.4052927771191865

Epoch: 5| Step: 10
Training loss: 1.7010691422320412
Validation loss: 2.4120932253799765

Epoch: 534| Step: 0
Training loss: 1.72873202763481
Validation loss: 2.381828719622446

Epoch: 5| Step: 1
Training loss: 1.9665148323522013
Validation loss: 2.3940425484446624

Epoch: 5| Step: 2
Training loss: 2.1741869579736366
Validation loss: 2.451396687844101

Epoch: 5| Step: 3
Training loss: 1.9123660919582397
Validation loss: 2.429864557332548

Epoch: 5| Step: 4
Training loss: 1.7775756684626947
Validation loss: 2.3691103136380263

Epoch: 5| Step: 5
Training loss: 1.8182420211274952
Validation loss: 2.4639395778115802

Epoch: 5| Step: 6
Training loss: 2.1929717158673614
Validation loss: 2.433723222012289

Epoch: 5| Step: 7
Training loss: 1.8245683290164159
Validation loss: 2.3925192031820903

Epoch: 5| Step: 8
Training loss: 1.4898660387073253
Validation loss: 2.4085875127059797

Epoch: 5| Step: 9
Training loss: 2.007035992157569
Validation loss: 2.423649503340707

Epoch: 5| Step: 10
Training loss: 2.4044729829897724
Validation loss: 2.4267929144926765

Epoch: 535| Step: 0
Training loss: 2.3723564493627958
Validation loss: 2.359830994124156

Epoch: 5| Step: 1
Training loss: 1.6531225943592935
Validation loss: 2.3168746247007954

Epoch: 5| Step: 2
Training loss: 1.912102267726513
Validation loss: 2.3554664410046997

Epoch: 5| Step: 3
Training loss: 1.7256467988745385
Validation loss: 2.3173171491814775

Epoch: 5| Step: 4
Training loss: 1.5441098399993403
Validation loss: 2.414947208003401

Epoch: 5| Step: 5
Training loss: 1.8349757495909622
Validation loss: 2.365648933123013

Epoch: 5| Step: 6
Training loss: 1.629857944781083
Validation loss: 2.3808568278965336

Epoch: 5| Step: 7
Training loss: 1.6723387600848267
Validation loss: 2.3904500188561015

Epoch: 5| Step: 8
Training loss: 2.2949057720512176
Validation loss: 2.3777724061651044

Epoch: 5| Step: 9
Training loss: 1.89276487454611
Validation loss: 2.400542537106502

Epoch: 5| Step: 10
Training loss: 2.796138389382422
Validation loss: 2.3439833450375955

Epoch: 536| Step: 0
Training loss: 1.99100168877017
Validation loss: 2.368422517567347

Epoch: 5| Step: 1
Training loss: 1.7163053292776564
Validation loss: 2.3823982845699327

Epoch: 5| Step: 2
Training loss: 1.7426844687765033
Validation loss: 2.371381788471136

Epoch: 5| Step: 3
Training loss: 1.9905536608885557
Validation loss: 2.3860912373152736

Epoch: 5| Step: 4
Training loss: 1.7165618839814911
Validation loss: 2.4213571188601195

Epoch: 5| Step: 5
Training loss: 1.7066647203752228
Validation loss: 2.3436316883483306

Epoch: 5| Step: 6
Training loss: 1.6485302795045158
Validation loss: 2.411197112841732

Epoch: 5| Step: 7
Training loss: 2.507983430640382
Validation loss: 2.3688565790772094

Epoch: 5| Step: 8
Training loss: 1.8853723369747264
Validation loss: 2.4290574138523926

Epoch: 5| Step: 9
Training loss: 1.4588065696541181
Validation loss: 2.4132928760074184

Epoch: 5| Step: 10
Training loss: 2.534976241623038
Validation loss: 2.4034141468240438

Epoch: 537| Step: 0
Training loss: 1.9450825628099855
Validation loss: 2.473903881741009

Epoch: 5| Step: 1
Training loss: 1.6696557262323946
Validation loss: 2.3906312647922485

Epoch: 5| Step: 2
Training loss: 1.8915759760869575
Validation loss: 2.3740114174180986

Epoch: 5| Step: 3
Training loss: 2.0153895987327552
Validation loss: 2.4247183926849387

Epoch: 5| Step: 4
Training loss: 2.183205339693757
Validation loss: 2.4087564587697132

Epoch: 5| Step: 5
Training loss: 1.7933936851774013
Validation loss: 2.3607584282033884

Epoch: 5| Step: 6
Training loss: 1.4871910287230443
Validation loss: 2.4049142199047715

Epoch: 5| Step: 7
Training loss: 2.478295426924323
Validation loss: 2.3709670292781597

Epoch: 5| Step: 8
Training loss: 1.793682280534833
Validation loss: 2.387593567624363

Epoch: 5| Step: 9
Training loss: 1.6979047934774267
Validation loss: 2.370082466234901

Epoch: 5| Step: 10
Training loss: 1.912160808428687
Validation loss: 2.3748808508682258

Epoch: 538| Step: 0
Training loss: 1.5770689197252847
Validation loss: 2.3407396843930997

Epoch: 5| Step: 1
Training loss: 1.8976509756907778
Validation loss: 2.421960016373351

Epoch: 5| Step: 2
Training loss: 2.4226262065816937
Validation loss: 2.4272931794389767

Epoch: 5| Step: 3
Training loss: 2.251691076775567
Validation loss: 2.3841128040680593

Epoch: 5| Step: 4
Training loss: 1.5282211710305742
Validation loss: 2.438353732662865

Epoch: 5| Step: 5
Training loss: 1.394198594367129
Validation loss: 2.423945092345133

Epoch: 5| Step: 6
Training loss: 1.3741859280308373
Validation loss: 2.383346332669779

Epoch: 5| Step: 7
Training loss: 2.1066938526878167
Validation loss: 2.3713958715882257

Epoch: 5| Step: 8
Training loss: 2.188537569570239
Validation loss: 2.370963035086605

Epoch: 5| Step: 9
Training loss: 1.7763963123385476
Validation loss: 2.395477203894227

Epoch: 5| Step: 10
Training loss: 1.8825965852517406
Validation loss: 2.3908486392696955

Epoch: 539| Step: 0
Training loss: 2.028923227902347
Validation loss: 2.37751168304551

Epoch: 5| Step: 1
Training loss: 2.2488902852192707
Validation loss: 2.4355022004199793

Epoch: 5| Step: 2
Training loss: 1.4322740680884976
Validation loss: 2.3671912119456695

Epoch: 5| Step: 3
Training loss: 1.8197546501157902
Validation loss: 2.422553523584069

Epoch: 5| Step: 4
Training loss: 2.0035325086644207
Validation loss: 2.3743145314562586

Epoch: 5| Step: 5
Training loss: 1.8975890347546718
Validation loss: 2.353845198848192

Epoch: 5| Step: 6
Training loss: 2.1101582521148754
Validation loss: 2.422591049506858

Epoch: 5| Step: 7
Training loss: 1.370844456976348
Validation loss: 2.4043210505219603

Epoch: 5| Step: 8
Training loss: 1.700917694688551
Validation loss: 2.3929178576243717

Epoch: 5| Step: 9
Training loss: 1.6908179266490762
Validation loss: 2.417959339242081

Epoch: 5| Step: 10
Training loss: 2.3751038980345136
Validation loss: 2.380457377016131

Epoch: 540| Step: 0
Training loss: 1.9132376002686617
Validation loss: 2.366921726348312

Epoch: 5| Step: 1
Training loss: 1.733018705655603
Validation loss: 2.4201716238597557

Epoch: 5| Step: 2
Training loss: 2.3511714166532096
Validation loss: 2.3707227689526484

Epoch: 5| Step: 3
Training loss: 1.6160723393414347
Validation loss: 2.4420204568033808

Epoch: 5| Step: 4
Training loss: 1.731434434231549
Validation loss: 2.383604200916352

Epoch: 5| Step: 5
Training loss: 1.8097656935048148
Validation loss: 2.475737442856449

Epoch: 5| Step: 6
Training loss: 1.9621981637722354
Validation loss: 2.3435146385887773

Epoch: 5| Step: 7
Training loss: 2.021720953472189
Validation loss: 2.4016591388250546

Epoch: 5| Step: 8
Training loss: 1.9652040428045499
Validation loss: 2.402946501532394

Epoch: 5| Step: 9
Training loss: 2.1113924904186314
Validation loss: 2.4171725133750566

Epoch: 5| Step: 10
Training loss: 1.662933253001084
Validation loss: 2.389312297945763

Epoch: 541| Step: 0
Training loss: 1.4912726508800112
Validation loss: 2.410785687989698

Epoch: 5| Step: 1
Training loss: 1.672662994927068
Validation loss: 2.390019369518415

Epoch: 5| Step: 2
Training loss: 2.433006546425626
Validation loss: 2.4060320276692697

Epoch: 5| Step: 3
Training loss: 1.902639691090554
Validation loss: 2.3517708503986112

Epoch: 5| Step: 4
Training loss: 1.3518624827263548
Validation loss: 2.398548754947288

Epoch: 5| Step: 5
Training loss: 1.9127383887621967
Validation loss: 2.36693177489586

Epoch: 5| Step: 6
Training loss: 1.6332915119084879
Validation loss: 2.373440739591644

Epoch: 5| Step: 7
Training loss: 2.41597477007949
Validation loss: 2.3976506157618216

Epoch: 5| Step: 8
Training loss: 2.2976880353786853
Validation loss: 2.4628552380639026

Epoch: 5| Step: 9
Training loss: 1.1283028121681942
Validation loss: 2.391994397510184

Epoch: 5| Step: 10
Training loss: 1.8091947739202843
Validation loss: 2.404551287449158

Epoch: 542| Step: 0
Training loss: 2.2177995607155325
Validation loss: 2.390865642230209

Epoch: 5| Step: 1
Training loss: 2.0620389480834462
Validation loss: 2.3742564348342676

Epoch: 5| Step: 2
Training loss: 1.5236558024179685
Validation loss: 2.3859109530771905

Epoch: 5| Step: 3
Training loss: 1.3617849363436496
Validation loss: 2.3584614809096025

Epoch: 5| Step: 4
Training loss: 1.88275627590456
Validation loss: 2.4591706464472844

Epoch: 5| Step: 5
Training loss: 2.740992011068699
Validation loss: 2.351888849623254

Epoch: 5| Step: 6
Training loss: 2.10082087140723
Validation loss: 2.3931685692234264

Epoch: 5| Step: 7
Training loss: 2.0674237520671888
Validation loss: 2.396059817558726

Epoch: 5| Step: 8
Training loss: 1.8758828945495827
Validation loss: 2.398960749216217

Epoch: 5| Step: 9
Training loss: 1.2203183475603385
Validation loss: 2.317194976494756

Epoch: 5| Step: 10
Training loss: 1.5268254876433676
Validation loss: 2.3172188924509847

Epoch: 543| Step: 0
Training loss: 1.8497396260112542
Validation loss: 2.3735377630549843

Epoch: 5| Step: 1
Training loss: 1.6030416217857129
Validation loss: 2.4284993700378656

Epoch: 5| Step: 2
Training loss: 2.214256033720453
Validation loss: 2.3876701139751955

Epoch: 5| Step: 3
Training loss: 1.4157971443788733
Validation loss: 2.375798008759675

Epoch: 5| Step: 4
Training loss: 2.328890476218177
Validation loss: 2.3922589550715667

Epoch: 5| Step: 5
Training loss: 1.918900832420239
Validation loss: 2.3829017303720907

Epoch: 5| Step: 6
Training loss: 1.6981077574246592
Validation loss: 2.3390239447848704

Epoch: 5| Step: 7
Training loss: 2.407342637921528
Validation loss: 2.4271127599644946

Epoch: 5| Step: 8
Training loss: 1.4635239822930222
Validation loss: 2.3569311513146065

Epoch: 5| Step: 9
Training loss: 1.5952427175355923
Validation loss: 2.3820920040851696

Epoch: 5| Step: 10
Training loss: 2.0382899432218076
Validation loss: 2.371871803017053

Epoch: 544| Step: 0
Training loss: 1.8457215323362866
Validation loss: 2.4109531269297158

Epoch: 5| Step: 1
Training loss: 1.8444261119434477
Validation loss: 2.392702979018896

Epoch: 5| Step: 2
Training loss: 2.2067666764468155
Validation loss: 2.430748960785995

Epoch: 5| Step: 3
Training loss: 1.5255125341409979
Validation loss: 2.4163960491680028

Epoch: 5| Step: 4
Training loss: 2.1197254692320158
Validation loss: 2.379997946546232

Epoch: 5| Step: 5
Training loss: 1.693679444930346
Validation loss: 2.409427657877396

Epoch: 5| Step: 6
Training loss: 2.1572535294262862
Validation loss: 2.405455412421373

Epoch: 5| Step: 7
Training loss: 1.528754867832921
Validation loss: 2.3550829955764185

Epoch: 5| Step: 8
Training loss: 1.3295175320465331
Validation loss: 2.3472522516771566

Epoch: 5| Step: 9
Training loss: 2.3546557185731065
Validation loss: 2.368221601153621

Epoch: 5| Step: 10
Training loss: 1.8729771828837503
Validation loss: 2.347784750859727

Epoch: 545| Step: 0
Training loss: 1.772982030516056
Validation loss: 2.3891434056557266

Epoch: 5| Step: 1
Training loss: 1.945799127097007
Validation loss: 2.438987103706696

Epoch: 5| Step: 2
Training loss: 1.8529907383777373
Validation loss: 2.4384739168296425

Epoch: 5| Step: 3
Training loss: 1.9174406175233996
Validation loss: 2.328538459181594

Epoch: 5| Step: 4
Training loss: 1.6903860349875361
Validation loss: 2.3739719065448828

Epoch: 5| Step: 5
Training loss: 1.633045015708596
Validation loss: 2.392914996060106

Epoch: 5| Step: 6
Training loss: 2.144557264826841
Validation loss: 2.390127944398077

Epoch: 5| Step: 7
Training loss: 2.2661843497292993
Validation loss: 2.3555767320478322

Epoch: 5| Step: 8
Training loss: 1.6835439723327557
Validation loss: 2.4122246660101725

Epoch: 5| Step: 9
Training loss: 1.878942604268392
Validation loss: 2.358905249179045

Epoch: 5| Step: 10
Training loss: 2.053944848743731
Validation loss: 2.393768011720597

Epoch: 546| Step: 0
Training loss: 1.394971847386832
Validation loss: 2.371540091428514

Epoch: 5| Step: 1
Training loss: 1.9970867874473257
Validation loss: 2.411497503583676

Epoch: 5| Step: 2
Training loss: 1.6550011875113753
Validation loss: 2.358324203319901

Epoch: 5| Step: 3
Training loss: 1.920234910780279
Validation loss: 2.3541605905129

Epoch: 5| Step: 4
Training loss: 2.1447375811488425
Validation loss: 2.407223512404282

Epoch: 5| Step: 5
Training loss: 2.484237763075179
Validation loss: 2.4116161485003107

Epoch: 5| Step: 6
Training loss: 2.0254315893579187
Validation loss: 2.434233114590847

Epoch: 5| Step: 7
Training loss: 1.7628375305119084
Validation loss: 2.3894140146977283

Epoch: 5| Step: 8
Training loss: 2.11065623614123
Validation loss: 2.369783399897214

Epoch: 5| Step: 9
Training loss: 1.9841790891042446
Validation loss: 2.3837680013755236

Epoch: 5| Step: 10
Training loss: 1.2259424671283479
Validation loss: 2.3812662213600113

Epoch: 547| Step: 0
Training loss: 1.5218712213300913
Validation loss: 2.403871351136825

Epoch: 5| Step: 1
Training loss: 1.8346627212849784
Validation loss: 2.3681655044475156

Epoch: 5| Step: 2
Training loss: 1.6415723699348874
Validation loss: 2.3682634833740512

Epoch: 5| Step: 3
Training loss: 1.932877286474359
Validation loss: 2.370499533959715

Epoch: 5| Step: 4
Training loss: 1.7091680983833901
Validation loss: 2.3615699049266277

Epoch: 5| Step: 5
Training loss: 1.6077632054964188
Validation loss: 2.407730248129112

Epoch: 5| Step: 6
Training loss: 1.8153945566505725
Validation loss: 2.373141404012901

Epoch: 5| Step: 7
Training loss: 1.5380777994576373
Validation loss: 2.420835791115984

Epoch: 5| Step: 8
Training loss: 2.2877787352457997
Validation loss: 2.3828039186976406

Epoch: 5| Step: 9
Training loss: 2.511478299842512
Validation loss: 2.400904590260981

Epoch: 5| Step: 10
Training loss: 2.2388135666504843
Validation loss: 2.4715944422298652

Epoch: 548| Step: 0
Training loss: 1.8235984253988897
Validation loss: 2.3735240120859062

Epoch: 5| Step: 1
Training loss: 1.5989645378994755
Validation loss: 2.3492479348861766

Epoch: 5| Step: 2
Training loss: 1.8082316553152302
Validation loss: 2.337453218371095

Epoch: 5| Step: 3
Training loss: 1.7640705524054687
Validation loss: 2.3587577211723474

Epoch: 5| Step: 4
Training loss: 1.938658152802062
Validation loss: 2.450560222393427

Epoch: 5| Step: 5
Training loss: 2.4088577407947867
Validation loss: 2.4337709113157837

Epoch: 5| Step: 6
Training loss: 1.9166765074546397
Validation loss: 2.365983899954786

Epoch: 5| Step: 7
Training loss: 1.4877974557386393
Validation loss: 2.3847367243265976

Epoch: 5| Step: 8
Training loss: 1.8954871214376559
Validation loss: 2.4259057266933

Epoch: 5| Step: 9
Training loss: 1.4786802575633673
Validation loss: 2.3996144184539783

Epoch: 5| Step: 10
Training loss: 2.3777875101874675
Validation loss: 2.319224590122192

Epoch: 549| Step: 0
Training loss: 1.924383247954982
Validation loss: 2.3340900844972166

Epoch: 5| Step: 1
Training loss: 1.9021109373910607
Validation loss: 2.4058036097156057

Epoch: 5| Step: 2
Training loss: 2.0262544678739394
Validation loss: 2.3524670866804898

Epoch: 5| Step: 3
Training loss: 2.171966441033973
Validation loss: 2.4148198430600516

Epoch: 5| Step: 4
Training loss: 1.5421069865288208
Validation loss: 2.391831961615841

Epoch: 5| Step: 5
Training loss: 2.239059765555182
Validation loss: 2.358770090699454

Epoch: 5| Step: 6
Training loss: 1.812823957233756
Validation loss: 2.346426997923729

Epoch: 5| Step: 7
Training loss: 1.8477297358990734
Validation loss: 2.376229314000318

Epoch: 5| Step: 8
Training loss: 1.7728476866927378
Validation loss: 2.362146642540962

Epoch: 5| Step: 9
Training loss: 1.9246807552020648
Validation loss: 2.4147393480132227

Epoch: 5| Step: 10
Training loss: 1.3944125084852725
Validation loss: 2.335045445300281

Epoch: 550| Step: 0
Training loss: 2.240759950029455
Validation loss: 2.3800937300711547

Epoch: 5| Step: 1
Training loss: 1.867278731284054
Validation loss: 2.387842442460215

Epoch: 5| Step: 2
Training loss: 1.363101368051676
Validation loss: 2.3497008929465086

Epoch: 5| Step: 3
Training loss: 1.6570379434222868
Validation loss: 2.4344980197963872

Epoch: 5| Step: 4
Training loss: 2.00834061026409
Validation loss: 2.3676620706761984

Epoch: 5| Step: 5
Training loss: 1.6447072421974134
Validation loss: 2.3850325935134933

Epoch: 5| Step: 6
Training loss: 1.85256518806532
Validation loss: 2.3744606448364043

Epoch: 5| Step: 7
Training loss: 1.8311870394875112
Validation loss: 2.374579770788464

Epoch: 5| Step: 8
Training loss: 2.2005091078105377
Validation loss: 2.3588260837081885

Epoch: 5| Step: 9
Training loss: 1.5664420183240872
Validation loss: 2.3709798238372635

Epoch: 5| Step: 10
Training loss: 1.8588684417812085
Validation loss: 2.4065204410320735

Testing loss: 2.54872675629563
